Compute cost estimates
Article • 04/20/2023

Compute refers to the hosting model for the computing resources that your application
runs on:
Infrastructure as a service (IaaS) lets you provision individual virtual machines (VMs)
along with the associated networking and storage components. Then you deploy
whatever software and applications you want onto those VMs. This model is the
closest to a traditional on-premises environment, except that Microsoft manages
the infrastructure. You still manage the individual VMs.
Platform as a service (PaaS) provides a managed hosting environment, where you
can deploy your application without needing to manage VMs or networking
resources. Azure App Service is a PaaS service.
Functions as a service (FaaS) goes even further in removing the need to worry
about the hosting environment. In a FaaS model, you deploy your code and the
service automatically runs it. Azure Functions is a FaaS service.

Hosting models
Whether you're hosting model is IaaS, PaaS, or FaaS, each resource requires you to
evaluate the tradeoffs that can affect your cost. To learn more about hosting models, see
Understand the hosting models.

What are the cost implications of choosing a hosting
model?
If your application can be broken down into short pieces of code, a FaaS hosting model
might be the best choice. You're charged only for the time it takes to run your code. For
example, Azure Functions is a FaaS service that processes events with serverless code.
Azure Functions lets you run short pieces of code, called functions, without worrying
about application infrastructure. Use one of the three Azure Functions pricing plans to
fit your need. For more information, see Introduction to Azure Functions.
If you want to deploy a larger or more complex application, PaaS might be the better
choice. With PaaS, your application is always running, as opposed to FaaS, where your
code runs only when needed. Since PaaS uses more resources, the cost increases.

If you're migrating your infrastructure from on-premises to Azure, IaaS greatly reduces
and optimizes infrastructure costs and salaries for IT staff who are no longer needed to
manage the infrastructure. Since IaaS uses more resources than PaaS and FaaS, your cost
could be highest.

What are the main cost drivers for Azure services?
You're charged differently for each service depending on your region, licensing plan
(such as, Azure Hybrid Benefit for Windows Server), number and type of instances you
need, operating system, lifespan, and other parameters required by the service. Assess
the need for each compute service by using the flowchart in Choose a candidate service.
Consider the tradeoffs that affect your cost by creating different estimates using the
Pricing Calculator. If your application consists of multiple workloads, we recommend
that you evaluate each workload separately. Refer to Consider limits and costs to do a
more detailed evaluation on service limits, cost, SLAs, and regional availability.

Are there payment options for Virtual Machines (VMs) to
help meet my budget?
The business requirements of your workload drive your best choice. If you have higher
SLA requirements, those requirements increase overall costs. You might need more VMs
to ensure uptime and connectivity. Other factors that affect cost are region, operating
system, and the number of instances you choose. Your cost also depends on the
workload life span. For more information, see Virtual machines and Use Azure Spot
Virtual Machines.
Pay as you go lets you pay for compute capacity by the second, with no long-term
commitment or upfront payments. This option lets you increase or decrease
compute capacity on demand. It's appropriate for applications with short-term,
spiky, or unpredictable workloads that can't be interrupted. You can also start or
stop usage at any time, resulting in paying only for what you use.
Reserved Virtual Machine Instances lets you purchase a VM for one or three years in
a specified region in advance. It's appropriate for applications with steady-state
usage. You might save more money compared to pay-as-you-go pricing.
Savings plans let you commit to one or three years, across all regions, for a fixed,
hourly dollar amount across compute services.
Spot pricing lets you purchase unused compute capacity at major discounts. If your
workload can tolerate interruptions, and the run time is flexible, using spot pricing

for VMs can significantly reduce the cost of running your workload in Azure.
Dev-Test pricing offers discounted rates on Azure to support your ongoing
development and testing. Dev-Test lets you quickly create consistent development
and test environments through a scalable, on-demand infrastructure. This
approach allows you to spin up what you need, when you need it, and explore
scenarios before going into production. To learn more about Azure Dev-Test
reduced rates, see Azure Dev/Test Pricing

.

For details on available sizes and options for the Azure VMs that you can use to run your
apps and workloads, see Sizes for virtual machines in Azure. For details on specific Azure
VM types, see Virtual Machine series .

Do I pay extra to run large-scale parallel and highperformance computing batch jobs?
Use Azure Batch to run large-scale parallel and high-performance computing (HPC)
batch jobs in Azure. You can save on VM cost because multiple apps can run on one VM.
Configure your workload with either the Low-priority tier, which is the cheapest option,
or the Standard tier, which provides better CPU performance. There's no cost for the
Azure Batch service. Charges accrue for the underlying resources that run your batch
workloads.

Use PaaS as an alternative to buying VMs
When you use the IaaS model, you have final control over the VMs. It might appear to
be a cheaper option at first, but when you add operational and maintenance costs, the
cost increases. When you use the PaaS model, these extra costs are included in the
pricing. In some cases, PaaS services can be a cheaper than managing VMs on your own.
For help with finding areas in the architecture where it might be natural to incorporate
PaaS options, see Managed services.

How can I cut costs with hosting my web apps in PaaS?
If you host your web apps in PaaS, choose an App Service plan to run your apps. The
plan defines the set of compute resources on which your app runs. If you have more
than one app, they run using the same resources. This situation is where you see the
most significant cost savings, because you don't incur cost for VMs.
If your apps are event-driven with a short-lived process using a microservices
architecture style, we recommend using Azure Functions. Run time and memory for a

single function run determine your cost. For pricing details, see Azure Functions
pricing .

Is it more cost-effective to deploy development testing
on a PaaS or IaaS hosting model?
If your development testing (dev-test) is built on Azure managed services such as Azure
DevOps, Azure SQL Database, Azure Cache for Redis, and Application Insights, the
cheapest solution might be using the PaaS hosting model. You don't incur the cost and
maintenance of hardware. If your dev-test is built on Azure managed services such as
Azure DevOps, Azure DevTest Labs, VMs, and Application Insights, you need to add the
cost of the VMs. This cost can greatly increase your cost. For details on evaluating, see
Azure Dev/Test Pricing

.

A special case of PaaS - Containers
How can I get the best cost savings for a containerized
workload that requires full orchestration?
Your business requirements might require that you store container images to provide
fast, scalable retrieval, and network-close container workload deployments. Although
there are choices as to how you run them, we recommend that you use Azure
Kubernetes Service (AKS) to set up instances with a minimum of three nodes. AKS
reduces the complexity and operational overhead of managing Kubernetes by
offloading much of that responsibility to Azure. There's no charge for AKS cluster
management. Any other costs are minimal. The containers themselves have no effect on
cost. You pay only for per-second billing and custom machine sizes.

Can I save money if my containerized workload doesn't
need full orchestration?
Your business requirements might not need full orchestration. If not, and you're using
App Service containers, we recommend that you use one of the App Service plans.
Choose the appropriate plan based on your environment and workload.
There's no charge to use SNI-based TLS/SSL. Standard and Premium service plans
include the right to use one IP TLS/SSL at no extra charge. Free and shared service plans
don't support TLS/SSL. You can purchase the right to use more TLS/SSL connections. In

all cases, you must purchase the TLS/SSL certificate itself separately. To learn more about
each plan, see App Services pricing

.

Where's the savings if my workload is event driven with a
short-lived process?
In this example, Service Fabric might be a better choice than AKS. The biggest difference
between the two is that AKS works only with Docker applications using Kubernetes.
Service Fabric works with microservices and supports many different runtime strategies.
Service Fabric can deploy Docker and Windows Server containers. Like AKS, Service
Fabric is built for the microservice and event-driven architectures. AKS is strictly a service
orchestrator and handles deployments, whereas Service Fabric also offers a
development framework that lets you build stateful/stateless applications.

Predict cost estimates using the pricing
calculator
Use the Pricing Calculator to create your total cost estimate. After you run your initial
scenario, you might find that your plan is beyond the scope of your budget. You can
adjust the overall cost and create various cost scenarios to make sure your needs are
met before you commit to purchasing.
７ Note
The costs in this example are based on the current price and are subject to change.
The calculation is for illustration purposes only. It shows the Collapsed view of the
cost in this estimate.



 Tip
You can start building your cost estimate at any time and re-visit it later. The
changes are saved until you modify or delete your estimate.

Next steps
Provisioning cloud resources to optimize cost
Virtual machines in Azure
Virtual machines pricing
Pricing calculator

Data store cost estimates
Article • 04/20/2023

Most cloud workloads adopt the polyglot persistence approach. Cloud workloads use a
mix of technologies instead of one data store service. You can achieve optimal cost
benefit from using this approach.
Each Azure data store has a different billing model. To establish a total cost estimate:
Identify the business transactions and their requirements
Break each transaction into operations
Identify a data store appropriate for the type of data
Perform these steps for each workload separately.
For example, take an e-commerce application. It needs to store data for transactions
such as orders, payments, and billing. The data structure is predetermined and not
expected to change frequently. Data integrity and consistency are crucial. There's a need
to store product catalogs, social media posts, and product reviews. In some cases, the
data is unstructured, and is likely to change over time. Media files must be stored and
data must be stored for auditing purposes.
Learn about data stores in Understand data store models.

Identify the business transactions and their
requirements
The following list of questions address the requirements that can have the greatest
effect on your cost estimate. For example, your monthly bill might be within your
budget now, but if you scale up or add storage space later, your cost might increase
well over your budget.
Does your data need to be migrated to on-premises, external data centers, or
other cloud-hosting environments?
What type of data do you intend to store?
How large are the entities you need to store?
What is the overall amount of storage capacity you need?
What kind of schemas apply to your data, for example, fixed schema, schema-onwrite, or schema-on-read?
What are your data performance requirements? For example, what are acceptable
response times for querying and aggregation of data once ingested?

What level of fault-tolerance do you need to provide for data consumers?
What kind of data replication capabilities do you require?
Do the limits of a particular data store support your requirements for scale,
number of connections, and throughput?
How many instances need to run to support your uptime and throughput
requirements? Consider operations costs in this calculation.
Can you partition your data to store it more cost effectively? For example, can you
move large objects out of an expensive relational database into an object store?
There are other requirements that might not have as great of an effect on your cost. For
example, the US East region is only slightly lower in cost than Canada Central. See
Criteria for choosing a data store for other business requirements.
Use the Azure pricing calculator

to determine different cost scenarios.

What are the network requirements that might affect
cost?
Do you need to restrict or otherwise manage access to your data from other
network resources?
Does data need to be accessible only from inside the Azure environment?
Does the data need to be accessible from specific IP addresses or subnets?
Does the data need to be accessible from applications or services hosted onpremises or in other external data centers?

Break each transaction into operations
For example, these distinct operations might process one business transaction:
10-15 database calls
Three append operations on blob
Two list operations on two separate file shares

How does data type in each operation affect cost?
Consider Azure Blob Storage Block Blobs instead of storing binary image data in
Azure SQL Database. Blob storage is cheaper than Azure SQL Database.
If your design requires SQL, store a lookup table in SQL Database and retrieve the
document when needed to serve it to the user in your application middle tier. SQL
Database is highly targeted for high-speed data lookups and set-based operations.

The hot access tier of Azure Block Blob Storage cost is cheaper than the equivalent
size of the Premium SSD volume that has the database.
For more information, see Understand data store models.

Identify a data store appropriate for the type of
data
７ Note
An inappropriate data store or one that's mis-configured can have a huge cost
impact on your design.

Relational database management systems cost
When you need strong consistency guarantees, we recommend relational database
management systems (RDBMS). An RDBMS typically supports a schema-on-write model,
where the data structure is defined ahead of time, and all read or write operations must
use the schema.

How can I save money if my data is on-premises and
already on SQL server?
The on-premises license with Software Assurance can be used to bring down the cost if
the workload is eligible for Azure Hybrid Benefit

. This option applies for Azure SQL

Database (PaaS) and SQL Server on Azure Virtual Machines (IaaS).
For open-source databases such as MySQL, MariaDB, or PostGreSQL, Azure provides
managed services that are easy to provision.

What are some design considerations that affect cost?
If the SLAs don't allow for downtime, can a read-only replica in a different region
enable business continuity?
If the database in the other region must be read/write, how is the data replicated?
Does the data need to be synchronous, or could consistency allow for
asynchronous replication?
How important is it for updates made in one node to appear in other nodes,
before further changes can be made?

Azure storage has several options to make sure data is copied and available when
needed. Locally redundant storage (LRS) synchronously replicates data in the primary
region. If the entire primary center is unavailable, the replicated data is lost. At the
expensive end, Geo-zone-redundant storage (GZRS) replicates data in availability zones
within the primary region and asynchronously copied to another paired region.
Databases that offer geo-redundant storage, such as SQL Database, are more expensive.
Most other OSS RDBMS databases use LRS storage, which contributes to the lower price
range.
For more information, see Automated backups in Azure SQL Database.
Azure Cosmos DB

offers five consistency levels: strong, bounded staleness, session,

consistent prefix, and eventual. Each level provides availability and performance
tradeoffs. Comprehensive SLAs back each level. The consistency level itself doesn't affect
cost.

How can I minimize compute cost?
Higher throughput and IOPS require higher compute, memory, I/O, and storage limits.
These limits are expressed in a vCore model. With higher vCore number, you buy more
resources, so the cost is higher. Azure SQL Database has more vCores and allows you to
scale in smaller increments. Azure Database for MySQL, PostgreSQL, and MariaDB have
fewer vCores and scaling up to a higher vCore can cost more. MySQL provides inmemory tables in the Memory Optimized tier, which can also increase the cost.
All options offer a consumption and provisioned pricing models. With preprovisioned
instances, you save more if you can commit to one or three years. To learn more, see
Azure Reserved Capacity for Databases .

How is primary and backup storage cost calculated?
With Azure SQL Database, the initial 32 GB of storage is included in the price. For the
other listed options, you need to buy storage separately and might increase the cost
depending on your storage needs.
For most databases, there's no charge for the price of backup storage that's equal in
size to primary storage. If you need more backup storage, you incur more cost.

Key/value and document databases cost

We recommend Azure Cosmos DB for key/value stores and document databases. Azure
Cache for Redis is also recommended for key/value stores.
For Azure Cosmos DB , here are some considerations that affect cost:
Can storage and item size be adjusted?
Can index policies be reduced or customized to bring down the extra cost for
writes and eliminate the requirement for more throughput capacity?
If data is no longer needed, can it be deleted from the Azure Cosmos DB account?
As an alternative, you can migrate the old data to another data store such as Azure
blob storage or Azure data warehouse.
For Azure Cache for Redis

, there's no upfront cost, no termination fees, you pay only

for what you use, and billing is per-hour.

Graphic databases cost
Cost considerations for graphic database stores include storage required by data and
indexes used across all the regions. For example, graphs need to scale beyond the
capacity of a single server, and make it easy to look up the index when processing a
query.
The Azure service that supports this functionality is Azure Cosmos DB for Apache
Gremlin. Cost is limited to the Azure Cosmos DB

usage. You pay for the operations

that you perform on the database and for the storage consumed by your data. Charges
are determined by the number of provisioned containers, the number of hours the
containers were online, and the provisioned throughput for each container.
If the on-premises data is already on an SQL server on Azure Virtual Machines (IaaS), the
license with Software Assurance can be used to bring down the cost if the workload is
eligible for Azure Hybrid Benefit

.

Data analytics cost
Cost considerations for data analytics stores include data storage, multiple servers to
maximize scalability, and the ability to access large data as external tables.
Azure has many services that support data analytics stores:
Azure Synapse Analytics
Azure Data Lake
Azure Data Explorer

Azure Analysis Services
Azure HDInsight
Azure Databricks
As an example of use, historical data is typically stored in data stores such as blob
storage or Azure Data Lake Storage Gen2. Azure Synapse, Databricks, or HDInsight
access these stores as external tables.
When you use Azure Synapse, you only pay for the capabilities that you opt in to use.
During public preview, there's no cost for provisioning an Azure Synapse workspace.
Enabling a managed virtual network and Customer Managed Keys might incur a
workspace fee after public preview. Pricing of workspaces with other capabilities will be
announced at a future date.

Column family database cost
The main cost consideration for a column family database is that it needs to be
massively scalable.
The Azure services that support column family databases are Azure Cosmos DB for
Apache Cassandra and HBase in HDInsight.
For Azure Cosmos DB for Apache Cassandra, you pay the cost of Azure Cosmos DB ,
which includes database operations and consumed storage. Azure Cosmos DB for
Apache Cassandra is open-source.
For HBase in HDInsight, you pay the cost of HDInsight

, which includes instance size

and number. HBase is open-source.

Search engine database cost
Cost incurs for a search engine database when applications need to search for
information held in external data stores. It also needs to index massive volumes of data
and provide near real-time access to these indexes.
Azure Cognitive Search

is a search service that uses AI capabilities to identify and

explore relevant content at scale.

Time series database cost
The main cost considerations for a time series database is the need to collect large
amounts of data in real time from a large number of sources. Although the records

written to a time series database are generally small, there are often a large number of
records, and total data size can grow rapidly, which drives up cost.
Azure Time Series Insights

might be the best option to minimize cost.

Object storage cost
Cost considerations include storing large binary objects such as:
Images
Text files
Video and audio streams
Large application data objects and documents
Virtual machine disk images
Azure services that support object storage are Azure Blob Storage

and Azure Data

Lake Storage Gen2 .

Shared files cost
The main cost consideration is having the ability to access files across a network. For
example, given appropriate security and concurrent access control mechanisms, sharing
data in this way can enable distributed services to provide highly scalable data access
for performing basic, low-level operations such as simple read and write requests.
The Azure service that supports shared files is Azure Files .

Azure messaging cost estimates
Article • 04/20/2023

The messaging services in this article have no up-front cost or termination fees. You pay
only for what you use. In some cases, combining two messaging services to increase the
efficiency of your messaging system is advantageous. For for more information, see
Crossover scenarios.
Cost is based on the number of operations or throughput units you use, depending on
the message service. If you use the wrong messaging service, you may incur higher
costs. Before choosing a service, first, determine the intent and requirements of the
messages. Then consider the tradeoffs between cost, operations units, and throughput
units. For tradeoff examples, see Technology choices for a message broker.
Use the Azure pricing calculator

for help with creating various cost scenarios.

Azure Service Bus cost
Connect on-premises and cloud-based applications and services to implement highly
secure messaging workflows by using Azure Service Bus. Cost is based on messaging
operations and the number of connections. The basic tier is the cheapest. If you want
more operations and features, choose the standard tier or the premium tier. For
example, the Azure Service Bus premium tier runs in dedicated resources to provide
higher throughput and more consistent performance.
For pricing details, see Azure Service Bus pricing

.

Event Grid cost
Use Event Grid to simplify event-based app development and manage events routing
from any source to any destination. Event Grid can route a massive number of events,
per second, per region. Cost is based on the number of operations you run. Examples of
operations you can run are event ingress, subscription delivery attempts, management
calls, and filtering by subject suffix.
For pricing details, see Event Grid pricing .

Event Hubs

Use Event Hubs to stream millions of events per second from any source, build dynamic
data pipelines, and immediately respond to business challenges. Cost is based on
throughput units. A key difference between Event Grid and Event Hubs is in the way
event data is made available to subscribers. For more information, see Pull model.
For frequently asked questions on pricing, see Event Hubs FAQs For details on Event
Hubs pricing, see Event Hubs pricing .

Networking cost estimates
Article • 04/20/2023

Traffic routing and load balancing
Most workloads have a load balancing service to route and distribute traffic. It's done in
such a way that a single resource isn't overloaded and the response time is minimum
with maximum throughput.
Do you need to distribute traffic within a region or across regions?
Azure Front Door and Azure Traffic Manager can distribute traffic to backends, clouds, or
hybrid on-premises services that reside in multiple regions. Otherwise, for regional
traffic that moves within virtual networks or zonal and zone-redundant service
endpoints within a region, choose Azure Application Gateway or Azure Load Balancer.
What is the type of traffic?
Load balancers such as Azure Application Gateway and Azure Front Door, route and
distribute traffic to web applications or HTTP/HTTPS endpoints. Those services support
Layer 7 features such as TLS/SSL offload, web application firewall, path-based load
balancing, and session affinity. For nonweb traffic, choose Azure Traffic Manager or
Azure Load Balancer. Azure Traffic Manager uses DNS to route traffic. Load Balancer
supports Layer 4 features for all UDP and TCP protocols.
Here's the matrix for choosing load balancers by considering both dimensions.
Service

Global/regional

Recommended traffic

Azure Front Door

Global

HTTP/HTTPS

Azure Traffic Manager

Global

non-HTTP/HTTPS

Azure Application Gateway

Regional

HTTP/HTTPS

Azure Load Balancer

Regional

non-HTTP/HTTPS

For more information, see Choose a load balancing service.

Example cost analysis
Consider a web application that receives traffic from users across regions over the
internet. To minimize the request response time, the load balancer can delegate the

responsibility from the web server to a different computer. The application is expected
to consume 10 TB of data. Routing rules direct incoming requests to various paths. Also,
use Web Application Firewall (WAF) to secure the application through policies.
By using the Azure pricing calculator , you can estimate the cost for these two services.
This example uses current prices and is subject to change. The example shows prices
and calculations for illustrative purposes only.

Azure Front Door (West US), Zone 1

Azure Application Gateway (West US)

Consider a similar example where the type of traffic changes. The application is a UDP
streaming service that deploys across regions and the traffic goes over the internet. Use
a combination of Azure Traffic Manager and Azure Load Balancer. Azure Traffic Manager
is a simple routing service that uses DNS to direct clients to a specific service. Here are
the cost estimates:

Azure Traffic Manager
Item

Example estimate

DNS queries

100 million x $0.54/per million = $54.00

Basic health checks

20 endpoints x $0.36/month = $7.20

Fast interval health checks

10 endpoints x $1.00/month = $10.00

External endpoints hosted outside
Azure

10 endpoints x $0.54/month = $5.40

Real user measurements

100 million user measurements x $2.00/month = $200.00

Traffic view

100 million data points processed x $2.00/month =
$200.00

Total

$486.60

Azure Load Balancer

Item

Example estimate

Rules

First five rules: $0.032/rule/hour x 730 = $18.25
Extra five rules: $0.013/rule/hour x 730 = $36.50

Data processed

10 TB x $0.005/GB = 51.20

Total

$105.95

Peering
Do you need to connect virtual networks?
Peering technology is a service that you use for Azure virtual networks to connect with
other virtual networks in the same or different Azure region. Hub and spoke
architectures often use peering technology.
There are extra costs incurred by peering connections on both egress and ingress traffic
traversing the peering connections.
If possible, keep the top talking services of a workload within the same virtual
network, zone, and region. Use virtual networks as shared resources for multiple
workloads against a single virtual network per workload approach. This approach
localizes traffic to a single virtual network and avoids the additional costs on peering
charges.

Example cost analysis
This example uses current prices and is subject to change. The example shows prices
and calculations for illustrative purposes only.
Peering within the same region is cheaper than peering between regions or Global
regions. For instance, you consume 50 TB per month by connecting two VNETs in
Central US. Using the current price here's the incurred cost.
Item

Example estimate

Ingress traffic

50 TB x $0.0100/GB = $512.50

Egress traffic

50 TB x $0.0100/GB = $512.50

Total

$1,025.00

Compare that cost for cross-region peering between Central US and East US.
Item

Example estimate

Ingress traffic

50 TB x $0.0350/GB = $1,792.00

Egress traffic

50 TB x $0.0350/GB = $1,792.00

Total

$3,584.00

Hybrid connectivity
There are two main options for connecting an on-premises datacenter to Azure
datacenters:
Azure VPN gateway. Use Azure VPN gateway to connect a virtual network to an
on-premises network through a VPN appliance or to Azure Stack through a site-tosite VPN tunnel.
Azure ExpressRoute. Create private connections between Azure datacenters and
infrastructure that's on premise or in a colocation environment.
What is the required throughput for cross-premises connectivity?
We recommend VPN gateway for development and test cases or small-scale production
workloads where throughput is less than 100 Mbps. Use Azure ExpressRoute for
enterprise and mission-critical workloads that access most Azure services. You can
choose bandwidth from 50 Mbps to 10 Gbps.
Another consideration is security. Unlike VPN gateway traffic, Azure ExpressRoute
connections don't go over the public internet. Industry-standard IPsec secures the VPN
gateway traffic.
For both services, inbound transfers are free and outbound transfers are billed per the
billing zone.
For more information, see Choose a solution for connecting an on-premises network to
Azure.
This article

provides a comparison of the two services.

Example cost analysis
This example compares the pricing details for VPN gateway and Azure ExpressRoute.

The example uses current prices and is subject to change. The example shows prices
and calculations for illustrative purposes only.

Azure VPN Gateway
Suppose you choose the VpnGw2AZ tier, which supports availability zones, 1 GB/s
bandwidth. The workload needs 15 site-to-site tunnels and 1 TB of outbound transfer.
The gateway is provisioned and available for 720 hours.
Item

Example estimate

VPN Hours

720 x $0.564 = $406.08

Site to Site
(S2S) Tunnels

The first 10 tunnels are free. For more tunnels, the cost is five tunnels x 720
hours x $0.015 per hour per tunnel = $54.00

Outbound

1024 GB x 0.086 = $88.65

transfer
Total cost per
month

$548.73

Azure ExpressRoute
Choose the Metered Data plan in billing zone of Zone 1.
Item

Example estimate

Circuit bandwidth

1 GB/s speed has a fixed rate of $436.00 for the standard price.

Outbound transfer

1 TB x $0.0025 = $25.60

Total cost per month

$461.00

The main cost driver is outbound data transfer. Azure ExpressRoute is more costeffective than Azure VPN Gateway when consuming large amounts of outbound data. If
you consume more than 200 TB per month, consider Azure ExpressRoute with the
Unlimited Data plan where you're charged a flat rate.
For pricing details, see:
Azure VPN Gateway pricing
Azure ExpressRoute pricing

Networking resources provisioning
Article • 04/20/2023

For design considerations, see Networking cost estimates.

Azure Front Door
Outbound data transfers, inbound data transfers, and routing rules all affect Azure Front
Door billing. Pricing information doesn't include the cost of accessing data from the
backend services and transferring it to Azure Front Door. Those costs are billed based
on data transfer charges described in Bandwidth pricing .
Another consideration is Web Application Firewall (WAF) settings. Adding policies drives
up the cost.
For more information, see Azure Front Door pricing

.

Azure Front Door reference architecture
Highly available multi-region web application uses Azure Front Door to route incoming
requests to the primary region. If the application that runs the primary region becomes
unavailable, Azure Front Door fails over to the secondary region.

Azure Application Gateway
There are two main pricing models:
Fixed price
You're charged from the time that you provision the gateway and it becomes
available. You're charged for the amount of data that the gateway processes. For
more information, see Application Gateway pricing .
Consumption price
This model applies to v2 SKUs that offer more features such as autoscaling, Azure
Kubernetes Service Ingress Controller, and zone redundancy. You're charged based
on the consumed capacity units. The capacity units measure the compute
resources, persistent connections, and throughput. Consumption price is charged
in addition to the fixed price.

For more information, see:
Scaling Application Gateway v2 and WAF v2
Application Gateway pricing

Application Gateway reference architecture
Microservices architecture on Azure Kubernetes Service uses Application Gateway
as the ingress controller.
Securely managed web applications uses Application Gateway as a web traffic load
balancer operating at Layer 7 that manages traffic to the web application. Web
Application Firewall is enabled to enhance security.

Azure ExpressRoute
There are two main pricing models:
Metered Data plan
There are two pricing tiers: Standard and Premium, which is priced higher. The tier
pricing is based on the circuit bandwidth.
If you don't need to access the services globally, choose Standard. With this tier,
you can connect to regions within the same zone at no extra cost. Outbound
cross-zonal traffic incurs more cost.
Unlimited Data plan
All inbound and outbound data transfer is included in the flat rate. There are two
pricing tiers: Standard and Premium, which is priced higher.
Calculate your usage and choose a billing plan. If you exceed about 68% of utilization,
we recommend the Unlimited Data plan.
For more information, see Azure ExpressRoute pricing .

ExpressRoute reference architecture
Connect an on-premises network to Azure using ExpressRoute connects an Azure virtual
network and an on-premises network connected using with VPN gateway failover.

Azure Firewall

Azure Firewall usage is charged at a fixed rate per deployment hour. There's extra cost
for the amount of data transferred.
There aren't extra costs for a firewall deployed in an availability zone. There are extra
costs for inbound and outbound data transfers associated with availability zones.
When compared to network virtual appliances (NVAs), with Azure Firewall you can save
up to 30-50%. For more information, see Azure Firewall and network virtual
appliances .

Azure Firewall reference architecture
Hub-spoke network topology in Azure
Deploy highly available NVAs

Azure Load Balancer
The Load Balancer service distributes inbound traffic according to the configured rules.
There are two tiers: Basic and Standard. The Basic tier is free.
For the Standard tier, you're charged only for the number of configured load-balancing
and outbound rules. Inbound NAT rules are free. There's no hourly charge for the load
balancer when no rules are configured.
For more information, see Azure Load Balancer pricing .

Load Balancer reference architecture
Connect an on-premises network to Azure using ExpressRoute: Multiple subnets
are connected through Azure load balancers.
SAP S/4HANA in Linux on Azure: Distribute traffic to virtual machines in the
application-tier subnet.
Configure ExpressRoute and Site-to-Site coexisting connections: Network traffic
from the VPN gateway is routed to the cloud application through an internal load
balancer. The load balancer resides in the front-end subnet of the application.

Azure VPN Gateway
When provisioning a VPN Gateway resource, choose between two gateway types:

VPN Gateway sends encrypted traffic across the public internet. Site-to-site, pointto-site, and virtual network-to-virtual network connections all use a VPN gateway.
ExpressRoute gateway sends network traffic on a private connection. Azure
ExpressRoute uses this configuration.
For VPN gateway, select Route-based or Policy-based depending on your VPN device
and the kind of VPN connection you want to create. A route-based gateway allows
point-to-site, inter-virtual network, or multiple site-to-site connections. Policy-based
only allows one site-to-site tunnel. Point-to-site isn't supported. So, route-based VPN is
more expensive.
You need to choose the SKU for Route-based VPN:
For developer and test workloads, use Basic.
For production workloads, an appropriate Generation1 or Generation2 SKU.
Each SKU has a range and pricing depends on the type of VPN gateway. Each type offers
different levels of bandwidth, site-to-site, and point-to-site tunnel options. Some of
those types also offer availability zones, which are more expensive. If you need higher
bandwidth, consider Azure ExpressRoute.
VPN Gateway can be the cost driver in a workload because charges are based on the
amount of time that the gateway is provisioned and available.
All inbound traffic is free. All outbound traffic is charged as per the bandwidth of the
VPN type. Bandwidth also varies depending on the billing zone.
For more information, see:
Hybrid connectivity
VPN Gateway pricing
Traffic across billing zones and regions
Bandwidth pricing

VPN Gateway reference architecture
Configure ExpressRoute and Site-to-Site coexisting connections connects the
virtual network to the on-premises network through a VPN device.

Azure Traffic Manager
Azure Traffic Manager uses DNS to route and load balanced traffic to service endpoints
in different Azure regions. An important use case is disaster recovery. In a workload, you

can use Azure Traffic Manager to route incoming requests to the primary region. If that
region becomes unavailable, Azure Traffic Manager fails over to the secondary region.
There are other features that can make the application highly responsive and available.
Those features cost money.
Determine the best web app to handle request based on geographic location.
Configure caching to reduce the response time.
Azure Traffic Manager isn't charged for bandwidth consumption. Billing is based on the
number of DNS queries received, with a discount for services receiving more than 1
billion monthly queries. You're also charged for each monitored endpoint.

Azure Traffic Manager reference architecture
Multi-region N-tier application uses Azure Traffic Manager to route incoming requests
to the primary region. If that region becomes unavailable, Azure Traffic Manager fails
over to the secondary region. For more information, see the section Traffic Manager
configuration.

DNS query charges
Azure Traffic Manager uses DNS to direct clients to specific services.
DNS queries that reach Azure Traffic Manager are charged in million query units.
Not all DNS queries reach Azure Traffic Manager. Recursive DNS servers run by
enterprises and ISPs first attempt to resolve the query by using cached DNS responses.
Those servers query Azure Traffic Manager at a regular interval to get updated DNS
entries. That interval value or time to live (TTL) is configurable in seconds.
TTL can affect cost. A longer TTL value increases the amount of caching and reduces
DNS query charges. Conversely, shorter a TTL value results in more queries.
Increased caching also affects how often the endpoint status refreshes. For example, the
user failover times, for an endpoint failure, becomes longer.

Health monitoring charges
When Azure Traffic Manager receives a DNS request, it chooses an available endpoint
based on configured state and health of the endpoint. To do this, Azure Traffic Manager
continually monitors the health of each service endpoint.

The number of monitored endpoints are charged. You can add endpoints for services
hosted in Azure and then add on endpoints for services hosted on-premises or with a
different hosting provider. The external endpoints are more expensive, but health checks
can provide high-availability applications that are resilient to endpoint failure, including
Azure region failures.

Real User Measurement charges
Real User Measurements evaluates network latency from the client applications to Azure
regions. That influences Azure Traffic Manager to select the best Azure region in which
the application is hosted. The number of measurements sent to Azure Traffic Manager is
billed.

Traffic View charges
By using Traffic View, you can get insight into the traffic patterns where you have
endpoints. The charges are based on the number of data points used to create the
insights presented.

Azure Virtual Network
Azure Virtual Network is free. You can create up to 50 virtual networks across all regions
within a subscription. Here are a few considerations:
Inbound and outbound data transfers are charged per the billing zone. Traffic that
moves across regions and billing zones is more expensive. For more information,
see:
Traffic across zones
Bandwidth pricing
Virtual Network peering has an extra cost. Peering within the same region is
cheaper than peering between regions or Global regions. Inbound and outbound
traffic is charged at both ends of the peered networks. For more information, see
Peering.
Managed services don't always need a virtual network. The cost of networking is
included in the service cost.

Web application cost estimates
Article • 04/18/2023

All web applications (apps) have no up-front cost or termination fees. Some charge only
for what you use and others charge per-second or per-hour. In addition, all web apps
run in Azure App Service plans. Together these costs can help determine the total cost
of a web app.
Use the Azure pricing calculator

to help create various cost scenarios.

App Service plans
App Service plans include the set of compute resources needed for the web app to run.
Except for with the Free tier, an App Service plan carries a charge on the compute
resources it uses. For a description of App Service plans, see Azure App Service plan
overview.
You can potentially save money by choosing one or more App Service plans. To help
find the solution that meets your business requirements, see Should I put an app in a
new plan or an existing plan?
The pricing tier of an App Service plan determines what App Service features you get
and how much you pay for the plan. The higher the tier, the higher the cost. For details
on the pricing tiers, see App Service pricing .
You don't get charged for using the App Service features that are available to you, such
as configuring custom domains, TLS/SSL certificates, deployment slots, and backups. For
a list of exceptions, see How much does my App Service plan cost?

App Service cost
App Service plans are billed on a per second basis. If your solution includes several App
Service apps, consider deploying them to separate App Service plans. This approach
enables you to scale apps independently because they run on separate instances. This
approach saves unnecessary cost.
Azure App Service supports two types of TLS/SSL connections: Server Name Indication
(SNI) TLS/SSL Connections and IP Address TLS/SSL Connections. SNI-based TLS/SSL
works on modern browsers while IP-based TLS/SSL works on all browsers. If your
business requirements allow, use SNI-based TLS/SSL instead of IP-based TLS/SSL.
There's no charge for SNI-based TLS/SSL. IP-based TLS/SSL incurs a cost per connection.

For more information, see SSL Connections in App Service pricing .

API Management cost
If you want to publish APIs hosted on Azure, on-premises, and in other clouds more
securely, reliably, and at scale, use Azure API Management. The pricing tier you choose
depends on the features needed based on your business requirements. For example,
depending on your requirements for scalability, the number of units required to scale
out range from 1 to 10. As the number of units increases, the cost of the pricing tier
increases.
Self-hosted gateways are available in the Developer and Premium pricing tiers. There's
an extra cost for this service if you use the Premium tier. There's no cost for self-hosted
gateways if you use the Developer tier.
For pricing details, see API Management pricing .

Content Delivery Network cost
If you want to offer users optimal online content delivery, use Azure Content Delivery
Network (CDN) in your workload. The data size that you use has the most significant
effect on cost. Choose based on your business requirements. Purchasing data in
increments of TB is significantly higher than purchasing data in increments of GB.
The provider you choose also can affect cost. Choose Microsoft as the provider to get
the lowest cost.
Some data, such as shopping carts, search results, and other dynamic content, isn't
cacheable. CDN offers Acceleration Data Transfers, also called Dynamic Site Acceleration
(DSA), to accelerate this type of web content. The price for DSA is the same across
Standard and Premium profiles.
For pricing details, see Content Delivery Network pricing

.

Azure Cognitive Search cost
To set up and scale a search experience quickly and cost-effectively, use Azure Cognitive
Search. When you create an Azure Cognitive Search service, a resource is created at a
pricing tier or SKU that's fixed for the lifetime of the service. Billing depends on the type
of service. For a list of services, see What are Azure Cognitive Services.

The charges are based on the number of transactions for each type of operation specific
to a service. Some transactions are free. If you need more transactions, choose from the
Standard instances.
For pricing details, see Azure Cognitive Search pricing

.

Azure SignalR Service cost
Use SignalR for any scenario that requires pushing data from server to client in real time.
For example, Azure SignalR Service provides secure and simplified communication
between client and app one-to-one, such as a chat window. It also provides
communication for one-to-many, such as instant broadcasting, IoT dashboards, or
notification to social network. To learn more about Azure SignalR Service, see What is
Azure SignalR Service?
We don't recommend the Free tier for a production environment. With the Standard tier,
you pay only for what you use. We recommend the Standard tier as an enterprise
solution because it offers a large number of concurrent connections and messages.
For pricing details, see Azure SignalR Service pricing

.

Notification Hubs cost
To broadcast push notifications to millions of users at once or tailor notifications to
individual users, use Azure Notification Hubs. Pricing is based on number of pushes. The
Free tier is a good starting point for exploring push capabilities but we don't
recommend it for production apps. If you require more pushes and features such as
scheduled pushes or multi-tenancy, you can buy more pushes per million.
For more information, see Push notifications with Azure Notification Hubs. For pricing
details, see Notification Hubs pricing .

Checklist - Monitor cost
Article • 04/20/2023

Use this checklist to monitor the cost of the workload.
Gather cost data from diverse sources to create reports. Start with tools like
Azure Advisor, Advisor Score, and Azure Cost Management. Build custom reports
relevant for the business by using Consumption APIs.
Cost reports
Review costs in Cost analysis
Use resource tag policies to build reports. Use tags to identify the owners of
systems or applications and create custom reports.
Follow a consistent tagging standard
Video: How to review tag policies with Azure Cost Management
Use Azure built-in roles for cost. Give access only to users who you want to view
and analyze cost reports. You can define roles by their scope. For example, use the
Cost Management Reader role to enable users to view costs for their resources in
subscriptions or resource groups.
Provide the right level of cost access
Azure RBAC scopes
Respond to alerts and have a response plan according to the constraints.
Respond to alerts quickly and identify possible causes and any required action.
Budget and alerts
Use cost alerts to monitor usage and spending
Adopt both proactive and reactive approaches for cost reviews. Conduct cost
reviews at a regular cadence to determine the cost trend. Also, review reports that
are created because of alerts.
Conduct cost reviews
Participate in central governance cost reviews
Analyze the cost at all scopes by using Cost analysis. Identify services that drive
the cost through different dimensions, such as location, usage meters, and so on.
Review whether certain optimizations bring results. For example, analyze costs
associated with reserved instances, savings plans, and Spot virtual machines (VMs)
against business goals.
Quickstart: Explore and analyze costs with Cost analysis

Detect anomalies and identify changes in business or applications that might
have contributed changes in cost. Focus on these factors:
Traffic pattern as the application scales
Budget for the usage meters on resources
Performance bottle necks
CPU utilization and network throughput
Storage footprint for blobs, backups, and archiving
Use Visualization tools to analyze cost information.
Create visuals and reports with in Power BI Desktop
Cost Management App

Set budgets and alerts
Article • 04/18/2023

Azure Cost Management has an alert feature. Alerts are generated when consumption
reaches a threshold.
Consider the metrics for each resource in the workload. For each metric, build alerts on
baseline thresholds. Admins are notified when the workload is using the services at
capacity. The admins can then tune the resources to target SKUs based on current load.
You can also set alerts on allowed budgets at the resource group or management
groups scopes. By setting alerts on metrics and budgets, you can balance both cloud
services performance and budget requirements.
Over time, you can optimize the workload to autoheal itself when it triggers alerts. For
more information about using alerts, see Use cost alerts to monitor usage and spending.

Respond to alerts
When you receive an alert, check the current consumption data because alerts don't
trigger in real time. There might be a delay between the alert and the current actual
cost. Look for significant difference between cost values when the alert occurred and the
current cost. Next, conduct a cost review to discuss the cost trend, possible causes, and
any required action.
Determine short and long-term actions that justify business value. Can a temporary
increase in the alert threshold be a feasible fix? Do you need to increase the longer-term
budget? Approval of any budget increase is mandatory.
If unnecessary or expensive resources trigger the alert, implement other Azure Policy
controls. You can also add budget automation to trigger resource scaling or shutdowns.
Once you've created your alerts, your can also proactively review your Azure Costs and
the forecast for upcoming weeks and months. For more information about Azure Cost
Analysis and how to create custom views for your application, see Start using Cost
analysis.

Revise budgets
After you identify and analyze your spending patterns, you can set budget limits for
applications or business units. You want to assign access to view or manage each

budget to the appropriate groups. Setting several alert thresholds for each budget can
help track your burn down rate.
Azure Cost Management also has a tool that helps to identify cost anomalies and
unexpected charges to your environment. To learn how to identify anomalies and
unexpected changes in cost, see Identify anomalies and unexpected changes in cost

Generate cost reports
Article • 04/03/2023

To monitor the cost of the workload, use Azure cost tools or custom reports. You can
scope reports to business units, applications, IT infrastructure shared services, and so on.
Make sure that the information is consistently shared with the stakeholders.

Azure cost tools
Azure provides the following cost tools that help track cloud spending and offer
recommendations.
Azure Advisor
Advisor Score
Azure Cost Management
Azure Cost Management Power BI app

Cost analysis
Cost analysis is a tool in Azure Cost Management that lets you view aggregated costs
over a period of time. This view helps you understand your spending trends.
You can view costs at different scopes, such as for a resource group or specific resource
tags. Cost analysis provides built-in charts and custom views. You can also download the
cost data in CSV format to analyze with other tools.
For more information, see Quickstart: Explore and analyze costs with cost analysis.
７ Note
There are many ways of purchasing Azure services. Not all services are supported
by Azure cost management. For example, you obtain detailed billing information
for services purchased through a Cloud Solution Provider (CSP) directly from the
CSP. For more information about the supported cost data, see Understand cost
management data.

Advisor recommendations
Azure Advisor recommendations for cost highlight any over-provisioned services and
ways to lower costs. For example, it can show virtual machines that should be resized to

a lower SKU, un-provisioned ExpressRoute circuits, and idle virtual network gateways.
For more information, see Advisor cost management recommendations.

Consumption APIs
Granular and custom reports help track cost over time. Azure provides a set of
consumption APIs to generate such reports. These APIs let you query and create various
cost data. Data includes usage data for Azure services and third-party services through
Azure Marketplace, balances, budgets, recommendations on reserved instances, and
others. You can configure Azure role-based access control (Azure RBAC) policies to let
only a certain set of users or applications access the data.
For example, you want to determine the cost of all resources used in your workload for a
given period. One way of getting this data is by querying usage meters and the rate of
those meters. You also need to know the billing period of the usage. By combining these
APIs, you can estimate the consumption cost.
The Azure consumption APIs include:
Billing account API: To get your billing account to manage your invoices, payments,
and track costs.
Billing periods API: To get billing periods that have consumption data.
Usage detail API: To get the breakdown of consumed quantities and estimated
charges.
Marketplace store charge API: To get usage-based marketplace charges for thirdparty services.
Price sheet API: To get the applicable rate for each meter.
You can import the results of the APIs into analysis tools.
７ Note
Consumption APIs are supported for enterprise enrollments and web direct
subscriptions (with exceptions). Check Consumption APIs for updates to determine
support for other types of Azure subscriptions.
For more information about common cost scenarios, see Billing automation scenarios.

Custom scripts

Use Azure APIs to schedule custom scripts that identify orphaned or empty resources.
For example, unattached managed disks, load balancers, application gateways, or Azure
SQL Servers with no databases. These resources incur a flat monthly charge while
unused. Other resources might be stale, for example, VM diagnostics data in blob or
table storage. To determine if you should delete the item, check its last use and
modification timestamps.

Analyze and visualize
Start with the usage details in the invoice. Review that information against relevant
business data and events. If there are anomalies, evaluate the significant changes in
business or applications that might have contributed those changes.
Power BI Desktop has a connector that lets you connect billing data from Azure Cost
Management. You can create custom dashboards and reports, ask questions of your
data, and publish and share your work.
７ Note
Sharing requires Power BI Premium licenses.
For more information, see Create visuals and reports with the Azure Cost Management
connector in Power BI Desktop.
You can also use the Cost Management App . The app uses the Azure Cost
Management Template app for Power BI. You can import and analyze usage data and
incurred cost within Power BI.

Azure Cost Management Power BI app
Use the Azure Cost Management Power BI app to analyze and manage your costs in
Power BI. You can use the app to monitor costs and usage trends, produce reports, and
identify cost optimization options to reduce expenditures.
The Cost Management Power BI app currently supports customers with an Enterprise
Agreement. For more information, see Azure Cost Management Power BI app.

Conduct cost reviews
Article • 04/18/2023

Cost monitoring tracks and reviews cloud costs to help establish budget controls and
prevent misuse. It's important to adopt proactive and reactive review approaches for
monitoring cost. Stakeholders should conduct cost reviews regularly and include
reactive cost reviews. For example, when a budget limit causes an alert.

Who should be included in a cost review?
Cost monitoring is a complex process that involves planning, cost estimation,
budgeting, and cost control, which can include several different teams.
Cloud Architect/Administrator​. Monitor and back-up cloud systems.
Product Owner. Needs awareness of the decisions.
Finance Owner and Financial Operations Practitioner. Need to understand cloud
billing to derive business benefits using financial metrics to make effective
decisions.
You can identify owners of systems or applications through resource tags.

Plan and schedule cost reviews
We recommend scheduling cost reviews as part of the regular business reviews.
Schedule a review during the billing period. This review creates awareness of the
estimated pending billing. You can use Azure Advisor, Advisor Score, and Azure
Cost Management – cost analysis to help with the reports.
Schedule a review after the billing period. This review shows the actual cost with
activity for that month. Use TimeframeType to generate monthly reports. The APIs
can query data that gets information on balances, new purchases, Azure
Marketplace service charges, adjustments, and overage charges. For more
information, see Cost Management automation overview.
Schedule a review when you receive a budget alert or Azure Advisor
recommendation.
Web Direct (pay-as-you-go) and Cloud Solution Provider (CSP) billing occurs monthly,
while Enterprise Agreement (EA) billing occurs annually. But ensure you do a cost review
regardless of the billing cycle.

Checklist - Optimize cost
Article • 07/27/2023

Continue to monitor and optimize the workload by using the right resources and sizes.
Use this checklist to optimize a workload.
Review the underutilized resources. Evaluate CPU utilization and network
throughput over time to check if the resources are used adequately. Azure Advisor
identifies underutilized virtual machines (VMs). You can choose to decommission,
resize, or shut down the machine to meet the cost requirements.
Resize virtual machines
Shutdown the underutilized instances
Auto start and stop VMs tool to non-production VMs. The start and stop VMs v2
feature starts or stops Azure Virtual Machines instances across multiple subscriptions. It
starts or stops virtual machines on user-defined schedules. For more information, see
Start and stop VMs.
Use Log Analytics to run log queries on data collected by Azure Monitor. You'll
be able to analyze utilization of resources. For more infomation, see Cost
optimization and Log Analytics.
Continuously take action on the cost reviews. Treat cost optimization as a
process, rather than a point-in-time activity. Use tooling in Azure that provides
recommendations on usage or cost optimization. Review the cost management
recommendations and take action. Make sure that all stakeholders are in
agreement about the implementation and timing of the change.
Recommendations for cost management in the Azure portal
Recommendations in the Cost Management Power BI app
Recommendations in Azure Advisor
Recommendations using Reservation REST APIs
Use savings plans. By using savings plans, you get the most flexible savings for
dynamic workloads, while accommodating planned or unplanned changes. With
savings plans, you commit to a fixed, hourly, dollar amount, collectively, on
compute services, globally.
Savings plans
Use Azure reservations. You get the greatest cost savings for stable, predictable
workloads with no planned changes by using reserved instances or capacity.
Consider that you're often committing to specific SKUs, throughput, or utilization
targets, often in a particular Azure region.

Azure reservations
Use discount prices. The following methods of buying Azure resources can lower
costs.
Azure Hybrid Benefit
Azure reservations
There are also payment plans offered at a lower cost:
Microsoft Azure Enterprise portal
Enterprise Dev Test Subscription
Microsoft Cloud Solution Provider program
Have a scale-in and scale-out policy. In a cost-optimized architecture, costs scale
linearly with demand. Increasing customer base shouldn't require more investment
in infrastructure. Conversely, if demand drops, scale-down unused resources. And
autoscale Azure resources when possible.
Autoscale instances
Azure SQL Database elastic pools are a simple, cost-effective solution for
managing and scaling multiple databases that have varying and unpredictable
usage demands
SQL Serverless is a compute tier for single databases in Azure SQL Database
that automatically scales compute based on workload demand and bills for the
amount of compute that you use per second.
Reevaluate design choices. Analyze the cost reports and forecast the capacity
needs. You might need to change some design choices.
Choose the right storage tier. Consider using hot, cool, and archive tiers for
storage account data. Storage accounts can provide automated tiering and
lifecycle management. For more information, see Review your storage options.
Choose the right data store. Instead of using one data store service, use a mix
of data store depending on the type of data you need to store for each
workload. For more information, see Choose the right data store .
Choose Spot VMs for low priority workloads. Spot VMs are ideal for workloads
that can be interrupted, such as highly parallel batch processing jobs.
Spot VMs
Optimize data transfer. Only deploy to multiple regions if your service levels
require it for either availability or geo-distribution. Data going out of Azure data
centers can add cost because pricing is based on billing zones.
Traffic across billing zones and regions

Reduce load on servers. Use Azure Content Delivery Network (CDN) and
caching service to reduce load on front-end servers. Caching is suitable for
servers that are continually rendering dynamic content that doesn't change
frequently.
Use managed services. Measure the cost of maintaining infrastructure and
replace it with Azure platform as a service (PaaS) or software as a service (SaaS)
services.
Managed services

Autoscale instances
Article • 04/20/2023

In Azure, it's easier to grow a service with little to no downtime compared to
downscaling a service, which usually requires deprovisioning or downtime. In general,
opt for scale-out instead of scale up.
For certain applications, capacity requirements might change over time. Autoscaling
policies allow for less error-prone operations and cost savings through robust
automation.

Virtual machine instances
For autoscaling, consider the choice of instance size. The size can significantly change
the cost of your workload.
Choose smaller instances where workload is highly variable and scale out to
get the desired level of performance, rather than up. This approach lets you make
your cost calculations and estimates granular.

Stateless applications

You can use many Azure services to improve the application's ability to scale
dynamically, even if the application isn't originally designed to scale dynamically.
For example, many ASP.NET stateful web applications can be made stateless. Then you
can autoscale them, which results in a cost benefit. You store state using Azure Redis
Cache, or Azure Cosmos DB as a back-end session state store through a session state
provider

.

Virtual machines
Article • 11/30/2022

Virtual machines can be deployed in fix-sized blocks. These VMs must be adequately
sized to meet capacity demand and reduce waste.
For example, look at a VM running the SAP on Azure project can show you how initially
the VM was sized based on the size of existing hardware server (with cost around €1 K
per month), but the real utilization of VM wasn't more than 25% - but simple choosing
the right VM size in the cloud we can achieve 75% saving (resize saving). And by
applying the snoozing you can get additional 14% of economy:

It's easy to handle cost comparison when you're well equipped and for this Microsoft
provides the set of specific services and tools that help you to understand and plan
costs. These include the TCO Calculator, Azure Pricing Calculator, Azure Cost
Management (Cloudyn), Azure Migrate, Azure Cosmos DB Sizing Calculator, and the
Azure Site Recovery Deployment Planner.
Here are some strategies that you can use to lower cost for virtual machines.

Resize virtual machines
You can lower cost by managing the size and the number of VMs.

Determine the load by analyzing the CPU utilization to make sure that the
instance is adequately utilized.
Ideally, with the right size, the current load should fit in a lower SKU of the same tier.
Another way is to lower the number instances and still keep the load below a reasonable
utilization. Azure Advisor

recommends load less than 80% utilization for non-user

facing workloads and 40% when user-facing workload. It also provides current and
target SKU information.
You can identify underutilized machines by adjusting the CPU utilization rule on each
subscription.
Resizing a virtual machine does require the machine to be shut down and restarted.
There might be a period of time when it will be unavailable. Make sure you carefully
time this action for minimal business impact.

Shut down underutilized instances
Use the Start/stop VMs during off-hours feature of virtual machines to minimize waste.
There are many configuration options to schedule the start and stop times. The feature
is suitable as a low-cost automation option. For information, see Start/stop VMs during
off-hours solution in Azure Automation.
Azure Advisor

evaluates virtual machines based on CPU and network utilization over a

time period. Then, the recommended actions are shut down or resize instances and cost
saving with both actions.

Spot VMs
Some workloads don't have a business requirement to complete a job within a period.
Can the workload be interrupted?
Spot VMs are ideal for workloads that can be interrupted, such as highly parallel batch
processing jobs. These VMs take advantage of the surplus capacity in Azure at a lower
cost. They're also well suited for experimental, development, and testing of large-scale
solutions.
For more information, see Use Spot VMs in Azure or check out our Azure Virtual
Machine Spot Eviction guide to learn how to create a reliable interruptible workload in
Azure.

Reserved VMs
Virtual machines are eligible for Azure Reservations. You can prepay for VM instances if
you can commit to one or three years. Reserved instances are appropriate for workloads
that have a long-term usage pattern.
The discount only applies to compute and not the other meters used to measure usage
for VMs. The discount can be extended to other services that emit VM usage, such as
Virtual machine scale sets and Container services, to name a few. For more information,
see Software costs not included with Azure Reserved VM Instances and Services that get
VM reservation discounts.
With reserved instances, you need to determine the VM size to buy. Analyze usage data
using Reservations Consumption APIs and follow the recommendations of Azure
portal

and Azure Advisor

to determine the size.

Reservations also apply to dedicated hosts. The discount is applied to all running hosts
that match the reservation scope and attributes. An important consideration is the SKU
for the host. When selecting a SKU, choose the VM series and type eligible to be a
dedicated host. For more information, see Azure Dedicated Hosts pricing .
For information about discounts on virtual machines, see How the Azure reservation
discount is applied to virtual machines.

Savings plans
Virtual machines are eligible for savings plans. By committing to a one-year or threeyear savings plan, you spend a fixed hourly dollar amount collectively on all compute
services. Consider that the commitment spans across all participating compute services
globally.
Using savings plans can save you up to 66% on cost.

Caching data
Article • 04/20/2023

Caching is a process that stores a copy of the data in front of the main data store. The
cache store locations are closer to the consumer than the main store. Advantages of
caching include faster response times and the ability to serve data quickly, which saves
on the overall cost. Check the built-in caching features of Azure services used in your
architecture. Azure also offers caching services such as Azure Cache for Redis or Azure
CDN.
For information about what data is suitable for caching, see Caching guidance.

Lower costs associated with reliability and
latency
Caching is a cost-effective way to store data, provide reliability, and reduce network
latency.
The type of data helps you determine if you need the complex capabilities of the
backend data store, such as data consistency. For fully static data, store it in a
caching store. If the data doesn't change frequently, consider placing a copy of the
data in a caching store and refresh it from time to time.
For example, an application stores images in blob storage. Then every time the
application requests an image, the business logic generates a thumbnail from
the main image and returns it to the caller. If the main image doesn't change
too often, then return the previously generated thumbnails stored in a cache.
This way, you can save on resources required to process the image and lower
the request response rate.
If the backend is unavailable, the cache continues to manage requests by using the
copy until the backend fails over to the backup data store.
Caching is also an effective way of reducing network latency. Instead of reaching
the central server, the response uses cache. That way, the client can receive the
response instantaneously. If you need higher network performance and the ability
to support more client connections, choose a higher tier of the caching service.
However, higher tiers incur more costs.

Caching can be expensive
Incorrect use of caching can result in severe business outcomes and higher costs.
If you choose to add a cache, your architecture will have multiple data stores.
There are added costs to keeping them coordinated. You might need to fill the
cache before putting it in production. If you fill the cache on the application's first
access, it can introduce latency. If you seed the cache, it can affect the application's
start time. If you don't refresh the cache, your customers can get stale data.
Invalidate the cache at the right time when there's latest information in the source
system. Use strategies to age out the cache when appropriate.
Add instrumentation to make sure the caching layer is working optimally.
Instrumentation adds complexity and implementation cost.
Caching services such as Azure Cache for Redis offer tiers by cost. Pricing per tier is
based on the cache size and network performance. A smaller cache increases latency.
Before you choose a tier, estimate a baseline. Try load testing the number of users and
cache size.

Tradeoffs for cost
Article • 04/25/2023

As you design a workload, consider tradeoffs between cost optimization and other
aspects of the design, such as security, scalability, resilience, and operability.
What is most important for the business: lowest cost, no downtime, high throughput?
An optimal design doesn't equate to a low-cost design. You might make risky choices in
favor of a cheaper solution.

Cost versus reliability
Cost has a direct correlation with reliability.
Does the cost of high availability components exceed the acceptable downtime?
Overall service level agreement (SLA), Recovery Time Objective (RTO), and Recovery
Point Objective (RPO) might lead to expensive design choices. If your service SLAs, RTOs,
and RPOs times are short, greater investment is inevitable for high availability and
disaster recovery options.
For example, to support high availability, you choose to host the application across
regions. This choice is costlier than single region because of the replication costs or the
need for provisioning extra nodes. Data transfer between regions also adds cost.
If the cost of high availability exceeds the cost of downtime, you can save by using
Azure platform-managed replication and then recover data from the backup storage.
For resiliency, availability, and reliability considerations, see the Reliability pillar.

Cost versus performance efficiency
Boosting performance leads to higher costs.
Many factors affect performance.
Fixed or consumption-based provisioning. Avoid cost estimating a workload at
consistently high utilization. Consumption-based pricing is more expensive than
the equivalent provisioned pricing. Smooth out the peaks to get a consistent flow
of compute and data. Ideally, use manual scaling and autoscaling to find the right
balance. Scaling up is more expensive than scaling out.

Azure regions. Cost scales directly with the number of regions. Locating resources
in cheaper regions shouldn't negate the cost of network ingress and egress or by
degraded application performance because of increased latency.
Caching. Every payload's render cycle consumes both compute and memory. Use
caching to reduce load on servers and save with precanned storage and bandwidth
costs. The savings can be dramatic, especially for static content services.
While caching can reduce cost, there are some performance tradeoffs. For
example, Azure Traffic Manager pricing is based on the number of DNS queries
that reach the service. Reduce that number through caching and configure how
often the cache refreshes. Relying on the cache that isn't frequently updated
causes longer user failover times if an endpoint is unavailable.
Batch or real-time processing. Using dedicated resources for batch processing of
long-running jobs increases the cost. You can lower cost by provisioning Spot VMs
but be prepared for the job to be interrupted every time Azure evicts the VM.
For performance considerations, see the Performance efficiency pillar.

Cost versus security
Increasing security of the workload increases costs.
As a rule, don't compromise on security. With certain workloads, you can't avoid security
costs. For example, for specific security and compliance requirements, deploying to
differentiated regions is more expensive. Premium security features can also increase the
cost. There are areas where you can reduce costs by using native security features. For
example, avoid implementing custom roles if you can use built-in roles.
For security considerations, see the Security pillar.

Cost versus operational excellence
Investing in systems monitoring and automation might increase the cost initially but
over time reduces cost. Integrate IT operations processes like user or application access
provisioning, incident response, and disaster recovery with the workload.
Cost of maintaining infrastructure is more expensive. With platform as a service (PaaS) or
software as a service (SaaS) services, infrastructure, platform management services, and
other operational efficiencies are included in the service pricing.
For operational considerations, see the Operational excellence pillar.

Operational excellence documentation
Apply reliable, predictable, and automated operations processes to your architecture to
keep an application running in production.

Key points

ｆ

QUICKSTART

Principles
Automation overview
Application design
Monitoring checklist
DevOps culture
DevOps patterns

ｄ

TRAINING

Operational Excellence

ｑ

VIDEO

Getting Started with DevOps

Automate business solutions

ｂ

GET STARTED

Activate resources on demand
Create repeatable and consistent environments
Configure programmatic infrastructure
Automate operational tasks

ｐ

CONCEPT

Understand automation best practices

Deploy infrastructure with code
Bootstrap automation
Understand Azure Functions

ｄ

TRAINING

Choose the best Azure service to automate your business processes

ｑ

VIDEO

Azure DevOps with ARM templates

Engineer modern release management strategies

ｅ

OVERVIEW

Configure development environments
Practice continuous integration
Perform release testing
Consider performance
Evaluate release deployment
Anticipate deployment issues

ｐ

CONCEPT

Understand source control
Consider continuous integration pipelines
Automate testing
Optimize your build
Document your release processes
Stage your workloads
Roll back with deployment slots

ｑ

VIDEO

Continuous Delivery and Release Management | DevOps for Mobile

Monitor for DevOps

ｐ

CONCEPT

Understand application and platform monitoring
Analyze alerts

ｑ

VIDEO

Continuous Monitoring for Web Perf and Accessibility
Monitoring AKS with Azure

DevOps tools and services

ｉ

REFERENCE

Azure DevOps
Azure Diagnostics
Azure Monitor
Azure Resource Manager
Azure Role-Based Access Control (RBAC)
Bridge to Kubernetes
Chef
Docker Desktop
Git
Terraform

DevOps APIs

ｉ

REFERENCE

Alerts - Get All
Automation REST API
Azure Container Instances
Azure Deployment Manager
Azure Resource Health
Network Monitoring

Overview of the operational excellence
pillar
Article • 05/04/2023

The operational excellence pillar covers the processes that keep an application running
in production. Deployments must be reliable and predictable. Automated deployments
reduce the chance of human error. Fast and routine deployment processes don't slow
down the release of new features or bug fixes. It's equally important to be able to
quickly roll backward or forward if an update has problems.
To assess your workload by using the tenets found in the Microsoft Azure WellArchitected Framework, see the Azure Well-Architected Review.
We recommend the following video to help you achieve operational excellence with the
Azure Well-Architected Framework:
https://learn.microsoft.com/shows/azure-enablement/achieve-operational-excellencewith-azure-well-architected-framework/player

Articles
The Microsoft Azure Well-Architected Framework includes the following articles in the
operational excellence pillar:
Operational
excellence articles

Description

Release engineering:

Provides guidance on how to design, build, and orchestrate workloads

application
development

with operational excellence principles in mind.

Monitoring operations
of cloud applications

Shows how monitoring and diagnostics are essential to any workload
and are crucial for cloud applications that run in a remote datacenter.

Performance
considerations for your
deployment

Describes the monitoring and management of performance and
availability of software applications through operational excellence.

infrastructure
Release engineering:

Shows how deploying your application code is one of the key factors

deployment

that determines your application stability.

Operational
excellence articles

Description

Repeatable
Infrastructure

Refers to best practices for deploying the platform where your
application runs. Infrastructure provisioning is frequently known as
Deployment Automation or Infrastructure as code.

Testing your application
and Azure environment

Shows how testing is fundamental for preparing for the unexpected
and catching mistakes before they affect users.

Enforcing resource-level rules through Azure Policy helps ensure adoption of
operational excellence best practices for all the assets, which supports your workload.
For example, Azure Policy can help ensure all the virtual machines (VMs) supporting
your workload adhere to a preapproved list of VM SKUs.
Azure Advisor provides a set of Azure Policy recommendations to help you quickly
identify opportunities to implement Azure Policy best practices for your workload.
Use the DevOps checklist as a starting point to assess your DevOps culture and process.

Next steps
Reference the operational excellence principles to guide you in your overall strategy.
Principles

Operational excellence design principles
Article • 03/15/2023

The principles of operational excellence are a series of considerations that can help
achieve superior operational practices.
To achieve a higher competency in operations, consider and improve how software is:
Developed
Deployed
Operated
Maintained
Equally important, provide a team culture, which includes:
Experimentation and growth
Solutions for rationalizing the current state of operations
Incident response plans
To assess your workload using the tenets found in the Azure Well-Architected
Framework, reference the Microsoft Azure Well-Architected Review.
The following design principles provide:
Context for questions
Why a certain aspect is important
How an aspect is applicable to Operational excellence
These critical design principles are used as lenses to assess the Operational excellence of
an application deployed on Azure. These lenses provide a framework for the application
assessment questions.

Optimize build and release processes
Embrace software engineering disciplines across your entire environment, which include
the following disciplines:
Provision with Infrastructure as Code
Build and release with continuous integration and continuous delivery (CI/CD)
pipelines
Use automated testing methods
Avoid configuration drift through configuration as code

This approach ensures the creation and management of environments throughout the
software development lifecycle. It enables:
Consistency
Repetition
Early detection of issues

Understand operational health
Implement systems and processes to monitor all aspects of your workload. Including:
Build and release processes
Infrastructure health
Application health
Robust monitoring ensures the observability of a workload and allows you to correlate
events and take proactive mitigating issues.
In addition, customer data is critical to understanding the health of a workload and
whether the service is meeting the business goals.
） Important
The Health Modeling section of the Azure Mission-Critical framework contains
further in-depth guidance and examples on how to build a health model for a given
workload.

Rehearse recovery and practice failure
Rehearse recovery and practice failure using the following methods:
Run disaster recovery (DR) drills on a regular cadence.
Use chaos engineering practices to identify and remediate weak points in
application reliability.
Rehearse failure to validate the effectiveness of recovery processes and ensure
teams are familiar with their responsibilities.
Document past failures and automate their remediation where possible.

Embrace continuous operational improvement

Teams that embrace continuous operational improvement continuously evaluate and
refine operational procedures and tasks. They strive to reduce complexity and ambiguity
whenever possible.
Adopting a continuous improvement culture helps organizations:
Evolve processes over time.
Optimize inefficiencies and associated processes.
Learn from failures.
Continuously evaluate new opportunities.

Use loosely coupled architecture
Use modern architecture patterns such as:
microservices
loosely coupled
serverless
and pair this with cloud design patterns such as:
Circuit breakers
Load-Leveling
Throttling
and advanced deployment strategies like:
Canary
Blue-green
Staggered
to enable teams to build and deploy services independently and minimize the impact if
there is a service failure.
This principle also extends to procedural decoupling. Teams will be able to take full
advantage of their loosely coupled architecture if they do not have to depend on
partner teams to support, approve, or operate their workloads.

Next step
Automation overview

Automation overview: Goals, best
practices, and types
Article • 05/30/2023

Automation has revolutionized how businesses operate and this trend continues to
evolve. Businesses have moved to automating manual processes so that engineers can
focus attention on tasks that add business value. Automating business solutions lets
you:
Activate resources on demand.
Deploy solutions rapidly.
Minimize human error in configuring repetitive tasks.
Produce consistent and repeatable results.
For more information, see Deployment considerations for automation.

Goals of automation
When you automate technical processes, a common approach for some organizations is
to automate what they can and leave the more difficult processes for humans to
perform manually.
A goal of automation is to make tools that do what humans can do, only better. For
example, a human can perform any given task once. But when the task requires
repetitive runs, especially over long time periods, an automated system is better
equipped to do that work with more predictable, error-free results. Increasing speed is
another goal in automation. When you practice these automation goals, you can build
systems that are faster, repetitive, and can run on a daily basis.

Automate toil to improve efficiency
Most automation involves a percentage of toil. Toil is the operational work that's related
to a process that's manual, repetitive, can be automated, and has minimal value. It's
counterproductive to automation but in many organizations, a small amount of toil is
unavoidable. It becomes an issue when too much toil slows progress. A project's
production velocity decreases if engineers are continuously interrupted by manual tasks
attributed to toil, either planned or unplanned. Too much toil can impact job
satisfaction. Engineers become dissatisfied when they find themselves spending too
much time on operational toil rather than on other projects.

Automation should be developed, and increased, so engineers can eliminate future toil.
By reducing toil, engineers can concentrate on innovating business solutions.
For more information, see Toil automation .

Automation best practices
Ensure consistency: The more manual processes are involved, the more prone to
human error. Manual processes can lead to mistakes, oversights, reduction of data
quality, and reliability problems.
Centralize mistakes: Choose a platform that lets you fix bugs in one place in order
to fix them everywhere. This best practice reduces the chance of error and the
possibility of the bug being reintroduced.
Identify issues quickly: Complex issue might not always be identifiable in a timely
manner. But with good automation, detection of these issues should occur quickly.
Maximize employee productivity: Automation leads to more innovative solutions,
and in general provides more value to the business. This improvement in turn
raises morale and job satisfaction. Once a process is automated, training and
maintenance can be greatly reduced or eliminated. This shift frees engineers to
spend less time on manual processes and more time on automating business
solutions.

Types of automation
Three types of automation are described in this article:
Infrastructure deployment
Infrastructure configuration
Operational tasks
These categories share the same goals and best practices mentioned previously. They
differ in areas where Azure provides solutions that help achieve optimal automation.
Other types of automation, such as continuous deployment and continuous delivery, are
described further in the Operational Excellence pillar.

Infrastructure deployment
As businesses move to the cloud, they need to repeatedly deploy their solutions and
know that their infrastructure is in a reliable state. To meet these challenges, you can
automate deployments by using a practice referred to as infrastructure as code. In code,
you define the infrastructure that needs to be deployed.

There are many deployment technologies you can use with Azure. Here are three
examples:
Azure Resource Manager (ARM) templates
Azure Bicep
Terraform
These technologies use a declarative approach. This approach lets you state what you
intend to deploy without having to write the sequence of programming commands to
create it. You can deploy not only virtual machines, but also the network infrastructure,
storage systems, and any other resources you might need.

Infrastructure configuration
If you don't manage configuration carefully, your business could encounter disruptions
such as systems outages and security issues. Optimal configuration enables you to
quickly detect and correct configurations that could interrupt or slow performance.
When creating new resources on Azure, you can take advantage of configuration as
code to bootstrap the deployment.
Configuration tools can also be used to configure and manage the ongoing state of
deployed resources.

Operational tasks
As the demand for speed in performing operational tasks increases over time, you're
expected to deliver things faster and faster. Manually performing operational tasks will
fail to scale as demand increases. This is where automation can help. To meet ondemand delivery using an automation platform, you need to develop automation
components (such as runbooks and configurations), create integrations to systems that
are already in place efficiently, and operate and troubleshoot.
Advantages of automating operational tasks include:
Optimize and extend existing processes.
Deliver flexible and reliable services.
Lower costs.
Improve predictability.
Two popular options for automating operational tasks are:

Azure Functions - Run code without managing the underlying infrastructure on
where the code is run.
Azure Automation - Uses programming and scripting language to automate
operational tasks in code and run on demand.
For more information, see Automation.

Next steps
Automate repeatable infrastructure

Repeatable infrastructure
Article • 05/03/2023

Historically, deploying a new service or application involves manual work such as
procuring and preparing hardware, configuring operating environments, and enabling
monitoring solutions. Ideally, an organization would have multiple environments in
which to test deployments. These test environments should be similar enough to
production that deployment and run time issues are detected before deployment to
production. This manual work takes time, is error-prone, and can produce
inconsistencies between the environments if not done well.
Cloud computing changes the way we procure infrastructure. No longer are we
unboxing, racking, and cabling physical infrastructure. We have internet accessible
management portals and REST interfaces to help us. We can now provision virtual
machines, databases, and other cloud services on demand and globally. When we no
longer need cloud services, they can be easily deleted.
However, cloud computing alone doesn't remove the effort and risk in provisioning
infrastructure. When you use a cloud portal to build systems, many of the same manual
configuration tasks remain. Application servers require configuration, databases need
networking, and firewalls need firewalling.

Azure landing zones (repeatable environment
configuration)
Organizations that manage, govern, or support multiple workloads in the cloud will
require repeatable and consistent environments. Azure landing zones provide central
operations teams (also known as platform teams) with a repeatable approach to
environmental configuration. To deliver consistent environments, all Azure landing zones
provide a set of common design areas, reference architecture, reference
implementation, and a process to modify that deployment to fit the organization design
requirements.
This environment is primarily enabled by using the Azure landing zone design principles
of policy-driven governance alongside subscription democratization.
The following links are from the Cloud Adoption Framework to help you deploy Azure
landing zones:
Azure landing zones adhere to a common set of design areas to guide
configuration of required environment considerations including: Identity, Network

topology and connectivity, Resource organization, Governance disciplines,
Operations baseline, and Business continuity and disaster recovery (BCDR)
Azure landing zones can be deployed through via:
Azure landing zones accelerator portal
Azure landing zones - Bicep modules design considerations
Azure landing zones - Terraform modules design considerations
To get started with Azure landing zones to create consistent, repeatable environment
configuration, see the article series on Azure landing zones.

Deploy infrastructure with code
To fully realize deployment optimization, reduce configuration effort, and automate full
environments' deployment, something more is required. One option is referred to as
infrastructure as code.
Infrastructure as code (IaC) is the management of infrastructure—such as virtual
machines, load balancers, and connection topology—in a descriptive model, using a
versioning system that's similar to what's used for source code. When you're creating an
application, the same source code generates the same binary every time it's compiled. In
a similar manner, an IaC model generates the same environment every time it's applied.
IaC is a key DevOps practice, and it's often used with continuous delivery.
Ultimately, IaC lets you and your team develop and release changes faster, but with
higher confidence in your deployments.

Gain higher confidence
One of the biggest benefits of IaC is the level of confidence you can have in your
deployments, and in your understanding of the infrastructure and its configuration.
Integrate with your process. If you have a process by which code changes are peer
reviewed, you can use that same process for reviewing infrastructure changes. This
review process can help proactively detect problematic configurations that are difficult
to see when making manual infrastructure changes.
Consistency. Following an IaC process ensures that the whole team follows a standard,
well-established process. Historically, some organizations designate a single or small set
of individuals responsible for deploying and configuring infrastructure. By following a
fully automated process, responsibility for infrastructure deployments moves from
individuals into the automation process and tooling. This move broadens the number of

team members who can initiate infrastructure deployments while maintaining
consistency and quality.
Automated scanning. Many types of IaC configurations can be scanned by automated
tooling. One such type of tooling is linting to check for errors in the code. Another type
will scan the proposed changes to your Azure infrastructure to ensure they follow
security and performance best practices. Automated scanning can be an important part
of a Continuous security approach.
Secret management. Most solutions require secrets to be maintained and managed.
These include connection strings, API keys, client secrets, and certificates. Following an
IaC approach means that you need to adopt best practices for managing secrets. For
example, Azure Key Vault is used to store secrets securely. It integrates with many IaC
tools and configurations to ensure that the person conducting the deployment doesn't
need access to production secrets. This approach, in turn, helps you adhere to the
security principle of least privilege.
Access control. A fully automated IaC deployment pipeline means that an automated
process should perform all changes to your infrastructure resources. This approach has
many security benefits. For more information, see Security design principles. By
automating your deployments, you can be confident that changes deployed to your
environment have followed the correct procedure. You can even consider expanding the
number of people who can initiate a deployment since the deployment itself is done in
a fully automated way. Ideally, you would remove the ability for humans to manually
modify your cloud resources and instead rely completely on automated processes. In
emergencies, you can allow for this ability to be overridden, by using a break glass
account or Privileged identity management.
Avoid configuration drift. When you use IaC, you can redeploy all of your environment
templates on every release of your solution. IaC tooling is built to be idempotent, which
means that it can run several times and will produce the same result each time.
Running infrastructure as code operations frequently has the following benefits:
Avoids deployment staleness, which prevents unforeseen issues during a
redeployment. For example, as part of a disaster recovery plan.
Reduces complexity overall as there's one process that's rehearsed often.
Helps to avoid configuration drift. Accidental changes outside of the regular
pipeline are corrected quickly and the source of truth for your environment's
configuration remains in code.

Manage multiple environments

Many organizations maintain multiple environments, for example, test, staging, and
production. In some cases, multiple production environments are maintained for multitenanted solutions or geographically distributed applications. Ensuring consistency
across these environments can be difficult; using infrastructure as code solutions can
help.
Manage non-production environments. A common pain point for organizations is
when nonproduction environments are dissimilar to production environments. Often,
when you build production and nonproduction environments by hand, the configuration
of each won't match. This mismatch slows down the testing of changes and reduces
confidence that changes won't harm a production system. When following an IaC
approach, this problem is minimized. When you use IaC automation, the same
infrastructure configuration files can be used for all environments, producing almost
identical environments. When needed, differentiation can be achieved by using input
parameters for each environment.
Dynamically provision environments. Once you have your IaC configurations defined,
you can use them to provision new environments more efficiently. This agility can be
enormously helpful when you're testing your solution. For example, you could quickly
provision a duplicate of your production environment that can then be used for security
penetration tests, load testing or help a developer track down a bug.
Scale production environments. IaC configurations can be used to deploy more
instances of your solution, ensuring consistency between all environments. This model
can be useful if you're following certain deployment patterns or if you need to extend
your services to a new geographic region. An example is the Deployment stamps
pattern.
Disaster recovery. In some situations, where recovery time might not be time-sensitive,
IaC configurations can be used as part of a disaster recovery plan. For example, if
infrastructure needs to be recreated in a second region, your IaC configurations can be
used to do so. You need to consider deployment time and restoring the state of your
infrastructure and the infrastructure itself.
When you plan for disaster and recovery, ensure that your disaster recovery plans are
fully tested and that they meet your Business metrics.

Better understand your cloud resources
IaC can also help you better understand the state of your cloud resources.
Audit changes. Changes to your IaC configurations will be version-controlled in the
same way as your code, such as through Git's version history. Version control means you

can review each change that has happened, and understand who made it and when.
This approach can be helpful if you're trying to understand why a resource is configured
a specific way.
Metadata. Many types of IaC configurations let you add metadata, like code comments,
to help explain why something is done a particular way. If your organization has a
culture of documenting your code, apply the same principles to your infrastructure
code.
Keep everything together. It's common for a developer to work on features that require
both code and infrastructure changes. By keeping infrastructure defined as code, you
can group application and infrastructure code to understand the relationship between
them better. For example, if you see a change to an IaC configuration on a feature
branch or in a pull request, you'll have a clearer understanding of what that change
relates to.
Better understand Azure itself. The Azure portal is a great way to provision and
configure resources; but it often simplifies the underlying resource model used. Using
IaC means that you gain a deeper understanding of what's happening in Azure and how
to troubleshoot it if something isn't working correctly. For example, when creating a set
of virtual machines in the Azure portal, some of the underlying resource creation is
abstracted for the deployment process. When you use IaC, not only do you have explicit
control over resource creation, little is abstracted from the process, which provides a
richer understanding of what's deployed and how it's configured.

Categories of IaC tooling
You can use many declarative infrastructure deployment technologies with Azure.
Deployment technologies fall into two main categories.
Imperative IaC involves writing scripts in a language like Bash, PowerShell, C#
script files, or Python. These scripts programmatically run a series of steps to create
or modify your resources. When you use imperative deployments, it's up to you to
manage things like dependency sequencing, error control, and resource updates.
Declarative IaC involves writing a definition of how you want your environment to
look; the tooling then figures out how to make this happen by inspecting your
current state, comparing it to the target state you've requested, and applying the
differences. For more information, see Use automation to redue effort and error.
There are great Azure tooling options for both models. Here we describe three of the
commonly used declarative IaC technologies for Azure: ARM templates, Bicep, and
Terraform.

Automate deployments with ARM Templates
Azure Resource Manager (ARM) Templates provide an Azure native infrastructure as
code solution. ARM Templates are written in a language derived from JavaScript Object
Notation (JSON), and they define the infrastructure and configurations for Azure
deployments. An ARM template is declarative. You state what you want to deploy,
provide configuration values, and the Azure engine takes care of making the necessary
Azure REST API put requests. Other benefits you gain by using ARM templates for
infrastructure deployments include:
Parallel resource deployment: The Azure deployment engine sequences resource
deployments based on defined dependencies. If dependencies don't exist between
two resources, they're deployed at the same time.
Modular deployments: ARM templates can be broken up into multiple template
files for reusability and modularization.
Day one resource support: ARM templates support all Azure resources and
resource properties as they're released.
Extensibility: Azure deployments can be extended by using deployment scripts
and other automation solutions.
Validation: Azure deployments are evaluated against a validation API to catch
configuration mistakes.
Testing: The ARM template test toolkit provides a static code analysis framework
for testing ARM templates.
Change preview: ARM template what-if lets you see what will be changed before
deploying an ARM template.
Tooling: Language service extensions are available for both Visual Studio Code and
Visual Studio to help you author ARM templates.
The following example demonstrates a simple ARM template that deploys a single Azure
Storage account. In this example, a single parameter is defined to take in a name for the
storage account. Under the resources section, a storage account is defined, the
storageName parameter is used to provide a name, and the storage account details are
defined. See the included documentation for an in-depth explanation of the different
sections and configurations for ARM templates.
JSON

{
"$schema": "https://schema.management.azure.com/schemas/2019-0401/deploymentTemplate.json#",
"contentVersion": "1.0.0.0",
"parameters": {
"storageName": {

"type": "string",
"defaultValue": "newStorageAccount"
}
},
"resources": [
{
"name": "[parameters('storageName')]",
"type": "Microsoft.Storage/storageAccounts",
"apiVersion": "2019-06-01",
"location": "[resourceGroup().location]",
"kind": "StorageV2",
"sku": {
"name": "Premium_LRS",
"tier": "Premium"
}
}
]
}

Learn more
What are ARM templates?
Deploy consistent infrastructure with ARM Templates
ARM templates code samples

Automate deployments with Bicep
Bicep is a domain-specific language (DSL) that uses declarative syntax to deploy Azure
resources. Bicep provides concise syntax, reliable type safety, and support for code
reuse.
You can think of Bicep as a revision to the Azure Resource Manager template (ARM
template) language rather than a new language. The syntax is different, but the core
functionality and runtime remain the same.
The following example shows a simple Bicep file that defines a storage account.
Bicep

param location string = resourceGroup().location
param storageAccountName string =
'toylaunch${uniqueString(resourceGroup().id)}'
resource storageAccount 'Microsoft.Storage/storageAccounts@2021-06-01' = {
name: storageAccountName
location: location
sku: {
name: 'Standard_LRS'
}

kind: 'StorageV2'
properties: {
accessTier: 'Hot'
}
}

Learn more
Documentation: What is Bicep?
Documentation: Frequently asked questions
Documentation: Key Bicep concepts

Automate deployments with Terraform
Terraform is a declarative framework for deploying and configuring infrastructure that
supports many private and public clouds, Azure being one of them. It has the main
advantage of offering a cloud-agnostic framework. While Terraform configurations are
specific to each cloud, the framework itself is the same for all of them. Terraform
configurations are written in a domain-specific language (DSL) called Hashicorp
Configuration Language.
The following example demonstrates a simple Terraform configuration that deploys an
Azure resource group and a single Azure Storage account.
HashiCorp Configuration Language

resource "azurerm_resource_group" "example" {
name
= "newStorageAccount"
location = "eastus"
}
resource "azurerm_storage_account" "example" {
name
= "storageaccountname"
resource_group_name
= azurerm_resource_group.example.name
location
= azurerm_resource_group.example.location
account_tier
= "Standard"
account_replication_type = "GRS"
}

Take note, the Terraform provider for Azure is an abstraction on top of Azure APIs. This
abstraction is beneficial because the API complexities are obfuscated. This abstraction
comes at a cost; the Terraform provider for Azure doesn't always provide parity with the
Azure APIs' capabilities. To learn more about using Terraform on Azure, see Using
Terraform on Azure

Automate infrastructure deployment with
Azure Deployment Environments
For organizations that support multiple development teams working in multiple
environments, it is essential to have repeatable, consistent, and secure environments.
Development infrastructure administrators (dev infra admins) are responsible for
managing and controlling access to the resources that developers need. Developers
typically work on multiple projects, and might need several different development
environments. Developers are usually organized in development teams, led by a
development team lead.
Dev infra admin teams must be able to provide developers with environments that
include the preconfigured resources needed for different environment types, like
development, test, and production. Ideally, developers should be able to create a new
instance of an environment appropriate for their project quickly and easily, rather than
having to wait for the dev infra team.
By using project level templates, organizations can create environment templates that
define the resources and configuration required for each project. Controlling access to
each project enables the organization to set an identity security perimeter, so only
approved admins can manage the project, and only approved developers can use it.
Creating environments by using templates helps organizations to avoid the pitfalls of
manually creating each environment. Creating environments manually is timeconsuming, often results in inconsistencies, and presents difficulties in ensuring
compliance. When evaluating the use of templates, organizations should consider the
overhead of creating new templates, maintaining existing templates, and retiring unused
templates.
Azure Deployment Environments enables development teams to deploy application
infrastructure with project-based templates. Dev infra admins can deploy preconfigured
resources defined by an IaC template. Environments can be platform as a service (PaaS)
or infrastructure as a service (IaaS) environments based on the content of the template.
Dev infra admins can automate development infrastructure deployment by using Azure
Deployment Environments. It automates the process of applying policies, permissions,
and settings on environments, controlling the resource configuration that developers
can create, and centrally tracking environments across projects by:
Providing a project-based, curated set of reusable IaC templates.
Defining specific Azure deployment configurations per project and per
environment type.

Providing self-service experience without giving control over subscriptions.
Tracking costs and ensuring compliance with enterprise governance policies.
Learn more
Azure Deployment Environments

Manual deployment
Manual deployment steps introduce significant risks regarding human error and also
increase overall deployment times. However, in some cases, manual steps might be
required. For these cases, ensure that any manual steps are documented, including roles
and responsibilities.

Hotfix process
In some cases, you might have an unplanned deployment need. For instance, to deploy
critical hotfixes or security remediation patches. A defined process for unplanned
deployments can help prevent service availability and other deployment issues during
these critical events.

Next steps
Automate infrastructure configuration

Configure infrastructure
Article • 05/03/2023

When you work with Azure, many services can be created and configured
programmatically by using automation or infrastructure as code tooling. These tools
access Azure through the exposed REST APIs or what we refer to as the Azure control
plane. For example, an Azure Network Security Group can be deployed, and security
group rules created using an Azure Resource Manager template. The Network Security
Group and its configuration are exposed through the Azure control plane and natively
accessible.
Other configurations, such as installing software on a virtual machine (VM), adding data
to a database, or starting pods in an Azure Kubernetes Service cluster can't be accessed
through the Azure control plane. These actions require a different set of configuration
tools. These configurations are on the Azure data plane side, or not exposed through
Azure REST APIs. The data plane enables tools to use agents, networking, or other
access methods to provide resource-specific configuration options.
For example, when deploying a set of VMs to Azure, you might also want to install and
configure a web server, stage the content, and then make the content available on the
internet. Also, if the VM configuration changes and no longer aligns with the
configuration definition, you might want a configuration management system to
remediate the configuration. Many options are available for these data plane
configurations. This article details several options and provides links for in-depth
information.

Bootstrap automation
When deploying to Azure, you might need to run post-deployment VM configuration or
run other arbitrary code to bootstrap the deployed Azure resources. Several options are
available for these bootstrapping tasks and are detailed in the following sections of this
article.

Azure VM extensions
Azure VM extensions are small packages that run post-deployment configuration and
automation on Azure VMs. Several extensions are available for many different
configuration tasks, such as running scripts, configuring antimalware solutions, and
configuring logging solutions. These extensions can be installed and run on VMs by
using an ARM template, the Azure CLI, Azure PowerShell module, or the Azure portal.

Each Azure VM has a VM Agent installed, and this agent manages the lifecycle of the
extension.
A typical VM extension use case is to use a custom script extension to install software,
run commands, and perform configurations on a virtual machine or Azure Virtual
Machine Scale Sets. The custom script extension uses the Azure VM Agent to download
and run a script. The custom script extensions can be configured to run as part of
infrastructure as code deployments such that the VM is created, and then the script
extension is run on the VM. Extensions can also be run outside of an Azure deployment
by using the Azure CLI, PowerShell module, or the Azure portal.
In the following example, the Azure CLI is used to deploy a custom script extension to
an existing virtual machine, which installs a Nginx webserver.

az vm extension set \
--resource-group myResourceGroup \
--vm-name myVM --name customScript \
--publisher Microsoft.Azure.Extensions \
--settings '{"commandToExecute": "apt-get install -y nginx"}'

Learn more
Use the included code sample to deploy a virtual machine and configure a web server
on that machine with the custom script extension. For more information, see:
Azure virtual machine extensions and features
Azure Well Architected Framework sample (custom script extension)

cloud-init
cloud-init is a known industry tool for configuring Linux virtual machines on first boot.
Much like the Azure custom script extension, cloud-init lets you install packages and run
commands on Linux virtual machines. cloud-init can be used for things like software
installation, system configurations, and content staging. Azure includes many cloud-init
enabled Marketplace virtual machine images across many of the most well-known Linux
distributions. For a full list, see cloud-init support for virtual machines in Azure.
1. To use cloud-init, create a text file named cloud-init.txt and enter your cloud-init
configuration. In this example, the Nginx package is added to the cloud-init
configuration.
YAML

#cloud-config
package_upgrade: true
packages:
- nginx

2. Create a resource group for the VM.
Azure CLI

az group create --name myResourceGroupAutomate --location eastus

3. Create the VM, specifying the --custom-data property with the cloud-init
configuration name.
Azure CLI

az vm create \
--resource-group myResourceGroupAutomate \
--name myAutomatedVM \
--image UbuntuLTS \
--admin-username azureuser \
--generate-ssh-keys \
--custom-data cloud-init.txt

On boot, cloud-init uses the system's native package management tool to install Nginx.
Learn more
cloud-init support for virtual machines in Azure

Azure deployment script resource
When you perform Azure deployments, you might need to run arbitrary code for
bootstrapping things like managing user accounts, Kubernetes pods, or querying data
from a non-Azure system. Because none of these operations are accessible through the
Azure control plane, some other mechanism is required for performing this automation.
To run arbitrary code with an Azure deployment, see the
Microsoft.Resources/deploymentScripts Azure resource.

The deployment script resource behaves like any other Azure resource in the following
ways:
Can be used in an ARM template.
Contains ARM template dependencies on other resources.
Consumes input, produces output.

Uses a user-assigned managed identity for authentication.
When deployed, the deployment script runs PowerShell or Azure CLI commands and
scripts. Script execution and logging can be observed in the Azure portal or with the
Azure CLI and PowerShell module. Many options can be configured like environment
variables for the execution environment, timeout options, and what to do with the
resource after a script failure.
The following example shows an ARM template snippet with the deployment script
resource configured to run a PowerShell script.
JSON

{
"type": "Microsoft.Resources/deploymentScripts",
"apiVersion": "2019-10-01-preview",
"name": "runPowerShellScript",
"location": "[resourceGroup().location]",
"kind": "AzurePowerShell",
"identity": {
"type": "UserAssigned",
"userAssignedIdentities": {"[parameters('identity')]": {}}
},
"properties": {
"forceUpdateTag": "1",
"azPowerShellVersion": "3.0",
"arguments": "[concat('-sqlServer ', parameters('sqlServer'))]",
"primaryScriptUri": "[variables('script')]",
"timeout": "PT30M",
"cleanupPreference": "OnSuccess",
"retentionInterval": "P1D"
}
}

Learn more
Documentation: Use deployment scripts in templates

Configuration Management
Configuration management tools can be used to configure and manage the ongoing
configuration and state of Azure virtual machines. Three popular options are Azure
Automation State Configuration, Chef, and Puppet.

Azure Automation State Configuration

Azure Automation State Configuration is a configuration management solution built on
top of PowerShell Desired State Configuration (DSC). State configuration works with
Azure virtual machines, on-premises machines, and machines in a cloud other than
Azure. When you use state configuration, you can import PowerShell DSC resources and
assign them to many virtual machines from a central location. Once each endpoint has
evaluated and applied the desired state, state compliance is reported to Azure and can
be seen on a built-in dashboard.
The following example uses PowerShell DSC to ensure the NGINX has been installed on
Linux systems.
PowerShell

configuration linuxpackage {
Import-DSCResource -Module nx
Node "localhost" {
nxPackage nginx {
Name = "nginx"
Ensure = "Present"
}
}
}

Once the module imports into Azure Automation State Configuration and is assigned to
nodes, the state configuration dashboard provides compliance results.

Learn more

Use the included code sample to deploy Azure Automation State Configuration and
several Azure virtual machines. The virtual machines are also onboarded to state
configuration, and a configuration applied.
Get started with Azure Automation State Configuration
Azure Automation State Configuration example scenario

Chef
Chef is an automation platform that helps define how your infrastructure is configured,
deployed, and managed. Other components included in Chef Habitat are for application
lifecycle automation, rather than the infrastructure and Chef InSpec that helps automate
compliance with security and policy requirements. Chef Clients are installed on target
machines, with one or more central Chef Servers that store and manage the
configurations.
Learn more
Documentation: An Overview of Chef

Puppet
Puppet is an enterprise-ready automation platform that handles the application delivery
and deployment process. Agents are installed on target machines to let Puppet run
manifests that define the desired configuration of the Azure infrastructure and virtual
machines. Puppet can integrate with other solutions such as Jenkins and GitHub for an
improved DevOps workflow.
Learn more
Documentation: How Puppet works

Next steps
Automate operational tasks

Automate operational tasks
Article • 05/01/2023

Operational tasks can include any action or activity you may perform while managing
systems, system access, and processes. Some examples include rebooting servers,
creating accounts, and shipping logs to a data store. These tasks may occur on a
schedule, as a response to an event or monitoring alert, or ad-hoc based on external
factors. Like many other activities related to managing computer systems, these
activities are often performed manually, which takes time, and are error-prone.
Many of these operational tasks can and should be automated. Using scripting
technologies and related solutions, you can shift effort from manually performing
operational tasks towards building automation for these tasks. In doing so, you achieve
so much, including:
Reduce time to perform an action.
Reduce risk in performing the action.
Automated response to events and alerts.
Increased human capacity for further innovation.
When working in Azure, you have many options for automating operational tasks. This
document details some of the more popular.

Azure Functions
Azure Functions allows you to run code without managing the underlying infrastructure
on where the code is run. Functions provide a cost-effective, scalable, and event-driven
platform for building applications and running operational tasks. Functions support
running code written in a range of programming languages including C#, Java,
JavaScript, Python, and PowerShell.
When creating a Function, a hosting plan is selected. Hosting plans controls how a
function app is scaled, resource availability, and availability of advanced features such as
virtual network connectivity and startup time. The hosting plan also influences the cost.
Functions hosting plans:
Consumption: Default hosting plan, pay only for Function execution time,
configurable timeout period, automatic scale.
Premium: Faster start, VNet connectivity, unlimited execution duration, premium
instance sizes, more predictable pricing.

Dedicated: Functions run on dedicated virtual machines and can use custom
images.
For full details on consumption plans, see Azure Functions scale and hosting.
Functions provide event-driven automation; each function has a trigger associated with
it. These triggers are what run the functions. Common triggers include:
HTTP / Webhook: Function is run when an HTTP request is received.
Queue: Function is run when a new message arrives in a message queue.
Blob storage: Function is run when a new blob is created in a storage container.
Timer: Function is run on a schedule.
Below are example triggers seen in the Azure portal when creating a new function

Once an event has occurred that initiates a Function, you may also want to consume
data from this event or from another source. Once a Function has been completed, you
may want to publish or push data to an Azure service such as a Blob Storage. Input and
output are achieved using input and output bindings. For more information about
triggers and bindings, see Azure Functions triggers and binding concepts.
Both PowerShell and Python are common languages for automating everyday
operational tasks. Because Azure Functions supports both of these languages, it is an

excellent platform for hosting, running, and auditing automated operational tasks. For
example, let's assume that you would like to build a solution to facilitate self-service
account creation. An Azure PowerShell Function could be used to create the account in
Azure Active Directory. An HTTP trigger can be used to initiate the Function, and an
input binding configured to pull the account details from the HTTP request body. The
only remaining piece would be a solution that consumes the account details and creates
the HTTP requests against the Function.
Learn more
Documentation: Azure Functions PowerShell developer guide
Documentation: Azure Functions Python developer guide

Azure Deployment Environments
Azure Deployment Environments enables development teams to quickly spin up
consistent app infrastructure by using project-based templates, minimizing setup time
while maximizing security, compliance, and cost efficiency. A deployment environment is
a collection of Azure resources deployed in predefined subscriptions. Development
infrastructure (dev infra) admins can enforce enterprise security policies and provide a
curated set of predefined infrastructure as code (IaC) templates.
Dev infra admins define deployment environments as catalog items hosted in a GitHub
or Azure DevOps repository called a catalog. A catalog item consists of an IaC template
and a manifest.yaml file.
The creation of deployment environments can be scripted, and the environments can be
managed programmatically.
Learn more
Azure Deployment Environments
Scenarios for using Azure Deployment Environments Preview
Create and access an environment by using the Azure CLI

Azure Automation
PowerShell and Python are popular programming languages for automating operational
tasks. Using these languages, performing operations like restarting services, moving logs
between data stores, and scaling infrastructure to meet demand can be expressed in
code and executed on demand. Alone, these languages do not offer a platform for
centralized management, version control, or execution history. The languages also lack a

native mechanism for responding to events like monitoring driven alerts. To provide
these capabilities, an automation platform is needed.
Azure Automation provides an Azure-hosted platform for hosting and running
PowerShell and Python code across Azure, non-Azure cloud, and on-premises
environments. PowerShell and Python code is stored in an Azure Automation Runbook,
which has the following attributes:
Execute Runbooks on demand, on a schedule, or through a webhook.
Execution history and logging.
Integrated secrets store.
Source Control integration.
As seen in the following image, Azure Automation provides a portal experience for
managing Azure Automation Runbooks. Use the included code sample (ARM template)
to deploy an Azure Automation account, automation runbook, and explore Azure
Automation for yourself.

Learn more
Documentation: What is Azure Automation?

Scale operations
So far, this document has detailed options for scripting operational tasks; however,
many Azure services come with built-in automation, particularly in scale operations. As
application demand increases (transactions, memory consumption, and compute

availability), you may need to scale the application hosting platform so that requests
continue to be served. As demand decreases, scaling back not only appropriately resizes
your application but also reduces operational cost.
In cloud computing, scale activities are classified into two buckets:
Scale-up: Adding extra resources to an existing system to meet demand.
Scale-out: Adding more infrastructure to meet demand.
Many Azure services can be scaled up by changing the pricing tier of that service.
Generally, this operation would need to be performed manually or using detection logic
and custom automation.
Some Azure services support automatic scale-out, which is the focus of this section.

Azure Monitor autoscale
Azure Monitor autoscale can be used to autoscale Virtual Machine Scale Sets, Cloud
Services, App Service Web Apps, and API Management service. To configure scale-out
operations for these services, while in the Azure portal, select the service, and then
scale-out under the resource settings. Select Custom To configure autoscaling rules.
Automatic scale operations can also be configured using an Azure Resource Manager
Template, the Azure PowerShell module, and the Azure CLI.

When creating the autoscale rules, configure minimum and maximum instance counts.
These settings prevent inadvertent costly scale operations. Next, configure autoscale
rules, at minimum one to add more instances, and one to remove instances when no
longer needed. Azure Monitor autoscale rules give you fine-grain control over when a
scale operation is initiated. See the Learn more section below for more information on
configuring these rules.

Learn more
Documentation: Azure Monitor autoscale overview

Azure Kubernetes Service
Azure Kubernetes Service (AKS) offers an Azure managed Kubernetes cluster. When
considering scale operations in Kubernetes, there are two components:
Pod scaling: Increase or decrease the number of load balanced pods to meet
application demand.
Node scaling: Increase or decrease the number of cluster nodes to meet cluster
demand.
Azure Kubernetes Service includes automation to facilitate both of these scale operation
types.

Horizontal pod autoscaler
Horizontal pod autoscaler (HPA) monitors resource demand and automatically scales
pod replicas. When configuring horizontal pod autoscaling, you provide the minimum
and maximum pod replicas that a cluster can run and the metrics and thresholds that
initiate the scale operation. To use horizontal pod autoscaling, each pod must be
configured with resource requests and limits, and a HorizontalPodAutoscaler
Kubernetes object must be created.
The following Kubernetes manifest demonstrates resource requests on a Kubernetes
pod and also the definition of a horizontal pod autoscaler object.
YAML

apiVersion: apps/v1
kind: Deployment
metadata:
name: azure-vote-back
spec:
replicas: 1
selector:
matchLabels:
app: azure-vote-back
template:
metadata:
labels:
app: azure-vote-back
spec:
nodeSelector:
"beta.kubernetes.io/os": linux
containers:
- name: azure-vote-back
image: redis
# Resource requests and limits
resources:
requests:
cpu: 100m
memory: 128Mi
limits:
cpu: 250m
memory: 256Mi
ports:
- containerPort: 6379
name: redis
--apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
name: azure-vote-back-hpa
spec:
maxReplicas: 10 # define max replica count
minReplicas: 3 # define min replica count
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: azure-vote-back
targetCPUUtilizationPercentage: 50 # target CPU utilization

Cluster autoscaler
Where horizontal pod autoscaling is a response to demand on a specific application of
service running in a Kubernetes cluster, cluster autoscaling responds to demand on the
entire Kubernetes cluster itself. If a Kubernetes cluster does not have enough compute
resources or nodes to facilitate all requested pods' resource requests, some of these

pods will enter a non-scheduled or pending state. In response to this situation, more
nodes can be automatically added to the cluster. Conversely, once compute resources
have been freed up, the cluster nodes can automatically be removed to match steadystate demand.
Cluster autoscaler can be configured when creating an AKS cluster. The following
example demonstrates this operation with the Azure CLI. This operation can also be
completed with an Azure Resource Manager template.
Azure CLI

az aks create \
--resource-group myResourceGroup \
--name myAKSCluster \
--node-count 1 \
--vm-set-type VirtualMachineScaleSets \
--load-balancer-sku standard \
--enable-cluster-autoscaler \
--min-count 1 \
--max-count 3

Cluster autoscaler can also be configured on an existing cluster using the following
Azure CLI command.
Azure CLI

az aks update \
--resource-group myResourceGroup \
--name myAKSCluster \
--enable-cluster-autoscaler \
--min-count 1 \
--max-count 3

See the included documentation for information on fine-grain control for cluster
autoscale operations.
Learn more
Documentation: Horizontal pod autoscaling
Documentation: AKS cluster autoscaler

Release engineering: Application
development
Article • 05/18/2023

One of the primary goals of adopting modern release management strategies is to build
systems that let your teams turn ideas into production-delivered software with as little
friction as possible. Throughout this section of the Well-Architected Framework,
methods and tools for quickly and reliably delivering software are examined. You'll learn
about continuous deployment, integration strategies, deployment environments, and
more. Samples are provided to help you quickly get hands-on with this technology.
However, release engineering doesn't start with fancy deployment software, multiple
deployment environments, or Kubernetes clusters. Before examining how we can quickly
and reliably release software, we need to first look at how software is developed.
Not only has the introduction of cloud computing affected how software is delivered
and run, it has also had a huge downstream effect on how software is developed. For
example, the introduction of container technology has changed how we can host, scale,
and deprecate software. That said, containers have also impacted dependency
management, host environment, and tooling as we develop software.
This article details many practices to consider when building strategies for developing
for the cloud, including:
Development environments, or where you write your code.
Source control and branching strategies, or how you manage, collaborate on, and
eventually deploy your code.

Development environments
When you develop software for the cloud or any other environment, care needs to be
taken to ensure that the development environment is set up for success. When setting
up a development environment, consider the following questions:
How do I ensure that all dependencies are in place?
How can I best configure my development environment to emulate a production
environment?
How do I develop code where service dependencies might exist with code already
in production?

Based on the chosen deployment method and target application, the answers to these
questions, and the tools used for what's often referred to the "inner-loop" development
process, can differ.
Package management solutions, for example, can help you manage dependencies in
your development environment. When you use containers, you can use a solution like
Docker Desktop for your development environment instead. When it comes to
effectively emulating a production environment, you can choose to set up a range of
dummy services locally.
Some Linux tools are a handy solution, and they're available on Windows machines too.
You can use them by configuring the Windows Subsystem for Linux.
Other tools let you provide developers with a preconfigured environment that they can
spin up whenever they need it, providing a consistent environment. Azure Deployment
Environments helps you to emulate environments, such as sandbox, testing, staging, or
production.
Finally, tools like Bridge for Kubernetes let you debug code in your development
environment while being connected to a Kubernetes cluster. This configuration can be
helpful when working on containerized microservices. You can work on one service
locally while services that you take a dependency on are spun up remotely.

Examples of inner-loop development tools
A development team might choose some of the following pieces of software when
developing a containerized application with a Kubernetes cluster as its deployment
target.
Azure Artifacts

is a package management solution that lets you host private

packages and provide upstream connectivity to public package repositories for
various package management systems such as NuGet .
Docker Desktop

is an application that provides a Docker environment on your

development system. Docker Desktop includes not only the Docker runtime but
also application development tools and local Kubernetes environments.
Windows Subsystem for Linux provides a Linux environment on your Windows
machines, including many command-line tools, utilities, and Linux applications.
Bridge to Kubernetes lets you run and debug code on your development system
while connected to a Kubernetes cluster. This configuration is helpful when
working on microservice type architectures.
Podman

is an open-source tool for working with containers.

Microsoft Dev Box provides a safe development environment for developers to
create code, supported through virtual networks and RBAC permissions that
control who can access the organizations network and who can use the dev box.
Azure deployment environments empowers development teams to quickly and
easily spin up app infrastructure with project-based templates that establish
consistency and best practices while maximizing security.

Source control
Source control management (SCM) systems provide a way to control, collaborate, and
peer review software changes. As software is merged into source control, the system
helps manage code conflicts. Ultimately, source control provides a running history of the
software, modifications, and contributors. Whether a piece of software is open-sourced
or private, the use of source control software has become a standardized method of
managing software development.
As cloud practices are adopted, and because so much of the cloud infrastructure is
managed through code, version control systems are an integral part of infrastructure
management.
Many source control systems are powered by Git. Git is a distributed version control
system with related tools that lets you and your team track source code changes during
the software development lifecycle. When you use Git, you can create a copy of the
software, make changes, propose the changes, and receive peer review on your
proposal. Modern source control software simplifies this peer review process and helps
you see the exact changes that a contributor wants to make.
Once the proposed changes have been approved, Git helps merge the changes into the
source, including conflict resolution. If at any point the changes need to be reverted, Git
can also manage rollback.

Version control and code changes
Beyond providing a place to store code, source control systems let you understand what
version of the software is current and to identify changes between the present and past
versions. Version control solutions also provide a method for reverting to the previous
version when needed.
The following image demonstrates how Git and GitHub are used to see the proposed
code changes.



Branches, forks and pull requests
When you use source control systems, you can create branches in an existing repo and
your own copies of the software. A copy of an existing repo is called a "fork".
While branches are usually short lived and will eventually be merged back into a repo
via a pull request, forks usually remain for more time. While they can also be merged
back via a pull request, some forks develop independently from their origin repo.
You can review our documentation on Forks and Branches to find out more about the
differences and similarities between them.

Peer review
As updates are made to software and infrastructure configurations, version control
software lets us propose these changes before merging them into the source. During
the proposal, peers can review the changes, recommend updates, and approve the
changes. Source control solutions provide an excellent platform for collaboration on
changes to the software.
To learn more about Git, see What is Git?

GitHub
GitHub

is a popular source control system that uses Git. In addition to core Git

functionality, GitHub includes features such as access control, collaboration features
such as code issues, project boards, wikis, and an automation platform called GitHub
actions.

Azure Repos

Azure DevOps is a collection of services for building, collaborating on, testing, and
delivering software to any environment. Azure DevOps Services includes Azure Repos,
which is its source control system. When you use Azure Repos, you also receive
unlimited free private Git repositories. Standard Git powers Azure Repos, and you can
use clients and tools of your choice for working with them.

Next steps
Release Engineering: Continuous integration

Release engineering: Continuous
integration
Article • 05/18/2023

As code is developed, updated, or even removed, having a friction-free and safe method
to integrate these changes into the main code branch is paramount to enabling
developers to provide value fast. As a developer, you can make small code changes,
push these changes to a code repository, and get almost instantaneous feedback on the
quality, test coverage, and introduced bugs. This process lets you work faster and with
more confidence and less risk. Continuous integration (CI) is a practice where source
control systems and software deployment pipelines are integrated to provide
automated build, test, and feedback mechanisms for software development teams.
Continuous integration is about ensuring that software is ready for deployment but
doesn't include the deployment itself. This article covers the basics of continuous
integration and offers links and examples for more in-depth content.

Continuous integration
Continuous integration is a software development practice with which developers
integrate software updates into a source control system on a regular cadence. The
continuous integration process starts when an engineer creates a pull request signaling
to the CI system that code changes are ready to be integrated. Ideally, integration
validates the code against several baselines and tests and provides quick feedback to
the requesting engineer on the status of these tests. Assuming baseline checks and
testing have gone well, the integration process produces and stages assets such as
compiled code and container images that will deploy the updated software.
As a software engineer, continuous integration can help you deliver quality software
more quickly by performing the following actions:
Run automated tests against the code, providing early detection of breaking
changes.
Run code analysis to ensure code standards, quality, and configuration.
Run compliance and security checks ensuring no known vulnerabilities.
Run acceptance or functional tested to ensure that the software operates as
expected.
To provide quick feedback on detected issues.
Where applicable, produce deployable assets or packages that include the
updated code.

To achieve continuous integration, use software solutions to manage, integrate, and
automate the process. A common practice is to use a continuous integration pipeline,
detailed in this article's next section.

Continuous integration pipelines
A continuous integration pipeline involves a piece of software, in many cases cloudhosted, that provides:
A platform for running automated tests
Compliance scans
Reporting
All other components that make up the continuous integration process
In most cases, the pipeline software is attached to source control such that when pull
requests are created or software is merged into a specific branch, the continuous
integration pipeline is run. Source control integration also provides the opportunity for
providing CI feedback directly on pull requests.
Many solutions provide continuous integration pipeline capabilities.
Learn more
To learn how to create a continuous integration pipeline with either GitHub or Azure
DevOps, see:
Create your first pipeline
Using starter workflows

Source control integration
The integration of your continuous integration pipeline with your source control system
is key to enabling fast, self-service, and friction-free code contributions.
As pull requests are created, the CI pipeline is run, including all tests, security
assessments, and other checks.
CI test results are provided to the pull request initiator directly in the pull request,
allowing for almost real-time feedback on quality.
Another popular practice is building small reports or badges that can be presented
in source control to make visible the current builds states
The following image shows the integration between GitHub and an Azure DevOps
pipeline. In this example, a pull request has been created, which in turn has triggered an

Azure DevOps pipeline. The pipeline status can be seen directly in the pull request.

Test integration
A key element of continuous integration is the continual building and testing of code as
code contributions are made. Testing pull requests as they're created gives quick
feedback that the commit hasn't introduced breaking changes. The advantage is that
the tests that are run by the continuous integration pipeline can be the same tests run
during test-driven development.
The following code snippet shows a test step from an Azure DevOps pipeline. There are
two actions occurring:
The first task uses a popular Python testing framework to run CI tests. These tests
reside in source control alongside the Python code. The test results are output to a
file named test-results.xml.
The second task consumes the test results and publishing them to the Azure
DevOps pipeline as an integrated report.
YAML

- script: |
pip3 install pytest
pytest azure-vote/azure-vote/tests/ --junitxml=junit/test-results.xml
continueOnError: true
- task: PublishTestResults@2
displayName: 'Publish Test Results'
inputs:
testResultsFormat: 'JUnit'
testResultsFiles: '**/test-results.xml'
failTaskOnFailedTests: true
testRunTitle: 'Python $(python.version)'

The following image shows the test results as seen in the Azure DevOps portal:

Failed tests
Failed tests should temporarily block a deployment and lead to a deeper analysis of
what has happened. Failed tests should also lead to either a refinement of the test or an
improvement of the change that caused the test to fail.

CI result badges
Many developers show that their code quality is high by displaying a status badge in
their repo. The following image shows an Azure Pipelines badge as displayed on the
Readme file for an open-source project on GitHub:

Learn more
To learn how to display badges in your repositories, see:
Add an Azure Pipeline status badge to your repository.
Add a GitHub workflow status badge to your repository .

Next steps
Release Engineering: Testing

Testing your application and Azure
environment
Article • 05/30/2023

Testing is a fundamental component in DevOps and agile development in general. If
automation gives DevOps the required speed and agility to deploy software quickly,
then only the extensive testing of those deployments achieves the required reliability
that customers demand.
A main tenet of system reliability is the "shift-left" principle. If developing and deploying
an application is a process depicted as a series of steps going from left to right, testing
shouldn't be limited to the very end of the process. It should be shifted as much to the
beginning, meaning to the left, as possible. Errors are cheaper to repair when caught
early. They can be expensive or impossible to fix later in the application life cycle.
Testing should occur on all code, including application code, infrastructure templates,
and configuration scripts. As described in Repeatable infrastructure, the environment
where applications are running should be version-controlled and deployed through the
same mechanisms as application code. The environment can then be tested and
validated by using the same testing paradigms that teams already use for application
code.
A range of testing tools are available to automate and streamline different kinds of
testing. These tools include Azure Load Testing, Azure Pipelines for automated testing
and Azure Test Plans for manual testing.
There are multiple stages at which tests can be performed in the life cycle of code, and
each of them has particularities that are important to understand. In this guide, you can
find a summary of the different tests that you should consider while developing and
deploying applications.

Automated testing
Automating tests is the best way to make sure that they're used consistently. Depending
on how frequently tests are performed, they're typically limited in duration and scope, as
the following different types of automated tests show.

Unit testing

Unit tests are tests typically run as part of the continuous integration routine. Unit Tests
should be extensive and quick, ideally covering 100% of the code, and running in under
30 seconds.
Unit testing can verify that the syntax and functionality of individual modules of code
are working the way they should, for example, producing a defined output for a known
input. Unit tests can also be used to verify that infrastructure as code assets are valid.
Unit tests should be applied to all code assets, including templates and scripts.

Smoke testing
Smoke tests verify that a workload can be stood up in a test environment and performs
as expected. Smoke tests don't go to the extent of integration tests as smoke tests don't
verify the interoperability of different components.
Instead, they verify that the deployment methodology for both the infrastructure and
the application works and that the system responds as intended once the process is
complete.

Integration testing
After making sure that the different application components operate individually,
integration testing determines whether they can interact with each other as they should.
Running a large integration test suite can take a considerable amount of time, which is
why tests should be performed as early as possible (the shift-left principle) in the
software development lifecycle. Integration tests should be reserved to scenarios that
can't be tested with a smoke or unit test.
Long running test processes can be run on a regular interval if needed. A regular interval
offers a good compromise, detecting interoperability issues between application
components no later than one day after they were introduced.

Manual testing
Manual testing is more expensive than automated testing, so it's usually run less
frequently. Manual testing is fundamental for the correct functioning of the DevOps
feedback loop. It corrects errors before they become too expensive to repair or cause
customer dissatisfaction.

Acceptance testing
Depending on the context, acceptance testing is sometimes performed manually. It can
be partially or fully automated. Its purpose is to determine whether the software system
has met the requirement specifications.
The main purpose of this test is to evaluate the system's compliance with the business
requirements and verify if it has met the required criteria for delivery to end users.
Tools that include Usage analysis with Application Insights can help you work out how
people use your application. These tools let you decide whether a new feature has
improved your applications without bringing unintended adverse effects.
The next section of this article covers methods for testing in production, which can be
used to verify if features have met business targets with real world user behavior.

Testing and experimentation in production
Depending on your chosen deployment strategy, Azure platform features might help
you conduct experiments in production.
Azure AppService, for example, comes with the slot functionality to Set up staging
environments that let you have two different versions of the same application running at
the same time. Slots let you use A/B testing by directing part of the user traffic to one
version of the application and the rest of the traffic to another.
There are multiple popular approaches to experimentation in production:
Blue/Green deployments: When deploying a new application version, you can
deploy it in parallel to the existing one. Deploying in parallel lets you start
redirecting clients to the new version. If everything goes well, you can then
decommission the old version. If there's any problem with the new deployment,
you can redirect the users back to the deployment with the previous version.
Canary releases: You can expose the new functionality of your application in a
staggered way to select groups of users. You can extend the functionality to a
larger group of users if:
Users are satisfied with the new functionality.
The functionality performs as expected with the control group.
A/B testing: A/B testing is similar to canary release testing. While canary release
testing focuses on mitigating risk, A/B testing focuses on evaluating two versions
of an application or feature side by side. For example, if you have two versions of

the layout of your application, you could send half of your users to one, half to the
other, and use metrics to see which layout is more successful.

Stress testing
As other sections of this framework have explained, designing your application code
and infrastructure for scalability is of paramount importance. Stress tests help you verify
regularly that your application and your environment adapt to changing load
conditions.
During these stress tests, it's critical to monitor all the components of the system to
identify potential bottlenecks and to establish a clear baseline to test against.
Every component of the system that isn't able to scale out can become a scale
limitation, such as active or passive network components or databases. It's important to
know the limits of components so that you can mitigate their effect when it comes to
application scale.
Another important part of stress tests is to verify that the infrastructure scales back
down to its normal condition in order to keep costs under control.

Business continuity testing
Business continuity testing offers opportunities for improving prevention and recovery
methods for large-scale incidents. It also increases resilience to mundane incidents such
as isolated hardware failure or data corruption.

Disaster recovery drills
Disaster recovery drills are important for verifying that the existing backup strategy is
complimented by a working recovery procedure. These drills should be conducted
regularly.
Tools such as Azure Site Recovery make it possible to start an isolated copy of the
primary workload location in a secondary environment, so that it can be verified that the
workload has recovered as it should in an emergency.
In case a problem occurs during the drill, the disaster recovery procedure can be
optimized, and the infrastructure in the secondary environment can be deleted safely.

Exploratory testing

During exploratory testing, experts explore the application in its entirety trying to find
faults or suboptimal implementations of functionality. These experts could be
developers, UX specialists, product owners, actual users, and other profiles.
Structured test plans are typically not used since testing is left to the ability of the
individual tester.

Fault injection
Fault injection tests if the application is resilient to infrastructure failures. The technique
introduces faults in the underlying infrastructure and observes how the application
behaves.
Fault injection testing is fundamental to build trust in your redundancy mechanisms.
Shutting down infrastructure components, purposely degrading performance, or
introducing faults are ways of verifying that the application is going to react as expected
when these situations occur.
Products like Azure Chaos Studio aim to simplify the process of fault injection testing.
Many larger organizations have built their own chaos engines that use automated
testing tools to achieve fault injection.

Summary
In order to deploy software quickly and reliably, testing is a fundamental component of
the development and deployment life cycle. It's important to make sure that the
application is going to perform as expected in every situation. We therefore need to not
only test application code but all aspects of our workload.

Next steps
Release Engineering: Performance

Performance considerations for your
deployment infrastructure
Article • 05/30/2023

Build status shows if your product is in a deployable state, so builds are the heartbeat of
your continuous delivery system. It's important to have a build process up and running
the first day of your product development. Since builds provide such crucial information
about the status of your product, you should always strive for fast builds.
It's difficult to fix a build problem if it takes longer to build. When delays happen and
become normalized, teams tend to become less motivated to fix the problem.

Build times
Here are few ways you can achieve faster builds:
Choosing agents that meet your performance requirements: Speeding up your
builds starts with selecting the right build machines. Fast machines can make the
difference between hours and minutes. If your pipelines are in Azure Pipelines,
then you've got a convenient option to run your jobs by using a Microsoft-hosted
agent. With Microsoft-hosted agents, maintenance and upgrades are taken care of
for you. For more information, see Microsoft-hosted agents.
Build server location: When you're building your code, data is sent across the wire.
Inputs to the builds are fetched from a source control repository and the artifact
repository. At the end, the output from the build process needs to be copied,
including not only the compiled artifacts, but also the test reports, the code
coverage results, and the debug symbols. It's important that these copy actions are
fast. If you use your own build server, ensure that the build server is located near
the sources and a target location. Fast uploads and downloads can reduce the
overall build time.
Scaling out build servers: A single build server might be sufficient for a small
product. As the size and the scope of the product and the number of teams
working on the product increases, a single server might not be enough. Scale your
infrastructure horizontally over multiple machines when you reach the limit. For
more information, see Create and manage agent pools.
Optimizing the build:

Add parallel jobs to speed up the build process. For more information, see
Configure and pay for parallel jobs.
Enable parallel test suite runs, which often save a large amount of time,
especially when running integration and UI tests. For more information, see Run
tests in parallel for any test runner.
Use the notion of a multiplier, where you can scale out your builds over multiple
build agents. For more information, see Specify jobs in your pipeline.
Consider moving integration, UI, and smoke tests to a release pipeline. Moving
to a release pipeline improves the build speed and the speed of the build
feedback loop.
Publish the build artifacts to a package management solution, such as NuGet or
Maven. Publishing to a package management solution lets you reuse your build
artifact more easily.

Human intervention
Your organization might choose to create several different kinds of builds to optimize
build times.
CI builds: The purpose of this build is to ensure code compiles and unit tests are
run. This build gets triggered at each commit. It serves as the heartbeat of the
project and provides quality feedback to the team immediately. For more
information, see Specify events that trigger pipelines.
Nightly build: The purpose of a nightly build isn't only to compile the code, but
also to ensure any larger test suites that are inefficient run for each build on a
regular cadence. Usually, these cadences are integration, UI, or smoke tests. For
more information, see Configure schedules for pipelines.
Release build: In addition to compiling and running tests, this build also compiles
the API documentation, compliance reports, code signing, and other steps that
aren't required every time the code is built. This build provides the golden copy
that's pushed to the release pipeline to finally deploy in the production
environment.
The types of builds needed by your organization depend on factors including your
team's and organization's maturity, the kind of product you're working on, and your
deployment strategy.

Next steps
Release Engineering: Deployment

Release engineering: Deployment
Article • 05/31/2023

As you provision and update Azure resources, application code, and configuration
settings, a repeatable and predictable process will help you avoid errors and downtime.
We recommend automated processes for deployment that you can run on demand and
rerun if something fails. After your deployment processes are running smoothly, process
documentation can keep them that way.

Automation
To activate resources on demand, deploy solutions rapidly, minimize human error, and
produce consistent and repeatable results, be sure to automate deployments and
updates.

Automate as many processes as possible
The most reliable deployment processes are automated and idempotent, as in,
repeatable to produce the same results.
To automate your infrastructure, you can use Azure Bicep.
To configure virtual machines (VMs), you can use cloud-init (for Linux VMs) or
Azure Automation State Configuration (DSC).
To automate application deployment, you can use Azure DevOps Services, GitHub,
Jenkins, or other CI/CD solutions.
As a best practice, create a repository of categorized automation scripts for quick access,
documented with explanations of parameters and examples of script use. Keep this
documentation in sync with your Azure deployments, and designate a primary person to
manage the repository.
Automation scripts can also activate resources on demand for disaster recovery. But
scripts can sometimes be a place where you accidentally expose secrets. To protect
against exposing secrets in your scripts, use secret variables and Azure Key vault. For
more information, see Set secret variables and Azure Key Vault.

Automate and test deployment and maintenance tasks
Distributed applications consist of multiple parts that must work together. Deployment
should take advantage of proven mechanisms, such as scripts, that can update and

validate configuration and automate the deployment process. Test all processes fully to
ensure that errors don't cause extra downtime.

Implement deployment security measures
All deployment tools must incorporate security restrictions to protect the deployed
application. Define and enforce deployment policies carefully, and minimize the need for
human intervention.

Release process
One of the challenges with automating deployment is the cut-over itself, taking software
from the final stage of testing to live production. You usually need to do this process
quickly in order to minimize downtime. The blue-green deployment approach minimizes
downtime by ensuring you have two production environments that are as identical as
possible.

Document release process
Without detailed release process documentation, an operator might deploy a bad
update or might improperly configure settings for your application. Clearly define and
document your release process, and ensure that it's available to the entire operations
team.

Stage your workloads
When you deploy to various stages and run tests and validations at each stage before
moving on to the next, you ensure friction-free production deployment.
With good use of staging and production environments, you can push updates to the
production environment in a highly controlled way and minimize disruption from
unanticipated deployment issues.
Blue-green deployment

involves deploying an update into a production

environment that's separate from the live application. After you validate the
deployment, switch the traffic routing to the updated version. One way to switch
traffic routing is to use the staging slots available in Azure App Service to stage a
deployment before moving it to production.
Canary releases

are similar to blue-green deployments. Instead of switching all

traffic to the updated application, you route only a small portion of the traffic to

the new deployment. If there's a problem, revert to the old deployment. If not,
gradually route more traffic to the new version. In Azure App Service, you can use
the testing in production feature to manage a canary release.

Test environments
If development and test environments don't match the production environment, it's
hard to test and diagnose problems. So, keep development and test environments as
close to the production environment as possible. Make sure that test data is consistent
with the data used in production, even if it's sample data and not real production data
for privacy or compliance reasons. Plan to generate and anonymize sample test data.

Log and audit
To capture as much version-specific information as possible, implement a robust
logging strategy. If you use staged deployment techniques, more than one version of
your application is running in production. If a problem occurs, determine which version
is causing it.

High availability considerations
An application that depends on a single instance of a service creates a single point of
failure. To improve resiliency and scalability, provision multiple instances.
The following are examples of how to provision multiple instances with Azure resources.
We recommend that you review the resiliency guidance for the resources that comprise
your workload to achieve high availability across it.
For Azure App Service, select an app service plan that offers multiple instances.
For Azure Virtual Machines, ensure that your architecture includes more than one
VM and that each VM is included in an availability set.

Consider deploying across multiple regions
We recommend deploying all but the least critical applications and application services
across multiple regions. If your application is deployed to a single region, in the rare
event that the entire region becomes unavailable, the application will also be
unavailable. If you choose to deploy to a single region, consider preparing to redeploy
to a secondary region as a response to an unexpected failure.

Redeploy to a secondary region
If you run applications and databases in a single, primary region with no replication,
your recovery strategy might be to redeploy to another region. This solution is
affordable but most appropriate for noncritical applications that can tolerate longer
recovery times. If you choose this strategy, automate the redeployment process as much
as possible. Also, include redeployment and data synchronization scenarios in your
disaster response and testing scenarios.
To automate your redeployment process, consider using Azure Site Recovery.

Next steps
Release engineering: Rollback

Release engineering: Rollback
Article • 05/26/2023

In some cases, a new software deployment can harm or degrade the functionality of a
software system. When building your solutions, it's essential to anticipate deployment
issues and to architect solutions that provide mechanisms for fixing problematic
deployments. Rolling back a deployment involves reverting the deployment to a known
good state.
Rollback can be accomplished in many different ways. Several Azure services support
native mechanisms for rolling back to a previous state. We recommend you use them
where available and where it makes sense for your chosen deployment model.
The following examples help you design your rollback and recovery plan.

Make use of staged deployments
If you use a staged deployment strategy, you deploy several versions of your application
and use networking techniques to switch between them. You can then quickly revert to a
previous version of the application if you detect a problem along the way.
With Azure App Service, you can use deployment slots to implement staged
deployment. Deployment slots are running instances of the application, each with a
separate host name.
Slots can be used to stage and test applications before promoting to a production slot.
A deployment slot can be created to hold the last known good instance of your
application. If there's an issue or problematic deployment, the production slot can be
swapped with the known good slot to bring the application back to a known good state
quickly.
The following image shows how to do it by way of an Azure DevOps pipeline.

Learn more
For more information on Azure App Service deployment slots, see Set up staging
environments in Azure App Service.

Apply desired state configuration
Depending on the resources in your workload, you might be able to use desired state
configurations to automate and govern your deployment process.
Kubernetes is among the solutions that let you express your deployment instructions as
a desired state configuration. With Kubernetes, you can use a deployment instruction to
describe the particular workload running in the cluster. A deployment might declare that
a workload consists of three replicas of a specific pod that should always be running.
The deployment object creates a ReplicaSet and the associate Pods. When you update a
workload, the deployment itself can be revised, which will generally roll out a new
container image to the deployment pods. Assuming multiple replicas of the pods exist,
this rollout can happen in a controlled and staged manner and in harmony with your
workload's overall deployment strategy.
Learn more
For more information, see Kubernetes deployments

.

Work with deployment records
A deployment record is created when deploying Azure infrastructure and solutions with
Azure Resource Manager (ARM) templates or other infrastructure as code solutions.

When you create a new deployment, you can provide a previously known good
deployment so that if the current deployment fails, the previous good deployment is
redeployed. There are several considerations and caveats when you use this
functionality. For more information, see Rollback on an error to successful deployment.


Learn more
For more information, see Rollback on an error to successful deployment.

Restore a known good state
Many managed services have a concept of versioning with a built-in restore function.
Azure Logic Apps, for example, create a new version of the application whenever an
update is made to it. Azure maintains a history of versions and can revert or promote
any previous version. To do so, in the Azure portal, select your logic app and choose
Versions. Previous versions can be selected on the versions pane, and the application
can be inspected both in the code view and the visual designer view. Select the version
you want to revert to, select the Promote option, and then Save.



Learn more

For more information, see Manage logic apps in the Azure portal.

Restore a database
You can do a point in time restore on any primary database on the same server. You
restore to an earlier point in time within the database's retention period.
Learn more
For more information, see Point in time restore

Monitoring operations of cloud
applications
Article • 03/15/2023

Distributed applications and services running in the cloud are, by nature, complex pieces
of software that include many moving parts. In a production environment, it's important
to track the way customers use your system and monitor the health, and performance of
your system. Use the following checklist as a diagnostic aid to detect, correct, and
prevent issues from occurring.

Checklist
How are you monitoring your workload?
＂ Ensure that the system remains healthy.
＂ Track the availability of the system and its component elements.
＂ Maintain performance to ensure that the throughput of the system doesn't degrade
unexpectedly as the volume of work increases.
＂ Guarantee that the system meets any service-level agreements (SLAs) established
with customers.
＂ Protect the privacy and security of the system, users, and their data.
＂ Track the operations performed for auditing or regulatory purposes.
＂ Monitor the day-to-day usage of the system and spot trends that might lead to
problems if they're not addressed.
＂ Track issues that occur, from initial report through to analysis of possible causes,
rectification, consequent software updates, and deployment.
＂ Trace operations and debug software releases.

In this section
Follow these questions to assess the workload at a deeper level.
Assessment

Description

Do you monitor your

Have an overall view of the workload resources. The information

resources?

can come from application code, frameworks, external sources with
which the application communicates, and the underlying
infrastructure.

Assessment

Description

Do you have detailed

Instrumentation lets you gather performance data, diagnose

instrumentation in the
application code?

problems, and make decisions.

Do you correlate
application events across
all application

Collect data from various sources, consolidate and clean various
formats, and store in reliable storage.

components?
Do you interpret the

Analyze the data collected from various data sources to assess the

collected data to spot
issues and trends in
application health?

overall well-being of the workload.

Do you visualize

Present the analyzed data in a way that an operator can quickly

monitoring data?

spot any trends or problems.

Do you have alerts and

Present the analyzed data in a way that an operator can quickly

response plans ready for
the relevant teams when

spot any trends or problems.

issues occur?
Do you use the Azure

Consider the underlying infrastructure such as virtual machines,

platform notifications and

networks, and storage services to collect important platform-level

updates?

diagnostic data.

Do you monitor and

Health monitoring generates a snapshot of the current health of

measure application
health?

the system so that you can verify all components are functioning
as expected.

Do you monitor and track
resource usage?

Usage monitoring tracks how the features and components of an
application are used.

Do you collect data from
the reported issues

Analyzing data, for unexpected events, can provide insight about
the application health.

Do you collect audit logs
for regulatory

Auditing can provide evidence useful for compliance attestations.

requirements?

Azure offering
You can use any monitoring platform to manage your Azure resources. Microsoft's first
party offering is Azure Monitor, a comprehensive solution for metrics and logs from the
infrastructure to the application code, including the ability to trigger alerts and
automated actions as well as data visualization.

Reference architecture
Enterprise monitoring with Azure Monitor illustrates an architecture that has centralized
monitoring management by using Azure Monitor features.

Related links
Distributed tracing
Application Insights

Next steps
Monitoring phases

Monitoring workloads
Article • 11/30/2022

Monitoring a workload and the infrastructure in which it runs, should not be processed
or analyzed in isolation. Build a pipeline that gives you holistic observability of the
system.
This article describes the stages of a common pipeline design.

1. Data sources and instrumentation
Monitoring data from many sources:
Application code Developers add trace messages in application code to track the
flow of control. For example, recording the entry and exit times can be useful. An
entry to a method in the application can emit a trace message that specifies the
name of the method, the current time, the value of each parameter, and any other
pertinent information.
Application frameworks Many applications use libraries and frameworks to
perform common tasks such as accessing a data store or communicating over a
network. The frameworks emit trace messages and raw diagnostic information,
such as transaction rates and data transmission successes and failures can be
important information for debugging issues.
Dependent services The application will access external services such as a web
server or database management system to complete business operations. Even a
single business operation can result in multiple point-to-point calls among all
services. The services might publish their own trace information, logs, and
performance counters. Examples include SQL Server Dynamic Management Views
for tracking operations performed against a SQL Server database, and IIS trace logs
for recording requests made to a web server.

Release pipeline As the components of a system are modified and new versions
are deployed, it's important to be able to attribute issues, events, and metrics to
each version. This way problems with a specific version of a component can be
tracked quickly and rectified.
７ Note
As a workload owner, you may not be monitoring infrastructure metrics actively.
However, this information can indicate systemic issues. Consider the underlying
infrastructure and components on which your system runs. Virtual machines, virtual
networks, and storage services can all be sources of important infrastructure-level
performance counters and other diagnostic data.
Another important source is the operating system where the application runs. It can
be a source of low-level system-wide information, such as performance counters
that indicate I/O rates, memory utilization, and CPU usage. Operating system errors
(such as the failure to open a file correctly) might also be reported.

More information: Sources of monitoring data

Instrumentation
Instrumentation is a process of generating diagnostics data in the data sources. The
captured data can help you assess performance, diagnose problems, and make
decisions. In the simplest case of a single application, it can be a log of trace messages.
In complex scenarios, there might be a need to correlate instrumentation data from
several sources with sufficient context so that the data can be mapped it to an action.
For example, at the framework level, a task might be identified by a thread ID. Within an
application, the same work might be associated with the user ID for the user who is
performing that task.
More information: Instrumentation best practices

2. Collection and storage
At this stage, the information generated by instrumentation is collected and
transformed so that it's easy to consume and then saved to a reliable storage. The
choices for the data collection and storage technologies will depend on some factors:

Do you need to analyze data quickly (hot path) or it can be stored for later analysis
(warm, cold path)?
Do you want the data collection tier to retrieve data actively (pull model) or wait
for the data to arrive (push model)?
Do you want to consolidate data from various sources before writing to storage?
Do you need to archive the data?
More information: Data collection and storage considerations

3. Analysis and diagnosis
The analysis and diagnosis stage takes the collected raw data and produces information
that can be used to determine the state of the system. This information is useful in
deciding remediation actions. In certain cases, the results can be fed back into the
instrumentation and collection stages.
More information: Analyzing data

4. Visualization and alerting
The purpose of visualization is to present information in near real time by using a series
of dashboards. Also, you can view historical data through reports, graphs, and charts to
identify long-term trends.
More information: Visualizing data

Alerting
If information indicates that the key indicators are likely to exceed acceptable bounds,
you can trigger alerts to raise awareness. It can trigger an automated process that
attempts to take corrective actions, such as autoscaling.
More information: Alerting

Next steps
More information: Sources of monitoring data

Data sources for diagnostic data
Article • 11/30/2022

The information that the monitoring process uses can come from several sources:
application code, frameworks, external sources with which the application
communicates, and the underlying infrastructure.
This article highlights some best practices of those sources and provides guidance on
the types of information that you should capture.

Application code
Developers add trace messages in the application code to track the flow of control. Here
are some best practices:
Follow a standard approach For example, recording the entry and exit times can
be useful. An entry to a method in the application can emit a trace message that
specifies the name of the method, the current time, the value of each parameter,
and any other pertinent information.
Log all exceptions and warnings Retain a full trace of any nested exceptions and
warnings. Ideally, you should also capture information that identifies the user who
is running the code, together with activity correlation information (to track
requests as they pass through the system).
Log attempts to access all dependent resources An application will communicate
with services such as message queues, databases, files, and other dependent
services. This information can be used for metering and auditing purposes.
Capture information that identifies the user who is running the code This
information with activity correlation information is useful in tracking requests as
they pass through the system.

Application frameworks
Many applications use libraries and frameworks to perform common tasks such as
accessing a data store or communicating over a network. Consider configuring the
frameworks to emit trace messages. These frameworks might be configurable to provide
their own trace messages and raw diagnostic information, such as transaction rates and
data transmission successes and failures.

７ Note
Many modern frameworks automatically publish performance and trace events.
Capturing this information is simply a matter of providing a means to retrieve and
store it where it can be processed and analyzed.

Dependent services
The application will access external services such as a web server or database
management system to complete business operations. Even a single business operation
can result in multiple point-to-point calls among all services. The services might publish
their own trace information, logs, and performance counters. Examples include SQL
Server Dynamic Management Views for tracking operations performed against a SQL
Server database, and IIS trace logs for recording requests made to a web server.

Operating system
Another important source is the operating system where the application runs. It can be a
source of low-level system-wide information, such as performance counters that
indicate I/O rates, memory utilization, and CPU usage. Operating system errors such as
the failure to open a file correctly might also be reported.

Infrastructure metrics
７ Note
As a workload owner, you may not be monitoring infrastructure metrics actively.
However, this information can indicate systemic issues. Consider the underlying
infrastructure and components on which your system runs. Virtual machines, virtual
networks, and storage services can all be sources of important infrastructure-level
performance counters and other diagnostic data.
Compute monitoring Collect metrics from compute resources on which the
application is running. This might be virtual machines, App Services, or Kubernetes.
Knowing the state of your infrastructure will allow you to react promptly if there
are any issues.
Data tier monitoring Include metrics the databases, storage accounts, and other
data sources that interact with the application. A low performance of the data tier

of an application could have serious consequences.
Container monitoring If your application run on Azure Kubernetes Service (AKS),
you'll need to monitor the state of your cluster, nodes, and pods. One option is to
the container insights feature in Azure Monitor. This feature delivers quick, visual,
and actionable information: from the CPU and memory pressure of your nodes to
the logs of individual Kubernetes pods.
Operators who prefer using the open-source Kubernetes monitoring tool
Prometheus can take advantage of its integration with container insights.
The Sidecar Pattern adds a separate container with responsibilities that are
required by the main container. A common use case is for running logging utilities
and monitoring agents.
Network monitoring Networking is key to an application running without issues.
Consider using Network Watcher, a collection of network monitoring and
troubleshooting tools. Some of these tools are:
Traffic Analytics shows the flows the virtual networks and uses Microsoft Threat
Intelligence databases to give you percentage traffic from malicious IP
addresses. You can identify bottlenecks by seeing the systems in your virtual
networks that generate most traffic.
Network Performance Monitor can generate synthetic traffic to measure the
performance of network connections over multiple links, giving you a
perspective on the evolution of WAN and Internet connections over time, as
well as offering valuable monitoring information about Microsoft ExpressRoute
circuits.
VPN diagnostics can help troubleshooting site-to-site VPN connections
connecting your applications to users on-premises.

Release pipeline
As the components of a system are modified and new versions are deployed, it's
important to be able to attribute issues, events, and metrics to each version.
Relate information about issues, events, and others to the release pipeline
Problems with a specific version of a component can be tracked quickly and
rectified.
Log security-related information for successful and failing requests Security
issues might occur at any point in the system. For example, a user might attempt
to sign in with an invalid user ID or password. An authenticated user might try to

obtain unauthorized access to a resource. Or a user might provide an invalid or
outdated key to access encrypted information.

Sources from application and system
monitoring
This strategy uses internal sources within the application, application frameworks,
operating system, and infrastructure. The application code can generate its own
monitoring data at notable points during the lifecycle of a client request. The application
can include tracing statements that might be selectively enabled or disabled as
circumstances dictate. It might also be possible to inject diagnostics dynamically by
using a diagnostics framework. These frameworks typically provide plug-ins that can
attach to various instrumentation points in your code and capture trace data at these
points.
Additionally, your code and/or the underlying infrastructure might raise events at critical
points. Monitoring agents that are configured to listen for these events can record the
event information.

Real user monitoring
This approach records the interactions between a user and the application and observes
the flow of each request and response. This information can have a two-fold purpose: it
can be used for metering usage by each user, and it can be used to determine whether
users are receiving a suitable quality of service (for example, fast response times, low
latency, and minimal errors). You can use the captured data to identify areas of concern
where failures occur most often. You can also use the data to identify elements where
the system slows down, possibly due to hotspots in the application or some other form
of bottleneck. If you implement this approach carefully, it might be possible to
reconstruct users' flows through the application for debugging and testing purposes.
） Important
You should consider the data that's captured by monitoring real users to be highly
sensitive because it might include confidential material. If you save captured data,
store it securely. If you want to use the data for performance monitoring or
debugging purposes, strip out all personally identifiable information first.

Synthetic user monitoring
In this approach, you write your own test client that simulates a user and performs a
configurable but typical series of operations. You can track the performance of the test
client to help determine the state of the system. You can also use multiple instances of
the test client as part of a load-testing operation to establish how the system responds
under stress, and what sort of monitoring output is generated under these conditions.
７ Note
You can implement real and synthetic user monitoring by including code that traces
and times the execution of method calls and other critical parts of an application.

Application profiling
This approach is primarily targeted at monitoring and improving application
performance. Rather than operating at the functional level of real and synthetic user
monitoring, it captures lower-level information as the application runs. You can
implement profiling by using periodic sampling of the execution state of an application
(determining which piece of code that the application is running at a given point in
time). You can also use instrumentation that inserts probes into the code at important
junctures (such as the start and end of a method call) and records which methods were
invoked, at what time, and how long each call took. You can then analyze this data to
determine which parts of the application might cause performance problems.

Endpoint monitoring
This technique uses one or more diagnostic endpoints that the application exposes
specifically to enable monitoring. An endpoint provides a pathway into the application
code and can return information about the health of the system. Different endpoints can
focus on various aspects of the functionality. You can write your own diagnostics client
that sends periodic requests to these endpoints and assimilate the responses. For more
information, see the Health Endpoint Monitoring pattern.
For maximum coverage, you should use a combination of these techniques.

Next steps
Instrumentation

Instrument an application
Article • 11/30/2022

Instrumentation is a critical part of the monitoring process. You can make meaningful
decisions about the performance and health of a system only if you first capture the
data that enables you to make these decisions. By using instrumentation, the
information that you gather should be sufficient to assess performance, diagnose
problems, and make decisions without requiring you to sign in to a remote production
server to perform tracing, and debugging manually. Instrumentation data typically
includes metrics and information that's written to trace logs.

Trace logs
The contents of a trace log can be the result of textual data that's written by the
application or binary data that's created as the result of a trace event, if the application
is using Event Tracing for Windows (ETW). Trace log contents can also be generated
from system logs that record events arising from parts of the infrastructure, such as a
web server. Textual log messages are often designed to be human-readable, but they
should also be written in a format that enables an automated system to parse them
easily.
Categorize logs and use separate logs to record the trace output from different
operational aspects of the system, instead of writing all trace data to a single log. You
can then quickly filter log messages by reading from the appropriate log rather than
having to process a single lengthy file. Never write information that has different
security requirements (such as audit information and debugging data) to the same log.
７ Note
A log may be implemented as a file on the file system, or it may be held in some
other format, such as a blob in blob storage. Log information may also be held in
more structured storage, such as rows in a table.

Metrics
Metrics will generally be a measure or count of some aspect or resource in the system at
a specific time, with one or more associated tags or dimensions, sometimes called a
sample. A single instance of a metric isn't useful in isolation. Instead, metrics have to be
captured over time. The key issue to consider is which metrics you should record and

how frequently. Generating data for metrics too often can impose a significant extra
load on the system; whereas, capturing metrics infrequently may cause you to miss the
circumstances that lead to a significant event. The considerations will vary from metric
to metric. For example, CPU utilization on a server might vary significantly from second
to second, but high utilization becomes an issue only if it's long-lived over many
minutes.

Information for correlating data
You can easily monitor individual system-level performance counters, capture metrics
for resources, and obtain application trace information from various log files. Some
forms of monitoring require the analysis and diagnostics stage in the monitoring
pipeline to correlate the data that's retrieved from several sources. This data may take
several forms in the raw data, and the analysis process must be provided with sufficient
instrumentation data to map these different forms. For example, at the application
framework level, a task may be identified by a thread ID. Within an application, the same
work may be associated with the user ID for the user who is completing that task.
Also, it's unlikely to be a 1:1 mapping between threads and user requests, because
asynchronous operations may reuse the same threads to do operations for more than
one user. To complicate matters further, a single request may be handled by more than
one thread as execution flows through the system. If possible, associate each request
with a unique activity Id that's propagated through the system as part of the request
context. The technique for generating and including activity IDs in trace information
depends on the technology that's used to capture the trace data.
All monitoring data should be timestamped in the same way. For consistency, record all
dates and times by using Coordinated Universal Time, which helps you trace sequences
of events with ease.
７ Note
Computers operating in different time zones and networks may not be
synchronized. Don't depend on timestamps alone for correlating instrumentation
data that spans multiple machines.

Information to include in the instrumentation
data

Consider the following points when you're deciding which instrumentation data you
need to collect.

Human-readable data
Ensure that information captured by trace events is machine and human readable.
Adopt well-defined schemas for this information to help automated processing of log
data across systems, and to provide consistency to operations and engineering staff
reading the logs.
Include the following environmental information, such as:
Deployment environment
Machine on which the process is running
Details of the process
Call stack

Consider performance tradeoffs
Enable profiling only when necessary because it can impose a significant overhead on
the system. By using instrumentation, profiling records an event, such as a method call,
every time it occurs; but, sampling records only selected events.
The selection can be time-based, such as once every n seconds, or frequency-based,
such as once every n requests. If events occur frequently, profiling by instrumentation
may cause too much of a burden and effect overall performance. In this case, the
sampling approach may be preferable.
However, if the frequency of events is low, sampling may miss them. In this case,
instrumentation may be the better approach.

Invest in traceability and correlation
Provide sufficient context to enable a developer or administrator to determine the
source of each request, which may include some form of activity Id that identifies a
specific instance of a request.
It may also include information that can be used to correlate this activity with the
computational work performed and the resources used. This work might cross process
and machine boundaries.

For metering, the context should also include, either directly or indirectly, through other
correlated information, a reference to the customer who caused the request. This
context provides valuable information about the application state at the time that the
monitoring data was captured.

Capture all relevant data
Record all requests, and the locations, or regions from which these requests are made.
This information can help determine whether there are any location-specific hotspots.
This information can also be useful to determine whether to repartition an application or
the data that it uses.
Record and capture the details of exceptions carefully. Often, critical debug information
is lost because of poor exception handling.
Capture the full details of exceptions that the application throws, including any inner
exceptions and other context information. Include the call stack, if possible.

Strive for data consistency
Consistent data can help analyze events and correlate them with user requests. Consider
using a comprehensive and configurable logging package to gather information, rather
than depending on developers to adopt the same approach as they implement different
parts of the system.
Gather data from key performance counters, such as the volume of I/O being
performed, network utilization, number of requests, memory use, and CPU utilization.
Some infrastructure services may provide their own specific performance counters, such
as:
The number of connections to a database.
The rate at which transactions are performed.
The number of transactions that succeed or fail. Applications might also define
their own specific performance counters.

Consider external dependencies
Log all external service calls made.
For example to:
Database systems

Web services
Other system-level services that are part of the infrastructure
Record information about the time taken to perform each call, and the success or failure
of the call. If possible, capture information about all retry attempts and failures for any
transient errors that occur.

Ensure compatibility with telemetry systems
In many cases, the instrumentation information is generated as a series of events and
passed to a separate telemetry system for processing and analysis. A telemetry system is
typically independent of any specific application or technology, but it expects
information to follow a specific format that's defined by a schema. The schema
effectively specifies a contract that defines the data fields and types that the telemetry
system can ingest. The schema should be generalized to allow for data arriving from a
range of platforms and devices.
A common schema should include fields that are common to all instrumentation events,
such as:
Event name
Event time
IP address of the sender
Details required for event correlation, such as:
User ID
Device ID
Application ID
Remember that any number of devices may raise events, so the schema shouldn't
depend on the device type. Various devices may raise events for the same application.
The application may support roaming or some other form of cross-device distribution.
The schema may also include domain fields that are relevant to a particular scenario
that's common across different applications, such as:
Information about exceptions
Application start and end events
Success or failure of web service API calls
All applications that use the same set of domain fields should emit the same set of
events, enabling a set of common reports and analytics to be built.

Finally, a schema may contain custom fields for capturing the details of applicationspecific events.

Best practices for instrumenting applications
The following list summarizes best practices for instrumenting a distributed application
running in the cloud:
Make logs easy to read and easy to parse. Use structured logging where possible.
Be concise and descriptive in log messages.
Identify the source of the log.
Add timing information as each log record is written.
Use the same time zone and format for all timestamps.
Categorize logs and write messages to the appropriate place.
Don't reveal sensitive information about the system or personal information about
users.
Scrub this information before it's logged, but ensure that you keep the relevant
details.
Log all critical exceptions, but enable the administrator to turn logging on and off
for lower levels of exceptions and warnings.
Capture and log all retry logic information. This data can be useful in monitoring
the transient health of the system.
Trace out process calls, such as requests to external web services or databases.
Don't mix log messages with different security requirements in the same log file.
Except for auditing events, make sure that all logging calls are fire-and-forget
operations that don't block the progress of business operations.
Auditing events are exceptional because they're critical to the business.
Make sure that logging is extensible (for example in code through the use of an
interface) and doesn't have any direct dependencies on a concrete target.
Make sure that all logging is fail-safe and never triggers any cascading errors.
Treat instrumentation as an ongoing iterative process and review logs regularly,
not just when there's a problem.

Next steps
Collection and storage

Collect, aggregate, and store
monitoring data for cloud applications
Article • 11/30/2022

In a distributed system, instrumentation can generate large volumes of data from many
components, held in various locations, and in different formats.
For example, an application code might generate trace log files and application event
log data. From the infrastructure perspective, performance counters are captured
through other technologies. Also, any third-party components and services that the
application uses might provide their own instrumentation information in different
formats, by using separate trace files, blob storage, or even a custom data store.
This article provides best practices for collecting data from various sources,
consolidating and cleaning various formats, and storing in reliable storage.

Best practices
Have technologies to collect and aggregate application logs across environments,
diagnostics logs from application resources, infrastructure, and other critical
dependencies.
The collection service should run as an out-of-process service and should be
simple to deploy.
All output from the collection service should be a structured and agnostic format.
The monitoring and data-collection process must be fail-safe and must not trigger
any cascading error conditions.
In the event of a transient failure in sending information to a data sink, the
collection service should be prepared to reorder telemetry data so that the newest
information is sent first. (The monitoring agent/data-collection service might elect
to drop the older data, or save it locally and transmit it later to catch up, at its own
discretion.)

Architecture
This image shows a typical instrumentation data-collection process.

Data collection service collects data from various sources is not necessarily a single
process and might comprise many constituent parts running on different
machines.
If you need data to be analyzed quickly, local components that operate outside the
collection service might perform the analysis tasks immediately. The results can be
sent directly to the visualization and alerting systems.
If you don't need immediate analysis, data is held in storage while it awaits
processing.
The hot, warm, and cold analysis patterns are summarized in Analysis patterns.

Application data
For applications, the collecting service can be an Application Performance
Management (APM) tool that can run autonomously from the application that
generates the instrumentation data. After APM is enabled, you'll have clear visibility of
important metrics both in real time and historically. Consider an appropriate level of
logging. Verbose logging can incur significant costs.
An example of an APM is Application Insights that aggregates application level logs and
events for subsequent analysis.
Application logs support the end-to-end application lifecycle. Logging is essential in
understanding how the application operates in various environments and what events
occur and under which conditions.
Collecting application logs and events across all major environments is recommended.
Separate the data between environments as much as possible. Have filters to ensure
non-critical environments do not convolute production log interpretation. Furthermore,
corresponding log entries across the application should capture a correlation ID for their
respective transactions.

Application events should be captured as a structured data type with machinereadable data points rather than unstructured string types. A structured format,
following well-known schema can help in parsing and analyzing logs. Also, structured
data can easily be indexed and searched, and reporting can be greatly simplified.
Also the data should be an agnostic format that's independent of the machine,
operating system, or network protocol. For example, emit information in a selfdescribing format such as JSON, MessagePack, or Protobuf rather than ETL/ETW. Using
a standard format enables the system to construct processing pipelines; components
that read, transform, and send data in the agreed format can be easily integrated.

Infrastructure data
You will also need to collect platform diagnostics to get a holistic view. For example,
Windows event logs, performance counters, diagnostic infrastructure logs, and logs
from the management plane. Azure platform logs addresses all those needs. Here are
some recommendations:
Collect Azure Activity Logs to get audit information. These logs are useful in
detecting configuration changes to Azure resources.
Enable resource- or infrastructure- level monitoring throughout the application.
This type of logs includes information emitted by platform services such as Azure
VMs, Express Route or SQL Database, and also third-party solutions. Configure
application resources to route diagnostic logs and metrics to the chosen log
aggregation technology.
Enforce consistency. You can use Azure Policy to ensure the consistent use of
diagnostic settings across the application, to enforce the desired configuration for
each Azure service.
Collect logs and metrics available for critical internal dependencies. This
information gives you visibility into the operational state of critical internal
dependencies, such as a shared NVA or Express Route connections, and others.
There are many options for a collection service for aggregating infrastructure and
resource logs. Azure Log Analytics or Splunk, are popular choices for collating logs and
metrics across all application components for subsequent evaluation. Resources may
include Azure IaaS, PaaS services, and third-party appliances such as firewalls or antimalware solutions used in the application. For instance, if Azure Event Hubs is used, the
Diagnostic Settings should be configured to push logs and metrics to the data sink.
Understanding usage helps with right-sizing of the workload, but additional cost for
logging needs to be accepted and included in the cost model.

Collection strategies
Avoid retrieving telemetry data manually from every component. Have a way of moving
the data a central location and consolidated. For a multiregion solution, it's
recommended that you first collect, consolidate, and store data on a region-by-region
basis, and then aggregate the regional data into a single central system.
To optimize the use of bandwidth, prioritize based on the importance of data. You can
transfer less urgent data in batches. However, the data must not be delayed indefinitely,
especially if it contains time-sensitive information.

Data collection models
The collection service can collect instrumentation data in mainly two models:
Pull model Actively retrieves data from the various logs and other sources for each
instance of the application.
Push model Passively waits for the data to be sent from the components that
constitute each instance of the application.

Monitoring agents
Monitoring agents work in pull model. Agents run locally in a separate process with
each instance of the application and periodically pull data and write this information
directly to centralized storage shared by all instances of the application.

For more information, see Azure Diagnostics extension.

７ Note
Using a monitoring agent is ideally suited to capturing instrumentation data that's
naturally pulled from a data source. It's appropriate for a small-scale application
running on a limited number of nodes in a single location. An example is
information from SQL Server Dynamic Management Views or the length of an
Azure Service Bus queue.

Performance considerations
A complex, highly scalable, application might generate huge volumes of data. The
amount of data can easily overwhelm the I/O bandwidth available with a single, central
location. The telemetry solution must not act as bottleneck and must be scalable as the
system expands. Ideally, the solution should incorporate a degree of redundancy to
reduce the risks of losing important monitoring information (such as auditing or billing
data) if part of the system fails.
One approach is through queuing.

In this architecture, the data-collection service posts data to a queue. A message queue
is suitable because it provides "at least once" semantics that help ensure that queued
data will not be lost after it's posted. You can implement the storage writing service by
using a separate worker role. You can implement this with the Priority Queue pattern.
For scalability, you can run multiple instances of the storage writing service. If there is a
high volume of events, you can use Event Hubs to dispatch the data to a different
compute for processing and storage.

Consolidation strategies

The data collected from a single instance of an application gives a localized view of the
health and performance of that instance. To assess the overall health of the system, it's
necessary to consolidate some aspects of the data in the local views. You can perform
this after the data has been stored, but in some cases, you can also achieve it as the
data is collected.

The instrumentation data can pass through a separate data consolidation service that
combines data and acts as a filter and cleanup process. For example, instrumentation
data that includes the same correlation information such as an activity ID can be
amalgamated. (It's possible that a user starts performing a business operation on one
node and then gets transferred to another node if a node fails, or depending on how
load balancing is configured.) This process can also detect and remove any duplicated
data (always a possibility if the telemetry service uses message queues to push
instrumentation data out to storage).

Storage strategies
When deciding the storage capabilities, consider the type of data, how it's used, and
how urgently the data is required.

Storage technologies
Consider a polyglot persistence approach where different types of information are
stored by using technologies that are most appropriate to the way in which each type is
likely to be used.
For example, Azure blob and table storage have some similarities in the way in which
they're accessed. But they have differences in the operations you can perform for each,
and the granularity of the data that they hold. If you need to perform more analytical
operations or require full-text search capabilities on the data, it might be more
appropriate to use data storage that provides capabilities that are optimized for specific
types of queries and data access. For example:

Performance counter data can be stored in a SQL database to enable ad hoc
analysis.
Trace logs might be better stored in Azure Cosmos DB.
Security information can be written to HDFS.
Information that requires full-text search can be stored through Elasticsearch
(which can also speed searches by using rich indexing).
The same instrumentation data might be required for more than one purpose. For
example, performance counters can be used to provide a historical view of system
performance over time. This information might be combined with other usage data to
generate customer billing information. In these situations, the same data might be sent
to more than one destination, such as a document database that can act as a long-term
store for holding billing information, and a multidimensional store for handling complex
performance analytics.

Consolidation service
You can implement another service that periodically retrieves the data from shared
storage, partitions and filters the data according to its purpose, and then writes it to an
appropriate set of data stores.

An alternative approach is to include this functionality in the consolidation and cleanup
process and write the data directly to these stores as it's retrieved rather than saving it
in an intermediate shared storage area.
Each approach has its advantages and disadvantages. Implementing a separate
partitioning service lessens the load on the consolidation and cleanup service, and it
enables at least some of the partitioned data to be regenerated if necessary (depending
on how much data is retained in shared storage). However, it consumes additional
resources. Also, there might be a delay between the receipt of instrumentation data
from each application instance and the conversion of this data into actionable
information.

Querying considerations
Consider how urgently the data is required. Data that generates alerts must be accessed
quickly, so it should be held in fast data storage and indexed or structured to optimize
the queries that the alerting system performs. In some cases, it might be necessary for
the collection service to format and save data locally so that a local instance of the
alerting system can send notifications quickly. The same data can be dispatched to the
storage writing service shown in the previous images and stored centrally if it's also
required for other purposes.

Data retention considerations
In some cases, after the data has been processed and transferred, the original raw
source data that was stored locally can be removed. In other cases, it might be
necessary or useful to save the raw information. For example, data that's generated for
debugging purposes might be best left available in its raw form but can then be
discarded quickly after any bugs have been rectified.
Performance data often has a longer life so that it can be used for spotting performance
trends and for capacity planning. The consolidated view of this data is usually kept
online for a finite period to enable fast access. After that, it can be archived or discarded.
It's useful to store historical data so you can spot long-term trends. Rather than saving
old data in its entirety, it might be possible to down-sample the data to reduce its
resolution and save storage costs. As an example, rather than saving minute-by-minute
performance indicators, you can consolidate data that's more than a month old to form
an hour-by-hour view.
Data gathered for metering and billing customers might need to be saved indefinitely.
Additionally, regulatory requirements might dictate that information collected for
auditing and security purposes also needs to be archived and saved. This data is also
sensitive and might need to be encrypted or otherwise protected to prevent tampering.
You should never record users' passwords or other information that might be used to
commit identity fraud. Such details should be scrubbed from the data before it's stored.

Next step
Information that's used for more considered analysis, for reporting, and for spotting
historical trends is less urgent and can be stored in a manner that supports data mining
and ad hoc queries.

Analyze monitoring data

Analyze monitoring data for cloud
applications
Article • 04/10/2023

After you've collected data from various data sources, analyze the data to assess the
overall well-being of the system. For analysis, have a clear understanding of:
How to structure data based on KPIs and performance metrics you've defined.
How to correlate the data captured in different metrics and log files. This is
important when tracking a sequence of events and help diagnose problems.
In most cases, data for each part component of the architecture is captured locally and
then accurately combined with data generated by other components.
For example, a three-tier application has:
Presentation tier that allows a user to connect to a website
Middle tier that hosts a set of microservices that processes business logic
Database tier that stores data associated with the operation
The usage data for a single business operation might span across all three tiers. This
information needs to be correlated to provide an overall view of the resource and
processing usage for the operation. The correlation might involve some preprocessing
and filtering of data on the database tier. On the middle tier, common tasks are
aggregation and formatting.

Best practices
Correlate application and resource level logs—Evaluate data at both levels to
optimize the detection of issues and troubleshooting of detected issues. You can
aggregate the data in a single data sink or have ways to query events across both
levels. Using a unified solution, such as Azure Log Analytics, is recommended to
aggregate and query application and resource level logs.
Define clear retention times on storage for cold analysis—This practice is
recommended to allow historic analysis over a specific period. Another benefit is
control on control storage costs. Have processes that make sure data gets archived
to cheaper storage and aggregate data for long-term trend analysis.
Analyze long-term trends analyzed to predict operational issues—Evaluate
across long-term data to form operational strategies and also to predict what

operational issues are likely to occur and when. For instance, if the average
response times have been slowly increasing over time and getting closer to the
maximum target.

Correlate data
Data generated by instrumentation and captured by Application Performance
Monitoring (APM) tools can provide a snapshot of the system state. The main purpose
of the snapshot to make this data actionable.
For example:
What has caused an intense I/O loading at the system level at a specific time?
Is it the result of a large number of database operations?
Is this reflected in the database response times, the number of transactions per
second, and application response times at the same juncture?
A possible remedial action to reduce the load might be to shard the data over more
servers. In addition, exceptions can arise because a fault in any tier. An exception in one
level often triggers another fault in the level above.
So it's recommended that you correlate different types of monitoring data at each level
to produce an overall view of the state of the system and the applications that are
running on it. This is vital for root cause analysis for failures. You can also use this
information to make decisions about whether the system is functioning acceptably or
not, and determine what can be done to improve the quality of the system.
In a distributed application, an operation will lead to multiple transactions. Each
transaction generates events in different services and those events must be correlated
and visualized. This way you can spot performance bottlenecks or failure hotspots
across services. Application Map is a popular choice for visualizing flows.
Telemetry correlation ensures that the transactions can be mapped through the end-toend application and critical system flows. Platform-level metrics and logs such as CPU
percentage, network in/out, and disk operations/sec should be collected from the
application to inform a health model and detect/predict issues. This can also help to
distinguish between transient and non-transient faults.
Here's an Application Map of an application that has several microservices. With this
visual representation, you can see that Workflow service is getting errors from the
Delivery service.

When you look at the end-to-end transaction, you can see that an exception is thrown
due to memory limits in Azure Cache for Redis.

When correlating data make sure that the raw instrumentation data includes sufficient
context and activity ID information to support the required aggregations for correlating
events. Additionally, this data might be held in different formats, and it might be
necessary to parse this information to convert it into a standardized format for analysis.

Analysis patterns
Analyzing and reformatting data for visualization, reporting, and alerting purposes can
be a complex process that consumes its own set of resources. Some forms of
monitoring are time-critical and require immediate analysis of data to be effective.
While other Other forms of analysis are less time-critical and might require some
computation and aggregation after the raw data has been received. This table shows the
patterns for analysis.
Pattern

Characteristics

Hot
analysis

Time-critical and
requires immediate
analysis.

Considerations
Data must be
structured and
available quickly for

Use case
Alerting.
Detecting a security attack
on the system.

efficient processing.
Move the analysis
processing to the
individual VMs that
stores the data.

Warm

Less time-critical and

Perform

Performance analysis on

analysis

might require
aggregation before
analysis.

computation and
aggregation on the
received raw data.

data from a series of events
to identify reliability issues.
Diagnosis of health issues

Aggregate a series
of events instead of
an isolated event.

by statistical evaluation of
the events leading up to a
health event.

Data is stored safely
after it has been
captured.

Usage monitoring and
auditing needs a view of
the system at points in

Aggregate a series
of events instead of

time.
Predictive health analysis

an isolated event.

with historical information.

Cold
analysis

Analysis can be
performed at a later
date.

Combined approach
In common monitoring use cases, you'll use a combination of all three patterns.
For example, a health event is typically processed through hot analysis and can raise an
alert immediately. An operator can then drill into the reasons for the health event by
examining the data from the warm path. This data should contain information about the
events leading up to the issue that caused the health event. An operator can also use

cold analysis to provide the data for predictive health analysis. The operator can gather
historical information over a specified period and use it in conjunction with the current
health data (retrieved from the hot path) to spot trends that might soon cause health
issues. In these cases, it might be necessary to raise an alert so that corrective action can
be taken.

Diagnose issues
Diagnosis requires the ability to determine the cause of faults or unexpected behavior,
including performing root cause analysis. The information that's required typically
includes:
Detailed information from event logs and traces, either for the entire system or for
a specified subsystem during a specified time window.
Complete stack traces resulting from exceptions and faults of any specified level
that occur within the system or a specified subsystem during a specified period.
Crash dumps for any failed processes either anywhere in the system or for a
specified subsystem during a specified time window.
Activity logs recording the operations that are performed either by all users or for
selected users during a specified period.
Analyzing data for troubleshooting purposes often requires a deep technical
understanding of the system architecture and the various components that compose the
solution. As a result, a large degree of manual intervention is often required to interpret
the data, establish the cause of problems, and recommend an appropriate strategy to
correct them. It might be appropriate simply to store a copy of this information in its
original format and make it available for cold analysis by an expert.

Next steps
Visualization

Visualize data and raise alerts
Article • 11/30/2022

An important aspect of any monitoring system is the ability to present the data in such a
way that an operator can quickly spot any trends or problems. Also important is the
ability to quickly inform an operator if a significant event has occurred that might
require attention.
Data presentation can take several forms, including visualization by using dashboards,
alerting, and reporting.

Use dashboards to visualize data
The most common way to visualize data is to use dashboards that can display
information as a series of charts, graphs, or some other illustration. These items can be
parameterized, and an analyst can select the important parameters, such as the time
period, for any specific situation.
Dashboards can be organized hierarchically. Top-level dashboards can give an overall
view of each aspect of the system but enable an operator to drill down to the details.
For example, a dashboard that shows the overall disk I/O for the system should allow an
analyst to view the I/O rates for each individual disk to determine whether one or more
specific devices account for a disproportionate volume of traffic. Ideally, the dashboard
should also display related information, such as the source of each request (the user or
activity) that's generating this I/O. This information can then be used to determine
whether (and how) to spread the load more evenly across devices, and whether the
system would perform better if more devices were added.
A dashboard may also use color-coding or some other visual cues to indicate values that
appear anomalous or that are outside an expected range. Using the previous example:
A disk with an I/O rate that's approaching its maximum capacity over an extended
period (a hot disk) can be highlighted in red.
A disk with an I/O rate that periodically runs at its maximum limit over short
periods (a warm disk) can be highlighted in yellow.
A disk that's showing normal usage can be displayed in green.
For a dashboard system to work effectively, it must have the raw data to work with. If
you're building your own dashboard system, or using a dashboard developed by
another organization, you must understand the following concepts:

Which instrumentation data do you need to collect?
At what levels of granularity?
How should you format data for the dashboard to consume?
A good dashboard doesn't only display information, it also enables an analyst to pose
improvised questions about that information. Some systems provide management tools
that an operator can use to complete these tasks and explore the underlying data.
Instead, depending on the repository that's used to hold this information, it may be
possible to query this data directly, or import it into tools such as Microsoft Excel for
further analysis and reporting.
７ Note
You should restrict access to dashboards to authorized personnel, because this
information may be commercially sensitive. You should also protect the underlying
data for dashboards to prevent users from changing it.

Reporting
Reporting is used to generate an overall view of the system. It may incorporate historical
data and current information. Reporting requirements fall into two broad categories:
operational reporting and security reporting.
Operational reporting typically includes the following aspects:
Aggregating statistics that you can use to understand resource utilization of the
overall system or specified subsystems during a specified time window.
Identifying trends in resource usage for the overall system or specified subsystems
during a specified period.
Monitoring exceptions that have occurred throughout the system or in specified
subsystems during a specified period.
Determining the efficiency of the application for the deployed resources, and
understanding whether the volume of resources, and their associated cost, can be
reduced without affecting performance unnecessarily.
Security reporting tracks customers' use of the system. It can include:
Auditing user operations: This method requires recording the individual requests
that each user completes, together with dates and times. The data should be
structured to enable an administrator to quickly reconstruct the sequence of
operations that a user completes over a specified period.

Tracking resource use by user: This method requires recording how each request for
a user accesses the various resources that compose the system, and for how long.
An administrator can use this data to generate a utilization report, by user, over a
specified period, possibly for billing purposes.
In many cases, batch processes can generate reports according to a defined schedule.
Latency isn't normally an issue. Batch processes should also be available for generation
on a spontaneous basis, if needed. As an example, if you are storing data in a relational
database, such as Azure SQL Database, you can use a tool such as SQL Server Reporting
Services to extract and format data, and present it as a set of reports.

Next steps
More information: Alerting

Alerting for operations
Article • 03/15/2023

Alerting is the process of generating notifications when a significant event is detected
after analyzing instrumentation data. It's an important part of any system that makes
performance, availability, and privacy guarantees to the users where an action is
required immediately.
To ensure that the system remains healthy, responsive, and secure, it's highly
recommended that you set alerts so that operators can respond to them in a timely
manner. An alert can contain enough contextual information to help you quickly get
started on diagnostic activities. Alerting can be used to invoke remediation functions
such as autoscaling. Alerts can also enable cost-awareness by watching budgets and
limits.

Best practices
Define a process for alert response that identifies the accountable owners and
actions.
Configure alerts for a well-defined scope (resource types and resource groups) and
adjust the verbosity to minimize noise.
Use an automated alerting solution, such as Splunk or Azure Monitor, instead of
having people actively look for issues.
Take advantage of alerts to operationalize remediation processes. For example,
automatically create tickets to track issues and resolutions.
Track the health of Azure services in regions, communication about outages,
planned maintenance activities, and other health advisories.

Alert use cases
Here are some common situations for which you might want to set alerts and associated
actions:
Security events—If the event logs indicate that repeated authentication or
authorization failures are occurring, the system might be under attack and an
operator should be informed.
Performance metrics—If a particular performance metric exceeds a specified
threshold, the system must quickly respond.
Availability information—If a fault is detected, it might be necessary to quickly
restart one or more subsystems, or failover to a backup resource. Repeated faults

in a subsystem might indicate more serious concerns.
Cost and usage information—If usage or cost exceeds the allocated budget, it
might be necessary to raise cost awareness to the department with notification
that spending has reached a fixed threshold of the quota.

Application alerts
An alerting system should allow customizing alerts. The appropriate values from the
underlying instrumentation data can be provided to the alerting system as parameters.
This approach enables an operator to filter data and focus on those thresholds or
combinations of interest values.
In some cases, the raw instrumentation data can be provided. In other situations, it
might be more appropriate to supply aggregated data. For example, an alert can be
triggered if the CPU utilization for a node has exceeded 90% over the past 10 minutes.
The details provided to the alerting system should also include any appropriate
summary and context information. This data can help reduce the possibility that falsepositive events will trip an alert.

Resource health alerts
Health alerts should be configured for specific resource groups and resource types, and
should be adjusted to maximize signal to noise ratios. For example, only send a
notification when a resource becomes unhealthy as per the defined requirements of the
application health model or due to an Azure platform-initiated event.
Consider transient issues when setting an appropriate threshold for resource
unavailability, such as configuring an alert for a virtual machine with a threshold of 1
minute for unavailability before an alert is triggered.
Azure Resource Health provides information about the health of individual resources
such as a specific virtual machine instance, and is useful when diagnosing unavailable
resources.
You can also view resource health in Azure Monitor. Alerts can be defined by using
many different data streams such as metric values, log search queries, and activity log
events.

Service health alerts

Get a view into the health of Azure services and regions, communications about
outages, planned maintenance activities, and other health advisories.
Consider configuring Azure Service Health alerts to operationalize Service Health events.
These alerts can provide useful information in interpreting issues that you might have
detected through the resource health alerts. You can use Service Health events to decide
the best way to respond operationally.
However, Service Health alerts aren't effective in detecting issues because of the
associated latencies. There's a 5 minute Service Level Objectives (SLOs) for automated
issues, but many issues require manual interpretation for root cause analysis.

Process considerations
Alerts proactively notify and in cases even respond to operational states that deviate
from the norm. When such an event occurs, an alert is triggered to notify the
accountable teams.
Having well-defined owners and response playbooks per alert is vital to optimizing
operational effectiveness. Alerts can be set for non-technical notifications. For example,
a budget owner should be made aware of capacity issues so that budgets can be
adjusted and discussed.
Instead of having teams actively monitor the systems and dashboard, send reliable alert
notifications to the owners.
The operators might receive alert information by using many delivery channels such as
email, text message, a pager device, or push notifications to a mobile app. Many alerting
systems support subscriber groups, and all operators who are members of the same
group can receive the same set of alerts.
In Azure Monitor, alerts use action groups to notify the owners.

Subscription notification e-mails
Subscription notification e-mails can contain important service notifications or security
alerts. Make sure that subscription notification emails are routed to the relevant
technical stakeholders.

IT Service Management (ITSM) integration

ITSM systems can help define workflows. You can use those systems to document issues,
notify and assign responsible parties, and track issues. For example, operational alerts
from the application could be integrated to automatically create new tickets to track
resolution.
Azure Monitor supports integrations with third-party ITSM systems. For example, if you
have a custom solution that ingests data through an incoming API, this can be engaged
with an Azure Monitor action group each time an alert is raised. Many partner
integrations are ready to use out of the box.
For information on Azure Monitor and ITSM integration, reference IT Service
Management Connector Overview.

Alert prioritization
All alerts being treated the same is going to reduce the efficacy of notifications.
When defining alerts, analyze the potential business impact and prioritize accordingly.
Prioritizing alerts helps operational teams in cases where multiple events require
intervention at the same time. For example, alerts concerning critical system flows might
require special attention. When creating an alert, ensure you establish and set the
correct priority.
One way of specifying the priority is by using a severity level that indicates how critical a
situation is. This image shows this case in Azure Monitor.

Next steps
Return to the operational excellence overview.
Use case: Health monitoring

Related links
Overview of alerts in Microsoft Azure
Create and manage action groups

Health monitoring
Article • 11/30/2022

A system is healthy if it's running and can process requests. Health monitoring
generates a snapshot of the current health of the system so that you can verify all
components are functioning as expected.

Requirements for health monitoring
If any part of the system is unhealthy, you're alerted within a matter of seconds. You'll
determine which parts of the system are functioning normally and which parts are
experiencing problems. A traffic light system indicates system health:
Red for unhealthy. The system has stopped.
Yellow for partially healthy. The system is running with reduced functionality.
Green for healthy.
A comprehensive health monitoring system enables you to drill down to view the health
status of subsystems and components. For example, if the overall system is partially
healthy, you can zoom in and determine which functionality is currently unavailable.

Best practices
Correlate events across all application components.
Use an Application Performance Management (APM) tool used to collect
application level logs.
Use Application Insights to gather key metrics.
Collect application logs from different application environments.
Consider using log levels used to capture different types of application events.
Capture log messages in a structured format.
Set out critical application performance targets and non-functional requirements
with clarity.
Identify known gaps in application observability that led to missed incidents or
false positives in the past.
Consider different log aggregation technologies to collect logs and metrics from
Azure resources.
Make logs and metrics available for [critical internal dependencies]#logs-forinternal-dependencies).
Implement black-box monitoring to measure platform services and the resulting
customer experience.

Implement detailed instrumentation in the application code to better understand
the customer experience.
Apply white-box monitoring to instrument the application with semantic logs and
metrics.

Distributed tracing
Trace the execution of user requests to generate raw data to determine which requests
have:
Succeeded
Failed
Taken too long
Distributed tracing allows you to build and visualize end-to-end transaction flows for
the application. Events coming from different application components or different
component tiers of the application should be correlated to build these flows.
For instance, using consistent correlation IDs transferred between components within a
transaction achieves end-to-end transaction flows.
Event correlation between application layers allows you to connect tracing data of the
complete application stack. You can create a complete picture of where time is spent at
each layer through tools that can query the tracing data repositories in correlation to a
unique identifier. This unique identifier represents a given transaction that flowed
through the system.

Application Performance Management (APM)
tools
To successfully maintain an application, it's important to turn the lights on to have clear
visibility into important metrics, both in real time and historically.
Application Performance Management (APM) tools
An APM technology, such as Application Insights, should be used to manage the
performance and availability of the application, aggregating application level logs, and
events for later interpretation. With cost in mind, consider the appropriate level of
logging required.
Application Insights is an extensible Application Performance Management (APM)
service for developers and DevOps professionals to monitor live applications. It

automatically detects performance anomalies and includes analytics tools to help users
diagnose issues, and to understand what customers do with your application.
Application Insights monitors diagnostic trace logs from your application so that you
can correlate trace events with requests.
Application Insights allows you to:
Verify that your application is running correctly.
Makes application troubleshooting easier.
Provides custom business telemetry to indicate whether your application is being
used as intended.
For more information about how Application Insights helps you monitor applications,
reference Monitoring workloads.

Logs and metrics
Logging is essential to understand how an application operates in various environments
and what events occur and under which conditions. There are two types of logs:
application-generated logs and platform logs.

Application logs
Application logs support the end-to-end application lifecycle. You should collect logs
and events across all major application environments. A sufficient degree of separation
and filtering should be in place to ensure non-critical environments don't convolute
production log interpretation. Corresponding log entries across the application should
capture a correlation ID for their respective transactions.

Log levels
Use the following log levels to capture different types of application events across
environments:
Info
Warning
Error
Debug
Pre-configure the preceding log levels and apply these levels within relevant
environments. Changing log levels includes simple configuration changes to support
operational scenarios where it's necessary to raise the log level within an environment.

Log messages
Capture log messages and application events in a structured format, following wellknown schema. The structured data type includes machine-readable data points rather
than unstructured string types.
Structured data can help you:
Parse and analyze logs.
Index and search logs.
Simplify reporting.

Metrics
Metrics are numerical values that are collected at regular intervals and describe some
aspect of a system at a particular time. They reflect the health and usage statistics of
your resources.

Analyze health data
Analyzing health data involves quickly indicating whether the system is running through
metrics. Hot analysis of the immediate data triggers an alert if a critical component is
detected as unhealthy.
For example, the component fails to respond to a consecutive series of pings. You can
then take the appropriate corrective action. For more information about analyzing data,
reference Analyze monitoring data for cloud applications.

Performance targets and non-functional requirements (NFRs)
Application-level metrics should include end-to-end transaction times of key technical
functions, such as:
Database queries.
Response times for external API calls.
Failure rates of processing steps, and so on.
To assess fully the health of key scenarios in the context of targets and NFRs, correlate
application log events, such as user login, across critical system flows.

Gaps in application monitoring

Known gaps in application observability lead to missed incidents and false positives.
 Tip
What you can't see, you can't measure. What you can't measure, you can't improve.

Platform logs
Platform logs provide detailed diagnostic and auditing information for the infrastructure,
and resources they depend on. Monitoring your platform includes collecting rich metrics
and logs to verify the state of your complete infrastructure, and to react promptly if
there are any issues.
Application Insights or a full-stack monitoring service like Azure Monitor can help you
keep tabs on your entire landscape.
For more information, reference Monitoring workloads.

Activity logs and diagnostic settings
Activity logs provide audit information about when a resource is modified, such as when
a virtual machine is started or stopped. Such information is useful for the interpretation
and troubleshooting of operational and performance issues because it provides
transparency around configuration changes.
 Tip
We recommend collecting and aggregating activity logs.

Log aggregation technologies
Log aggregation technologies, such as Azure Log Analytics or Splunk, should be used to
collate logs and metrics across all application components for later evaluation.
Resources may include Azure IaaS

and PaaS

services, and third-party appliances

such as firewalls or anti-malware solutions used in the application. For instance, if Azure
Event Hub

is used, the Diagnostic Settings should be configured to push logs and

metrics to the data sink. Understanding usage helps with right-sizing of the workload,
but extra costs for logging should be accepted and included in the cost model.

All application resources should be configured to route diagnostic logs and metrics to
the chosen log aggregation technology. Use Azure Policy to ensure the consistent use of
diagnostic settings across the application and to enforce the configuration you want for
each Azure service.

Logs for internal dependencies
To build a robust application health model, it's vital to have visibility into the operational
state of critical internal dependencies, such as a shared Network Virtual Appliance (NVA)
or ExpressRoute connection.

Black-box monitoring
Black-box monitoring tests externally visible application behavior without knowledge of
the internals of the system. This type of monitoring is a common approach to measuring
customer-centric SLIs, SLOs, and SLAs.
For more information, reference Azure Monitor.

Instrumentation
Instrumentation of your code allows precise detection of underperforming pieces when
you apply load or stress tests. It's critical to have this data available to improve and
identify performance opportunities in the application code. Use Application
Performance Monitoring (APM) tools, such as Application Insights, to manage the
performance and availability of the application, along with aggregating application-level
logs, and events for later interpretation.
For more resources about instrumentation, reference Monitor performance.

White-box monitoring
Application-level metrics and logs, such as current memory consumption or request
latency, should be collected from the application to inform a health model, detect, and
predict issues.
For more information, reference Instrumenting an application with Application Insights
and Instrument an application.

Next steps

Usage monitoring

Usage monitoring
Article • 11/30/2022

Usage monitoring tracks how the features and components of an application are used.
This article describes how you can use the data gathered from usage monitoring to gain
insight into operational events that affect your application and workloads.

Benefits of usage monitoring
The following list explores the use cases for data gathered from usage monitoring:
Determine which features you use heavily and determine any potential hotspots in
the system. High-traffic elements may benefit from functional partitioning or even
replication to spread the load more evenly. You can use this information to figure
out which features you don't use often and possible candidates for retirement, or
replacement in a future version of the system.
Collect information about the operational events of the system under normal use.
For example, in an e-commerce site, you can record the statistical information
about the number of transactions and the volume of customers that are
responsible for them. You can use this information for capacity planning as the
number of customers grows.
Detect user satisfaction with the performance or functionality of the system. For
example, if a large number of customers in an e-commerce system regularly
abandon their shopping carts, this behavior may mean there's a problem with the
checkout functionality.
Generate billing information. An application or service may charge customers for
the resources they use.
Enforce quotas. If a user exceeds their paid quota of processing time or resource
usage during a specific period, the system can limit their access or throttle
processing.

Requirements for usage monitoring
To analyze system usage, you'll need monitoring information that includes:
The number of requests that each subsystem processes and directs to each
resource.
The work that each user does.
The volume of data storage that each user occupies.

The resources that each user accesses.

Requirements for data collection
You can track usage at a relatively high level. Usage tracking can note the start and end
times of each request and the nature of the request, such as read, write, and so on,
depending on the resource in question. Retrieve this information through the following
ways:
Trace user activity.
Capture performance counters that measure the usage for each resource.
Monitor each users' resource consumption.
For accounting purposes, you'll want to identify which users are responsible for doing
which operations, and the resources that these operations use. The gathered
information should be detailed enough for accurate billing.

Next steps
Issue tracking

Issue tracking
Article • 11/30/2022

If unexpected events occur in the system, customers and other users may report these
issues.
Issue tracking involves:
Managing issues.
Associating issues with efforts to resolve underlying problems in the system.
Informing customers of possible resolutions.

Requirements for issue tracking
You can often track issues using a separate system that lets you record and report the
details of problems that users report. These details can include:
Tasks the user was doing.
Symptoms of the problem.
Sequence of events.
Error or warning messages.

Requirements for data collection
The user who initially reported the issue is considered the primary data source. This user
can provide more information, such as:
A crash dump, if the application includes a component that runs on the user's
desktop.
A screenshot.
The date and time the error occurred.
The user's location.
You can use this information to help debug and create a backlog for future software
releases.

Analyze data
Consider the following scenarios when you analyze issue-tracking data:

Different users may report the same problem. The issue-tracking system should
associate common reports.
Record the debugging progress against each issue report.
Inform customers of the solution when you've resolved the issue.
If a user reports an issue that has a known solution in the issue-tracking system,
inform the user of the solution immediately.

Next steps
Tracing and debugging

Tracing and debugging
Article • 11/30/2022

When a user reports an issue, the user is often aware only of the immediate effect that
the issue has on their operations. The user can only report the results of their own
experience back to the person who is responsible for maintaining the system. These
experiences are a visible symptom of one or more fundamental problems.

Root cause analysis
In many cases, an analyst must dig through the chronology of the underlying operations
to establish the root cause of the problem. This process is called a root cause analysis.
A root cause analysis may uncover inefficiencies in application design. In these
situations, you can try to rework the affected elements and deploy them as part of a
later release. This process requires careful control, and you should monitor the updated
components closely.

Requirements for tracing and debugging
For tracing unexpected events and other problems, consider the following requirements:
Monitoring data must provide enough information to enable an analyst to trace
the origin of an issue and reconstruct the sequence of events that lead up to the
issue.
Data must be sufficient for the analyst to identify a root cause.
A root cause enables the developer to make the necessary changes to prevent the
issue from recurring.

Requirements for data collection
Troubleshooting involves the following data collection requirements:
Trace all methods and their parameters used in an operation to create a model that
shows the logical flow through the system when a customer makes a specific
request.
Capture and log exceptions, and warnings that the system generates because of
this flow.
To support debugging, the system should provide the following data:

Hooks that enable you to capture state information at critical points in the
system.
Step-by-step information as selected operations continue.
７ Note
Capturing detailed data can cause extra load on the system and should be a
temporary process. Only capture detailed data when an unusual series of events
occur, which are difficult to replicate. Alternately, only capture detailed data when
you're monitoring a new release to ensure that new elements in the system
function as expected.

Next steps
Auditing

Auditing
Article • 11/30/2022

Depending on the application, there may be legal requirements for auditing users'
operations and recording all data access. Auditing can provide evidence that links
customers to specific requests. Affirming validity is an important factor in many online
business systems to help maintain trust between the customer and the business
responsible for the application, or service.

Requirements for auditing
An analyst can trace the sequence of business operations that users perform so that you
can reconstruct users' actions. Tracing the sequence of operations may be necessary as a
matter of record, or as part of a forensic investigation.
Audit information is highly sensitive. This information includes data that identifies the
users of the system and the tasks that they're doing. Reports contain sensitive audit
information available only to trusted analysts. An analyst can generate a range of
reports. For example, reports may list the following activities:
All users' activities occurring during a specified time frame.
The chronology of a single user's activity.
The sequence of operations performed against one or more resources.

Requirements for data collection
The primary sources of auditing information can include:
The security system that manages user authentication.
Trace logs that record user activity.
Security logs that track all network requests.
Regulatory requirements may dictate the format of the audit data and the way it's
stored. For example, it may not be possible to clean the data in any way. It must be
recorded in its original format. Access to the data repository must be protected to
prevent tampering.

Analyze audit data

An analyst must access all the raw data in its original form. Aside from the common
audit report requirement, the tools for analyzing this data are specialized and external to
the system.

Next steps
DevOps Checklist

Operational Excellence patterns
Article • 06/16/2023

Cloud applications run in a remote datacenter where you don't have full control of the
infrastructure or, in some cases, the operating system. This situation can make
management and monitoring more difficult than an on-premises deployment.
Applications must expose runtime information that administrators and operators can
use to manage and monitor the system. Applications must also support changing
business requirements and customization without requiring the application to be
stopped or redeployed.
Pattern

Summary

Ambassador

Create helper services that send network requests on behalf of a consumer
service or application.

Anticorruption
Layer

Implement a façade or adapter layer between a modern application and a
legacy system.

External
Configuration
Store

Move configuration information out of the application deployment package to
a centralized location.

Gateway
Aggregation

Use a gateway to aggregate multiple individual requests into a single request.

Gateway
Offloading

Offload shared or specialized service functionality to a gateway proxy.

Gateway
Routing

Route requests to multiple services using a single endpoint.

Geodes

Deploy backend services into a set of geographical nodes, each of which can
service any client request in any region.

Health
Endpoint
Monitoring

Implement functional checks in an application that external tools can access
through exposed endpoints at regular intervals.

Materialized
View

Generate prepopulated views over the data in one or more data stores when
the data isn't ideally formatted for required query operations.

Sidecar

Deploy components of an application into a separate process or container to
provide isolation and encapsulation.

Strangler Fig

Incrementally migrate a legacy system by gradually replacing specific pieces of
functionality with new applications and services.

Performance efficiency documentation
Apply the principles of scalability and testing to your architecture to meet users'
demands efficiently.

Key points

ｆ

QUICKSTART

Overview
Principles
Performance efficiency checklist
Test checklist
Optimize checklist
Performance efficiency patterns
Tradeoffs

ｄ

TRAINING

Performance efficiency

ｑ

VIDEO

Azure SQL Capacity Planning: Overview

Design for performance efficiency

ｂ

GET STARTED

Design applications for performance
Design applications for efficiency
Design applications for scalability
Design applications for capacity

ｐ

CONCEPT

Consider team expertise
Choose the right data storage
Reduce response time with asynchronous programming
Plan for growth
Scale out rather than scaling up

ｄ

TRAINING

Advance resilience through chaos engineering and fault injection

ｑ

VIDEO

Autoscale applications on Kubernetes with Kubernetes Event-Driven Autoscaling (KEDA)

Test for performance efficiency

ｅ

OVERVIEW

Performance testing
Load testing
Stress testing
Multiregion testing
Testing tools

ｐ

CONCEPT

Establish baselines
Configure the environment based on testing results
Consider caching data
Use a content delivery network

ｄ

TRAINING

Identify performance bottlenecks with Azure Load Testing
Identify performance regressions through automated load testing

ｑ

VIDEO

Scale your cloud app with Azure Cache for Redis

Monitor for performance efficiency

ｅ

OVERVIEW

Monitor distributed architectures
Consider application profiling
Analyze infrastructure metrics and logs
Integrate performance data
Monitor performance for scalability and reliability

ｐ

CONCEPT

Collect application logs
View platform metrics
Correlate and evaluate data
Consider cost implications

ｑ

VIDEO

Azure Application Insights Profiler

Optimize for performance efficiency

ｅ

OVERVIEW

Cache data for performance optimization
Partition data for performance optimization
Sustain performance efficiency over time

ｐ

CONCEPT

Consider capacity

Determine acceptable performance optimization
Optimize autoscaling
Partition critical workloads

ｑ

VIDEO

Optimize costs of your backups with Azure Backup

Performance tools and services

ｉ

REFERENCE

Application Insights
Azure Autoscale
Azure Blob Storage
Azure Content Delivery Network (CDN)
Azure Cosmos DB
Azure Front Door
Azure Load Testing
Azure Monitor
B-series virtual machines
Microsoft Well-Architected Review

Performance APIs

ｉ

REFERENCE

Azure CDN REST API
Azure Monitor REST API
Azure Cache for Redis API
Azure Storage REST API

Overview of the performance efficiency
pillar
Article • 05/30/2023

Performance efficiency is the ability of your workload to scale to meet the demands
placed on it by users in an efficient manner. Before cloud computing became popular,
when it came to planning how a system would handle increases in load, many
organizations intentionally provisioned oversized workloads to meet business
requirements. This decision made sense for on-premises environments because it
ensured capacity during peak usage. Capacity reflects resource availability (CPU and
memory). Capacity was a major consideration for processes that would be in place for
many years.
Just as you need to anticipate increases in load for on-premises environments, you need
to expect increases in cloud environments to meet business requirements. One
difference is that you might no longer need to make long-term predictions for expected
changes to ensure you'll have enough capacity in the future. Another difference is in the
approach used to manage performance.
To assess your workload using the tenets found in the Microsoft Azure Well-Architected
Framework, see the Microsoft Azure Well-Architected Review.
To boost performance efficiency, you should watch Performance Efficiency: Fast &
Furious: Optimizing for Quick and Reliable VM Deployments.

Articles
The performance efficiency pillar covers the following articles to help you effectively
scale your workload:
Performance

Description

efficiency
article
Performance
efficiency

Review your application architecture to ensure your workload scales to meet
the demands placed on it by users in an efficient manner.

checklist
Performance
principles

Learn about the principles that can guide you in your overall strategy for
improving performance efficiency.

Performance
efficiency

Description

article
Design for
performance

Review your application architecture from a performance design standpoint.

Consider
scalability

Plan for growth by understanding your current workloads.

Plan for capacity

Plan to scale your application tier by adding extra infrastructure to meet
demand.

Performance
monitoring
checklist

Monitor services and check the health state of current workloads to maintain
overall workload performance.

Performance
patterns

Implement design patterns to build more performant workloads.

Tradeoffs

Consider tradeoffs between performance optimization and other aspects of
the design, such as reliability, security, cost efficiency, and operability.

Next steps
Reference the performance efficiency principles intended to guide you in your overall
strategy.
Principles

Performance efficiency principles
Article • 11/22/2022

Performance efficiency is the ability of your workload to adjust to changes in demands
placed on it by users in an efficient manner. These principles are intended to guide you
in your overall strategy for improving performance efficiency.

Design for horizontal scaling
Horizontal scaling allows for elasticity. Instances are added (scale-out) or removed
(scale-in) in response to changes in load. Scaling out can improve resiliency by building
redundancy. Scaling in can help reduce costs by shutting down excess capacity.
Approach

Benefit

Define a capacity
model according to
the business
requirements

Test the limits for predicted and random spikes and fluctuations in load to
make sure the application can scale. Factor in the SKU service limits and
regional limits so that application scales as expected if there's a regional
failure.

Use PaaS offerings

Take advantage of the built-in capabilities that automatically trigger
scaling operations instead of investing in manual scaling efforts that often
require custom implementations and can be error prone.

Choose the right
resources and
right-size

Determine if the resources can support the anticipated load. Also, justify
the cost implications of the choices.

Apply strategies in
your design early

Accelerate adoption without significant changes. For example, strive for
stateless application and store state externally in a database or distributed
cache. Use caching where possible, to minimize the processing load.

An alternate approach is vertical scaling (scale up). However, you eventually may reach a
limit where there isn't a larger system, and you can't scale up anymore. At that point,
any further scaling must be horizontal. So it's good practice to employ a scale-out
architecture early on.

Shift-left on performance testing
Test early and test often to catch issues early.
Approach

Benefit

Approach

Benefit

Run load and
stress tests

Measure the application's performance under predetermined amounts of load
and also the maximum load your application and its infrastructure can
withstand.

Establish
performance

Determine the current efficiency of the application and its supporting
infrastructure. You'll be able to identify bottlenecks early before it worsens with

baselines

load. Also, this strategy can lead to strategies for improvements and determine
if the application is meeting the business goals.

Run the test in
the continuous

Detect issues early. Any development effort must go through continuous
performance testing to make sure changes to the codebase doesn't negatively

integration (CI)
build pipeline.

affect performance.

Continuously monitor for performance in
production
Observe the system holistically to evaluate the overall health of the solution. Capture the
test results not only in dev/test environment but also in production. Monitoring and
logging in production can help identify bottlenecks and opportunities for improvement.
Approach

Benefit

Monitor the health of

Know about the scalability and resiliency of the infrastructure, application,

the entire solution

and dependent services. Gather and review key performance counters
regularly.

Capture data from
repeatable processes

Evaluate the metrics over time that would allow for autoscaling with
demand. For reliability, look for early warning signs that might require
proactive intervention.

Reevaluate the needs

Identify improvement opportunities with resolution planning. This effort

of the workload

may require updated configurations and deprecations in favor of more-

continuously

appropriate solutions.

Next section
Use this checklist to review your application architecture from a performance design
standpoint.
Design checklist

Related links
Performance efficiency impacts the entire architecture spectrum and is interrelated
with other pillars of the Microsoft Azure Well-Architected Framework.
Assess your workload using the Microsoft Azure Well-Architected Review tool.

Challenges of monitoring distributed
architectures
Article • 05/30/2023

Most cloud deployments are based on distributed architectures where components are
distributed across various services. Troubleshooting monolithic applications often
requires only one or two lenses—the application and the database. With distributed
architectures, troubleshooting is complex and challenging because of various factors.
This article describes some of those challenges.

Key points
＂ The team might not have the expertise across all the services used in an
architecture.
＂ Uncovering and resolving bottlenecks by monitoring all of your services and their
infrastructure is complex.
＂ Antipatterns in design and code causes issues if the application is under pressure.
＂ Resilience in any service might impact your application's ability to meet current
load.

Team expertise
Distributed architectures require many areas of expertise. To adequately monitor
performance, it's critical that telemetry is captured throughout the application, across all
services, and is rich. Also, your team should have the necessary skills to troubleshoot all
services used in the architecture. When making technology choices, it's advantageous
and simple to choose a service over another because of the team's expertise. As the
collective skill set grows, you can incorporate other technologies.

Scaling issues
For monolithic applications, scale is two-dimensional. An application usually consists of
a group of application servers, some web front ends (WFEs), and database servers.
Uncovering bottlenecks is simpler but resolving them can require considerable effort. For
distributed applications, complexity increases exponentially in both aspects for
performance issues. You must consider each application, its supporting service, and the
latency between all the application layers.

Performance efficiency is a complex mixture of applications and infrastructure (IaaS and
PaaS). First, you must ensure that all services can scale to support the expected load and
that one service doesn't cause a bottleneck. Second, while performance testing, you
might realize that certain services scale under different load conditions as opposed to
scaling all services uniformly. Monitoring all of your services and their infrastructure can
help fine-tune your application for optimal performance.
For more information about monitoring for scalability, see Monitor performance for
scalability and reliability.

Antipatterns in design
Antipatterns in design and code are a common cause for performance problems when
an application is under pressure. For example, an application behaves as expected
during performance testing. However, when it's released to production and starts to
handle live workloads, performance decreases. Problems such as rejecting user requests,
stalling, or throwing exceptions might arise. To learn how to identify and fix these
antipatterns, see Performance antipatterns for cloud applications.

Fault tracking
If a service in the architecture fails, how does it affect your application's overall
performance? Is the error transient, allowing your application to continue to function.
Or, will the application experience a critical failure? If the error is transient, does your
application experience a decrease in performance?
Resiliency plays a significant role in performance efficiency because the failure of any
service can impact your application's ability to meet your business goals and scale to
meet current load. Chaos testing—the introduction of random failures within your
infrastructure—against your application can help determine how well your application
continues to perform under varying stages of load.
For more information about reliability impacts on performance, see Monitor
performance for scalability and reliability.

Next
Design scalable Azure applications

Community links
To learn more about chaos testing, see Advancing resilience through chaos engineering
and fault injection

.

Related links
Performance antipatterns for cloud applications
Monitor performance for scalability and reliability
Back to the main article

Design applications for performance
Article • 05/18/2023

Application design is critical to ensuring performance efficiency as load increases. This
article gives you insights on the most important aspects of designing applications for
performance. For more performance efficiency design related articles, see:
Design applications for efficiency
Design applications for scale
Design applications for capacity

Choose the right storage
The overall design of the storage tier can greatly affect an application's performance
and scalability. The Azure Storage platform is designed to be massively scalable to meet
the data storage and performance needs of modern applications.
Data services in the Azure Storage platform include:
Azure Blob, a massively scalable object store for text and binary data, which
includes support for big data analytics through Azure Data Lake Analytics .
Azure Files, managed file shares for cloud or on-premises deployments.
Azure Queue, a messaging store for reliable messaging between application
components.
Azure Tables, a NoSQL store for schemaless storage of structured data.
Azure Disks, block-level storage volumes for Azure virtual machines (VMs).
Each service is accessed through an Azure Storage account. To get started, see Create a
storage account.
Most cloud workloads adopt the polyglot persistence approach. Instead of one data
store service, a mix of technologies is used. Your application might require more than
one type of data store. For guidance about when to use different storage types, see
Sample scenarios for Azure Storage services.

Choose the right databases
The choice of database can affect an application's performance and scalability. Database
reads and writes involve network calls and storage input and output (I/O), both of which
are expensive. Choosing the right database service to store and retrieve data is therefore
critical to ensure application performance. Azure has many database services to fit most

needs. There are also third-party options from Azure Marketplace

that can be

considered.
To help choose a database type, determine whether the application's storage
requirements fit a relational design (SQL), or a key-value, document, or graph design
(NoSQL). Some applications might use both a SQL and a NoSQL database for different
storage needs.
For a detailed description of NoSQL and relational databases, see Understand
distributed NoSQL databases. See Select an Azure data store for your application to help
find appropriate managed data storage solutions.

Relational databases
Use a relational database when strong consistency guarantees are important. All
changes are atomic, and transactions always leave the data in a consistent state. A
relational database can't usually scale out horizontally without sharding the data, and
implementing manual sharding can be time consuming. Also, the data in relational
database must be normalized, which isn't appropriate for every data set.
If a relational database is optimal, Azure offers several platform as a service (PaaS)
options that fully manage database hosting and operations. Azure SQL can host single
databases or multiple databases through Azure SQL Managed Instance. Azure database
offerings span performance, scale, size, reliability, disaster recovery, and migration
compatibility requirements. Azure offers the following PaaS relational database services:
Azure SQL Database
Azure Database for MySQL
Azure Database for PostgreSQL
Azure Database for MariaDB

NoSQL databases
Use a NoSQL database when application performance and availability are more
important than strong consistency. NoSQL databases are ideal for handling large,
unrelated, indeterminate, or rapidly changing data. NoSQL databases also have
tradeoffs. For more information, see Understand distributed NoSQL database
challenges.
Azure provides two managed services that optimize for NoSQL solutions, Azure Cosmos
DB

and Azure Cache for Redis

. For document and graph databases, Azure Cosmos

DB offers extreme scale and performance.

Choose the right VM sizes
Choose VM sizing requirements based on your needs. Choosing the wrong VM size can
result in capacity issues as VMs approach their limits, or can lead to unnecessary costs.
To choose the right VM size, consider your workloads, number of CPUs, RAM capacity,
disk size, and speed according to business requirements. For more information about
Azure VM sizes and their purposes, see Sizes for virtual machines in Azure.
Azure offers the following categories of VM sizes, each designed to run different
workloads.
General-purpose provide balanced CPU-to-memory ratio ideal for testing and
development, small to medium databases, and low to medium traffic web servers.
Memory optimized offer a high memory-to-CPU ratio good for relational database
servers, medium to large caches, and in-memory analytics.
Compute optimized have a high CPU-to-memory ratio good for medium traffic
web servers, network appliances, batch processes, and application servers.
GPU optimized are available with single, multiple, or fractional GPUs designed for
compute-intensive, graphics-intensive, and visualization workloads.
High performance compute are designed to deliver leadership-class performance,
scalability, and cost efficiency for various real-world high performance computing
(HPC) workloads.
Storage optimized offer high disk throughput and I/O ideal for big data, SQL and
NoSQL databases, data warehousing, and large transactional databases. Example
databases include Cassandra, MongoDB, Cloudera, and Redis.

Use microservices architectures
Microservices are a popular architectural style for building applications that are resilient,
highly scalable, independently deployable, and able to evolve quickly. A microservices
architecture consists of a collection of small, autonomous services. Each service is selfcontained and implements a single business capability.
Breaking up larger entities into small discrete pieces doesn't ensure sizing and scaling
capabilities. Application logic must be written to control these capabilities. One benefit
of microservices is that they can be scaled independently. Independent scaling lets you
scale out subsystems that require more resources without scaling the entire application.
Another microservices benefit is fault isolation. If an individual microservice becomes
unavailable, it doesn't disrupt the entire application. Upstream microservices should be
designed to handle faults correctly, for example by implementing circuit breaking. To

learn more about the benefits of microservices, see Microservices architecture design
benefits.
Building with microservices has challenges for development and testing. Writing a small
service that relies on other dependent services requires a different approach than
writing a traditional monolithic or layered application. Existing tools aren't always
designed to work with service dependencies. Refactoring across service boundaries can
be difficult. It's also challenging to test service dependencies, especially when the
application is evolving quickly. For more information about the potential drawbacks of a
microservices architecture, see Microservices architecture design challenges.

Dynamic service discovery for microservices applications
In a microservices application, many separate services or instances of services in play
must receive instructions on who to contact and other configuration information. Hard
coding this information is flawed. Through service discovery, a service instance can spin
up and dynamically discover the configuration information it needs to become
functional without having the information hard coded.
Use an orchestration platform designed to execute and manage individual services, such
as Kubernetes or Service Fabric. Individual services can be right sized, scaled up, scaled
down, and dynamically configured to match user demand. An orchestrator like Azure
Kubernetes Service (AKS) or Azure Service Fabric can pack a higher density of services
onto a single host, which allows for more efficient resource utilization.
Both platforms provide built-in services for executing, scaling, and operating a
microservices architecture. These services include discovery and finding where a
particular service is running. AKS supports pod autoscaling and cluster autoscaling. For
more information, see Advanced AKS microservices architecture autoscaling. Service
Fabric architecture takes a different approach to scaling for stateless and stateful
services. For more information, see Azure Service Fabric scaling services.
 Tip
Decomposing an application into microservices is a level of decoupling that is an
architectural best practice when appropriate. A microservices architecture can also
bring some challenges. The design patterns in Design patterns for microservices
can help mitigate these challenges.

Use connection pooling

Establishing a connection to a database is an expensive operation that requires creating
an authenticated network connection to the remote database server. Database
connections are especially expensive for applications that open new connections
frequently. Connection pooling reuses existing connections and avoids the expense of
opening a new connection for each request. Connection pooling reduces connection
latency and enables higher database throughput (transactions per second) on the
server.

Pool size limits
Azure limits the number of network connections for a VM or Azure App Service instance.
Exceeding this limit causes connections to be slowed down or terminated. With
connection pooling, a fixed set of connections is established at startup time and
maintained. In many cases, a default pool size might consist only of a small handful of
connections that perform quickly in basic test scenarios.
The default pool size might become a bottleneck under scale when the pool is
exhausted. It's a best practice to establish a pool size that maps to the number of
concurrent transactions supported on each application instance.
Each database and application platform has slightly different requirements for the right
way to set up and use the pool. For a .NET code example that uses SQL Server and
Azure SQL Database, see SQL Server connection pooling. In all cases, testing is
important to ensure a connection pool is properly established and works as designed
under load.
 Tip
Choose a pool size that uses the same number of concurrent connections. Choose
a size that can handle more than the existing connections so you can quickly
handle a new incoming request.

Integrated security and connection pools
Integrated security is a single unified solution that uses a set of common policies and
configuration settings to protect every service that a business runs. Integrated security
reduces scaling, provisioning, and management issues, including higher costs and
complexity, and increases control and overall security.
However, sometimes you might not want to use connection pooling for security
reasons. For example, although connection pooling improves the performance of

subsequent database requests for a single user, that user can't take advantage of
connections made by other users. Connection pooling also results in at least one
connection per user to the database server.
Measure your business' security requirements against the advantages and
disadvantages of connection pooling. For more information, see Pool fragmentation.

Next steps
Design for efficiency

Design applications for efficiency
Article • 05/18/2023

Making choices that improve performance efficiency is critical to application design. This
article describes the most important aspects of designing efficient applications. For
more performance efficiency design related guidance, see Design for performance.

Reduce response time with asynchronous
programming
The time for a caller to receive a response can range from milliseconds to minutes.
During that time, the thread is held by the process until the response comes back or an
exception occurs. Holding the thread is inefficient, because no other requests can be
processed during the response wait time. An example of multiple requests in flight
being inefficient is a bank account where only one resource can operate on the request
at a time. Another example is when connection pools can't be shared, so all the requests
need separate connections to complete.
Asynchronous processing is an alternative approach that enables a remote service to be
executed without waiting and blocking resources on the client. Asynchronous
programming is a critical pattern for enabling performance efficiency, and is available in
most modern programming languages and platforms.
There are many ways to inject asynchronous programming into an application design.
For APIs and services that work across the internet, consider using the asynchronous
request-reply pattern. In code, remote calls can be asynchronously executed by using
built-in language constructs like async / await in .NET C#. For more information, see
Asynchronous programming with async and await.
The .NET platform has other built-in support for asynchronous programming. For more
information, see Task-based asynchronous pattern and Event-based asynchronous
pattern.

Process faster by queuing and batching
requests
Similar to asynchronous programming, queuing is a scalable mechanism to hand off
processing work to a service. Highly scalable queuing services are natively supported in
Azure. The queue is a storage buffer located between the caller and the processing

service. The queue takes requests, stores them in a buffer, and queues the requests to
provide services for the reliable delivery and management of the queued data.
A queue is often the best way to hand off work to a processing service. The processing
service receives work by listening on the queue and dequeuing messages. If items to be
processed enter too quickly, the queuing service keeps them in the queue until the
processing service has available resources and asks for a new message.
The processing service can use dynamic Azure Functions to easily autoscale on demand
as the queue builds up to meet the intake pressure. Developing processor logic with
Azure Functions to run task logic from a queue is a common, scalable, and cost effective
way to use queuing between a client and a processor.
Azure provides some native first-party queueing services. These services include Azure
Queue Storage, a simple queuing service based on Azure Storage, and Azure Service
Bus, a message broker service that supports transactions and reduced latency. Many
third-party options are also available through Azure Marketplace .
To learn more about queue-based load leveling, see Queue-based load leveling pattern.
Also see Storage queues and Service Bus queues compared and contrasted.

Optimize with data compression
A well-known optimization best practice is to use a compression strategy to compress
and bundle web pages or API responses. Compression shrinks the data returned from a
page or API back to the browser or client app. Compressing the data returned to clients
optimizes network traffic and accelerates the application.
Azure Front Door can do file compression, and .NET has built-in framework support for
this technique with gzip compression. For more information, see Response compression
in ASP.NET Core.

Improve performance with session affinity
If your application is stateful, and data or state is stored locally in the instance of the
application, enabling session affinity might improve application performance. When
session affinity is enabled, subsequent requests to the application are directed to the
same server that processed the first request. If session affinity isn't enabled, subsequent
requests are directed to the next available server, depending on the load balancing
rules.

Session affinity allows the instance to have some persistent or cached data or context,
which can speed subsequent requests. However, if your application doesn't store large
amounts of state or cached data in memory, session affinity might decrease your
throughput. One host could get overloaded with requests, while others are dormant.
 Tip
For information about best practices for stateless services when an application is
migrated from old Azure Cloud Services to Azure Service Fabric, see Migrate an
Azure Cloud Services application to Azure Service Fabric.

Run background jobs to meet integration
needs
Many types of applications require background tasks that run independently of the user
interface. Examples include batch jobs, intensive processing tasks, and long-running
processes such as workflows. Background jobs can be run without requiring user
interaction. The application can start the job and continue to process interactive
requests from users. For more information, see Background jobs.
Background tasks must perform adequately to ensure that they don't block the
application or cause inconsistencies due to delayed operation when the system is under
load. Performance can be improved by scaling the compute instances that host the
background tasks. For a list of considerations, see Background jobs scaling and
performance considerations.
Logic Apps is a serverless pay-per-use consumption service that enables a set of readyto-use out-of-the-box connectors and a long-running workflow engine to quickly meet
cloud-native integration needs. Logic Apps is flexible enough to support scenarios like
running tasks or jobs, advanced scheduling, and triggering.
Logic Apps also has advanced hosting options to allow it to run within enterprise
restricted cloud environments. Logic Apps can be combined with and complement other
Azure services, or can be used independently.
Like all serverless services, Logic Apps doesn't require VM instances to be purchased,
enabled, or scaled up and down. Logic Apps scales automatically on serverless platform
as a service (PaaS)-provided instances, and you pay only for usage.

Next steps

Design for scaling

Design applications for scaling
Article • 05/18/2023

Scalability, an important aspect of performance efficiency, is the ability of a system to
resize to handle changing load. This article describes the types of scaling, how to plan
for growth, and how to use platform autoscaling features to manage load.
To achieve scalability, consider how your application scales, and implement platform as
a service (PaaS) offerings that have built-in scaling operations. Azure services that
autoscale can scale automatically to match demand to workload. For example, these
services automatically scale out to ensure capacity during workload peaks, and return to
normal automatically when the load drops. For more information about autoscaling, see
Use autoscaling to manage load.

Understand horizontal and vertical scaling
Two ways an application can scale include vertical scaling and horizontal scaling. Vertical
scaling, or scaling up, increases the capacity of a resource, for example by using a larger
virtual machine (VM) size. Horizontal scaling, or scaling out, adds new instances of a
resource, such as VMs or database replicas.
An advantage of vertical scaling is that you can scale up without changing the
application. But at some point, you hit a limit where you can't scale up anymore. Any
further scaling must then be horizontal.
Horizontal scaling has the following significant advantages over vertical scaling:
True cloud scale applications can be designed to run on hundreds or even
thousands of nodes, reaching scales that aren't possible on a single node.
Horizontal scale is elastic, so you can add more instances if load increases, or
remove instances during quieter periods.
Scaling out can be triggered automatically, either on a schedule or in response to
changes in load.
Scaling out might be cheaper than scaling up, because running several small VMs
can cost less than running a single large VM.
Horizontal scaling can improve reliability by adding redundancy. If an instance
goes down, the application keeps running.
Horizontal scale must be designed into the system. For example, you can scale out VMs
by placing them behind a load balancer. But each VM in the pool must handle any client
request, so the application must be stateless or store state externally, as in a distributed

cache. Managed PaaS services often have horizontal scaling and autoscaling built in. The
ease of scaling is a major advantage of using PaaS services.
Simply adding more instances doesn't mean an application can scale. Adding more
instances could push a bottleneck somewhere else. For example, scaling a web front end
to handle more client requests might trigger lock contentions in the database. You need
to consider measures such as optimistic concurrency or data partitioning to enable
more throughput to the database.
Always conduct performance and load testing to find potential bottlenecks. Resolving
one bottleneck might reveal other bottlenecks elsewhere. The stateful parts of a system,
such as databases, are the most common cause of bottlenecks, and require careful
design to scale horizontally.

Plan for growth
Use the Performance efficiency checklist to review your design from a scalability
standpoint. For more information about how to determine the upper and maximum
limits of an application's capacity, see Performance testing.
In the cloud, the ability to take advantage of scalability depends on your infrastructure
and services. Platforms like Kubernetes were built with scaling in mind. VMs might not
scale as easily, although scale operations are possible. With VMs, you might want to
plan ahead to avoid scaling infrastructure in the future. Another option is to select a
different platform such as virtual machine scale sets.
Planning for growth starts with understanding your current workloads, which can help
you anticipate scaling needs based on predictive usage scenarios. An example of a
predictive usage scenario is an e-commerce site that recognizes that its infrastructure
should scale appropriately for an expected high volume of holiday traffic.
Carry out load tests and stress tests to determine the necessary infrastructure to support
predicted spikes in workloads. A good plan includes incorporating a buffer to
accommodate random spikes. With scalable platforms, you can predict the current
average and peak times for your workload and manage this prediction with payment
plans. You pay either per minute or per hour, depending on the service.
Another critical component of planning for scale is to make sure the region that host
your application supports the necessary capacity to accommodate the load increase. If
you use a multiregion architecture, make sure the secondary regions can also support
the increase.

A region might offer a product, but might not have the necessary SKUs to support the
predicted load increase, so you need to verify capacity. To verify your regions and
available SKUs, first select the product and regions in Products available by region .
Then, use the Azure portal to check the SKUs that are available.



Determine scale units
For each resource, know the upper scaling limits, and use sharding or decomposition if
you need to go beyond those limits. Design the application so that it's easily scaled by
adding one or more scale units, such as by using the deployment stamps pattern.
Use built-in scaling features or tools to understand which resources need to scale
concurrently with other resources. Then use well-defined sets of resources to determine
the scale units for the system. For example, adding X number of front-end VMs might
require Y number of extra queues and Z number of storage accounts to handle the extra
workload. So a scale unit could consist of X VM instances, Y queues, and Z storage
accounts.

Use autoscaling to manage load

Autoscaling lets you automatically run the right amount of resources to handle your app
load. Autoscaling adds resources, or scales out, to handle increases in load such as
seasonal workloads. Autoscaling also removes idle resources, or scales in, to save money
during decreases in load, such as nights and weekends for some corporate apps.
Autoscaling automatically scales between the minimum and maximum number of
instances to run, and automatically adds or removes VMs based on a set of rules.

Understand autoscaling delays
Horizontal scaling changes the number of identical instances, and vertical scaling
switches to more or less powerful instances. Scaling operations can be fast, but they
take time to complete. It's important to understand how the delay affects the
application under load, and whether degraded performance is acceptable.
For more information, see Best practices for autoscale.

Use platform autoscaling features
To get the greatest benefit from autoscaling, follow these practices:
Use built-in autoscaling features when possible, rather than custom or third-party
mechanisms.
Use scheduled scaling rules where possible, to ensure that resources are available.
Add reactive autoscaling to the rules where appropriate, to cope with unexpected
changes in demand.
７ Note
If your application is explicitly designed to handle the termination of some of its
instances, use autoscaling to reduce operational costs. Scale down and scale in
resources that are no longer necessary for the load.
For more information, see Autoscaling.

Autoscale compute or memory-intensive applications
CPU or memory-intensive applications can require scaling up to a larger machine SKU
with more CPU or memory. Once the demand for CPU or memory decreases, instances
can revert back to the original size.
For example, you might have an application that processes images, videos, or music.
While scaling out allows the system to process more files simultaneously, it doesn't
improve the processing speed for each individual file. Given the process and
requirements, it might make sense to scale up a server by adding CPU or memory to
quickly process a large media file.

Use Azure services that autoscale
Autoscaling works by collecting resource metrics like CPU and memory usage, and
application metrics like requests queued and requests per second. Rules can then be
created to add and remove instances depending on how the rule evaluates.
An Azure App Service plan allows autoscale rules to be set for scale out/scale in and
scale up/scale down. The App Service autoscaling sample

shows how to create an

Azure App Service plan, which includes an Azure App Service. Scaling also applies to
Azure Automation.
Azure Kubernetes Service (AKS) offers two levels of autoscale:
Horizontal autoscale can be enabled on service containers to add more or fewer
pod instances within the cluster.
Cluster autoscale can be enabled on the agent VM instances running an agent
node pool to add or remove VM instances dynamically.
The following Azure services also offer autoscaling capability:
Azure Service Fabric with virtual machine scale sets offers autoscale capabilities for
true infrastructure as a service (IaaS) scenarios.
Azure Application Gateway and Azure API Management are PaaS offerings for
ingress services that enable autoscale.
Azure Functions, Azure Logic Apps, and Azure App Services provide serverless payper-use consumption modeling that inherently provides autoscaling capabilities.
Azure SQL Database is a PaaS platform that changes performance characteristics of
a database on the fly and assigns more resources when needed or releases them
when not needed. SQL Database allows scaling up/down, read scale-out, and
global scale-out/sharding capabilities.

Each service documents its autoscale capabilities. For a general discussion about Azure
platform autoscaling, see Overview of autoscale in Azure.
７ Note
If your application doesn't have built-in autoscale ability, or isn't configured to scale
out automatically as load increases, its services might fail if they become saturated
with user requests. For possible solutions, see Set up a scaling tool by using Azure
Automation and Azure Logic Apps for Azure Virtual Desktop.

Next steps
Design for capacity

Design applications for capacity
Article • 05/18/2023

This article describes ways you can design applications for capacity to optimize
performance efficiency. Azure offers many options to meet capacity requirements while
minimizing costs as your business grows.

Scale out rather than scaling up
With cloud technologies, it's often easier, cheaper, and more effective to scale out, or
add resources, than to scale up, or increase the capacity of a resource. If you plan to
scale up by increasing the capacity allocated to your hosts, you reach a limit where it
becomes cost-prohibitive to scale any further. Scaling up also often requires downtime
for servers to reboot. Instead, plan to scale your application tier by adding extra
infrastructure to meet demand. Be sure to remove the resources when they're no longer
needed.

Prepare infrastructure for large-scale events
Large-scale application design takes careful planning and possibly complex
implementation. Work with your business and marketing teams to prepare for largescale events like sports matches, sales, or marketing campaigns. You can anticipate
sudden spikes in traffic caused by these events to prepare infrastructure ahead of time.
A fundamental Azure design principle is to scale out by adding machines or service
instances based on increased demand. Scaling out can be better than purchasing more
hardware that might not be in your budget. Depending on your payment plan, you
don't have to pay for idle virtual machines (VMs), or need to reserve capacity in advance.
A pay-as-you-go plan can be ideal for applications that need to meet planned spikes in
traffic.
７ Note
Don't plan for capacity to always meet the highest level of expected demand. An
inappropriate or misconfigured service can impact cost. For example, building a
multiregion service when the service levels don't require high availability or
georedundancy increases cost without reasonable business justification.

Choose the right resources
Right-sizing your infrastructure to meet the needs of your applications can save costs
compared to the one-size-fits-all solution often employed with on-premises hardware.
You can choose various options when you deploy Azure VMs to support workloads.
Each VM type has specific features and different combinations of CPU, memory, and
disks. For example, B-Series VMs are ideal for workloads that don't need full CPU
performance continuously, like web servers, proofs of concept, small databases, and
development build environments. B-Series VMs offer a cost effective way to deploy
workloads that have bursts in performance and don't need full continuous CPU
performance. For a list of VM sizes and recommended uses, see Sizes for virtual
machines in Azure.
Continually monitor workloads to find out if your VMs aren't optimized or have frequent
unused periods. It makes sense to either downscale these VMs by using virtual machine
scale sets, or shut them down. You can optimize VMs with Azure Automation, virtual
machine scale sets, autoshutdown, or scripted or third-party solutions. For more
information, see Automate VM optimization.
Along with choosing the right VMs, selecting the right storage type can save your
organization significant monthly costs. For a list of storage data types, access tiers,
storage account types, and storage redundancy options, see Select the right storage.

Use metrics to fine-tune scaling
It can be difficult to understand the relationship between metrics and capacity
requirements, especially when an application is initially deployed. Provision a little extra
capacity at the beginning, and then monitor and tune the autoscaling rules to bring
capacity closer to the actual load.
Autoscaling lets you run the right amount of resources to handle your app load by
adding resources or scaling out to handle increases in load such as seasonal workloads
and customer-facing applications. After you configure the autoscaling rules, monitor the
performance of your application over time. Use the results of this monitoring to adjust
how the system scales if necessary.
Azure Monitor autoscale provides a common set of autoscaling functionality for virtual
machine scale sets and Azure App Service. Scaling can be done on a schedule, or based
on a runtime metric, such as CPU or memory usage. For example, you can scale out by
one instance if average CPU usage is above 70%, and scale in by one instance if CPU
usage falls below 50 percent.

The default autoscaling rules execute an autoscaling action at the correct time to
prevent the system from reacting too quickly. For more information, see Autoscaling.
For a list of built-in metrics, see Azure Monitor autoscaling common metrics. You can
also implement custom metrics by using Application Insights to monitor the
performance of your live applications. Some Azure services use different scaling
methods.

Scale preemptively and schedule autoscaling
Preemptive scaling based on historical data can help ensure your application performs
consistently, even though your metrics haven't yet indicated the need to scale.
Schedule-based rules allow you to scale when you see time patterns in your load, and
you want to scale before a possible load increase or decrease occurs. For example, you
can set a trigger attribute to scale out to 10 instances on weekdays, and scale in to four
instances on weekends.
If you can predict the load on the application, consider using scheduled autoscaling,
which adds and removes instances to meet anticipated peaks in demand. For more
information, see Use Azure Monitor autoscale.

Test capacity planning
The business should communicate any fluctuation in expected load for performance
testing. Load can be impacted by world events, such as political, economic, or weather
changes. Load can also be affected by marketing initiatives, such as sales or promotions,
or by seasonal events like holidays.
Test load variations prior to events, including unexpected load variations, to ensure that
your application can scale. You should also ensure that all regions can adequately scale
to support total load if one region fails.

Next steps
Performance testing

Capacity best practices
Article • 05/18/2023

The following practices help you maintain sufficient capacity to meet your application's
performance efficiency needs.

Use Content Delivery Networks
Content Delivery Networks (CDNs) cache and deliver static objects from Azure Blob
Storage, web applications, or any publicly accessible web server by using the closest
point of presence (POP) server. CDNs can also accelerate dynamic content, which can't
be cached, by using various network and routing optimizations. For more information,
see What is a content delivery network on Azure.

Prepare for large-scale events
Work with your business and marketing teams to prepare for large-scale events like
sports matches, sales, or marketing campaigns. You can anticipate sudden spikes in
traffic caused by these events to prepare infrastructure ahead of time.

Choose the right resources
Right sizing your infrastructure to meet the needs of your applications can save costs
compared to the one-size-fits-all solution often employed with on-premises hardware.
Identify the needs of your application and choose the resources that best fit those
needs. For more information, see Sizes for virtual machines in Azure.

Use metrics to adjust scaling policies
Autoscaling rules can use a detection mechanism based on a measured trigger attribute,
such as CPU usage or queue length. These rules use an aggregated value over time,
rather than instantaneous values, to trigger an autoscaling action. By default, the
aggregate is an average of the values, which prevents the system from reacting too
quickly or causing rapid oscillation. For more information, see Use Azure Monitor
autoscale.

Preemptively scale based on trends

Preemptive scaling based on historical data can ensure your application has consistent
performance, even though your metrics haven't yet indicated the need to scale. If you
can predict the load on the application, consider using scheduled autoscaling, which
adds and removes instances to meet anticipated peaks in demand. For more
information, see Autoscaling.

Automate scale operations
Fluctuation in application traffic is expected. To ensure optimal operation conditions are
maintained, such variations should be met by automated scalability operations.
Autoscaling enables a platform as a service (PaaS) or infrastructure as a service (IaaS) to
scale within a preconfigured range of resources. However, provisioning or
deprovisioning capacity, for example adding scale units like clusters, compute instances,
or deployments, is more advanced and complex. The process should be codified and
automated, and the effects of adding or removing capacity should be well understood.
For more information, see Repeatable infrastructure.

Monitor application health
Any change in the health state of application components can influence the capacity
demands on other components. These impacts need to be fully understood, and
autoscaling measures must be in place to handle the impacts. For example, if an outage
in an external API is mitigated by writing messages into a retry queue, this queue needs
to handle the sudden spikes in load.

