AWS Well-
Architected Framework
AWS Well-
Architected Framework

AWS Well-Architected Framework
AWS Well-Architected Framework
Copyright © 2023 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.
Amazon's trademarks and trade dress may not be used in connection with any product or service that is not
Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or
discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may
or may not be aﬃliated with, connected to, or sponsored by Amazon.

AWS Well-Architected Framework
Table of Contents
Abstract and introduction ................................................................................................................... 1
Introduction .............................................................................................................................. 1
Deﬁnitions ................................................................................................................................. 1
On architecture .......................................................................................................................... 3
General design principles ............................................................................................................ 4
The pillars of the framework ............................................................................................................... 5
Operational excellence ................................................................................................................ 5
Design principles ................................................................................................................ 5
Deﬁnition .......................................................................................................................... 6
Best practices .................................................................................................................... 6
Resources ........................................................................................................................ 12
Security ................................................................................................................................... 12
Design principles .............................................................................................................. 12
Deﬁnition ........................................................................................................................ 13
Best practices .................................................................................................................. 13
Resources ........................................................................................................................ 19
Reliability ................................................................................................................................ 20
Design principles .............................................................................................................. 20
Deﬁnition ........................................................................................................................ 20
Best practices .................................................................................................................. 21
Resources ........................................................................................................................ 24
Performance eﬃciency .............................................................................................................. 24
Design principles .............................................................................................................. 25
Deﬁnition ........................................................................................................................ 25
Best practices .................................................................................................................. 26
Resources ........................................................................................................................ 30
Cost optimization ..................................................................................................................... 30
Design principles .............................................................................................................. 31
Deﬁnition ........................................................................................................................ 31
Best practices .................................................................................................................. 32
Resources ........................................................................................................................ 36
Sustainability ........................................................................................................................... 36
Design principles .............................................................................................................. 36
Deﬁnition ........................................................................................................................ 37
Best practices .................................................................................................................. 37
Resources ........................................................................................................................ 41
The review process ........................................................................................................................... 43
Conclusion ....................................................................................................................................... 45
Contributors .................................................................................................................................... 46
Further reading ................................................................................................................................ 47
Document revisions .......................................................................................................................... 48
Appendix: Questions and best practices .............................................................................................. 50
Operational excellence ............................................................................................................. 50
Organization ................................................................................................................... 50
Prepare .......................................................................................................................... 73
Operate ........................................................................................................................ 114
Evolve .......................................................................................................................... 140
Security ................................................................................................................................. 151
Security foundations ....................................................................................................... 151
Identity and access management ...................................................................................... 163
Detection ....................................................................................................................... 191
Infrastructure protection ................................................................................................. 197
Data protection .............................................................................................................. 209
Incident response ........................................................................................................... 223
iii

AWS Well-Architected Framework
Application security ........................................................................................................ 233
Reliability .............................................................................................................................. 246
Foundations ................................................................................................................... 246
Workload architecture ..................................................................................................... 272
Change management ...................................................................................................... 299
Failure management ....................................................................................................... 319
Performance eﬃciency ............................................................................................................ 378
Selection ....................................................................................................................... 378
Review .......................................................................................................................... 437
Monitoring ..................................................................................................................... 441
Tradeoﬀs ....................................................................................................................... 448
Cost optimization ................................................................................................................... 454
Practice Cloud Financial Management ............................................................................... 454
Expenditure and usage awareness ..................................................................................... 468
Cost-eﬀective resources ................................................................................................... 495
Manage demand and supply resources .............................................................................. 514
Optimize over time ......................................................................................................... 521
Sustainability ......................................................................................................................... 526
Region selection ............................................................................................................. 527
Alignment to demand ..................................................................................................... 528
Software and architecture ............................................................................................... 537
Data .............................................................................................................................. 543
Hardware and services .................................................................................................... 556
Process and culture ......................................................................................................... 562
Notices .......................................................................................................................................... 568
AWS glossary ................................................................................................................................. 569
iv

AWS Well-Architected Framework
Introduction
AWS Well-Architected Framework
Publication date: April 10, 2023 (Document revisions (p. 48))
The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make
while building systems on AWS. By using the Framework you will learn architectural best practices for
designing and operating reliable, secure, eﬃcient, cost-eﬀective, and sustainable systems in the cloud.
Introduction
The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make
while building systems on AWS. Using the Framework helps you learn architectural best practices for
designing and operating secure, reliable, eﬃcient, cost-eﬀective, and sustainable workloads in the AWS
Cloud. It provides a way for you to consistently measure your architectures against best practices and
identify areas for improvement. The process for reviewing an architecture is a constructive conversation
about architectural decisions, and is not an audit mechanism. We believe that having well-architected
systems greatly increases the likelihood of business success.
AWS Solutions Architects have years of experience architecting solutions across a wide variety
of business verticals and use cases. We have helped design and review thousands of customers’
architectures on AWS. From this experience, we have identiﬁed best practices and core strategies for
architecting systems in the cloud.
The AWS Well-Architected Framework documents a set of foundational questions that help you to
understand if a speciﬁc architecture aligns well with cloud best practices. The framework provides a
consistent approach to evaluating systems against the qualities you expect from modern cloud-based
systems, and the remediation that would be required to achieve those qualities. As AWS continues to
evolve, and we continue to learn more from working with our customers, we will continue to reﬁne the
deﬁnition of well-architected.
This framework is intended for those in technology roles, such as chief technology oﬃcers (CTOs),
architects, developers, and operations team members. It describes AWS best practices and strategies
to use when designing and operating a cloud workload, and provides links to further implementation
details and architectural patterns. For more information, see the AWS Well-Architected homepage.
AWS also provides a service for reviewing your workloads at no charge. The AWS Well-Architected
Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and
measure your architecture using the AWS Well-Architected Framework. The AWS WA Tool provides
recommendations for making your workloads more reliable, secure, eﬃcient, and cost-eﬀective.
To help you apply best practices, we have created AWS Well-Architected Labs, which provides you with
a repository of code and documentation to give you hands-on experience implementing best practices.
We also have teamed up with select AWS Partner Network (APN) Partners, who are members of the AWS
Well-Architected Partner program. These AWS Partners have deep AWS knowledge, and can help you
review and improve your workloads.
Deﬁnitions
Every day, experts at AWS assist customers in architecting systems to take advantage of best practices
in the cloud. We work with you on making architectural trade-oﬀs as your designs evolve. As you deploy
1

AWS Well-Architected Framework
Deﬁnitions
these systems into live environments, we learn how well these systems perform and the consequences of
those trade-oﬀs.
Based on what we have learned, we have created the AWS Well-Architected Framework, which provides
a consistent set of best practices for customers and partners to evaluate architectures, and provides a set
of questions you can use to evaluate how well an architecture is aligned to AWS best practices.
The AWS Well-Architected Framework is based on six pillars — operational excellence, security, reliability,
performance eﬃciency, cost optimization, and sustainability.
Table 1. The pillars of the AWS Well-Architected Framework
Name
Description
Operational excellence
The ability to support development and run
workloads eﬀectively, gain insight into their
operations, and to continuously improve
supporting processes and procedures to deliver
business value.
Security
The security pillar describes how to take
advantage of cloud technologies to protect data,
systems, and assets in a way that can improve
your security posture.
Reliability
The reliability pillar encompasses the ability of
a workload to perform its intended function
correctly and consistently when it’s expected to.
This includes the ability to operate and test the
workload through its total lifecycle. This paper
provides in-depth, best practice guidance for
implementing reliable workloads on AWS.
Performance eﬃciency
The ability to use computing resources eﬃciently
to meet system requirements, and to maintain
that eﬃciency as demand changes and
technologies evolve.
Cost optimization
Sustainability
The ability to run systems to deliver business
value at the lowest price point.
The ability to continually improve sustainability
impacts by reducing energy consumption and
increasing eﬃciency across all components of a
workload by maximizing the beneﬁts from the
provisioned resources and minimizing the total
resources required.
In the AWS Well-Architected Framework, we use these terms:
• A component is the code, conﬁguration, and AWS Resources that together deliver against a
requirement. A component is often the unit of technical ownership, and is decoupled from other
components.
• The term workload is used to identify a set of components that together deliver business value. A
workload is usually the level of detail that business and technology leaders communicate about.
• We think about architecture as being how components work together in a workload. How components
communicate and interact is often the focus of architecture diagrams.
2

AWS Well-Architected Framework
On architecture
• Milestones mark key changes in your architecture as it evolves throughout the product lifecycle
(design, implementation, testing, go live, and in production).
• Within an organization the technology portfolio is the collection of workloads that are required for
the business to operate.
• The level of eﬀort is categorizing the amount of time, eﬀort, and complexity a task requires for
implementation. Each organization needs to consider the size and expertise of the team and the
complexity of the workload for additional context to properly categorize the level of eﬀort for the
organization.
• High: The work might take multiple weeks or multiple months. This could be broken out into
multiple stories, releases, and tasks.
• Medium: The work might take multiple days or multiple weeks. This could be broken out into
multiple releases and tasks.
• Low: The work might take multiple hours or multiple days. This could be broken out into multiple
tasks.
When architecting workloads, you make trade-oﬀs between pillars based on your business context. These
business decisions can drive your engineering priorities. You might optimize to improve sustainability
impact and reduce cost at the expense of reliability in development environments, or, for mission-critical
solutions, you might optimize reliability with increased costs and sustainability impact. In ecommerce
solutions, performance can aﬀect revenue and customer propensity to buy. Security and operational
excellence are generally not traded-oﬀ against the other pillars.
On architecture
In on-premises environments, customers often have a central team for technology architecture that acts
as an overlay to other product or feature teams to verify they are following best practice. Technology
architecture teams typically include a set of roles such as: Technical Architect (infrastructure), Solutions
Architect (software), Data Architect, Networking Architect, and Security Architect. Often these teams use
TOGAF or the Zachman Framework as part of an enterprise architecture capability.
At AWS, we prefer to distribute capabilities into teams rather than having a centralized team with
that capability. There are risks when you choose to distribute decision making authority, for example,
verifying that teams are meeting internal standards. We mitigate these risks in two ways. First, we have
practices (ways of doing things, process, standards, and accepted norms) that focus on allowing each
team to have that capability, and we put in place experts who verify that teams raise the bar on the
standards they need to meet. Second, we implement mechanisms that carry out automated checks to
verify standards are being met.
“Good intentions never work, you need good mechanisms to make anything happen” — Jeﬀ
Bezos.
This means replacing a human's best eﬀorts with mechanisms (often automated) that check for
compliance with rules or process. This distributed approach is supported by the Amazon leadership
principles, and establishes a culture across all roles that works back from the customer. Working
backward is a fundamental part of our innovation process. We start with the customer and what they
want, and let that deﬁne and guide our eﬀorts. Customer-obsessed teams build products in response to
a customer need.
For architecture, this means that we expect every team to have the capability to create architectures
and to follow best practices. To help new teams gain these capabilities or existing teams to raise their
bar, we activate access to a virtual community of principal engineers who can review their designs and
help them understand what AWS best practices are. The principal engineering community works to make
best practices visible and accessible. One way they do this, for example, is through lunchtime talks that
focus on applying best practices to real examples. These talks are recorded and can be used as part of
onboarding materials for new team members.
3

AWS Well-Architected Framework
General design principles
AWS best practices emerge from our experience running thousands of systems at internet scale. We
prefer to use data to deﬁne best practice, but we also use subject matter experts, like principal engineers,
to set them. As principal engineers see new best practices emerge, they work as a community to verify
that teams follow them. In time, these best practices are formalized into our internal review processes,
and also into mechanisms that enforce compliance. The Well-Architected Framework is the customer-
facing implementation of our internal review process, where we have codiﬁed our principal engineering
thinking across ﬁeld roles, like Solutions Architecture and internal engineering teams. The Well-
Architected Framework is a scalable mechanism that lets you take advantage of these learnings.
By following the approach of a principal engineering community with distributed ownership of
architecture, we believe that a Well-Architected enterprise architecture can emerge that is driven by
customer need. Technology leaders (such as a CTOs or development managers), carrying out Well-
Architected reviews across all your workloads will permit you to better understand the risks in your
technology portfolio. Using this approach, you can identify themes across teams that your organization
could address by mechanisms, training, or lunchtime talks where your principal engineers can share their
thinking on speciﬁc areas with multiple teams.
General design principles
The Well-Architected Framework identiﬁes a set of general design principles to facilitate good design in
the cloud:
• Stop guessing your capacity needs: If you make a poor capacity decision when deploying a workload,
you might end up sitting on expensive idle resources or dealing with the performance implications of
limited capacity. With cloud computing, these problems can go away. You can use as much or as little
capacity as you need, and scale up and down automatically.
• Test systems at production scale: In the cloud, you can create a production-scale test environment on
demand, complete your testing, and then decommission the resources. Because you only pay for the
test environment when it's running, you can simulate your live environment for a fraction of the cost
of testing on premises.
• Automate with architectural experimentation in mind: Automation permits you to create and
replicate your workloads at low cost and avoid the expense of manual eﬀort. You can track changes to
your automation, audit the impact, and revert to previous parameters when necessary.
• Consider evolutionary architectures: In a traditional environment, architectural decisions are often
implemented as static, onetime events, with a few major versions of a system during its lifetime.
As a business and its context continue to evolve, these initial decisions might hinder the system's
ability to deliver changing business requirements. In the cloud, the capability to automate and test on
demand lowers the risk of impact from design changes. This permits systems to evolve over time so
that businesses can take advantage of innovations as a standard practice.
• Drive architectures using data: In the cloud, you can collect data on how your architectural choices
aﬀect the behavior of your workload. This lets you make fact-based decisions on how to improve
your workload. Your cloud infrastructure is code, so you can use that data to inform your architecture
choices and improvements over time.
• Improve through game days: Test how your architecture and processes perform by regularly
scheduling game days to simulate events in production. This will help you understand where
improvements can be made and can help develop organizational experience in dealing with events.
4

AWS Well-Architected Framework
Operational excellence
The pillars of the framework
Creating a software system is a lot like constructing a building. If the foundation is not solid, structural
problems can undermine the integrity and function of the building. When architecting technology
solutions, if you neglect the six pillars of operational excellence, security, reliability, performance
eﬃciency, cost optimization, and sustainability, it can become challenging to build a system that delivers
on your expectations and requirements. Incorporating these pillars into your architecture will help you
produce stable and eﬃcient systems. This will allow you to focus on the other aspects of design, such as
functional requirements.
Pillars
• Operational excellence (p. 5)
• Security (p. 12)
• Reliability (p. 20)
• Performance eﬃciency (p. 24)
• Cost optimization (p. 30)
• Sustainability (p. 36)
Operational excellence
The Operational Excellence pillar includes the ability to support development and run workloads
eﬀectively, gain insight into their operations, and to continuously improve supporting processes and
procedures to deliver business value.
The operational excellence pillar provides an overview of design principles, best practices, and questions.
You can ﬁnd prescriptive guidance on implementation in the Operational Excellence Pillar whitepaper.
Topics
• Design principles (p. 5)
• Deﬁnition (p. 6)
• Best practices (p. 6)
• Resources (p. 12)
Design principles
There are ﬁve design principles for operational excellence in the cloud:
• Perform operations as code: In the cloud, you can apply the same engineering discipline that you use
for application code to your entire environment. You can deﬁne your entire workload (applications,
infrastructure) as code and update it with code. You can implement your operations procedures
as code and automate their run process by initiating them in response to events. By performing
operations as code, you limit human error and achieve consistent responses to events.
• Make frequent, small, reversible changes: Design workloads to permit components to be updated
regularly. Make changes in small increments that can be reversed if they fail (without aﬀecting
customers when possible).
5

AWS Well-Architected Framework
Deﬁnition
• Reﬁne operations procedures frequently: As you use operations procedures, look for opportunities
to improve them. As you evolve your workload, evolve your procedures appropriately. Set up regular
game days to review and validate that all procedures are eﬀective and that teams are familiar with
them.
• Anticipate failure: Perform “pre-mortem” exercises to identify potential sources of failure so that
they can be removed or mitigated. Test your failure scenarios and validate your understanding of their
impact. Test your response procedures to verify that they are eﬀective, and that teams are familiar
with their process. Set up regular game days to test workloads and team responses to simulated
events.
• Learn from all operational failures: Drive improvement through lessons learned from all operational
events and failures. Share what is learned across teams and through the entire organization.
Deﬁnition
There are four best practice areas for operational excellence in the cloud:
• Organization
• Prepare
• Operate
• Evolve
Your organization’s leadership deﬁnes business objectives. Your organization must understand
requirements and priorities and use these to organize and conduct work to support the achievement of
business outcomes. Your workload must emit the information necessary to support it. Implementing
services to achieve integration, deployment, and delivery of your workload will create an increased ﬂow
of beneﬁcial changes into production by automating repetitive processes.
There may be risks inherent in the operation of your workload. Understand those risks and make an
informed decision to enter production. Your teams must be able to support your workload. Business and
operational metrics derived from desired business outcomes will permit you to understand the health of
your workload, your operations activities, and respond to incidents. Your priorities will change as your
business needs and business environment changes. Use these as a feedback loop to continually drive
improvement for your organization and the operation of your workload.
Best practices
Topics
• Organization (p. 6)
• Prepare (p. 9)
• Operate (p. 10)
• Evolve (p. 11)
Organization
Your teams must have a shared understanding of your entire workload, their role in it, and shared
business goals to set the priorities that will achieve business success. Well-deﬁned priorities will
maximize the beneﬁts of your eﬀorts. Evaluate internal and external customer needs involving key
stakeholders, including business, development, and operations teams, to determine where to focus
eﬀorts. Evaluating customer needs will verify that you have a thorough understanding of the support
that is required to achieve business outcomes. Verify that you are aware of guidelines or obligations
6

AWS Well-Architected Framework
Best practices
deﬁned by your organizational governance and external factors, such as regulatory compliance
requirements and industry standards that may mandate or emphasize speciﬁc focus. Validate that you
have mechanisms to identify changes to internal governance and external compliance requirements.
If no requirements are identiﬁed, validate that you have applied due diligence to this determination.
Review your priorities regularly so that they can be updated as needs change.
Evaluate threats to the business (for example, business risk and liabilities, and information security
threats) and maintain this information in a risk registry. Evaluate the impact of risks, and tradeoﬀs
between competing interests or alternative approaches. For example, accelerating speed to market for
new features may be emphasized over cost optimization, or you may choose a relational database for
non-relational data to simplify the eﬀort to migrate a system without refactoring. Manage beneﬁts and
risks to make informed decisions when determining where to focus eﬀorts. Some risks or choices may be
acceptable for a time, it may be possible to mitigate associated risks, or it may become unacceptable to
permit a risk to remain, in which case you will take action to address the risk.
Your teams must understand their part in achieving business outcomes. Teams must understand their
roles in the success of other teams, the role of other teams in their success, and have shared goals.
Understanding responsibility, ownership, how decisions are made, and who has authority to make
decisions will help focus eﬀorts and maximize the beneﬁts from your teams. The needs of a team
will be shaped by the customer they support, their organization, the makeup of the team, and the
characteristics of their workload. It's unreasonable to expect a single operating model to be able to
support all teams and their workloads in your organization.
Verify that there are identiﬁed owners for each application, workload, platform, and infrastructure
component, and that each process and procedure has an identiﬁed owner responsible for its deﬁnition,
and owners responsible for their performance.
Having understanding of the business value of each component, process, and procedure, of why those
resources are in place or activities are performed, and why that ownership exists will inform the actions
of your team members. Clearly deﬁne the responsibilities of team members so that they may act
appropriately and have mechanisms to identify responsibility and ownership. Have mechanisms to
request additions, changes, and exceptions so that you do not constrain innovation. Deﬁne agreements
between teams describing how they work together to support each other and your business outcomes.
Provide support for your team members so that they can be more eﬀective in taking action and
supporting your business outcomes. Engaged senior leadership should set expectations and measure
success. Senior leadership should be the sponsor, advocate, and driver for the adoption of best
practices and evolution of the organization. Let team members take action when outcomes are at
risk to minimize impact and encourage them to escalate to decision makers and stakeholders when
they believe there is a risk so that it can be addressed and incidents avoided. Provide timely, clear, and
actionable communications of known risks and planned events so that team members can take timely
and appropriate action.
Encourage experimentation to accelerate learning and keep team members interested and engaged.
Teams must grow their skill sets to adopt new technologies, and to support changes in demand and
responsibilities. Support and encourage this by providing dedicated structured time for learning. Verify
that your team members have the resources, both tools and team members, to be successful and scale
to support your business outcomes. Leverage cross-organizational diversity to seek multiple unique
perspectives. Use this perspective to increase innovation, challenge your assumptions, and reduce the
risk of conﬁrmation bias. Grow inclusion, diversity, and accessibility within your teams to gain beneﬁcial
perspectives.
If there are external regulatory or compliance requirements that apply to your organization, you
should use the resources provided by AWS Cloud Compliance to help educate your teams so that they
can determine the impact on your priorities. The Well-Architected Framework emphasizes learning,
measuring, and improving. It provides a consistent approach for you to evaluate architectures, and
implement designs that will scale over time. AWS provides the AWS Well-Architected Tool to help you
review your approach before development, the state of your workloads before production, and the
7

AWS Well-Architected Framework
Best practices
state of your workloads in production. You can compare workloads to the latest AWS architectural best
practices, monitor their overall status, and gain insight into potential risks. AWS Trusted Advisor is a tool
that provides access to a core set of checks that recommend optimizations that may help shape your
priorities. Business and Enterprise Support customers receive access to additional checks focusing on
security, reliability, performance, cost-optimization, and sustainability that can further help shape their
priorities.
AWS can help you educate your teams about AWS and its services to increase their understanding of
how their choices can have an impact on your workload. Use the resources provided by AWS Support
(AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS Documentation
to educate your teams. Reach out to AWS Support through AWS Support Center for help with your AWS
questions. AWS also shares best practices and patterns that we have learned through the operation of
AWS in The Amazon Builders' Library. A wide variety of other useful information is available through the
AWS Blog and The Oﬃcial AWS Podcast. AWS Training and Certiﬁcation provides some training through
self-paced digital courses on AWS fundamentals. You can also register for instructor-led training to
further support the development of your teams’ AWS skills.
Use tools or services that permit you to centrally govern your environments across accounts, such as
AWS Organizations, to help manage your operating models. Services like AWS Control Tower expand
this management capability by allowing you to deﬁne blueprints (supporting your operating models) for
the setup of accounts, apply ongoing governance using AWS Organizations, and automate provisioning
of new accounts. Managed Services providers such as AWS Managed Services, AWS Managed Services
Partners, or Managed Services Providers in the AWS Partner Network, provide expertise implementing
cloud environments, and support your security and compliance requirements and business goals. Adding
Managed Services to your operating model can save you time and resources, and lets you keep your
internal teams lean and focused on strategic outcomes that will diﬀerentiate your business, rather than
developing new skills and capabilities.
The following questions focus on these considerations for operational excellence. (For a list of
operational excellence questions and best practices, see the Appendix (p. 50).)
OPS 1: How do you determine what your priorities are?
Everyone must understand their part in achieving business success. Have shared goals in order to set
priorities for resources. This will maximize the beneﬁts of your eﬀorts.
OPS 2: How do you structure your organization to support your business outcomes?
Your teams must understand their part in achieving business outcomes. Teams must understand their
roles in the success of other teams, the role of other teams in their success, and have shared goals.
Understanding responsibility, ownership, how decisions are made, and who has authority to make
decisions will help focus eﬀorts and maximize the beneﬁts from your teams.
OPS 3: How does your organizational culture support your business outcomes?
Provide support for your team members so that they can be more eﬀective in taking action and
supporting your business outcome.
You might ﬁnd that you want to emphasize a small subset of your priorities at some point in time.
Use a balanced approach over the long term to verify the development of needed capabilities and
management of risk. Review your priorities regularly and update them as needs change. When
responsibility and ownership are undeﬁned or unknown, you are at risk of both not performing necessary
8

AWS Well-Architected Framework
Best practices
action in a timely fashion and of redundant and potentially conﬂicting eﬀorts emerging to address
those needs. Organizational culture has a direct impact on team member job satisfaction and retention.
Activate the engagement and capabilities of your team members to achieve the success of your business.
Experimentation is required for innovation to happen and turn ideas into outcomes. Recognize that an
undesired result is a successful experiment that has identiﬁed a path that will not lead to success.
Prepare
To prepare for operational excellence, you have to understand your workloads and their expected
behaviors. You will then be able to design them to provide insight to their status and build the
procedures to support them.
Design your workload so that it provides the information necessary for you to understand its internal
state (for example, metrics, logs, events, and traces) across all components in support of observability
and investigating issues. Iterate to develop the telemetry necessary to monitor the health of your
workload, identify when outcomes are at risk, and activate eﬀective responses. When instrumenting your
workload, capture a broad set of information to achieve situational awareness (for example, changes in
state, user activity, permission access, utilization counters), knowing that you can use ﬁlters to select the
most useful information over time.
Adopt approaches that improve the ﬂow of changes into production and that achieves refactoring, fast
feedback on quality, and bug ﬁxing. These accelerate beneﬁcial changes entering production, limit issues
deployed, and activate rapid identiﬁcation and remediation of issues introduced through deployment
activities or discovered in your environments.
Adopt approaches that provide fast feedback on quality and achieves rapid recovery from changes
that do not have desired outcomes. Using these practices mitigates the impact of issues introduced
through the deployment of changes. Plan for unsuccessful changes so that you are able to respond
faster if necessary and test and validate the changes you make. Be aware of planned activities in your
environments so that you can manage the risk of changes impacting planned activities. Emphasize
frequent, small, reversible changes to limit the scope of change. This results in faster troubleshooting
and remediation with the option to roll back a change. It also means you are able to get the beneﬁt of
valuable changes more frequently.
Evaluate the operational readiness of your workload, processes, procedures, and personnel to
understand the operational risks related to your workload. Use a consistent process (including manual or
automated checklists) to know when you are ready to go live with your workload or a change. This will
also help you to ﬁnd any areas that you must make plans to address. Have runbooks that document your
routine activities and playbooks that guide your processes for issue resolution. Understand the beneﬁts
and risks to make informed decisions to permit changes to enter production.
AWS allows you to view your entire workload (applications, infrastructure, policy, governance, and
operations) as code. This means you can apply the same engineering discipline that you use for
application code to every element of your stack and share these across teams or organizations to
magnify the beneﬁts of development eﬀorts. Use operations as code in the cloud and the ability to
safely experiment to develop your workload, your operations procedures, and practice failure. Using AWS
CloudFormation allows you to have consistent, templated, sandbox development, test, and production
environments with increasing levels of operations control.
The following questions focus on these considerations for operational excellence.
OPS 4: How do you design your workload so that you can understand its state?
Design your workload so that it provides the information necessary across all components (for
example, metrics, logs, and traces) for you to understand its internal state. This allows you to provide
eﬀective responses when appropriate.
9

AWS Well-Architected Framework
Best practices
OPS 5: How do you reduce defects, ease remediation, and improve ﬂow into production?
Adopt approaches that improve ﬂow of changes into production that achieve refactoring fast feedback
on quality, and bug ﬁxing. These accelerate beneﬁcial changes entering production, limit issues
deployed, and achieve rapid identiﬁcation and remediation of issues introduced through deployment
activities.
OPS 6: How do you mitigate deployment risks?
Adopt approaches that provide fast feedback on quality and achieve rapid recovery from changes that
do not have desired outcomes. Using these practices mitigates the impact of issues introduced through
the deployment of changes.
OPS 7: How do you know that you are ready to support a workload?
Evaluate the operational readiness of your workload, processes and procedures, and personnel to
understand the operational risks related to your workload.
Invest in implementing operations activities as code to maximize the productivity of operations
personnel, minimize error rates, and achieve automated responses. Use “pre-mortems” to anticipate
failure and create procedures where appropriate. Apply metadata using Resource Tags and AWS Resource
Groups following a consistent tagging strategy to achieve identiﬁcation of your resources. Tag your
resources for organization, cost accounting, access controls, and targeting the running of automated
operations activities. Adopt deployment practices that take advantage of the elasticity of the cloud to
facilitate development activities, and pre-deployment of systems for faster implementations. When
you make changes to the checklists you use to evaluate your workloads, plan what you will do with live
systems that no longer comply.
Operate
Successful operation of a workload is measured by the achievement of business and customer outcomes.
Deﬁne expected outcomes, determine how success will be measured, and identify metrics that will be
used in those calculations to determine if your workload and operations are successful. Operational
health includes both the health of the workload and the health and success of the operations activities
performed in support of the workload (for example, deployment and incident response). Establish
metrics baselines for improvement, investigation, and intervention, collect and analyze your metrics,
and then validate your understanding of operations success and how it changes over time. Use
collected metrics to determine if you are satisfying customer and business needs, and identify areas for
improvement.
Eﬃcient and eﬀective management of operational events is required to achieve operational excellence.
This applies to both planned and unplanned operational events. Use established runbooks for well-
understood events, and use playbooks to aid in investigation and resolution of issues. Prioritize
responses to events based on their business and customer impact. Verify that if an alert is raised in
response to an event, there is an associated process to run with a speciﬁcally identiﬁed owner. Deﬁne
in advance the personnel required to resolve an event and include escalation processes to engage
additional personnel, as it becomes necessary, based on urgency and impact. Identify and engage
individuals with the authority to make a decision on courses of action where there will be a business
impact from an event response not previously addressed.
Communicate the operational status of workloads through dashboards and notiﬁcations that are tailored
to the target audience (for example, customer, business, developers, operations) so that they may take
10

AWS Well-Architected Framework
Best practices
appropriate action, so that their expectations are managed, and so that they are informed when normal
operations resume.
In AWS, you can generate dashboard views of your metrics collected from workloads and natively from
AWS. You can leverage CloudWatch or third-party applications to aggregate and present business,
workload, and operations level views of operations activities. AWS provides workload insights through
logging capabilities including AWS X-Ray, CloudWatch, CloudTrail, and VPC Flow Logs to identify
workload issues in support of root cause analysis and remediation.
The following questions focus on these considerations for operational excellence.
OPS 8: How do you understand the health of your workload?
Deﬁne, capture, and analyze workload metrics to gain visibility to workload events so that you can take
appropriate action.
OPS 9: How do you understand the health of your operations?
Deﬁne, capture, and analyze operations metrics to gain visibility to operations events so that you can
take appropriate action.
OPS 10: How do you manage workload and operations events?
Prepare and validate procedures for responding to events to minimize their disruption to your
workload.
All of the metrics you collect should be aligned to a business need and the outcomes they support.
Develop scripted responses to well-understood events and automate their performance in response to
recognizing the event.
Evolve
Learn, share, and continuously improve to sustain operational excellence. Dedicate work cycles to
making nearly continuous incremental improvements. Perform post-incident analysis of all customer
impacting events. Identify the contributing factors and preventative action to limit or prevent recurrence.
Communicate contributing factors with aﬀected communities as appropriate. Regularly evaluate
and prioritize opportunities for improvement (for example, feature requests, issue remediation, and
compliance requirements), including both the workload and operations procedures.
Include feedback loops within your procedures to rapidly identify areas for improvement and capture
learnings from running operations.
Share lessons learned across teams to share the beneﬁts of those lessons. Analyze trends within lessons
learned and perform cross-team retrospective analysis of operations metrics to identify opportunities
and methods for improvement. Implement changes intended to bring about improvement and evaluate
the results to determine success.
On AWS, you can export your log data to Amazon S3 or send logs directly to Amazon S3 for long-term
storage. Using AWS Glue, you can discover and prepare your log data in Amazon S3 for analytics, and
store associated metadata in the AWS Glue Data Catalog. Amazon Athena, through its native integration
with AWS Glue, can then be used to analyze your log data, querying it using standard SQL. Using a
business intelligence tool like Amazon QuickSight, you can visualize, explore, and analyze your data.
Discovering trends and events of interest that may drive improvement.
11

AWS Well-Architected Framework
Resources
The following question focuses on these considerations for operational excellence.
OPS 11: How do you evolve operations?
Dedicate time and resources for nearly continuous incremental improvement to evolve the
eﬀectiveness and eﬃciency of your operations.
Successful evolution of operations is founded in: frequent small improvements; providing safe
environments and time to experiment, develop, and test improvements; and environments in which
learning from failures is encouraged. Operations support for sandbox, development, test, and production
environments, with increasing level of operational controls, facilitates development and increases the
predictability of successful results from changes deployed into production.
Resources
Refer to the following resources to learn more about our best practices for Operational Excellence.
Documentation
• DevOps and AWS
Whitepaper
• Operational Excellence Pillar
Video
• DevOps at Amazon
Security
The Security pillar encompasses the ability to protect data, systems, and assets to take advantage of
cloud technologies to improve your security.
The security pillar provides an overview of design principles, best practices, and questions. You can ﬁnd
prescriptive guidance on implementation in the Security Pillar whitepaper.
Topics
• Design principles (p. 12)
• Deﬁnition (p. 13)
• Best practices (p. 13)
• Resources (p. 19)
Design principles
In the cloud, there are a number of principles that can help you strengthen your workload security:
• Implement a strong identity foundation: Implement the principle of least privilege and enforce
separation of duties with appropriate authorization for each interaction with your AWS resources.
Centralize identity management, and aim to eliminate reliance on long-term static credentials.
12

AWS Well-Architected Framework
Deﬁnition
• Maintain traceability: Monitor, alert, and audit actions and changes to your environment in real time.
Integrate log and metric collection with systems to automatically investigate and take action.
• Apply security at all layers: Apply a defense in depth approach with multiple security controls. Apply
to all layers (for example, edge of network, VPC, load balancing, every instance and compute service,
operating system, application, and code).
• Automate security best practices: Automated software-based security mechanisms improve your
ability to securely scale more rapidly and cost-eﬀectively. Create secure architectures, including the
implementation of controls that are deﬁned and managed as code in version-controlled templates.
• Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such
as encryption, tokenization, and access control where appropriate.
• Keep people away from data: Use mechanisms and tools to reduce or eliminate the need for direct
access or manual processing of data. This reduces the risk of mishandling or modiﬁcation and human
error when handling sensitive data.
• Prepare for security events: Prepare for an incident by having incident management and investigation
policy and processes that align to your organizational requirements. Run incident response simulations
and use tools with automation to increase your speed for detection, investigation, and recovery.
Deﬁnition
There are seven best practice areas for security in the cloud:
• Security foundations
• Identity and access management
• Detection
• Infrastructure protection
• Data protection
• Incident response
• Application security
Before you architect any workload, you need to put in place practices that inﬂuence security. You
will want to control who can do what. In addition, you want to be able to identify security incidents,
protect your systems and services, and maintain the conﬁdentiality and integrity of data through data
protection. You should have a well-deﬁned and practiced process for responding to security incidents.
These tools and techniques are important because they support objectives such as preventing ﬁnancial
loss or complying with regulatory obligations.
The AWS Shared Responsibility Model helps organizations that adopt the cloud to achieve their security
and compliance goals. Because AWS physically secures the infrastructure that supports our cloud
services, as an AWS customer you can focus on using services to accomplish your goals. The AWS Cloud
also provides greater access to security data and an automated approach to responding to security
events.
Best practices
Topics
• Security (p. 14)
• Identity and access management (p. 14)
• Detection (p. 16)
• Infrastructure protection (p. 16)
• Data protection (p. 17)
• Incident response (p. 18)
13

AWS Well-Architected Framework
Best practices
• Application security (p. 18)
Security
The following question focuses on these considerations for security. (For a list of security questions and
best practices, see the Appendix (p. 151).).
SEC 1: How do you securely operate your workload?
To operate your workload securely, you must apply overarching best practices to every area of security.
Take requirements and processes that you have deﬁned in operational excellence at an organizational
and workload level, and apply them to all areas.
Staying up to date with recommendations from AWS, industry sources, and threat intelligence helps
you evolve your threat model and control objectives. Automating security processes, testing, and
validation allow you to scale your security operations.
In AWS, segregating diﬀerent workloads by account, based on their function and compliance or data
sensitivity requirements, is a recommended approach.
Identity and access management
Identity and access management are key parts of an information security program, ensuring that only
authorized and authenticated users and components are able to access your resources, and only in a
manner that you intend. For example, you should deﬁne principals (that is, accounts, users, roles, and
services that can perform actions in your account), build out policies aligned with these principals, and
implement strong credential management. These privilege-management elements form the core of
authentication and authorization.
In AWS, privilege management is primarily supported by the AWS Identity and Access Management (IAM)
service, which allows you to control user and programmatic access to AWS services and resources. You
should apply granular policies, which assign permissions to a user, group, role, or resource. You also
have the ability to require strong password practices, such as complexity level, avoiding re-use, and
enforcing multi-factor authentication (MFA). You can use federation with your existing directory service.
For workloads that require systems to have access to AWS, IAM allows for secure access through roles,
instance proﬁles, identity federation, and temporary credentials.
The following questions focus on these considerations for security.
SEC 2: How do you manage identities for people and machines?
There are two types of identities you need to manage when approaching operating secure AWS
workloads. Understanding the type of identity you need to manage and grant access helps you verify
the right identities have access to the right resources under the right conditions.
Human Identities: Your administrators, developers, operators, and end users require an identity to
access your AWS environments and applications. These are members of your organization, or external
users with whom you collaborate, and who interact with your AWS resources via a web browser, client
application, or interactive command line tools.
Machine Identities: Your service applications, operational tools, and workloads require an identity to
make requests to AWS services, for example, to read data. These identities include machines running
in your AWS environment such as Amazon EC2 instances or AWS Lambda functions. You may also
manage machine identities for external parties who need access. Additionally, you may also have
machines outside of AWS that need access to your AWS environment.
14

AWS Well-Architected Framework
Best practices
SEC 3: How do you manage permissions for people and machines?
Manage permissions to control access to people and machine identities that require access to AWS and
your workload. Permissions control who can access what, and under what conditions.
Credentials must not be shared between any user or system. User access should be granted using
a least-privilege approach with best practices including password requirements and MFA enforced.
Programmatic access, including API calls to AWS services, should be performed using temporary and
limited-privilege credentials, such as those issued by the AWS Security Token Service.
Users need programmatic access if they want to interact with AWS outside of the AWS Management
Console. The way to grant programmatic access depends on the type of user that's accessing AWS.
To grant users programmatic access, choose one of the following options.
Which user needs
programmatic access?
Workforce identity
(Users managed in IAM Identity
Center)
To
Use temporary credentials to
sign programmatic requests to
the AWS CLI, AWS SDKs, or AWS
APIs.
By
Following the instructions for
the interface that you want to
use.
• For the AWS CLI, see
Conﬁguring the AWS CLI to
use AWS IAM Identity Center
(successor to AWS Single Sign-
On) in the AWS Command Line
Interface User Guide.
• For AWS SDKs, tools, and
AWS APIs, see IAM Identity
Center authentication in the
AWS SDKs and Tools Reference
Guide.
IAM
Use temporary credentials to
sign programmatic requests to
the AWS CLI, AWS SDKs, or AWS
APIs.
IAM
(Not recommended)
Use long-term credentials to
sign programmatic requests to
the AWS CLI, AWS SDKs, or AWS
APIs.
Following the instructions in
Using temporary credentials
with AWS resources in the IAM
User Guide.
Following the instructions for
the interface that you want to
use.
• For the AWS CLI, see
Authenticating using IAM
user credentials in the AWS
Command Line Interface User
Guide.
• For AWS SDKs and tools, see
Authenticate using long-term
credentials in the AWS SDKs
and Tools Reference Guide.
• For AWS APIs, see Managing
access keys for IAM users in
the IAM User Guide.
15

AWS Well-Architected Framework
Best practices
AWS provides resources that can help you with identity and access management. To help learn best
practices, explore our hands-on labs on managing credentials & authentication, controlling human
access, and controlling programmatic access.
Detection
You can use detective controls to identify a potential security threat or incident. They are an essential
part of governance frameworks and can be used to support a quality process, a legal or compliance
obligation, and for threat identiﬁcation and response eﬀorts. There are diﬀerent types of detective
controls. For example, conducting an inventory of assets and their detailed attributes promotes more
eﬀective decision making (and lifecycle controls) to help establish operational baselines. You can also
use internal auditing, an examination of controls related to information systems, to verify that practices
meet policies and requirements and that you have set the correct automated alerting notiﬁcations based
on deﬁned conditions. These controls are important reactive factors that can help your organization
identify and understand the scope of anomalous activity.
In AWS, you can implement detective controls by processing logs, events, and monitoring that allows
for auditing, automated analysis, and alarming. CloudTrail logs, AWS API calls, and CloudWatch provide
monitoring of metrics with alarming, and AWS Conﬁg provides conﬁguration history. Amazon GuardDuty
is a managed threat detection service that continuously monitors for malicious or unauthorized behavior
to help you protect your AWS accounts and workloads. Service-level logs are also available, for example,
you can use Amazon Simple Storage Service (Amazon S3) to log access requests.
The following question focuses on these considerations for security.
SEC 4: How do you detect and investigate security events?
Capture and analyze events from logs and metrics to gain visibility. Take action on security events and
potential threats to help secure your workload.
Log management is important to a Well-Architected workload for reasons ranging from security
or forensics to regulatory or legal requirements. It is critical that you analyze logs and respond to
them so that you can identify potential security incidents. AWS provides functionality that makes log
management easier to implement by giving you the ability to deﬁne a data-retention lifecycle or deﬁne
where data will be preserved, archived, or eventually deleted. This makes predictable and reliable data
handling simpler and more cost eﬀective.
Infrastructure protection
Infrastructure protection encompasses control methodologies, such as defense in depth, necessary to
meet best practices and organizational or regulatory obligations. Use of these methodologies is critical
for successful, ongoing operations in either the cloud or on-premises.
In AWS, you can implement stateful and stateless packet inspection, either by using AWS-native
technologies or by using partner products and services available through the AWS Marketplace. You
should use Amazon Virtual Private Cloud (Amazon VPC) to create a private, secured, and scalable
environment in which you can deﬁne your topology—including gateways, routing tables, and public and
private subnets.
The following questions focus on these considerations for security.
SEC 5: How do you protect your network resources?
Any workload that has some form of network connectivity, whether it’s the internet or a private
network, requires multiple layers of defense to help protect from external and internal network-based
threats.
16

AWS Well-Architected Framework
Best practices
SEC 6: How do you protect your compute resources?
Compute resources in your workload require multiple layers of defense to help protect from external
and internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions,
database services, IoT devices, and more.
Multiple layers of defense are advisable in any type of environment. In the case of infrastructure
protection, many of the concepts and methods are valid across cloud and on-premises models. Enforcing
boundary protection, monitoring points of ingress and egress, and comprehensive logging, monitoring,
and alerting are all essential to an eﬀective information security plan.
AWS customers are able to tailor, or harden, the conﬁguration of an Amazon Elastic Compute Cloud
(Amazon EC2), Amazon Elastic Container Service (Amazon ECS) container, or AWS Elastic Beanstalk
instance, and persist this conﬁguration to an immutable Amazon Machine Image (AMI). Then, whether
launched by Auto Scaling or launched manually, all new virtual servers (instances) launched with this AMI
receive the hardened conﬁguration.
Data protection
Before architecting any system, foundational practices that inﬂuence security should be in place.
For example, data classiﬁcation provides a way to categorize organizational data based on levels of
sensitivity, and encryption protects data by way of rendering it unintelligible to unauthorized access.
These tools and techniques are important because they support objectives such as preventing ﬁnancial
loss or complying with regulatory obligations.
In AWS, the following practices facilitate protection of data:
• As an AWS customer you maintain full control over your data.
• AWS makes it easier for you to encrypt your data and manage keys, including regular key rotation,
which can be easily automated by AWS or maintained by you.
• Detailed logging that contains important content, such as ﬁle access and changes, is available.
• AWS has designed storage systems for exceptional resiliency. For example, Amazon S3 Standard,
S3 Standard–IA, S3 One Zone-IA, and Amazon Glacier are all designed to provide 99.999999999%
durability of objects over a given year. This durability level corresponds to an average annual expected
loss of 0.000000001% of objects.
• Versioning, which can be part of a larger data lifecycle management process, can protect against
accidental overwrites, deletes, and similar harm.
• AWS never initiates the movement of data between Regions. Content placed in a Region will remain in
that Region unless you explicitly use a feature or leverage a service that provides that functionality.
The following questions focus on these considerations for security.
SEC 7: How do you classify your data?
Classiﬁcation provides a way to categorize data, based on criticality and sensitivity in order to help you
determine appropriate protection and retention controls.
SEC 8: How do you protect your data at rest?
Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access
or mishandling.
17

AWS Well-Architected Framework
Best practices
SEC 9: How do you protect your data in transit?
Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized
access or loss.
AWS provides multiple means for encrypting data at rest and in transit. We build features into our
services that make it easier to encrypt your data. For example, we have implemented server-side
encryption (SSE) for Amazon S3 to make it easier for you to store your data in an encrypted form. You
can also arrange for the entire HTTPS encryption and decryption process (generally known as SSL
termination) to be handled by Elastic Load Balancing (ELB).
Incident response
Even with extremely mature preventive and detective controls, your organization should still put
processes in place to respond to and mitigate the potential impact of security incidents. The architecture
of your workload strongly aﬀects the ability of your teams to operate eﬀectively during an incident, to
isolate or contain systems, and to restore operations to a known good state. Putting in place the tools
and access ahead of a security incident, then routinely practicing incident response through game days,
will help you verify that your architecture can accommodate timely investigation and recovery.
In AWS, the following practices facilitate eﬀective incident response:
• Detailed logging is available that contains important content, such as ﬁle access and changes.
• Events can be automatically processed and launch tools that automate responses through the use of
AWS APIs.
• You can pre-provision tooling and a “clean room” using AWS CloudFormation. This allows you to carry
out forensics in a safe, isolated environment.
The following question focuses on these considerations for security.
SEC 10: How do you anticipate, respond to, and recover from incidents?
Preparation is critical to timely and eﬀective investigation, response to, and recovery from security
incidents to help minimize disruption to your organization.
Verify that you have a way to quickly grant access for your security team, and automate the isolation of
instances as well as the capturing of data and state for forensics.
Application security
Application security (AppSec) describes the overall process of how you design, build, and test the
security properties of the workloads you develop. You should have appropriately trained people in
your organization, understand the security properties of your build and release infrastructure, and use
automation to identify security issues.
Adopting application security testing as a regular part of your software development lifecycle (SDLC) and
post release processes help validate that you have a structured mechanism to identify, ﬁx, and prevent
application security issues entering your production environment.
Your application development methodology should include security controls as you design, build, deploy,
and operate your workloads. While doing so, align the process for continuous defect reduction and
18

AWS Well-Architected Framework
Resources
minimizing technical debt. For example, using threat modeling in the design phase helps you uncover
design ﬂaws early, which makes them easier and less costly to ﬁx as opposed to waiting and mitigating
them later.
The cost and complexity to resolve defects is typically lower the earlier you are in the SDLC. The easiest
way to resolve issues is to not have them in the ﬁrst place, which is why starting with a threat model
helps you focus on the right outcomes from the design phase. As your AppSec program matures, you can
increase the amount of testing that is performed using automation, improve the ﬁdelity of feedback to
builders, and reduce the time needed for security reviews. All of these actions improve the quality of the
software you build, and increase the speed of delivering features into production.
These implementation guidelines focus on four areas: organization and culture, security of the pipeline,
security in the pipeline, and dependency management. Each area provides a set of principles that you
can implement and provides an end-to-end view of how you design, develop, build, deploy, and operate
workloads.
In AWS, there are a number of approaches you can use when addressing your application security
program. Some of these approaches rely on technology while others focus on the people and
organizational aspects of your application security program.
The following question focuses on these considerations for application security.
SEC 11: How do you incorporate and validate the security properties of applications throughout
the design, development, and deployment lifecycle?
Training people, testing using automation, understanding dependencies, and validating the security
properties of tools and applications help to reduce the likelihood of security issues in production
workloads.
Resources
Refer to the following resources to learn more about our best practices for Security.
Documentation
• AWS Cloud Security
• AWS Compliance
• AWS Security Blog
• AWS Security Maturity Model
Whitepaper
• Security Pillar
• AWS Security Overview
• AWS Risk and Compliance
Video
• AWS Security State of the Union
• Shared Responsibility Overview
19

AWS Well-Architected Framework
Reliability
Reliability
The Reliability pillar encompasses the ability of a workload to perform its intended function correctly
and consistently when it’s expected to. This includes the ability to operate and test the workload
through its total lifecycle. This paper provides in-depth, best practice guidance for implementing reliable
workloads on AWS.
The reliability pillar provides an overview of design principles, best practices, and questions. You can ﬁnd
prescriptive guidance on implementation in the Reliability Pillar whitepaper.
Topics
• Design principles (p. 20)
• Deﬁnition (p. 20)
• Best practices (p. 21)
• Resources (p. 24)
Design principles
There are ﬁve design principles for reliability in the cloud:
• Automatically recover from failure: By monitoring a workload for key performance indicators
(KPIs), you can start automation when a threshold is breached. These KPIs should be a measure of
business value, not of the technical aspects of the operation of the service. This provides for automatic
notiﬁcation and tracking of failures, and for automated recovery processes that work around or repair
the failure. With more sophisticated automation, it’s possible to anticipate and remediate failures
before they occur.
• Test recovery procedures: In an on-premises environment, testing is often conducted to prove
that the workload works in a particular scenario. Testing is not typically used to validate recovery
strategies. In the cloud, you can test how your workload fails, and you can validate your recovery
procedures. You can use automation to simulate diﬀerent failures or to recreate scenarios that led to
failures before. This approach exposes failure pathways that you can test and ﬁx before a real failure
scenario occurs, thus reducing risk.
• Scale horizontally to increase aggregate workload availability: Replace one large resource with
multiple small resources to reduce the impact of a single failure on the overall workload. Distribute
requests across multiple, smaller resources to verify that they don’t share a common point of failure.
• Stop guessing capacity: A common cause of failure in on-premises workloads is resource saturation,
when the demands placed on a workload exceed the capacity of that workload (this is often the
objective of denial of service attacks). In the cloud, you can monitor demand and workload utilization,
and automate the addition or removal of resources to maintain the more eﬃcient level to satisfy
demand without over- or under-provisioning. There are still limits, but some quotas can be controlled
and others can be managed (see Manage Service Quotas and Constraints).
• Manage change in automation: Changes to your infrastructure should be made using automation. The
changes that must be managed include changes to the automation, which then can be tracked and
reviewed.
Deﬁnition
There are four best practice areas for reliability in the cloud:
• Foundations
• Workload architecture
20

AWS Well-Architected Framework
Best practices
• Change management
• Failure management
To achieve reliability, you must start with the foundations — an environment where Service Quotas and
network topology accommodate the workload. The workload architecture of the distributed system
must be designed to prevent and mitigate failures. The workload must handle changes in demand or
requirements, and it must be designed to detect failure and automatically heal itself.
Best practices
Topics
• Foundations (p. 21)
• Workload architecture (p. 22)
• Change management (p. 22)
• Failure management (p. 23)
Foundations
Foundational requirements are those whose scope extends beyond a single workload or project. Before
architecting any system, foundational requirements that inﬂuence reliability should be in place. For
example, you must have suﬃcient network bandwidth to your data center.
With AWS, most of these foundational requirements are already incorporated or can be addressed
as needed. The cloud is designed to be nearly limitless, so it’s the responsibility of AWS to satisfy the
requirement for suﬃcient networking and compute capacity, permitting you to change resource size and
allocations on demand.
The following questions focus on these considerations for reliability. (For a list of reliability questions and
best practices, see the Appendix (p. 246).).
REL 1: How do you manage Service Quotas and constraints?
For cloud-based workload architectures, there are Service Quotas (which are also referred to as service
limits). These quotas exist to prevent accidentally provisioning more resources than you need and
to limit request rates on API operations so as to protect services from abuse. There are also resource
constraints, for example, the rate that you can push bits down a ﬁber-optic cable, or the amount of
storage on a physical disk.
REL 2: How do you plan your network topology?
Workloads often exist in multiple environments. These include multiple cloud environments (both
publicly accessible and private) and possibly your existing data center infrastructure. Plans must
include network considerations such as intra- and inter-system connectivity, public IP address
management, private IP address management, and domain name resolution.
For cloud-based workload architectures, there are Service Quotas (which are also referred to as service
limits). These quotas exist to prevent accidentally provisioning more resources than you need and to
limit request rates on API operations to protect services from abuse. Workloads often exist in multiple
environments. You must monitor and manage these quotas for all workload environments. These include
multiple cloud environments (both publicly accessible and private) and may include your existing data
center infrastructure. Plans must include network considerations, such as intrasystem and intersystem
21

AWS Well-Architected Framework
Best practices
connectivity, public IP address management, private IP address management, and domain name
resolution.
Workload architecture
A reliable workload starts with upfront design decisions for both software and infrastructure. Your
architecture choices will impact your workload behavior across all of the Well-Architected pillars. For
reliability, there are speciﬁc patterns you must follow.
With AWS, workload developers have their choice of languages and technologies to use. AWS SDKs
take the complexity out of coding by providing language-speciﬁc APIs for AWS services. These SDKs,
plus the choice of languages, permits developers to implement the reliability best practices listed
here. Developers can also read about and learn from how Amazon builds and operates software in The
Amazon Builders' Library.
The following questions focus on these considerations for reliability.
REL 3: How do you design your workload service architecture?
Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a
microservices architecture. Service-oriented architecture (SOA) is the practice of making software
components reusable via service interfaces. Microservices architecture goes further to make
components smaller and simpler.
REL 4: How do you design interactions in a distributed system to prevent failures?
Distributed systems rely on communications networks to interconnect components, such as servers
or services. Your workload must operate reliably despite data loss or latency in these networks.
Components of the distributed system must operate in a way that does not negatively impact other
components or the workload. These best practices prevent failures and improve mean time between
failures (MTBF).
REL 5: How do you design interactions in a distributed system to mitigate or withstand failures?
Distributed systems rely on communications networks to interconnect components (such as servers
or services). Your workload must operate reliably despite data loss or latency over these networks.
Components of the distributed system must operate in a way that does not negatively impact other
components or the workload. These best practices permit workloads to withstand stresses or failures,
more quickly recover from them, and mitigate the impact of such impairments. The result is improved
mean time to recovery (MTTR).
Change management
Changes to your workload or its environment must be anticipated and accommodated to achieve reliable
operation of the workload. Changes include those imposed on your workload, such as spikes in demand,
and also those from within, such as feature deployments and security patches.
Using AWS, you can monitor the behavior of a workload and automate the response to KPIs. For
example, your workload can add additional servers as a workload gains more users. You can control who
has permission to make workload changes and audit the history of these changes.
The following questions focus on these considerations for reliability.
22

AWS Well-Architected Framework
Best practices
REL 6: How do you monitor workload resources?
Logs and metrics are powerful tools to gain insight into the health of your workload. You can conﬁgure
your workload to monitor logs and metrics and send notiﬁcations when thresholds are crossed
or signiﬁcant events occur. Monitoring allows your workload to recognize when low-performance
thresholds are crossed or failures occur, so it can recover automatically in response.
REL 7: How do you design your workload to adapt to changes in demand?
A scalable workload provides elasticity to add or remove resources automatically so that they closely
match the current demand at any given point in time.
REL 8: How do you implement change?
Controlled changes are necessary to deploy new functionality, and to verify that the workloads and the
operating environment are running known software and can be patched or replaced in a predictable
manner. If these changes are uncontrolled, then it makes it diﬃcult to predict the eﬀect of these
changes, or to address issues that arise because of them.
When you architect a workload to automatically add and remove resources in response to changes
in demand, this not only increases reliability but also validates that business success doesn't become
a burden. With monitoring in place, your team will be automatically alerted when KPIs deviate from
expected norms. Automatic logging of changes to your environment permits you to audit and quickly
identify actions that might have impacted reliability. Controls on change management certify that you
can enforce the rules that deliver the reliability you need.
Failure management
In any system of reasonable complexity, it is expected that failures will occur. Reliability requires
that your workload be aware of failures as they occur and take action to avoid impact on availability.
Workloads must be able to both withstand failures and automatically repair issues.
With AWS, you can take advantage of automation to react to monitoring data. For example, when a
particular metric crosses a threshold, you can initiate an automated action to remedy the problem. Also,
rather than trying to diagnose and ﬁx a failed resource that is part of your production environment, you
can replace it with a new one and carry out the analysis on the failed resource out of band. Since the
cloud allows you to stand up temporary versions of a whole system at low cost, you can use automated
testing to verify full recovery processes.
The following questions focus on these considerations for reliability.
REL 9: How do you back up data?
Back up data, applications, and conﬁguration to meet your requirements for recovery time objectives
(RTO) and recovery point objectives (RPO).
REL 10: How do you use fault isolation to protect your workload?
Fault isolated boundaries limit the eﬀect of a failure within a workload to a limited number of
components. Components outside of the boundary are unaﬀected by the failure. Using multiple fault
isolated boundaries, you can limit the impact on your workload.
23

AWS Well-Architected Framework
Resources
REL 11: How do you design your workload to withstand component failures?
Workloads with a requirement for high availability and low mean time to recovery (MTTR) must be
architected for resiliency.
REL 12: How do you test reliability?
After you have designed your workload to be resilient to the stresses of production, testing is the only
way to verify that it will operate as designed, and deliver the resiliency you expect.
REL 13: How do you plan for disaster recovery (DR)?
Having backups and redundant workload components in place is the start of your DR strategy. RTO
and RPO are your objectives for restoration of your workload. Set these based on business needs.
Implement a strategy to meet these objectives, considering locations and function of workload
resources and data. The probability of disruption and cost of recovery are also key factors that help to
inform the business value of providing disaster recovery for a workload.
Regularly back up your data and test your backup ﬁles to verify that you can recover from both logical
and physical errors. A key to managing failure is the frequent and automated testing of workloads to
cause failure, and then observe how they recover. Do this on a regular schedule and verify that such
testing is also initiated after signiﬁcant workload changes. Actively track KPIs, and also the recovery time
objective (RTO) and recovery point objective (RPO), to assess a workload's resiliency (especially under
failure-testing scenarios). Tracking KPIs will help you identify and mitigate single points of failure. The
objective is to thoroughly test your workload-recovery processes so that you are conﬁdent that you can
recover all your data and continue to serve your customers, even in the face of sustained problems. Your
recovery processes should be as well exercised as your normal production processes.
Resources
Refer to the following resources to learn more about our best practices for Reliability.
Documentation
• AWS Documentation
• AWS Global Infrastructure
• AWS Auto Scaling: How Scaling Plans Work
• What Is AWS Backup?
Whitepaper
• Reliability Pillar: AWS Well-Architected
• Implementing Microservices on AWS
Performance eﬃciency
The Performance Eﬃciency pillar includes the ability to use computing resources eﬃciently to meet
system requirements, and to maintain that eﬃciency as demand changes and technologies evolve.
24

AWS Well-Architected Framework
Design principles
The performance eﬃciency pillar provides an overview of design principles, best practices, and
questions. You can ﬁnd prescriptive guidance on implementation in the Performance Eﬃciency Pillar
whitepaper.
Topics
• Design principles (p. 25)
• Deﬁnition (p. 25)
• Best practices (p. 26)
• Resources (p. 30)
Design principles
There are ﬁve design principles for performance eﬃciency in the cloud:
• Democratize advanced technologies: Make advanced technology implementation smoother for
your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to
learn about hosting and running a new technology, consider consuming the technology as a service.
For example, NoSQL databases, media transcoding, and machine learning are all technologies that
require specialized expertise. In the cloud, these technologies become services that your team can
consume, permitting your team to focus on product development rather than resource provisioning
and management.
• Go global in minutes: Deploying your workload in multiple AWS Regions around the world permits
you to provide lower latency and a better experience for your customers at minimal cost.
• Use serverless architectures: Serverless architectures remove the need for you to run and maintain
physical servers for traditional compute activities. For example, serverless storage services can act as
static websites (removing the need for web servers) and event services can host code. This removes the
operational burden of managing physical servers, and can lower transactional costs because managed
services operate at cloud scale.
• Experiment more often: With virtual and automatable resources, you can quickly carry out
comparative testing using diﬀerent types of instances, storage, or conﬁgurations.
• Consider mechanical sympathy: Understand how cloud services are consumed and always use the
technology approach that aligns with your workload goals. For example, consider data access patterns
when you select database or storage approaches.
Deﬁnition
There are four best practice areas for performance eﬃciency in the cloud:
• Selection
• Review
• Monitoring
• Tradeoﬀs
Take a data-driven approach to building a high-performance architecture. Gather data on all aspects of
the architecture, from the high-level design to the selection and conﬁguration of resource types.
Reviewing your choices on a regular basis validates that you are taking advantage of the continually
evolving AWS Cloud. Monitoring veriﬁes that you are aware of any deviance from expected performance.
Make trade-oﬀs in your architecture to improve performance, such as using compression or caching, or
relaxing consistency requirements.
25

AWS Well-Architected Framework
Best practices
Best practices
Topics
• Selection (p. 26)
• Review (p. 29)
• Monitoring (p. 29)
• Tradeoﬀs (p. 30)
Selection
The more eﬀective solution for a particular workload varies, and solutions often combine multiple
approaches. Well-architected workloads use multiple solutions and activate diﬀerent features to improve
performance.
AWS resources are available in many types and conﬁgurations so you can ﬁnd an approach that closely
matches your workload needs. You can also ﬁnd options that are not eﬃciently achievable with on-
premises infrastructure. For example, a managed service such as Amazon DynamoDB provides a fully
managed NoSQL database with single-digit millisecond latency at any scale.
The following question focuses on these considerations for performance eﬃciency. (For a list of
performance eﬃciency questions and best practices, see the Appendix (p. 378).).
PERF 1: How do you select the best performing architecture?
Often, multiple approaches are required for more eﬀective performance across a workload. Well-
architected systems use multiple solutions and features to improve performance.
Use a data-driven approach to select the patterns and implementation for your architecture and achieve
a cost eﬀective solution. AWS Solutions Architects, AWS Reference Architectures, and AWS Partner
Network (APN) partners can help you select an architecture based on industry knowledge, but data
obtained through benchmarking or load testing will be required to optimize your architecture.
Your architecture will likely combine a number of diﬀerent architectural approaches (for example, event-
driven, ETL, or pipeline). The implementation of your architecture will use the AWS services that are
speciﬁc to the optimization of your architecture's performance. In the following sections we discuss the
four main resource types to consider (compute, storage, database, and network).
Compute
Selecting compute resources that meet your requirements, performance needs, and provide great
eﬃciency of cost and eﬀort will permit you to accomplish more with the same number of resources.
When evaluating compute options, be aware of your requirements for workload performance and cost
requirements and use this to make informed decisions.
In AWS, compute is available in three forms: instances, containers, and functions:
• Instances are virtualized servers, permitting you to change their capabilities with a button or an API
call. Because resource decisions in the cloud aren’t ﬁxed, you can experiment with diﬀerent server
types. At AWS, these virtual server instances come in diﬀerent families and sizes, and they oﬀer a wide
variety of capabilities, including solid-state drives (SSDs) and graphics processing units (GPUs).
• Containers are a method of operating system virtualization that permit you to run an application and
its dependencies in resource-isolated processes. AWS Fargate is serverless compute for containers or
26

AWS Well-Architected Framework
Best practices
Amazon EC2 can be used if you need control over the installation, conﬁguration, and management
of your compute environment. You can also choose from multiple container orchestration platforms:
Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS).
• Functions abstract the run environment from the code you want to apply. For example, AWS Lambda
permits you to run code without running an instance.
The following question focuses on these considerations for performance eﬃciency.
PERF 2: How do you select your compute solution?
The more eﬃcient compute solution for a workload varies based on application design, usage patterns,
and conﬁguration settings. Architectures can use diﬀerent compute solutions for various components
and turn on diﬀerent features to improve performance. Selecting the wrong compute solution for an
architecture can lead to lower performance eﬃciency.
When architecting your use of compute you should take advantage of the elasticity mechanisms
available to verify you have suﬃcient capacity to sustain performance as demand changes.
Storage
Cloud storage is a critical component of cloud computing, holding the information used by your
workload. Cloud storage is typically more reliable, scalable, and secure than traditional on-premises
storage systems. Select from object, block, and ﬁle storage services, and cloud data migration options
for your workload.
In AWS, storage is available in three forms: object, block, and ﬁle:
• Object Storage provides a scalable, durable platform to make data accessible from any internet
location for user-generated content, active archive, serverless computing, Big Data storage or backup
and recovery. Amazon Simple Storage Service (Amazon S3) is an object storage service that oﬀers
industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for
99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all
around the world.
• Block Storage provides highly available, consistent, low-latency block storage for each virtual host
and is analogous to direct-attached storage (DAS) or a Storage Area Network (SAN). Amazon Elastic
Block Store (Amazon EBS) is designed for workloads that require persistent storage accessible by EC2
instances that helps you tune applications with the right storage capacity, performance and cost.
• File Storage provides access to a shared ﬁle system across multiple systems. File storage solutions like
Amazon Elastic File System (Amazon EFS) are ideal for use cases such as large content repositories,
development environments, media stores, or user home directories. Amazon FSx makes it eﬃcient and
cost eﬀective to launch and run popular ﬁle systems so you can leverage the rich feature sets and fast
performance of widely used open source and commercially-licensed ﬁle systems.
The following question focuses on these considerations for performance eﬃciency.
PERF 3: How do you select your storage solution?
The more eﬃcient storage solution for a system varies based on the kind of access operation (block,
ﬁle, or object), patterns of access (random or sequential), required throughput, frequency of access
(online, oﬄine, archival), frequency of update (WORM, dynamic), and availability and durability
constraints. Well-architected systems use multiple storage solutions and turn on diﬀerent features to
improve performance and use resources eﬃciently.
27

AWS Well-Architected Framework
Best practices
When you select a storage solution, verifying that it aligns with your access patterns will be critical to
achieving the performance you want.
Database
The cloud oﬀers purpose-built database services that address diﬀerent problems presented by your
workload. You can choose from many purpose-built database engines including relational, key-value,
document, in-memory, graph, time series, and ledger databases. By selecting the most eﬀective database
to solve a speciﬁc problem (or a group of problems), you can break away from restrictive one-size-ﬁts-
all monolithic databases and focus on building applications to meet the performance needs of your
customers.
In AWS you can choose from multiple purpose-built database engines including relational, key-value,
document, in-memory, graph, time series, and ledger databases. With AWS databases, you don’t need
to worry about database management tasks such as server provisioning, patching, setup, conﬁguration,
backups, or recovery. AWS continuously monitors your clusters to keep your workloads up and running
with self-healing storage and automated scaling, so that you can focus on higher value application
development.
The following question focuses on these considerations for performance eﬃciency.
PERF 4: How do you select your database solution?
The most eﬀective database solution for a system varies based on requirements for availability,
consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems
use diﬀerent database solutions for various subsystems and turn on diﬀerent features to improve
performance. Selecting the wrong database solution and features for a system can lead to lower
performance eﬃciency.
Your workload's database approach has a signiﬁcant impact on performance eﬃciency. It's often an
area that is chosen according to organizational defaults rather than through a data-driven approach. As
with storage, it is critical to consider the access patterns of your workload, and also to consider if other
non-database solutions could solve the problem more eﬃciently (such as using graph, time series, or in-
memory storage database).
Network
Since the network is between all workload components, it can have great impacts, both positive and
negative, on workload performance and behavior. There are also workloads that are heavily dependent
on network performance such as high performance computing (HPC) where deep network understanding
is important to increase cluster performance. Determine the workload requirements for bandwidth,
latency, jitter, and throughput.
On AWS, networking is virtualized and is available in a number of diﬀerent types and conﬁgurations. This
makes it eﬃcient to match your networking operations with your needs. AWS oﬀers product features
(for example, Enhanced Networking, Amazon EBS-optimized instances, Amazon S3 transfer acceleration,
and dynamic Amazon CloudFront) to optimize network traﬃc. AWS also oﬀers networking features (for
example, Amazon Route 53 latency routing, Amazon VPC endpoints, AWS Direct Connect, and AWS
Global Accelerator) to reduce network distance or jitter.
The following question focuses on these considerations for performance eﬃciency.
PERF 5: How do you conﬁgure your networking solution?
The most eﬃcient network solution for a workload varies based on latency, throughput requirements,
jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location
options. These constraints can be oﬀset with edge locations or resource placement.
28

AWS Well-Architected Framework
Best practices
You must consider location when deploying your network. You can choose to place resources close to
where they will be used to reduce distance. Use networking metrics to make changes to networking
conﬁguration as the workload evolves. By taking advantage of Regions, placement groups, and edge
services, you can signiﬁcantly improve performance. Cloud based networks can be quickly re-built
or modiﬁed, so evolving your network architecture over time is necessary to maintain performance
eﬃciency.
Review
Cloud technologies are rapidly evolving and you must verify that workload components are using the
latest technologies and approaches to continually improve performance. You must continually evaluate
and consider changes to your workload components to verify you are meeting its performance and cost
objectives. New technologies, such as machine learning and artiﬁcial intelligence (AI), can permit you to
reimagine customer experiences and innovate across all of your business workloads.
Take advantage of the continual innovation at AWS driven by customer need. We release new Regions,
edge locations, services, and features regularly. Any of these releases could positively improve the
performance eﬃciency of your architecture.
The following question focuses on these considerations for performance eﬃciency.
PERF 6: How do you evolve your workload to take advantage of new releases?
When architecting workloads, there are ﬁnite options that you can choose from. However, over time,
new technologies and approaches become available that could improve the performance of your
workload.
Architectures performing poorly are usually the result of a non-existent or broken performance review
process. If your architecture is performing poorly, implementing a performance review process will
permit you to apply Deming’s plan-do-check-act (PDCA) cycle to drive iterative improvement.
Monitoring
After you implement your workload, you must monitor its performance so that you can remediate any
issues before they impact your customers. Monitoring metrics should be used to raise alarms when
thresholds are breached.
Amazon CloudWatch is a monitoring and observability service that provides you with data and
actionable insights to monitor your workload, respond to system-wide performance changes, optimize
resource utilization, and get a uniﬁed view of operational health. CloudWatch collects monitoring and
operational data in the form of logs, metrics, and events from workloads that run on AWS and on-
premises servers. AWS X-Ray helps developers analyze and debug production, distributed applications.
With AWS X-Ray, you can glean insights into how your application is performing and discover root
causes and identify performance bottlenecks. You can use these insights to react quickly and keep your
workload running smoothly.
The following question focuses on these considerations for performance eﬃciency.
PERF 7: How do you monitor your resources to verify they are performing?
System performance can degrade over time. Monitor system performance to identify degradation and
remediate internal or external factors, such as the operating system or application load.
Validating that you do not see false positives is key to an eﬀective monitoring solution. Automated
initiation functions avoid human error and can reduce the time it takes to ﬁx problems. Plan for game
29

AWS Well-Architected Framework
Resources
days, where simulations are conducted in the production environment, to test your alarm solution and
verify that it correctly recognizes issues.
Tradeoﬀs
When you architect solutions, think about tradeoﬀs to validate a more eﬃcient approach. Depending
on your situation, you could trade consistency, durability, and space for time or latency, to deliver higher
performance.
Using AWS, you can go global in minutes and deploy resources in multiple locations across the globe to
be closer to your end users. You can also dynamically add read only replicas to information stores (such
as database systems) to reduce the load on the primary database.
The following question focuses on these considerations for performance eﬃciency.
PERF 8: How do you use tradeoﬀs to improve performance?
When architecting solutions, determining tradeoﬀs permits you to select an more eﬃcient approach.
Often you can improve performance by trading consistency, durability, and space for time and latency.
As you make changes to the workload, collect and evaluate metrics to determine the impact of those
changes. Measure the impacts to the system and to the end user to understand how your trade-oﬀs
impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoﬀ
improves performance.
Resources
Refer to the following resources to learn more about our best practices for Performance Eﬃciency.
Documentation
• Amazon S3 Performance Optimization
• Amazon EBS Volume Performance
Whitepaper
• Performance Eﬃciency Pillar
Video
• AWS re:Invent 2019: Amazon EC2 foundations (CMP211-R2)
• AWS re:Invent 2019: Leadership session: Storage state of the union (STG201-L)
• AWS re:Invent 2019: Leadership session: AWS purpose-built databases (DAT209-L)
• AWS re:Invent 2019: Connectivity to AWS and hybrid AWS network architectures (NET317-R1)
• AWS re:Invent 2019: Powering next-gen Amazon EC2: Deep dive into the Nitro system (CMP303-R2)
• AWS re:Invent 2019: Scaling up to your ﬁrst 10 million users (ARC211-R)
Cost optimization
The Cost Optimization pillar includes the ability to run systems to deliver business value at the lowest
price point.
30

AWS Well-Architected Framework
Design principles
The cost optimization pillar provides an overview of design principles, best practices, and questions. You
can ﬁnd prescriptive guidance on implementation in the Cost Optimization Pillar whitepaper.
Topics
• Design principles (p. 31)
• Deﬁnition (p. 31)
• Best practices (p. 32)
• Resources (p. 36)
Design principles
There are ﬁve design principles for cost optimization in the cloud:
• Implement Cloud Financial Management: To achieve ﬁnancial success and accelerate business
value realization in the cloud, invest in Cloud Financial Management and Cost Optimization. Your
organization should dedicate time and resources to build capability in this new domain of technology
and usage management. Similar to your Security or Operational Excellence capability, you need to
build capability through knowledge building, programs, resources, and processes to become a cost-
eﬃcient organization.
• Adopt a consumption model: Pay only for the computing resources that you require and increase or
decrease usage depending on business requirements, not by using elaborate forecasting. For example,
development and test environments are typically only used for eight hours a day during the work
week. You can stop these resources when they are not in use for a potential cost savings of 75% (40
hours versus 168 hours).
• Measure overall eﬃciency: Measure the business output of the workload and the costs associated
with delivering it. Use this measure to know the gains you make from increasing output and reducing
costs.
• Stop spending money on undiﬀerentiated heavy lifting: AWS does the heavy lifting of data center
operations like racking, stacking, and powering servers. It also removes the operational burden of
managing operating systems and applications with managed services. This permits you to focus on
your customers and business projects rather than on IT infrastructure.
• Analyze and attribute expenditure: The cloud makes it simple to accurately identify the usage and
cost of systems, which then permits transparent attribution of IT costs to individual workload owners.
This helps measure return on investment (ROI) and gives workload owners an opportunity to optimize
their resources and reduce costs.
Deﬁnition
There are ﬁve best practice areas for cost optimization in the cloud:
• Practice Cloud Financial Management
• Expenditure and usage awareness
• Cost-eﬀective resources
• Manage demand and supply resources
• Optimize over time
As with the other pillars within the Well-Architected Framework, there are tradeoﬀs to consider, for
example, whether to optimize for speed-to-market or for cost. In some cases, it’s more eﬃcient to
optimize for speed, going to market quickly, shipping new features, or meeting a deadline, rather
than investing in upfront cost optimization. Design decisions are sometimes directed by haste rather
31

AWS Well-Architected Framework
Best practices
than data, and the temptation always exists to overcompensate “just in case” rather than spend time
benchmarking for the most cost-optimal deployment. This might lead to over-provisioned and under-
optimized deployments. However, this is a reasonable choice when you must “lift and shift” resources
from your on-premises environment to the cloud and then optimize afterwards. Investing the right
amount of eﬀort in a cost optimization strategy up front permits you to realize the economic beneﬁts of
the cloud more readily by achieving a consistent adherence to best practices and avoiding unnecessary
over provisioning. The following sections provide techniques and best practices for both the initial and
ongoing implementation of Cloud Financial Management and cost optimization of your workloads.
Best practices
Topics
• Practice Cloud Financial Management (p. 32)
• Expenditure and usage awareness (p. 33)
• Cost-eﬀective resources (p. 34)
• Manage demand and supply resources (p. 35)
• Optimize over time (p. 35)
Practice Cloud Financial Management
With the adoption of cloud, technology teams innovate faster due to shortened approval, procurement,
and infrastructure deployment cycles. A new approach to ﬁnancial management in the cloud is required
to realize business value and ﬁnancial success. This approach is Cloud Financial Management, and builds
capability across your organization by implementing organizational wide knowledge building, programs,
resources, and processes.
Many organizations are composed of many diﬀerent units with diﬀerent priorities. The ability to align
your organization to an agreed set of ﬁnancial objectives, and provide your organization the mechanisms
to meet them, will create a more eﬃcient organization. A capable organization will innovate and build
faster, be more agile and adjust to any internal or external factors.
In AWS you can use Cost Explorer, and optionally Amazon Athena and Amazon QuickSight with the
Cost and Usage Report (CUR), to provide cost and usage awareness throughout your organization. AWS
Budgets provides proactive notiﬁcations for cost and usage. The AWS blogs provide information on new
services and features to verify you keep up to date with new service releases.
The following question focuses on these considerations for cost optimization. (For a list of cost
optimization questions and best practices, see the Appendix (p. 454).).
COST 1: How do you implement cloud ﬁnancial management?
Implementing Cloud Financial Management helps organizations realize business value and ﬁnancial
success as they optimize their cost and usage and scale on AWS.
When building a cost optimization function, use members and supplement the team with experts in CFM
and cost optimization. Existing team members will understand how the organization currently functions
and how to rapidly implement improvements. Also consider including people with supplementary or
specialist skill sets, such as analytics and project management.
When implementing cost awareness in your organization, improve or build on existing programs and
processes. It is much faster to add to what exists than to build new processes and programs. This will
result in achieving outcomes much faster.
32

AWS Well-Architected Framework
Best practices
Expenditure and usage awareness
The increased ﬂexibility and agility that the cloud provides encourages innovation and fast-paced
development and deployment. It decreases the manual processes and time associated with provisioning
on-premises infrastructure, including identifying hardware speciﬁcations, negotiating price quotations,
managing purchase orders, scheduling shipments, and then deploying the resources. However, the ease
of use and virtually unlimited on-demand capacity requires a new way of thinking about expenditures.
Many businesses are composed of multiple systems run by various teams. The capability to attribute
resource costs to the individual organization or product owners drives eﬃcient usage behavior and helps
reduce waste. Accurate cost attribution permits you to know which products are truly proﬁtable, and
permits you to make more informed decisions about where to allocate budget.
In AWS, you create an account structure with AWS Organizations or AWS Control Tower, which provides
separation and assists in allocation of your costs and usage. You can also use resource tagging to apply
business and organization information to your usage and cost. Use AWS Cost Explorer for visibility into
your cost and usage, or create customized dashboards and analytics with Amazon Athena and Amazon
QuickSight. Controlling your cost and usage is done by notiﬁcations through AWS Budgets, and controls
using AWS Identity and Access Management (IAM), and Service Quotas.
The following questions focus on these considerations for cost optimization.
COST 2: How do you govern usage?
Establish policies and mechanisms to validate that appropriate costs are incurred while objectives are
achieved. By employing a checks-and-balances approach, you can innovate without overspending.
COST 3: How do you monitor usage and cost?
Establish policies and procedures to monitor and appropriately allocate your costs. This permits you to
measure and improve the cost eﬃciency of this workload.
COST 4: How do you decommission resources?
Implement change control and resource management from project inception to end-of-life. This
facilitates shutting down unused resources to reduce waste.
You can use cost allocation tags to categorize and track your AWS usage and costs. When you apply tags
to your AWS resources (such as EC2 instances or S3 buckets), AWS generates a cost and usage report
with your usage and your tags. You can apply tags that represent organization categories (such as cost
centers, workload names, or owners) to organize your costs across multiple services.
Verify that you use the right level of detail and granularity in cost and usage reporting and monitoring.
For high level insights and trends, use daily granularity with AWS Cost Explorer. For deeper analysis and
inspection use hourly granularity in AWS Cost Explorer, or Amazon Athena and Amazon QuickSight with
the Cost and Usage Report (CUR) at an hourly granularity.
Combining tagged resources with entity lifecycle tracking (employees, projects) makes it possible to
identify orphaned resources or projects that are no longer generating value to the organization and
should be decommissioned. You can set up billing alerts to notify you of predicted overspending.
33

AWS Well-Architected Framework
Best practices
Cost-eﬀective resources
Using the appropriate instances and resources for your workload is key to cost savings. For example, a
reporting process might take ﬁve hours to run on a smaller server but one hour to run on a larger server
that is twice as expensive. Both servers give you the same outcome, but the smaller server incurs more
cost over time.
A well-architected workload uses the most cost-eﬀective resources, which can have a signiﬁcant and
positive economic impact. You also have the opportunity to use managed services to reduce costs. For
example, rather than maintaining servers to deliver email, you can use a service that charges on a per-
message basis.
AWS oﬀers a variety of ﬂexible and cost-eﬀective pricing options to acquire instances from Amazon EC2
and other services in a way that more eﬀectively ﬁts your needs. On-Demand Instances permit you to pay
for compute capacity by the hour, with no minimum commitments required. Savings Plans and Reserved
Instances oﬀer savings of up to 75% oﬀ On-Demand pricing. With Spot Instances, you can leverage
unused Amazon EC2 capacity and oﬀer savings of up to 90% oﬀ On-Demand pricing. Spot Instances are
appropriate where the system can tolerate using a ﬂeet of servers where individual servers can come and
go dynamically, such as stateless web servers, batch processing, or when using HPC and big data.
Appropriate service selection can also reduce usage and costs; such as CloudFront to minimize data
transfer, or decrease costs, such as utilizing Amazon Aurora on Amazon RDS to remove expensive
database licensing costs.
The following questions focus on these considerations for cost optimization.
COST 5: How do you evaluate cost when you select services?
Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such
as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By
selecting the appropriate building blocks and managed services, you can optimize this workload for
cost. For example, using managed services, you can reduce or remove much of your administrative and
operational overhead, freeing you to work on applications and business-related activities.
COST 6: How do you meet cost targets when you select resource type, size and number?
Verify that you choose the appropriate resource size and number of resources for the task at hand. You
minimize waste by selecting the most cost eﬀective type, size, and number.
COST 7: How do you use pricing models to reduce cost?
Use the pricing model that is most appropriate for your resources to minimize expense.
COST 8: How do you plan for data transfer charges?
Verify that you plan and monitor data transfer charges so that you can make architectural decisions to
minimize costs. A small yet eﬀective architectural change can drastically reduce your operational costs
over time.
By factoring in cost during service selection, and using tools such as Cost Explorer and AWS Trusted
Advisor to regularly review your AWS usage, you can actively monitor your utilization and adjust your
deployments accordingly.
34

AWS Well-Architected Framework
Best practices
Manage demand and supply resources
When you move to the cloud, you pay only for what you need. You can supply resources to match the
workload demand at the time they’re needed, this decreases the need for costly and wasteful over
provisioning. You can also modify the demand, using a throttle, buﬀer, or queue to smooth the demand
and serve it with less resources resulting in a lower cost, or process it at a later time with a batch service.
In AWS, you can automatically provision resources to match the workload demand. Auto Scaling using
demand or time-based approaches permit you to add and remove resources as needed. If you can
anticipate changes in demand, you can save more money and validate that your resources match
your workload needs. You can use Amazon API Gateway to implement throttling, or Amazon SQS to
implementing a queue in your workload. These will both permit you to modify the demand on your
workload components.
The following question focuses on these considerations for cost optimization.
COST 9: How do you manage demand, and supply resources?
For a workload that has balanced spend and performance, verify that everything you pay for is used
and avoid signiﬁcantly underutilizing instances. A skewed utilization metric in either direction has an
adverse impact on your organization, in either operational costs (degraded performance due to over-
utilization), or wasted AWS expenditures (due to over-provisioning).
When designing to modify demand and supply resources, actively think about the patterns of usage, the
time it takes to provision new resources, and the predictability of the demand pattern. When managing
demand, verify you have a correctly sized queue or buﬀer, and that you are responding to workload
demand in the required amount of time.
Optimize over time
As AWS releases new services and features, it's a best practice to review your existing architectural
decisions to verify they continue to be the most cost eﬀective. As your requirements change, be
aggressive in decommissioning resources, entire services, and systems that you no longer require.
Implementing new features or resource types can optimize your workload incrementally, while
minimizing the eﬀort required to implement the change. This provides continual improvements in
eﬃciency over time and provides you remain on the most updated technology to reduce operating
costs. You can also replace or add new components to the workload with new services. This can provide
signiﬁcant increases in eﬃciency, so it's essential to regularly review your workload, and implement new
services and features.
The following questions focus on these considerations for cost optimization.
COST 10: How do you evaluate new services?
As AWS releases new services and features, it's a best practice to review your existing architectural
decisions to verify they continue to be the most cost eﬀective.
When regularly reviewing your deployments, assess how newer services can help save you money. For
example, Amazon Aurora on Amazon RDS can reduce costs for relational databases. Using serverless
such as Lambda can remove the need to operate and manage instances to run code.
35

AWS Well-Architected Framework
Resources
COST 11: How do you evaluate the cost of eﬀort?
Evaluate the cost of eﬀort for operations in the cloud, review your time-consuming cloud operations,
and automate them to reduce human eﬀorts and cost by adopting related AWS services, third-party
products, or custom tools.
Resources
Refer to the following resources to learn more about our best practices for Cost Optimization.
Documentation
• AWS Documentation
Whitepaper
• Cost Optimization Pillar
Sustainability
The Sustainability pillar focuses on environmental impacts, especially energy consumption and eﬃciency,
since they are important levers for architects to inform direct action to reduce resource usage. You can
ﬁnd prescriptive guidance on implementation in the Sustainability Pillar whitepaper.
Topics
• Design principles (p. 36)
• Deﬁnition (p. 37)
• Best practices (p. 37)
• Resources (p. 41)
Design principles
There are six design principles for sustainability in the cloud:
• Understand your impact: Measure the impact of your cloud workload and model the future impact
of your workload. Include all sources of impact, including impacts resulting from customer use of
your products, and impacts resulting from their eventual decommissioning and retirement. Compare
the productive output with the total impact of your cloud workloads by reviewing the resources and
emissions required per unit of work. Use this data to establish key performance indicators (KPIs),
evaluate ways to improve productivity while reducing impact, and estimate the impact of proposed
changes over time.
• Establish sustainability goals: For each cloud workload, establish long-term sustainability goals
such as reducing the compute and storage resources required per transaction. Model the return on
investment of sustainability improvements for existing workloads, and give owners the resources
they must invest in sustainability goals. Plan for growth, and architect your workloads so that growth
results in reduced impact intensity measured against an appropriate unit, such as per user or per
transaction. Goals help you support the wider sustainability goals of your business or organization,
identify regressions, and prioritize areas of potential improvement.
36

AWS Well-Architected Framework
Deﬁnition
• Maximize utilization: Right-size workloads and implement eﬃcient design to verify high utilization
and maximize the energy eﬃciency of the underlying hardware. Two hosts running at 30% utilization
are less eﬃcient than one host running at 60% due to baseline power consumption per host. At the
same time, reduce or minimize idle resources, processing, and storage to reduce the total energy
required to power your workload.
• Anticipate and adopt new, more eﬃcient hardware and software oﬀerings: Support the upstream
improvements your partners and suppliers make to help you reduce the impact of your cloud
workloads. Continually monitor and evaluate new, more eﬃcient hardware and software oﬀerings.
Design for ﬂexibility to permit the rapid adoption of new eﬃcient technologies.
• Use managed services: Sharing services across a broad customer base helps maximize resource
utilization, which reduces the amount of infrastructure needed to support cloud workloads. For
example, customers can share the impact of common data center components like power and
networking by migrating workloads to the AWS Cloud and adopting managed services, such as AWS
Fargate for serverless containers, where AWS operates at scale and is responsible for their eﬃcient
operation. Use managed services that can help minimize your impact, such as automatically moving
infrequently accessed data to cold storage with Amazon S3 Lifecycle conﬁgurations or Amazon EC2
Auto Scaling to adjust capacity to meet demand.
• Reduce the downstream impact of your cloud workloads: Reduce the amount of energy or resources
required to use your services. Reduce the need for customers to upgrade their devices to use
your services. Test using device farms to understand expected impact and test with customers to
understand the actual impact from using your services.
Deﬁnition
There are six best practice areas for sustainability in the cloud:
• Region selection
• Alignment to demand
• Software and architecture
• Data
• Hardware and services
• Process and culture
Sustainability in the cloud is a nearly continuous eﬀort focused primarily on energy reduction and
eﬃciency across all components of a workload by achieving the maximum beneﬁt from the resources
provisioned and minimizing the total resources required. This eﬀort can range from the initial
selection of an eﬃcient programming language, adoption of modern algorithms, use of eﬃcient data
storage techniques, deploying to correctly sized and eﬃcient compute infrastructure, and minimizing
requirements for high-powered end user hardware.
Best practices
Topics
• Region selection (p. 38)
• Alignment to demand (p. 38)
• Software and architecture (p. 39)
• Data (p. 39)
• Hardware and services (p. 40)
• Process and culture (p. 41)
37

AWS Well-Architected Framework
Best practices
Region selection
The choice of Region for your workload signiﬁcantly aﬀects its KPIs, including performance, cost, and
carbon footprint. To improve these KPIs, you should choose Regions for your workloads based on both
business requirements and sustainability goals.
The following question focuses on these considerations for sustainability. (For a list of sustainability
questions and best practices, see the Appendix (p. 526).)
SUS 1: How do you select Regions for your workload?
The choice of Region for your workload signiﬁcantly aﬀects its KPIs, including performance, cost, and
carbon footprint. To improve these KPIs, you should choose Regions for your workloads based on both
business requirements and sustainability goals.
Alignment to demand
The way users and applications consume your workloads and other resources can help you identify
improvements to meet sustainability goals. Scale infrastructure to continually match demand and verify
that you use only the minimum resources required to support your users. Align service levels to customer
needs. Position resources to limit the network required for users and applications to consume them.
Remove unused assets. Provide your team members with devices that support their needs and minimize
their sustainability impact.
The following question focuses on this consideration for sustainability:
SUS 2: How do you align cloud resources to your demand?
The way users and applications consume your workloads and other resources can help you identify
improvements to meet sustainability goals. Scale infrastructure to continually match demand and
verify that you use only the minimum resources required to support your users. Align service levels to
customer needs. Position resources to limit the network required for users and applications to consume
them. Remove unused assets. Provide your team members with devices that support their needs and
minimize their sustainability impact.
Scale infrastructure with user load: Identify periods of low or no utilization and scale resources to reduce
excess capacity and improve eﬃciency.
Align SLAs with sustainability goals: Deﬁne and update service level agreements (SLAs) such as
availability or data retention periods to minimize the number of resources required to support your
workload while continuing to meet business requirements.
Decrease creation and maintenance of unused assets: Analyze application assets (such as pre-compiled
reports, datasets, and static images) and asset access patterns to identify redundancy, underutilization,
and potential decommission targets. Consolidate generated assets with redundant content (for example,
monthly reports with overlapping or common datasets and outputs) to reduce the resources consumed
when duplicating outputs. Decommission unused assets (for example, images of products that are no
longer sold) to release consumed resources and reduce the number of resources used to support the
workload.
Optimize geographic placement of workloads for user locations: Analyze network access patterns to
identify where your customers are connecting from geographically. Select Regions and services that
reduce the distance that network traﬃc must travel to decrease the total network resources required to
support your workload.
38

AWS Well-Architected Framework
Best practices
Optimize team member resources for activities performed: Optimize resources provided to team
members to minimize the sustainability impact while supporting their needs. For example, perform
complex operations, such as rendering and compilation, on highly used shared cloud desktops instead of
on under-utilized high-powered single user systems.
Software and architecture
Implement patterns for performing load smoothing and maintaining consistent high utilization of
deployed resources to minimize the resources consumed. Components might become idle from lack
of use because of changes in user behavior over time. Revise patterns and architecture to consolidate
under-utilized components to increase overall utilization. Retire components that are no longer required.
Understand the performance of your workload components, and optimize the components that consume
the most resources. Be aware of the devices that your customers use to access your services, and
implement patterns to minimize the need for device upgrades.
The following questions focus on these considerations for sustainability:
SUS 3: How do you take advantage of software and architecture patterns to support your
sustainability goals?
Implement patterns for performing load smoothing and maintaining consistent high utilization of
deployed resources to minimize the resources consumed. Components might become idle from lack
of use because of changes in user behavior over time. Revise patterns and architecture to consolidate
under-utilized components to increase overall utilization. Retire components that are no longer
required. Understand the performance of your workload components, and optimize the components
that consume the most resources. Be aware of the devices that your customers use to access your
services, and implement patterns to minimize the need for device upgrades.
Optimize software and architecture for asynchronous and scheduled jobs: Use eﬃcient software designs
and architectures to minimize the average resources required per unit of work. Implement mechanisms
that result in even utilization of components to reduce resources that are idle between tasks and
minimize the impact of load spikes.
Remove or refactor workload components with low or no use: Monitor workload activity to identify
changes in utilization of individual components over time. Remove components that are unused and no
longer required, and refactor components with little utilization, to limit wasted resources.
Optimize areas of code that consume the most time or resources: Monitor workload activity to identify
application components that consume the most resources. Optimize the code that runs within these
components to minimize resource usage while maximizing performance.
Optimize impact on customer devices and equipment: Understand the devices and equipment that your
customers use to consume your services, their expected lifecycle, and the ﬁnancial and sustainability
impact of replacing those components. Implement software patterns and architectures to minimize the
need for customers to replace devices and upgrade equipment. For example, implement new features
using code that is backward compatible with earlier hardware and operating system versions, or manage
the size of payloads so they don’t exceed the storage capacity of the target device.
Use software patterns and architectures that most eﬀectively supports data access and storage patterns:
Understand how data is used within your workload, consumed by your users, transferred, and stored.
Select technologies to minimize data processing and storage requirements.
Data
The following question focuses on these considerations for sustainability:
39

AWS Well-Architected Framework
Best practices
SUS 4: How do you take advantage of data management policies and patterns to support your
sustainability goals?
Implement data management practices to reduce the provisioned storage required to support your
workload, and the resources required to use it. Understand your data, and use storage technologies
and conﬁgurations that most eﬀectively supports the business value of the data and how it’s used.
Lifecycle data to more eﬃcient, less performant storage when requirements decrease, and delete data
that’s no longer required.
Implement a data classiﬁcation policy: Classify data to understand its signiﬁcance to business outcomes.
Use this information to determine when you can move data to more energy-eﬃcient storage or safely
delete it.
Use technologies that support data access and storage patterns: Use storage that most eﬀectively
supports how your data is accessed and stored to minimize the resources provisioned while supporting
your workload. For example, solid state devices (SSDs) are more energy intensive than magnetic drives
and should be used only for active data use cases. Use energy-eﬃcient, archival-class storage for
infrequently accessed data.
Use lifecycle policies to delete unnecessary data: Manage the lifecycle of all your data and automatically
enforce deletion timelines to minimize the total storage requirements of your workload.
Minimize over-provisioning in block storage: To minimize total provisioned storage, create block storage
with size allocations that are appropriate for the workload. Use elastic volumes to expand storage as
data grows without having to resize storage attached to compute resources. Regularly review elastic
volumes and shrink over-provisioned volumes to ﬁt the current data size.
Remove unneeded or redundant data: Duplicate data only when necessary to minimize total storage
consumed. Use backup technologies that deduplicate data at the ﬁle and block level. Limit the use of
Redundant Array of Independent Drives (RAID) conﬁgurations except where required to meet SLAs.
Use shared ﬁle systems or object storage to access common data: Adopt shared storage and single
sources of truth to avoid data duplication and reduce the total storage requirements of your workload.
Fetch data from shared storage only as needed. Detach unused volumes to release resources. Minimize
data movement across networks: Use shared storage and access data from Regional data stores to
minimize the total networking resources required to support data movement for your workload.
Back up data only when diﬃcult to recreate: To minimize storage consumption, only back up data that
has business value or is required to satisfy compliance requirements. Examine backup policies and
exclude ephemeral storage that doesn’t provide value in a recovery scenario.
Hardware and services
Look for opportunities to reduce workload sustainability impacts by making changes to your hardware
management practices. Minimize the amount of hardware needed to provision and deploy, and select the
most eﬃcient hardware and services for your individual workload.
The following question focuses on these considerations for sustainability:
SUS 5: How do you select and use cloud hardware and services in your architecture to support your
sustainability goals?
Look for opportunities to reduce workload sustainability impacts by making changes to your hardware
management practices. Minimize the amount of hardware needed to provision and deploy, and select
the most eﬃcient hardware and services for your individual workload.
40

AWS Well-Architected Framework
Resources
Use the minimum amount of hardware to meet your needs: Using the capabilities of the cloud, you can
make frequent changes to your workload implementations. Update deployed components as your needs
change.
Use instance types with the least impact: Continually monitor the release of new instance types and
take advantage of energy eﬃciency improvements, including those instance types designed to support
speciﬁc workloads such as machine learning training and inference, and video transcoding.
Use managed services: Managed services shift responsibility for maintaining high average utilization,
and sustainability optimization of the deployed hardware, to AWS. Use managed services to distribute
the sustainability impact of the service across all tenants of the service, reducing your individual
contribution.
Optimize your use of GPUs: Graphics processing units (GPUs) can be a source of high-power
consumption, and many GPU workloads are highly variable, such as rendering, transcoding, and machine
learning training and modeling. Only run GPUs instances for the time needed, and decommission them
with automation when not required to minimize resources consumed.
Process and culture
Look for opportunities to reduce your sustainability impact by making changes to your development,
test, and deployment practices.
The following question focuses on these considerations for sustainability:
SUS 6: How do your organizational processes support your sustainability goals?
Look for opportunities to reduce your sustainability impact by making changes to your development,
test, and deployment practices.
Adopt operations that can rapidly introduce sustainability improvements: Test and validate potential
improvements before deploying them to production. Account for the cost of testing when calculating
potential future beneﬁt of an improvement. Develop low-cost testing operations to drive delivery of
small improvements.
Keep your workload up to date: Up-to-date operating systems, libraries, and applications can improve
workload eﬃciency and create adoption of more eﬃcient technologies. Up-to-date software might
also include features to measure the sustainability impact of your workload more accurately, as vendors
deliver features to meet their own sustainability goals.
Increase utilization of build environments: Use automation and infrastructure as code to bring up pre-
production environments when needed and take them down when not used. A common pattern is
to schedule periods of availability that coincide with the working hours of your development team
members. Hibernation is a useful tool to preserve state and rapidly bring instances online only when
needed. Use instance types with burst capacity, Spot Instances, elastic database services, containers, and
other technologies to align development and test capacity with use.
Use managed device farms for testing: Managed device farms spread the sustainability impact of
hardware manufacturing and resource usage across multiple tenants. Managed device farms oﬀer diverse
device types so you can support earlier, less popular hardware, and avoid customer sustainability impact
from unnecessary device upgrades.
Resources
Refer to the following resources to learn more about our best practices for sustainability.
41

AWS Well-Architected Framework
Resources
Whitepaper
• Sustainability Pillar
Video
• The Climate Pledge
42

AWS Well-Architected Framework
The review process
The review of architectures must be done in a consistent manner, with a blame-free approach that
encourages diving deep. It should be a lightweight process (hours not days) that is a conversation and
not an audit. The purpose of reviewing an architecture is to identify any critical issues that might need
addressing or areas that could be improved. The outcome of the review is a set of actions that should
improve the experience of a customer using the workload.
As discussed in the “On Architecture” section, you will want each team member to take responsibility
for the quality of its architecture. We recommend that the team members who build an architecture use
the Well-Architected Framework to continually review their architecture, rather than holding a formal
review meeting. A nearly continuous approach permits your team members to update answers as the
architecture evolves, and improve the architecture as you deliver features.
The AWS Well-Architected Framework is aligned to the way that AWS reviews systems and services
internally. It is premised on a set of design principles that inﬂuences architectural approach, and
questions that verify that people don’t neglect areas that often featured in Root Cause Analysis (RCA).
Whenever there is a signiﬁcant issue with an internal system, AWS service, or customer, we look at the
RCA to see if we could improve the review processes we use.
Reviews should be applied at key milestones in the product lifecycle, early on in the design phase to
avoid one-way doors that are diﬃcult to change, and then before the go-live date. (Many decisions are
reversible, two-way doors. Those decisions can use a lightweight process. One-way doors are hard or
impossible to reverse and require more inspection before making them.) After you go into production,
your workload will continue to evolve as you add new features and change technology implementations.
The architecture of a workload changes over time. You must follow good hygiene practices to stop
its architectural characteristics from degrading as you evolve it. As you make signiﬁcant architecture
changes, you should follow a set of hygiene processes including a Well-Architected review.
If you want to use the review as a one-time snapshot or independent measurement, you will want to
verify that you have all the right people in the conversation. Often, we ﬁnd that reviews are the ﬁrst
time that a team truly understands what they have implemented. An approach that works well when
reviewing another team's workload is to have a series of informal conversations about their architecture
where you can glean the answers to most questions. You can then follow up with one or two meetings
where you can gain clarity or dive deep on areas of ambiguity or perceived risk.
Here are some suggested items to facilitate your meetings:
• A meeting room with whiteboards
• Print outs of any diagrams or design notes
• Action list of questions that require out-of-band research to answer (for example, “did we activate
encryption or not?”)
After you have done a review, you should have a list of issues that you can prioritize based on your
business context. You will also want to take into account the impact of those issues on the day-to-day
work of your team. If you address these issues early, you could free up time to work on creating business
value rather than solving recurring problems. As you address issues, you can update your review to see
how the architecture is improving.
While the value of a review is clear after you have done one, you may ﬁnd that a new team might be
resistant at ﬁrst. Here are some objections that can be handled through educating the team on the
beneﬁts of a review:
• “We are too busy!” (Often said when the team is getting ready for a signiﬁcant launch.)
43

AWS Well-Architected Framework
• If you are getting ready for a big launch, you will want it to go smoothly. The review will permit you
to understand any problems you might have missed.
• We recommend that you carry out reviews early in the product lifecycle to uncover risks and develop
a mitigation plan aligned with the feature delivery roadmap.
• “We don’t have time to do anything with the results!” (Often said when there is an immovable event,
such as the Super Bowl, that they are targeting.)
• These events can’t be moved. Do you really want to go into it without knowing the risks in your
architecture? Even if you don’t address all of these issues you can still have playbooks for handling
them if they materialize.
• “We don’t want others to know the secrets of our solution implementation!”
• If you point the team at the questions in the Well-Architected Framework, they will see that none of
the questions reveal any commercial or technical proprietary information.
As you carry out multiple reviews with teams in your organization, you might identify thematic issues.
For example, you might see that a group of teams has clusters of issues in a particular pillar or topic.
You will want to look at all your reviews in a holistic manner, and identify any mechanisms, training, or
principal engineering talks that could help address those thematic issues.
44

AWS Well-Architected Framework
Conclusion
The AWS Well-Architected Framework provides architectural best practices across the six pillars for
designing and operating reliable, secure, eﬃcient, cost-eﬀective, and sustainable systems in the
cloud. The Framework provides a set of questions that allows you to review an existing or proposed
architecture. It also provides a set of AWS best practices for each pillar. Using the Framework in your
architecture will help you produce stable and eﬃcient systems, which allow you to focus on your
functional requirements.
45

AWS Well-Architected Framework
Contributors
The following individuals and organizations contributed to this document:
• Brian Carlson, Operations Lead Well-Architected, Amazon Web Services
• Ben Potter, Security Lead Well-Architected, Amazon Web Services
• Seth Eliot, Reliability Lead Well-Architected, Amazon Web Services
• Eric Pullen, Sr. Solutions Architect, Amazon Web Services
• Rodney Lester, Principal Solutions Architect, Amazon Web Services
• Jon Steele, Sr. Technical Account Manager, Amazon Web Services
• Max Ramsay, Principal Security Solutions Architect, Amazon Web Services
• Callum Hughes, Solutions Architect, Amazon Web Services
• Aden Leirer, Content Program Manager Well-Architected, Amazon Web Services
46

AWS Well-Architected Framework
Further reading
AWS Architecture Center
AWS Cloud Compliance
AWS Well-Architected Partner program
AWS Well-Architected Tool
AWS Well-Architected homepage
Operational Excellence Pillar whitepaper
Security Pillar whitepaper
Reliability Pillar whitepaper
Performance Eﬃciency Pillar whitepaper
Cost Optimization Pillar whitepaper
Sustainability Pillar whitepaper
The Amazon Builders' Library
47

AWS Well-Architected Framework
Document revisions
To be notiﬁed about updates to this whitepaper, subscribe to the RSS feed.
Change
Description
Updates for new
Framework (p. 48)
Best practices updated with
prescriptive guidance and new
best practices added. New
questions added to the Security
and Cost Optimization pillars.
Minor update (p. 48)
Whitepaper updated (p. 48)
Major update (p. 36)
Minor update (p. 48)
Minor update (p. 48)
Minor update (p. 48)
Updates for new
Framework (p. 48)
Whitepaper updated (p. 48)
Added deﬁnition for level
of eﬀort and updated best
practices in the appendix.
Added Sustainability Pillar and
updated links.
Sustainability Pillar added to the
framework.
Removed non-inclusive
language.
Fixed numerous links.
Minor editorial changes
throughout.
Review and rewrite of most
questions and answers.
Addition of AWS Well-
Architected Tool, links to AWS
Well-Architected Labs, and
AWS Well-Architected Partners,
minor ﬁxes to enable multiple
language version of framework.
Whitepaper updated (p. 48)
Review and rewrite of most
questions and answers, to ensure
questions focus on one topic
at a time. This caused some
previous questions to be split
into multiple questions. Added
common terms to deﬁnitions
(workload, component etc).
Changed presentation of
question in main body to include
descriptive text.
Whitepaper updated (p. 48)
Updates to simplify question
text, standardize answers, and
improve readability.
48
Date
April 10, 2023
October 20, 2022
December 2, 2021
November 20, 2021
April 22, 2021
March 10, 2021
July 15, 2020
July 8, 2020
July 1, 2019
November 1, 2018
June 1, 2018

AWS Well-Architected Framework
Whitepaper updated (p. 48)
Operational Excellence moved to
front of pillars and rewritten so
it frames other pillars. Refreshed
other pillars to reﬂect evolution
of AWS.
Whitepaper updated (p. 48)
Updated the Framework to
include operational excellence
pillar, and revised and updated
the other pillars to reduce
duplication and incorporate
learnings from carrying out
reviews with thousands of
customers.
Minor updates (p. 48)
Initial publication (p. 48)
Updated the Appendix with
current Amazon CloudWatch
Logs information.
AWS Well-Architected
Framework published.
November 1, 2017
November 1, 2016
November 1, 2015
October 1, 2015
49

AWS Well-Architected Framework
Operational excellence
Appendix: Questions and best
practices
This appendix summarizes all the questions and best practices in the AWS Well-Architected Framework.
Pillars
• Operational excellence (p. 50)
• Security (p. 151)
• Reliability (p. 246)
• Performance eﬃciency (p. 378)
• Cost optimization (p. 454)
• Sustainability (p. 526)
Operational excellence
The Operational Excellence pillar includes the ability to support development and run workloads
eﬀectively, gain insight into your operations, and to continuously improve supporting processes and
procedures to deliver business value. You can ﬁnd prescriptive guidance on implementation in the
Operational Excellence Pillar whitepaper.
Best practice areas
• Organization (p. 50)
• Prepare (p. 73)
• Operate (p. 114)
• Evolve (p. 140)
Organization
Questions
• OPS 1. How do you determine what your priorities are? (p. 50)
• OPS 2. How do you structure your organization to support your business outcomes? (p. 59)
• OPS 3. How does your organizational culture support your business outcomes? (p. 65)
OPS 1. How do you determine what your priorities are?
Everyone should understand their part in enabling business success. Have shared goals in order to set
priorities for resources. This will maximize the beneﬁts of your eﬀorts.
Best practices
• OPS01-BP01 Evaluate external customer needs (p. 51)
• OPS01-BP02 Evaluate internal customer needs (p. 51)
50

AWS Well-Architected Framework
Organization
• OPS01-BP03 Evaluate governance requirements (p. 52)
• OPS01-BP04 Evaluate compliance requirements (p. 54)
• OPS01-BP05 Evaluate threat landscape (p. 56)
• OPS01-BP06 Evaluate tradeoﬀs (p. 57)
• OPS01-BP07 Manage beneﬁts and risks (p. 58)
OPS01-BP01 Evaluate external customer needs
Involve key stakeholders, including business, development, and operations teams, to determine where to
focus eﬀorts on external customer needs. This will ensure that you have a thorough understanding of the
operations support that is required to achieve your desired business outcomes.
Common anti-patterns:
• You have decided not to have customer support outside of core business hours, but you haven't
reviewed historical support request data. You do not know whether this will have an impact on your
customers.
• You are developing a new feature but have not engaged your customers to ﬁnd out if it is desired, if
desired in what form, and without experimentation to validate the need and method of delivery.
Beneﬁts of establishing this best practice: Customers whose needs are satisﬁed are much more likely to
remain customers. Evaluating and understanding external customer needs will inform how you prioritize
your eﬀorts to deliver business value.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Understand business needs: Business success is created by shared goals and understanding across
stakeholders, including business, development, and operations teams.
• Review business goals, needs, and priorities of external customers: Engage key stakeholders,
including business, development, and operations teams, to discuss goals, needs, and priorities of
external customers. This ensures that you have a thorough understanding of the operational support
that is required to achieve business and customer outcomes.
• Establish shared understanding: Establish shared understanding of the business functions of the
workload, the roles of each of the teams in operating the workload, and how these factors support
your shared business goals across internal and external customers.
Resources
Related documents:
• AWS Well-Architected Framework Concepts – Feedback loop
OPS01-BP02 Evaluate internal customer needs
Involve key stakeholders, including business, development, and operations teams, when determining
where to focus eﬀorts on internal customer needs. This will ensure that you have a thorough
understanding of the operations support that is required to achieve business outcomes.
Use your established priorities to focus your improvement eﬀorts where they will have the greatest
impact (for example, developing team skills, improving workload performance, reducing costs,
automating runbooks, or enhancing monitoring). Update your priorities as needs change.
51

AWS Well-Architected Framework
Organization
Common anti-patterns:
• You have decided to change IP address allocations for your product teams, without consulting them,
to make managing your network easier. You do not know the impact this will have on your product
teams.
• You are implementing a new development tool but have not engaged your internal customers to ﬁnd
out if it is needed or if it is compatible with their existing practices.
• You are implementing a new monitoring system but have not contacted your internal customers to
ﬁnd out if they have monitoring or reporting needs that should be considered.
Beneﬁts of establishing this best practice: Evaluating and understanding internal customer needs will
inform how you prioritize your eﬀorts to deliver business value.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Understand business needs: Business success is created by shared goals and understanding across
stakeholders including business, development, and operations teams.
• Review business goals, needs, and priorities of internal customers: Engage key stakeholders,
including business, development, and operations teams, to discuss goals, needs, and priorities of
internal customers. This ensures that you have a thorough understanding of the operational support
that is required to achieve business and customer outcomes.
• Establish shared understanding: Establish shared understanding of the business functions of the
workload, the roles of each of the teams in operating the workload, and how these factors support
shared business goals across internal and external customers.
Resources
Related documents:
• AWS Well-Architected Framework Concepts – Feedback loop
OPS01-BP03 Evaluate governance requirements
Governance is the set of policies, rules, or frameworks that a company uses to achieve its business goals.
Governance requirements are generated from within your organization. They can aﬀect the types of
technologies you choose or inﬂuence the way you operate your workload. Incorporate organizational
governance requirements into your workload. Conformance is the ability to demonstrate that you have
implemented governance requirements.
Desired outcome:
• Governance requirements are incorporated into the architectural design and operation of your
workload.
• You can provide proof that you have followed governance requirements.
• Governance requirements are regularly reviewed and updated.
Common anti-patterns:
• Your organization mandates that the root account has multi-factor authentication. You failed to
implement this requirement and the root account is compromised.
• During the design of your workload, you choose an instance type that is not approved by the IT
department. You are unable to launch your workload and must conduct a redesign.
52

AWS Well-Architected Framework
Organization
• You are required to have a disaster recovery plan. You did not create one and your workload suﬀers an
extended outage.
• Your team wants to use new instances but your governance requirements have not been updated to
allow them.
Beneﬁts of establishing this best practice:
• Following governance requirements aligns your workload with larger organization policies.
• Governance requirements reﬂect industry standards and best practices for your organization.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Identify governance requirement by working with stakeholders and governance organizations. Include
governance requirements into your workload. Be able to demonstrate proof that you’ve followed
governance requirements.
Customer example
At AnyCompany Retail, the cloud operations team works with stakeholders across the organization to
develop governance requirements. For example, they prohibit SSH access into Amazon EC2 instances. If
teams need system access, they are required to use AWS Systems Manager Session Manager. The cloud
operations team regularly updates governance requirements as new services become available.
Implementation steps
1. Identify the stakeholders for your workload, including any centralized teams.
2. Work with stakeholders to identify governance requirements.
3. Once you’ve generated a list, prioritize the improvement items, and begin implementing them into
your workload.
a. Use services like AWS Conﬁg to create governance-as-code and validate that governance
requirements are followed.
b. If you use AWS Organizations, you can leverage Service Control Policies to implement governance
requirements.
4. Provide documentation that validates the implementation.
Level of eﬀort for the implementation plan: Medium. Implementing missing governance requirements
may result in rework of your workload.
Resources
Related best practices:
• OPS01-BP04 Evaluate compliance requirements (p. 54) - Compliance is like governance but comes
from outside an organization.
Related documents:
• AWS Management and Governance Cloud Environment Guide
• Best Practices for AWS Organizations Service Control Policies in a Multi-Account Environment
• Governance in the AWS Cloud: The Right Balance Between Agility and Safety
• What is Governance, Risk, And Compliance (GRC)?
53

AWS Well-Architected Framework
Organization
Related videos:
• AWS Management and Governance: Conﬁguration, Compliance, and Audit - AWS Online Tech Talks
• AWS re:Inforce 2019: Governance for the Cloud Age (DEM12-R1)
• AWS re:Invent 2020: Achieve compliance as code using AWS Conﬁg
• AWS re:Invent 2020: Agile governance on AWS GovCloud (US)
Related examples:
• AWS Conﬁg Conformance Pack Samples
Related services:
• AWS Conﬁg
• AWS Organizations - Service Control Policies
OPS01-BP04 Evaluate compliance requirements
Regulatory, industry, and internal compliance requirements are an important driver for deﬁning your
organization’s priorities. Your compliance framework may preclude you from using speciﬁc technologies
or geographic locations. Apply due diligence if no external compliance frameworks are identiﬁed.
Generate audits or reports that validate compliance.
If you advertise that your product meets speciﬁc compliance standards, you must have an internal
process for ensuring continuous compliance. Examples of compliance standards include PCI DSS,
FedRAMP, and HIPAA. Applicable compliance standards are determined by various factors, such as what
types of data the solution stores or transmits and which geographic regions the solution supports.
Desired outcome:
• Regulatory, industry, and internal compliance requirements are incorporated into architectural
selection.
• You can validate compliance and generate audit reports.
Common anti-patterns:
• Parts of your workload fall under the Payment Card Industry Data Security Standard (PCI-DSS)
framework but your workload stores credit cards data unencrypted.
• Your software developers and architects are unaware of the compliance framework that your
organization must adhere to.
• The yearly Systems and Organizations Control (SOC2) Type II audit is happening soon and you are
unable to verify that controls are in place.
Beneﬁts of establishing this best practice:
• Evaluating and understanding the compliance requirements that apply to your workload will inform
how you prioritize your eﬀorts to deliver business value.
• You choose the right locations and technologies that are congruent with your compliance framework.
• Designing your workload for auditability helps you to prove you are adhering to your compliance
framework.
Level of risk exposed if this best practice is not established: High
54

AWS Well-Architected Framework
Organization
Implementation guidance
Implementing this best practice means that you incorporate compliance requirements into your
architecture design process. Your team members are aware of the required compliance framework. You
validate compliance in line with the framework.
Customer example
AnyCompany Retail stores credit card information for customers. Developers on the card storage team
understand that they need to comply with the PCI-DSS framework. They’ve taken steps to verify that
credit card information is stored and accessed securely in line with the PCI-DSS framework. Every year
they work with their security team to validate compliance.
Implementation steps
1. Work with your security and governance teams to determine what industry, regulatory, or internal
compliance frameworks that your workload must adhere to. Incorporate the compliance frameworks
into your workload.
a. Validate continual compliance of AWS resources with services like AWS Compute Optimizer and
AWS Security Hub.
2. Educate your team members on the compliance requirements so they can operate and evolve
the workload in line with them. Compliance requirements should be included in architectural and
technological choices.
3. Depending on the compliance framework, you may be required to generate an audit or compliance
report. Work with your organization to automate this process as much as possible.
a. Use services like AWS Audit Manager to generate validate compliance and generate audit reports.
b. You can download AWS security and compliance documents with AWS Artifact.
Level of eﬀort for the implementation plan: Medium. Implementing compliance frameworks can be
challenging. Generating audit reports or compliance documents adds additional complexity.
Resources
Related best practices:
• SEC01-BP03 Identify and validate control objectives - Security control objectives are an important part
of overall compliance.
• SEC01-BP06 Automate testing and validation of security controls in pipelines - As part of your
pipelines, validate security controls. You can also generate compliance documentation for new
changes.
• SEC07-BP02 Deﬁne data protection controls - Many compliance frameworks have data handling and
storage policies based.
• SEC10-BP03 Prepare forensic capabilities - Forensic capabilities can sometimes be used in auditing
compliance.
Related documents:
• AWS Compliance Center
• AWS Compliance Resources
• AWS Risk and Compliance Whitepaper
• AWS Shared Responsibility Model
• AWS services in scope by compliance programs
Related videos:
55

AWS Well-Architected Framework
Organization
• AWS re:Invent 2020: Achieve compliance as code using AWS Compute Optimizer
• AWS re:Invent 2021 - Cloud compliance, assurance, and auditing
• AWS Summit ATL 2022 - Implementing compliance, assurance, and auditing on AWS (COP202)
Related examples:
• PCI DSS and AWS Foundational Security Best Practices on AWS
Related services:
• AWS Artifact
• AWS Audit Manager
• AWS Compute Optimizer
• AWS Security Hub
OPS01-BP05 Evaluate threat landscape
Evaluate threats to the business (for example, competition, business risk and liabilities, operational risks,
and information security threats) and maintain current information in a risk registry. Include the impact
of risks when determining where to focus eﬀorts.
The Well-Architected Framework emphasizes learning, measuring, and improving. It provides a
consistent approach for you to evaluate architectures, and implement designs that will scale over time.
AWS provides the AWS Well-Architected Tool to help you review your approach prior to development,
the state of your workloads prior to production, and the state of your workloads in production. You
can compare them to the latest AWS architectural best practices, monitor the overall status of your
workloads, and gain insight to potential risks.
AWS customers are eligible for a guided Well-Architected Review of their mission-critical workloads to
measure their architectures against AWS best practices. Enterprise Support customers are eligible for an
Operations Review, designed to help them to identify gaps in their approach to operating in the cloud.
The cross-team engagement of these reviews helps to establish common understanding of your
workloads and how team roles contribute to success. The needs identiﬁed through the review can help
shape your priorities.
AWS Trusted Advisor is a tool that provides access to a core set of checks that recommend optimizations
that may help shape your priorities. Business and Enterprise Support customers receive access to
additional checks focusing on security, reliability, performance, and cost-optimization that can further
help shape their priorities.
Common anti-patterns:
• You are using an old version of a software library in your product. You are unaware of security updates
to the library for issues that may have unintended impact on your workload.
• Your competitor just released a version of their product that addresses many of your customers'
complaints about your product. You have not prioritized addressing any of these known issues.
• Regulators have been pursuing companies like yours that are not compliant with legal regulatory
compliance requirements. You have not prioritized addressing any of your outstanding compliance
requirements.
Beneﬁts of establishing this best practice: Identifying and understanding the threats to your
organization and workload helps your determination of which threats to address, their priority, and the
resources necessary to do so.
56

AWS Well-Architected Framework
Organization
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Evaluate threat landscape: Evaluate threats to the business (for example, competition, business risk
and liabilities, operational risks, and information security threats), so that you can include their impact
when determining where to focus eﬀorts.
• AWS Latest Security Bulletins
• AWS Trusted Advisor
• Maintain a threat model: Establish and maintain a threat model identifying potential threats,
planned and in place mitigations, and their priority. Review the probability of threats manifesting as
incidents, the cost to recover from those incidents and the expected harm caused, and the cost to
prevent those incidents. Revise priorities as the contents of the threat model change.
Resources
Related documents:
• AWS Cloud Compliance
• AWS Latest Security Bulletins
• AWS Trusted Advisor
OPS01-BP06 Evaluate tradeoﬀs
Evaluate the impact of tradeoﬀs between competing interests or alternative approaches, to help make
informed decisions when determining where to focus eﬀorts or choosing a course of action. For example,
accelerating speed to market for new features may be emphasized over cost optimization, or you may
choose a relational database for non-relational data to simplify the eﬀort to migrate a system, rather
than migrating to a database optimized for your data type and updating your application.
AWS can help you educate your teams about AWS and its services to increase their understanding of
how their choices can have an impact on your workload. You should use the resources provided by
AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS
Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help
with your AWS questions.
AWS also shares best practices and patterns that we have learned through the operation of AWS in The
Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog
and The Oﬃcial AWS Podcast.
Common anti-patterns:
• You are using a relational database to manage time series and non-relational data. There are database
options that are optimized to support the data types you are using but you are unaware of the beneﬁts
because you have not evaluated the tradeoﬀs between solutions.
• Your investors request that you demonstrate compliance with Payment Card Industry Data Security
Standards (PCI DSS). You do not consider the tradeoﬀs between satisfying their request and continuing
with your current development eﬀorts. Instead you proceed with your development eﬀorts without
demonstrating compliance. Your investors stop their support of your company over concerns about the
security of your platform and their investments.
Beneﬁts of establishing this best practice: Understanding the implications and consequences of your
choices helps you to prioritize your options.
Level of risk exposed if this best practice is not established: Medium
57

AWS Well-Architected Framework
Organization
Implementation guidance
• Evaluate tradeoﬀs: Evaluate the impact of tradeoﬀs between competing interests, to help make
informed decisions when determining where to focus eﬀorts. For example, accelerating speed to
market for new features might be emphasized over cost optimization.
• AWS can help you educate your teams about AWS and its services to increase their understanding of
how their choices can have an impact on your workload. You should use the resources provided by
AWS Support (AWS Knowledge Center, AWS Discussion Forums, and AWS Support Center) and AWS
Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for
help with your AWS questions.
• AWS also shares best practices and patterns that we have learned through the operation of AWS in The
Amazon Builders' Library. A wide variety of other useful information is available through the AWS Blog
and The Oﬃcial AWS Podcast.
Resources
Related documents:
• AWS Blog
• AWS Cloud Compliance
• AWS Discussion Forums
• AWS Documentation
• AWS Knowledge Center
• AWS Support
• AWS Support Center
• The Amazon Builders' Library
• The Oﬃcial AWS Podcast
OPS01-BP07 Manage beneﬁts and risks
Manage beneﬁts and risks to make informed decisions when determining where to focus eﬀorts. For
example, it may be beneﬁcial to deploy a workload with unresolved issues so that signiﬁcant new
features can be made available to customers. It may be possible to mitigate associated risks, or it may
become unacceptable to allow a risk to remain, in which case you will take action to address the risk.
You might ﬁnd that you want to emphasize a small subset of your priorities at some point in time.
Use a balanced approach over the long term to ensure the development of needed capabilities and
management of risk. Update your priorities as needs change
Common anti-patterns:
• You have decided to include a library that does everything you need that one of your developers found
on the internet. You have not evaluated the risks of adopting this library from an unknown source and
do not know if it contains vulnerabilities or malicious code.
• You have decided to develop and deploy a new feature instead of ﬁxing an existing issue. You have not
evaluated the risks of leaving the issue in place until the feature is deployed and do not know what the
impact will be on your customers.
• You have decided to not deploy a feature frequently requested by customers because of unspeciﬁed
concerns from your compliance team.
Beneﬁts of establishing this best practice: Identifying the available beneﬁts of your choices, and being
aware of the risks to your organization, helps you to make informed decisions.
58

AWS Well-Architected Framework
Organization
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Manage beneﬁts and risks: Balance the beneﬁts of decisions against the risks involved.
• Identify beneﬁts: Identify beneﬁts based on business goals, needs, and priorities. Examples include
time-to-market, security, reliability, performance, and cost.
• Identify risks: Identify risks based on business goals, needs, and priorities. Examples include time-to-
market, security, reliability, performance, and cost.
• Assess beneﬁts against risks and make informed decisions: Determine the impact of beneﬁts and
risks based on goals, needs, and priorities of your key stakeholders, including business, development,
and operations. Evaluate the value of the beneﬁt against the probability of the risk being realized
and the cost of its impact. For example, emphasizing speed-to-market over reliability might provide
competitive advantage. However, it may result in reduced uptime if there are reliability issues.
OPS 2. How do you structure your organization to support your
business outcomes?
Your teams must understand their part in achieving business outcomes. Teams should understand their
roles in the success of other teams, the role of other teams in their success, and have shared goals.
Understanding responsibility, ownership, how decisions are made, and who has authority to make
decisions will help focus eﬀorts and maximize the beneﬁts from your teams.
Best practices
• OPS02-BP01 Resources have identiﬁed owners (p. 59)
• OPS02-BP02 Processes and procedures have identiﬁed owners (p. 61)
• OPS02-BP03 Operations activities have identiﬁed owners responsible for their
performance (p. 62)
• OPS02-BP04 Team members know what they are responsible for (p. 62)
• OPS02-BP05 Mechanisms exist to identify responsibility and ownership (p. 62)
• OPS02-BP06 Mechanisms exist to request additions, changes, and exceptions (p. 63)
• OPS02-BP07 Responsibilities between teams are predeﬁned or negotiated (p. 64)
OPS02-BP01 Resources have identiﬁed owners
Resources for your workload must have identiﬁed owners for change control, troubleshooting, and other
functions. Owners are assigned for workloads, accounts, infrastructure, platforms, and applications.
Ownership is recorded using tools like a central register or metadata attached to resources. The business
value of components informs the processes and procedures applied to them.
Desired outcome:
• Resources have identiﬁed owners using metadata or a central register.
• Team members can identify who owns resources.
• Accounts have a single owner where possible.
Common anti-patterns:
• The alternate contacts for your AWS accounts are not populated.
• Resources lack tags that identify what teams own them.
• You have an ITSM queue without an email mapping.
59

AWS Well-Architected Framework
Organization
• Two teams have overlapping ownership of a critical piece of infrastructure.
Beneﬁts of establishing this best practice:
• Change control for resources is straightforward with assigned ownership.
• You can involve the right owners when troubleshooting issues.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Deﬁne what ownership means for the resource use cases in your environment. Ownership can mean who
oversees changes to the resource, supports the resource during troubleshooting, or who is ﬁnancially
accountable. Specify and record owners for resources, including name, contact information, organization,
and team.
Customer example
AnyCompany Retail deﬁnes ownership as the team or individual that owns changes and support for
resources. They leverage AWS Organizations to manage their AWS accounts. Alternate account contacts
are conﬁguring using group inboxes. Each ITSM queue maps to an email alias. Tags identify who own
AWS resources. For other platforms and infrastructure, they have a wiki page that identiﬁes ownership
and contact information.
Implementation steps
1. Start by deﬁning ownership for your organization. Ownership can imply who owns the risk for the
resource, who owns changes to the resource, or who supports the resource when troubleshooting.
Ownership could also imply ﬁnancial or administrative ownership of the resource.
2. Use AWS Organizations to manage accounts. You can manage the alternate contacts for your accounts
centrally.
a. Using company owned email addresses and phone numbers for contact information helps you to
access them even if the individuals whom they belong to are no longer with your organization. For
example, create separate email distribution lists for billing, operations, and security and conﬁgure
these as Billing, Security, and Operations contacts in each active AWS account. Multiple people will
receive AWS notiﬁcations and be able to respond, even if someone is on vacation, changes roles, or
leaves the company.
b. If an account is not managed by AWS Organizations, alternate account contacts help AWS get in
contact with the appropriate personnel if needed. Conﬁgure the account’s alternate contacts to
point to a group rather than an individual.
3. Use tags to identify owners for AWS resources. You can specify both owners and their contact
information in separate tags.
a. You can use AWS Conﬁg rules to enforce that resources have the required ownership tags.
b. For in-depth guidance on how to build a tagging strategy for your organization, see AWS Tagging
Best Practices whitepaper.
4. For other resources, platforms, and infrastructure, create documentation that identiﬁes ownership.
This should be accessible to all team members.
Level of eﬀort for the implementation plan: Low. Leverage account contact information and tags to
assign ownership of AWS resources. For other resources you can use something as simple as a table in a
wiki to record ownership and contact information, or use an ITSM tool to map ownership.
Resources
Related best practices:
60

AWS Well-Architected Framework
Organization
• OPS02-BP02 Processes and procedures have identiﬁed owners (p. 61) - The processes and
procedures to support resources depends on resource ownership.
• OPS02-BP04 Team members know what they are responsible for (p. 62) - Team members should
understand what resources they are owners of.
• OPS02-BP05 Mechanisms exist to identify responsibility and ownership (p. 62) - Ownership needs
to be discoverable using mechanisms like tags or account contacts.
Related documents:
• AWS Account Management - Updating contact information
• AWS Conﬁg Rules - required-tags
• AWS Organizations - Updating alternative contacts in your organization
• AWS Tagging Best Practices whitepaper
Related examples:
• AWS Conﬁg Rules - Amazon EC2 with required tags and valid values
Related services:
• AWS Conﬁg
• AWS Organizations
OPS02-BP02 Processes and procedures have identiﬁed owners
Understand who has ownership of the deﬁnition of individual processes and procedures, why those
speciﬁc process and procedures are used, and why that ownership exists. Understanding the reasons that
speciﬁc processes and procedures are used aids in identiﬁcation of improvement opportunities.
Beneﬁts of establishing this best practice: Understanding ownership identiﬁes who can approve
improvements, implement those improvements, or both.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Process and procedures have identiﬁed owners responsible for their deﬁnition: Capture the processes
and procedures used in your environment and the individual or team responsible for their deﬁnition.
• Identify process and procedures: Identify the operations activities conducted in support of your
workloads. Document these activities in a discoverable location.
• Deﬁne who owns the deﬁnition of a process or procedure: Uniquely identify the individual or team
responsible for the speciﬁcation of an activity. They are responsible to ensure it can be successfully
performed by an adequately skilled team member with the correct permissions, access, and tools. If
there are issues with performing that activity, the team members performing it are responsible to
provide the detailed feedback necessary for the activitiy to be improved.
• Capture ownership in the metadata of the activity artifact: Procedures automated in services like
AWS Systems Manager, through documents, and AWS Lambda, as functions, support capturing
metadata information as tags. Capture resource ownership using tags or resource groups, specifying
ownership and contact information. Use AWS Organizations to create tagging polices and ensure
ownership and contact information are captured.
61

AWS Well-Architected Framework
Organization
OPS02-BP03 Operations activities have identiﬁed owners responsible for their
performance
Understand who has responsibility to perform speciﬁc activities on deﬁned workloads and why that
responsibility exists. Understanding who has responsibility to perform activities informs who will conduct
the activity, validate the result, and provide feedback to the owner of the activity.
Beneﬁts of establishing this best practice: Understanding who is responsible to perform an activity
informs whom to notify when action is needed and who will perform the action, validate the result, and
provide feedback to the owner of the activity.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Operations activities have identiﬁed owners responsible for their performance: Capture the
responsibility for performing processes and procedures used in your environment
• Identify process and procedures: Identify the operations activities conducted in support of your
workloads. Document these activities in a discoverable location.
• Deﬁne who is responsible to perform each activity: Identify the team responsible for an activity.
Ensure they have the details of the activity, and the necessary skills and correct permissions, access,
and tools to perform the activity. They must understand the condition under which it is to be
performed (for example, on an event or schedule). Make this information discoverable so that
members of your organization can identify who they need to contact, team or individual, for speciﬁc
needs.
OPS02-BP04 Team members know what they are responsible for
Understanding the responsibilities of your role and how you contribute to business outcomes informs the
prioritization of your tasks and why your role is important. This helps team members to recognize needs
and respond appropriately.
Beneﬁts of establishing this best practice: Understanding your responsibilities informs the decisions
you make, the actions you take, and your hand oﬀ activities to their proper owners.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Ensure team members understand their roles and responsibilities: Identify team members roles and
responsibilities and ensure they understand the expectations of their role. Make this information
discoverable so that members of your organization can identify who they need to contact, team or
individual, for speciﬁc needs.
OPS02-BP05 Mechanisms exist to identify responsibility and ownership
Where no individual or team is identiﬁed, there are deﬁned escalation paths to someone with the
authority to assign ownership or plan for that need to be addressed.
Beneﬁts of establishing this best practice: Understanding who has responsbility or ownership allows
you to reach out to the proper team or team member to make a request or transition a task. Having an
identiﬁed person who has the authority to assign responsbility or ownership or plan to address needs
reduces the risk of inaction and needs not being addressed.
Level of risk exposed if this best practice is not established: High
62

AWS Well-Architected Framework
Organization
Implementation guidance
• Mechanisms exist to identify responsibility and ownership: Provide accessible mechanisms for
members of your organization to discover and identify ownership and responsibility. These
mechanisms will help them to identify who to contact, team or individual, for speciﬁc needs.
OPS02-BP06 Mechanisms exist to request additions, changes, and exceptions
You can make requests to owners of processes, procedures, and resources. Requests include additions,
changes, and exceptions. These requests go through a change management process. Make informed
decisions to approve requests where viable and determined to be appropriate after an evaluation of
beneﬁts and risks.
Desired outcome:
• You can make requests to change processes, procedures, and resources based on assigned ownership.
• Changes are made in a deliberate manner, weighing beneﬁts and risks.
Common anti-patterns:
• You must update the way you deploy your application, but there is no way to request a change to the
deployment process from the operations team.
• The disaster recovery plan must be updated, but there is no identiﬁed owner to request changes to.
Beneﬁts of establishing this best practice:
• Processes, procedures, and resources can evolve as requirements change.
• Owners can make informed decisions when to make changes.
• Changes are made in a deliberate manner.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
To implement this best practice, you need to be able to request changes to processes, procedures, and
resources. The change management process can be lightweight. Document the change management
process.
Customer example
AnyCompany Retail uses a responsibility assignment (RACI) matrix to identify who owns changes for
processes, procedures, and resources. They have a documented change management process that’s
lightweight and easy to follow. Using the RACI matrix and the process, anyone can submit change
requests.
Implementation steps
1. Identify the processes, procedures, and resources for your workload and the owners for each.
Document them in your knowledge management system.
a. If you have not implemented OPS02-BP01 Resources have identiﬁed owners (p. 59), OPS02-
BP02 Processes and procedures have identiﬁed owners (p. 61), or OPS02-BP03 Operations
activities have identiﬁed owners responsible for their performance (p. 62), start with those ﬁrst.
2. Work with stakeholders in your organization to develop a change management process. The process
should cover additions, changes, and exceptions for resources, processes, and procedures.
63

AWS Well-Architected Framework
Organization
a. You can use AWS Systems Manager Change Manager as a change management platform for
workload resources.
3. Document the change management process in your knowledge management system.
Level of eﬀort for the implementation plan: Medium. Developing a change management process
requires alignment with multiple stakeholders across your organization.
Resources
Related best practices:
• OPS02-BP01 Resources have identiﬁed owners (p. 59) - Resources need identiﬁed owners before
you build a change management process.
• OPS02-BP02 Processes and procedures have identiﬁed owners (p. 61) - Processes need identiﬁed
owners before you build a change management process.
• OPS02-BP03 Operations activities have identiﬁed owners responsible for their performance (p. 62)
- Operations activities need identiﬁed owners before you build a change management process.
Related documents:
• AWS Prescriptive Guidance - Foundation palybook for AWS large migrations: Creating RACI matrices
• Change Management in the Cloud Whitepaper
Related services:
• AWS Systems Manager Change Manager
OPS02-BP07 Responsibilities between teams are predeﬁned or negotiated
Have deﬁned or negotiated agreements between teams describing how they work with and support each
other (for example, response times, service level objectives, or service-level agreements). Inter-team
communications channels are documented. Understanding the impact of the teams’ work on business
outcomes and the outcomes of other teams and organizations informs the prioritization of their tasks
and helps them respond appropriately.
When responsibility and ownership are undeﬁned or unknown, you are at risk of both not addressing
necessary activities in a timely fashion and of redundant and potentially conﬂicting eﬀorts emerging to
address those needs.
Desired outcome:
• Inter-team working or support agreements are agreed to and documented.
• Teams that support or work with each other have deﬁned communication channels and response
expectations.
Common anti-patterns:
• An issue occurs in production and two separate teams start troubleshooting independent of each
other. Their siloed eﬀorts extend the outage.
• The operations team needs assistance from the development team but there is no agreed to response
time. The request is stuck in the backlog.
Beneﬁts of establishing this best practice:
64

AWS Well-Architected Framework
Organization
• Teams know how to interact and support each other.
• Expectations for responsiveness are known.
• Communications channels are clearly deﬁned.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Implementing this best practice means that there is no ambiguity about how teams work with
each other. Formal agreements codify how teams work together or support each other. Inter-team
communication channels are documented.
Customer example
AnyCompany Retail’s SRE team has a service level agreement with their development team. Whenever
the development team makes a request in their ticketing system, they can expect a response within
ﬁfteen minutes. If there is a site outage, the SRE team takes lead in the investigation with support from
the development team.
Implementation steps
1. Working with stakeholders across your organization, develop agreements between teams based on
processes and procedures.
a. If a process or procedure is shared between two teams, develop a runbook on how the teams will
work together.
b. If there are dependencies between teams, agree to a response SLA for requests.
2. Document responsibilities in your knowledge management system.
Level of eﬀort for the implementation plan: Medium. If there are no existing agreements between
teams, it can take eﬀort to come to agreement with stakeholders across your organization.
Resources
Related best practices:
• OPS02-BP02 Processes and procedures have identiﬁed owners (p. 61) - Process ownership must be
identiﬁed before setting agreements between teams.
• OPS02-BP03 Operations activities have identiﬁed owners responsible for their performance (p. 62)
- Operations activities ownership must be identiﬁed before setting agreements between teams.
Related documents:
• AWS Executive Insights - Empowering Innovation with the Two-Pizza Team
• Introduction to DevOps on AWS - Two-Pizza Teams
OPS 3. How does your organizational culture support your
business outcomes?
Provide support for your team members so that they can be more eﬀective in taking action and
supporting your business outcome.
Best practices
65

AWS Well-Architected Framework
Organization
• OPS03-BP01 Executive Sponsorship (p. 66)
• OPS03-BP02 Team members are empowered to take action when outcomes are at risk (p. 66)
• OPS03-BP03 Escalation is encouraged (p. 67)
• OPS03-BP04 Communications are timely, clear, and actionable (p. 67)
• OPS03-BP05 Experimentation is encouraged (p. 69)
• OPS03-BP06 Team members are encouraged to maintain and grow their skill sets (p. 71)
• OPS03-BP07 Resource teams appropriately (p. 72)
• OPS03-BP08 Diverse opinions are encouraged and sought within and across teams (p. 73)
OPS03-BP01 Executive Sponsorship
Senior leadership clearly sets expectations for the organization and evaluates success. Senior leadership
is the sponsor, advocate, and driver for the adoption of best practices and evolution of the organization
Beneﬁts of establishing this best practice: Engaged leadership, clearly communicated expectations,
and shared goals ensures that team members know what is expected of them. Evaluating success aids in
identiﬁcation of barriers to success so that they can be addressed through intervention by the sponsor
advocate or their delegates.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Executive Sponsorship: Senior leadership clearly sets expectations for the organization and evaluates
success. Senior leadership is the sponsor, advocate, and driver for the adoption of best practices and
evolution of the organization
• Set expectations: Deﬁne and publish goals for your organizations including how they will be
measured.
• Track achievement of goals: Measure the incremental achievement of goals regularly and share the
results so that appropriate action can be taken if outcomes are at risk.
• Provide the resources necessary to achieve your goals: Regularly review if resources are still
appropriate, of if additional resources are needed based on: new information, changes to goals,
responsibilities, or your business environment.
• Advocate for your teams: Remain engaged with your teams so that you understand how they are
doing and if there are external factors aﬀecting them. When your teams are impacted by external
factors, reevaluate goals and adjust targets as appropriate. Identify obstacles that are impeding
your teams progress. Act on behalf of your teams to help address obstacles and remove unnecessary
burdens.
• Be a driver for adoption of best practices: Acknowledge best practices that provide quantiﬁable
beneﬁts and recognize the creators and adopters. Encourage further adoption to magnify the
beneﬁts achieved.
• Be a driver for evolution of for your teams: Create a culture of continual improvement. Encourage
both personal and organizational growth and development. Provide long term targets to strive for
that will require incremental achievement over time. Adjust this vision to compliment your needs,
business goals, and business environment as they change.
OPS03-BP02 Team members are empowered to take action when outcomes are
at risk
The workload owner has deﬁned guidance and scope empowering team members to respond when
outcomes are at risk. Escalation mechanisms are used to get direction when events are outside of the
deﬁned scope.
66

AWS Well-Architected Framework
Organization
Beneﬁts of establishing this best practice: By testing and validating changes early, you are able
to address issues with minimized costs and limit the impact on your customers. By testing prior to
deployment you minimize the introduction of errors.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Team members are empowered to take action when outcomes are at risk: Provide your team members
the permissions, tools, and opportunity to practice the skills necessary to respond eﬀectively.
• Give your team members opportunity to practice the skills necessary to respond: Provide alternative
safe environments where processes and procedures can be tested and trained upon safely. Perform
game days to allow team members to gain experience responding to real world incidents in
simulated and safe environments.
• Deﬁne and acknowledge team members' authority to take action: Speciﬁcally deﬁne team members
authority to take action by assigning permissions and access to the workloads and components they
support. Acknowledge that they are empowered to take action when outcomes are at risk.
OPS03-BP03 Escalation is encouraged
Team members have mechanisms and are encouraged to escalate concerns to decision makers and
stakeholders if they believe outcomes are at risk. Escalation should be performed early and often so that
risks can be identiﬁed, and prevented from causing incidents.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Encourage early and frequent escalation: Organizationally acknowledge that escalation early and
often is the best practice. Organizationally acknowledge and accept that escalations may prove to
be unfounded, and that it is better to have the opportunity to prevent an incident then to miss that
opportunity by not escalating.
• Have a mechanism for escalation: Have documented procedures deﬁning when and how escalation
should occur. Document the series of people with increasing authority to take action or approve
action and their contact information. Escalation should continue until the team member is satisﬁed
that they have handed oﬀ the risk to a person able to address it, or they have contacted the person
who owns the risk and liability for the operation of the workload. It is that person who ultimately
owns all decisions with respect to their workload. Escalations should include the nature of the risk,
the criticality of the workload, who is impacted, what the impact is, and the urgency, that is, when is
the impact expected.
• Protect employees who escalate: Have policy that protects team members from retribution if they
escalate around a non-responsive decision maker or stakeholder. Have mechanisms in place to
identify if this is occurring and respond appropriately.
OPS03-BP04 Communications are timely, clear, and actionable
Mechanisms exist and are used to provide timely notice to team members of known risks and planned
events. Necessary context, details, and time (when possible) are provided to support determining if
action is necessary, what action is required, and to take action in a timely manner. For example, providing
notice of software vulnerabilities so that patching can be expedited, or providing notice of planned
sales promotions so that a change freeze can be implemented to avoid the risk of service disruption.
Planned events can be recorded in a change calendar or maintenance schedule so that team members
can identify what activities are pending.
Desired outcome:
67

AWS Well-Architected Framework
Organization
• Communications provide context, details, and time expectations.
• Team members have a clear understanding of when and how to act in response to communications.
• Leverage change calendars to provide notice of expected changes.
Common anti-patterns:
• An alert happens several times per week that is a false positive. You mute the notiﬁcation each time it
happens.
• You are asked to make a change to your security groups but are not given an expectation of when it
should happen.
• You receive constant notiﬁcations in chat when systems scale up but no action is necessary. You avoid
the chat channel and miss an important notiﬁcation.
• A change is made to production without informing the operations team. The change creates an alert
and the on-call team is activated.
Beneﬁts of establishing this best practice:
• Your organization avoids alert fatigue.
• Team members can act with the necessary context and expectations.
• Changes can be made during change windows, reducing risk.
Level of risk exposed if this best practice is not established: High
Implementation guidance
To implement this best practice, you must work with stakeholders across your organization to agree to
communication standards. Publicize those standards to your organization. Identify and remove alerts
that are false-positive or always on. Utilize change calendars so team members know when actions
can be taken and what activities are pending. Verify that communications lead to clear actions with
necessary context.
Customer example
AnyCompany Retail uses chat as their main communication medium. Alerts and other information
populate speciﬁc channels. When someone must act, the desired outcome is clearly stated, and in many
cases, they are given a runbook or playbook to use. They use a change calendar to schedule major
changes to production systems.
Implementation steps
1. Analyze your alerts for false-positives or alerts that are constantly created. Remove or change them
so that they start when human intervention is required. If an alert is initiated, provide a runbook or
playbook.
a. You can use AWS Systems Manager Documents to build playbooks and runbooks for alerts.
2. Mechanisms are in place to provide notiﬁcation of risks or planned events in a clear and actionable
way with enough notice to allow appropriate responses. Use email lists or chat channels to send
notiﬁcations ahead of planned events.
a. AWS Chatbot can be used to send alerts and respond to events within your organizations
messaging platform.
3. Provide an accessible source of information where planned events can be discovered. Provide
notiﬁcations of planned events from the same system.
a. AWS Systems Manager Change Calendar can be used to create change windows when changes can
occur. This provides team members notice when they can make changes safely.
68

AWS Well-Architected Framework
Organization
4. Monitor vulnerability notiﬁcations and patch information to understand vulnerabilities in the wild and
potential risks associated to your workload components. Provide notiﬁcation to team members so that
they can act.
a. You can subscribe to AWS Security Bulletins to receive notiﬁcations of vulnerabilities on AWS.
Resources
Related best practices:
• OPS07-BP03 Use runbooks to perform procedures (p. 105) - Make communications actionable by
supplying a runbook when the outcome is known.
• OPS07-BP04 Use playbooks to investigate issues (p. 108) - In the case where the outcome is
unknown, playbooks can make communications actionable.
Related documents:
• AWS Security Bulletins
• Open CVE
Related examples:
• Well-Architected Labs: Inventory and Patch Management (Level 100)
Related services:
• AWS Chatbot
• AWS Systems Manager Change Calendar
• AWS Systems Manager Documents
OPS03-BP05 Experimentation is encouraged
Experimentation is a catalyst for turning new ideas into products and features. It accelerates learning and
keeps team members interested and engaged. Team members are encouraged to experiment often to
drive innovation. Even when an undesired result occurs, there is value in knowing what not to do. Team
members are not punished for successful experiments with undesired results.
Desired outcome:
• Your organization encourages experimentation to foster innovation.
• Experiments are used as an opportunity to learn.
Common anti-patterns:
• You want to run an A/B test but there is no mechanism to run the experiment. You deploy a UI change
without the ability to test it. It results in a negative customer experience.
• Your company only has a stage and production environment. There is no sandbox environment to
experiment with new features or products so you must experiment within the production environment.
Beneﬁts of establishing this best practice:
• Experimentation drives innovation.
• You can react faster to feedback from users through experimentation.
69

AWS Well-Architected Framework
Organization
• Your organization develops a culture of learning.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Experiments should be run in a safe manner. Leverage multiple environments to experiment without
jeopardizing production resources. Use A/B testing and feature ﬂags to test experiments. Provide team
members the ability to conduct experiments in a sandbox environment.
Customer example
AnyCompany Retail encourages experimentation. Team members can use 20% of their work week to
experiment or learn new technologies. They have a sandbox environment where they can innovate. A/B
testing is used for new features to validate them with real user feedback.
Implementation steps
1. Work with leadership across your organization to support experimentation. Team members should be
encouraged to conduct experiments in a safe manner.
2. Provide your team members with an environment where they can safely experiment. They must have
access to an environment that is like production.
a. You can use a separate AWS account to create a sandbox environment for experimentation. AWS
Control Tower can be used to provision these accounts.
3. Use feature ﬂags and A/B testing to experiment safely and gather user feedback.
a. AWS AppConﬁg Feature Flags provides the ability to create feature ﬂags.
b. Amazon CloudWatch Evidently can be used to run A/B tests over a limited deployment.
c. You can use AWS Lambda versions to deploy a new version of a function for beta testing.
Level of eﬀort for the implementation plan: High. Providing team members with an environment to
experiment in and a safe way to conduct experiments can require signiﬁcant investment. You may also
need to modify application code to use feature ﬂags or support A/B testing.
Resources
Related best practices:
• OPS11-BP02 Perform post-incident analysis (p. 142) - Learning from incidents is an important driver
for innovation along with experimentation.
• OPS11-BP03 Implement feedback loops (p. 143) - Feedback loops are an important part of
experimentation.
Related documents:
• An Inside Look at the Amazon Culture: Experimentation, Failure, and Customer Obsession
• Best practices for creating and managing sandbox accounts in AWS
• Create a Culture of Experimentation Enabled by the Cloud
• Enabling experimentation and innovation in the cloud at SulAmérica Seguros
• Experiment More, Fail Less
• Organizing Your AWS Environment Using Multiple Accounts - Sandbox OU
• Using AWS AppConﬁg Feature Flags
Related videos:
70

AWS Well-Architected Framework
Organization
• AWS On Air ft. Amazon CloudWatch Evidently | AWS Events
• AWS On Air San Fran Summit 2022 ft. AWS AppConﬁg Feature Flags integration with Jira
• AWS re:Invent 2022 - A deployment is not a release: Control your launches w/feature ﬂags (BOA305-R)
• Programmatically Create an AWS account with AWS Control Tower
• Set Up a Multi-Account AWS Environment that Uses Best Practices for AWS Organizations
Related examples:
• AWS Innovation Sandbox
• End-to-end Personalization 101 for E-Commerce
Related services:
• Amazon CloudWatch Evidently
• AWS AppConﬁg
• AWS Control Tower
OPS03-BP06 Team members are encouraged to maintain and grow their skill
sets
Teams must grow their skill sets to adopt new technologies, and to support changes in demand and
responsibilities in support of your workloads. Growth of skills in new technologies is frequently a
source of team member satisfaction and supports innovation. Support your team members’ pursuit
and maintenance of industry certiﬁcations that validate and acknowledge their growing skills. Cross
train to promote knowledge transfer and reduce the risk of signiﬁcant impact when you lose skilled and
experienced team members with institutional knowledge. Provide dedicated structured time for learning.
AWS provides resources, including the AWS Getting Started Resource Center, AWS Blogs, AWS Online
Tech Talks, AWS Events and Webinars, and the AWS Well-Architected Labs, that provide guidance,
examples, and detailed walkthroughs to educate your teams.
AWS also shares best practices and patterns that we have learned through the operation of AWS in The
Amazon Builders' Library and a wide variety of other useful educational material through the AWS Blog
and The Oﬃcial AWS Podcast.
You should take advantage of the education resources provided by AWS such as the Well-Architected
labs, AWS Support (AWS Knowledge Center, AWS Discussion Forms, and AWS Support Center) and AWS
Documentation to educate your teams. Reach out to AWS Support through AWS Support Center for help
with your AWS questions.
AWS Training and Certiﬁcation provides some free training through self-paced digital courses on AWS
fundamentals. You can also register for instructor-led training to further support the development of
your teams’ AWS skills.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Team members are encouraged to maintain and grow their skill sets: To adopt new technologies,
support innovation, and to support changes in demand and responsibilities in support of your
workloads continuing education is necessary.
• Provide resources for education: Provided dedicated structured time, access to training materials,
lab resources, and support participation in conferences and professional organizations that provide
71

AWS Well-Architected Framework
Organization
opportunities for learning from both educators and peers. Provide junior team members' access
to senior team members as mentors or allow them to shadow their work and be exposed to their
methods and skills. Encourage learning about content not directly related to work in order to have a
broader perspective.
• Team education and cross-team engagement: Plan for the continuing education needs of your
team members. Provide opportunities for team members to join other teams (temporarily or
permanently) to share skills and best practices beneﬁting your entire organization
• Support pursuit and maintenance of industry certiﬁcations: Support your team members acquiring
and maintaining industry certiﬁcations that validate what they have learned, and acknowledge their
accomplishments.
Resources
Related documents:
• AWS Getting Started Resource Center
• AWS Blogs
• AWS Cloud Compliance
• AWS Discussion Forms
• AWS Documentation
• AWS Online Tech Talks
• AWS Events and Webinars
• AWS Knowledge Center
• AWS Support
• AWS Training and Certiﬁcation
• AWS Well-Architected Labs,
• The Amazon Builders' Library
• The Oﬃcial AWS Podcast.
OPS03-BP07 Resource teams appropriately
Maintain team member capacity, and provide tools and resources to support your workload needs.
Overtasking team members increases the risk of incidents resulting from human error. Investments in
tools and resources (for example, providing automation for frequently performed activities) can scale the
eﬀectiveness of your team, helping them to support additional activities.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Resource teams appropriately: Ensure you have an understanding of the success of your teams and
the factors that contribute to their success or lack of success. Act to support teams with appropriate
resources.
• Understand team performance: Measure the achievement of operational outcomes and the
development of assets by your teams. Track changes in output and error rate over time. Engage
with teams to understand the work related challenges that impact them (for example, increasing
responsibilities, changes in technology, loss of personnel, or increase in customers supported).
• Understand impacts on team performance: Remain engaged with your teams so that you
understand how they are doing and if there are external factors aﬀecting them. When your teams
are impacted by external factors, reevaluate goals and adjust targets as appropriate. Identify
obstacles that are impeding your teams progress. Act on behalf of your teams to help address
obstacles and remove unnecessary burdens.
72

AWS Well-Architected Framework
Prepare
• Provide the resources necessary for teams to be successful: Regularly review if resources are still
appropriate, of if additional resources are needed, and make appropriate adjustments to support
teams.
OPS03-BP08 Diverse opinions are encouraged and sought within and across
teams
Leverage cross-organizational diversity to seek multiple unique perspectives. Use this perspective
to increase innovation, challenge your assumptions, and reduce the risk of conﬁrmation bias. Grow
inclusion, diversity, and accessibility within your teams to gain beneﬁcial perspectives.
Organizational culture has a direct impact on team member job satisfaction and retention. Foster the
engagement and capabilities of your team members to create the success of your business.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Seek diverse opinions and perspectives: Encourage contributions from everyone. Give voice to under-
represented groups. Rotate roles and responsibilities in meetings.
• Expand roles and responsibilities: Provide opportunity for team members to take on roles that they
might not otherwise. They will gain experience and perspective from the role, and from interactions
with new team members with whom they might not otherwise interact. They will bring their
experience and perspective to the new role and team members they interact with. As perspective
increases, additional business opportunities may emerge, or new opportunities for improvement
may be identiﬁed. Have members within a team take turns at common tasks that others typically
perform to understand the demands and impact of performing them.
• Provide a safe and welcoming environment: Have policy and controls that protect team members'
mental and physical safety within your organization. Team members should be able to interact
without fear of reprisal. When team members feel safe and welcome they are more likely to be
engaged and productive. The more diverse your organization the better your understanding can be
of the people you support including your customers. When your team members are comfortable,
feel free to speak, and are conﬁdent they will be heard, they are more likely to share valuable
insights (for example, marketing opportunities, accessibility needs, unserved market segments,
unacknowledged risks in your environment).
• Enable team members to participate fully: Provide the resources necessary for your employees
to participate fully in all work related activities. Team members that face daily challenges have
developed skills for working around them. These uniquely developed skills can provide signiﬁcant
beneﬁt to your organization. Supporting team members with necessary accommodations will
increase the beneﬁts you can receive from their contributions.
Prepare
Questions
• OPS 4. How do you design your workload so that you can understand its state? (p. 74)
• OPS 5. How do you reduce defects, ease remediation, and improve ﬂow into production? (p. 83)
• OPS 6. How do you mitigate deployment risks? (p. 96)
• OPS 7. How do you know that you are ready to support a workload? (p. 102)
73

AWS Well-Architected Framework
Prepare
OPS 4. How do you design your workload so that you can
understand its state?
Design your workload so that it provides the information necessary across all components (for example,
metrics, logs, and traces) for you to understand its internal state. This allows you to provide eﬀective
responses when appropriate.
Best practices
• OPS04-BP01 Implement application telemetry (p. 74)
• OPS04-BP02 Implement and conﬁgure workload telemetry (p. 76)
• OPS04-BP03 Implement user activity telemetry (p. 78)
• OPS04-BP04 Implement dependency telemetry (p. 79)
• OPS04-BP05 Implement transaction traceability (p. 81)
OPS04-BP01 Implement application telemetry
Application telemetry is the foundation for observability of your workload. Your application should
emit telemetry that provides insight into the state of the application and the achievement of business
outcomes. From troubleshooting to measuring the impact of a new feature, application telemetry
informs the way you build, operate, and evolve your workload.
Application telemetry consists of metrics and logs. Metrics are diagnostic information, such as your pulse
or temperature. Metrics are used collectively to describe the state of your application. Collecting metrics
over time can be used to develop baselines and detect anomalies. Logs are messages that the application
sends about its internal state or events that occur. Error codes, transaction identiﬁers, and user actions
are examples of events that are logged.
Desired Outcome:
• Your application emits metrics and logs that provide insight into its health and the achievement of
business outcomes.
• Metrics and logs are stored centrally for all applications in the workload.
Common anti-patterns:
• Your application doesn't emit telemetry. You are forced to rely upon your customers to tell you when
something is wrong.
• A customer has reported that your application is unresponsive. You have no telemetry and are unable
to conﬁrm that the issue exists or characterize the issue without using the application yourself to
understand the current user experience.
Beneﬁts of establishing this best practice:
• You can understand the health of your application, the user experience, and the achievement of
business outcomes.
• You can react quickly to changes in your application health.
• You can develop application health trends.
• You can make informed decisions about improving your application.
• You can detect and resolve application issues faster.
Level of risk exposed if this best practice is not established: High
74

AWS Well-Architected Framework
Prepare
Implementation guidance
Implementing application telemetry consists of three steps: identifying a location to store telemetry,
identifying telemetry that describes the state of the application, and instrumenting the application to
emit telemetry.
Customer example
AnyCompany Retail has a microservices based architecture. As part of their architectural design process,
they identiﬁed application telemetry that would help them understand the state of each microservice.
For example, the user cart service emits telemetry about events like add to cart, abandon cart, and
length of time it took to add an item to the cart. All microservices log errors, warnings, and transaction
information. Telemetry is sent to Amazon CloudWatch for storage and analysis.
Implementation steps
1. Identify a central location for telemetry storage for the applications in your workload. The location
should support both collection of telemetry and analysis capabilities. Anomaly detection and
automated insights are recommended features.
a. Amazon CloudWatch provides telemetry collection, dashboards, analysis, and event generation
capabilities.
2. To identify what telemetry you need, start by answering this question: what is the state of my
application? Your application should emit logs and metrics that collectively answer this question.
If you can’t answer the questions with the existing application telemetry, work with business and
engineering stakeholders to create a list of telemetry requirements.
a. You can request expert technical advice from your AWS account team as you identify and develop
new application telemetry.
3. Once the additional application telemetry has been identiﬁed, work with your engineering
stakeholders to instrument your application.
a. The AWS Distro for Open Telemetry provides APIs, libraries, and agents that collect application
telemetry. This example demonstrates how to instrument a JavaScript application with custom
metrics.
b. If you want to understand the observability services that AWS oﬀers, work through the One
Observability Workshop or request support from your AWS account team.
c. For a deeper dive into application telemetry, read the Instrumenting distributed systems for
operational visibility article in the Amazon Builder’s Library, which explains how Amazon
instruments applications and can serve as a guide for developing your own instrumentation
guidelines.
Level of eﬀort for the implementation plan: High. Instrumenting your application and centralizing
telemetry storage can take signiﬁcant investment.
Resources
Related best practices:
the section called “OPS04-BP02 Implement and conﬁgure workload telemetry” (p. 76) – Application
telemetry is a component of workload telemetry. In order to understand the health of the overall
workload you need to understand the health of individual applications that make up the workload.
the section called “OPS04-BP03 Implement user activity telemetry” (p. 78) – User activity telemetry is
often a subset of application telemetry. User activity like add to cart events, click streams, or completed
transactions provide insight into the user experience.
the section called “OPS04-BP04 Implement dependency telemetry” (p. 79) – Dependency checks
are related to application telemetry and may be instrumented into your application. If your application
75

AWS Well-Architected Framework
Prepare
relies on external dependencies like DNS or a database your application can emit metrics and logs on
reachability, timeouts, and other events.
the section called “OPS04-BP05 Implement transaction traceability” (p. 81) – Tracing transactions
across a workload requires each application to emit information about how they process shared events.
The way individual applications handle these events is emitted through their application telemetry.
the section called “OPS08-BP02 Deﬁne workload metrics” (p. 115) – Workload metrics are the key
health indicators for your workload. Key application metrics are a part of workload metrics.
Related documents:
• AWS Builders Library – Instrumenting Distributed Systems for Operational Visibility
• AWS Distro for OpenTelemetry
• AWS Well-Architected Operational Excellence Whitepaper – Design Telemetry
• Creating metrics from log events using ﬁlters
• Implementing Logging and Monitoring with Amazon CloudWatch
• Monitoring application health and performance with AWS Distro for OpenTelemetry
• New – How to better monitor your custom application metrics using Amazon CloudWatch Agent
• Observability at AWS
• Scenario – Publish metrics to CloudWatch
• Start Building – How to Monitor your Applications Eﬀectively
• Using CloudWatch with an AWS SDK
Related videos:
• AWS re:Invent 2021 - Observability the open-source way
• Collect Metrics and Logs from Amazon EC2 instances with the CloudWatch Agent
• How to Easily Setup Application Monitoring for Your AWS Workloads - AWS Online Tech Talks
• Mastering Observability of Your Serverless Applications - AWS Online Tech Talks
• Open Source Observability with AWS - AWS Virtual Workshop
Related examples:
• AWS Logging & Monitoring Example Resources
• AWS Solution: Amazon CloudWatch Monitoring Framework
• AWS Solution: Centralized Logging
• One Observability Workshop
Related services:
• Amazon CloudWatch
OPS04-BP02 Implement and conﬁgure workload telemetry
Design and conﬁgure your workload to emit information about its internal state and current status, for
example, API call volume, HTTP status codes, and scaling events. Use this information to help determine
when a response is required.
Use a service such as Amazon CloudWatch to aggregate logs and metrics from workload components
(for example, API logs from AWS CloudTrail, AWS Lambda metrics, Amazon VPC Flow Logs, and other
services).
76

AWS Well-Architected Framework
Prepare
Common anti-patterns:
• Your customers are complaining about poor performance. There are no recent changes to your
application and so you suspect an issue with a workload component. You have no telemetry to analyze
to determine what component or components are contributing to the poor performance.
• Your application is unreachable. You lack the telemetry to determine if it's a networking issue.
Beneﬁts of establishing this best practice: Understanding what is going on inside your workload helps
you to respond if necessary.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Implement log and metric telemetry: Instrument your workload to emit information about its internal
state, status, and the achievement of business outcomes. Use this information to determine when a
response is required.
• Gaining better observability of your VMs with Amazon CloudWatch - AWS Online Tech Talks
• How Amazon CloudWatch works
• What is Amazon CloudWatch?
• Using Amazon CloudWatch metrics
• What is Amazon CloudWatch Logs?
• Implement and conﬁgure workload telemetry: Design and conﬁgure your workload to emit
information about its internal state and current status (for example, API call volume, HTTP status
codes, and scaling events).
• Amazon CloudWatch metrics and dimensions reference
• AWS CloudTrail
• What Is AWS CloudTrail?
• VPC Flow Logs
Resources
Related documents:
• AWS CloudTrail
• Amazon CloudWatch Documentation
• Amazon CloudWatch metrics and dimensions reference
• How Amazon CloudWatch works
• Using Amazon CloudWatch metrics
• VPC Flow Logs
• What Is AWS CloudTrail?
• What is Amazon CloudWatch Logs?
• What is Amazon CloudWatch?
Related videos:
• Application Performance Management on AWS
• Gaining Better Observability of Your VMs with Amazon CloudWatch
• Gaining better observability of your VMs with Amazon CloudWatch - AWS Online Tech Talks
77

AWS Well-Architected Framework
Prepare
OPS04-BP03 Implement user activity telemetry
Instrument your application code to emit information about user activity. Examples of user activity
include click streams or started, abandoned, and completed transactions. Use this information to
help understand how the application is used, patterns of usage, and to determine when a response is
required. Capturing real user activity allows you to build synthetic activity that can be used to monitor
and test your workload in production.
Desired outcome:
• Your workload emits telemetry about user activity across all applications.
• You leverage synthetic user activity to monitor your application during oﬀ-peak hours.
Common anti-patterns:
• Your developers have deployed a new feature without user telemetry. You cannot tell if your
customers are using the feature without asking them.
• After a deployment to your front-end application, you see increased utilization. Because you lack user
activity telemetry, it is diﬃcult to identify the exact issue.
• An issue occurs in your application during oﬀ-peak hours. You do not notice the issue until the
morning when your users come online because you have not conﬁgured synthetic user activity.
Beneﬁts of establishing this best practice:
• Understand common user patterns or unexpected behaviors to optimize functionality of the
application to ﬁt your business goals.
• Monitor the application from the perspective of your users to detect problems with user experience,
such as broken links or slow click responses
• Identify the root cause of issues by tracing the steps your impacted user has taken.
• Synthetic user activity can provide early warning signs of performance degradation during oﬀ-peak
hours, allowing you to take corrective action before actual users are aﬀected.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Design your application code to emit information about user activity. Use this information to help
understand how the application is used, patterns of usage, and to determine when a response is
required. Utilize synthetic user activity to provide insight into application performance during oﬀ-peak
hours.
Customer example
AnyCompany Retail implements user activity telemetry at several layers in their application. The
front-end telemetry tracks pointer and movement events while the backend microservices emit
telemetry tracking events like adding an item to the user's cart and checking out. Together they provide
observability into the user experience. AnyCompany Retail also uses synthetic user telemetry to catch
problems when there are fewer users on the workload.
Implementation steps
1. Instrument your application to emit telemetry (metrics, events, logs, and traces) about user activity.
Once instrumented, front-end components emit telemetry automatically as the user interacts with the
user interface. Backend applications emit telemetry on user events and transactions.
a. Amazon CloudWatch RUM can provide insight into end user experience for front-end applications.
78

AWS Well-Architected Framework
Prepare
b. You can use the AWS Distro for Open Telemetry to instrument and capture telemetry from your
applications.
c. Amazon Pinpoint can analyze user behavior through campaigns, providing insight on user
engagement.
d. Customers with Enterprise Support can request the Building a Monitoring Strategy Workshop from
their Technical Account Manager. This workshop helps you build an observability strategy for your
workload.
2. Establish synthetic user activity to monitor your application. Synthetic user activity simulates user
actions to validate that your application is working properly.
a. Amazon CloudWatch Synthetics can simulate user activity using canaries.
Level of eﬀort for the implementation plan: High. It may take signiﬁcant development eﬀort to fully
instrument your application to collect user activity telemetry.
Resources
Related best practices:
• OPS04-BP01 Implement application telemetry (p. 74) - Application telemetry is required in order to
build in user activity telemetry.
• OPS04-BP02 Implement and conﬁgure workload telemetry (p. 76) - Some user activity telemetry
may also be considered workload telemetry.
Related documents:
• How to Monitor your Applications Eﬀectively
Related videos:
• AWS re:Invent 2020: Monitoring production services at Amazon
• AWS re:Invent 2021 - Optimize applications through end user insights with Amazon CloudWatch RUM
• Testing and Monitoring APIs on AWS - AWS Online Tech Talks
Related examples:
• Amazon CloudWatch RUM Web Client
• AWS Distro for Open Telemetry
• Implementing Real User Monitoring of Amplify Application using Amazon CloudWatch RUM
• One Observability Workshop
Related services:
• Amazon CloudWatch RUM
• Amazon CloudWatch Synthetics
• Amazon Pinpoint
OPS04-BP04 Implement dependency telemetry
Design and conﬁgure your workload to emit information about the status of resources it depends on.
These are resources that are external to your workload. Examples of external dependencies can include
79

AWS Well-Architected Framework
Prepare
external databases, DNS, and network connectivity. Use this information to determine when a response
is required and provide additional context on workload state.
Desired outcome:
• Your workload emits telemetry about the status of external dependencies.
• You are notiﬁed when dependencies are unhealthy.
Common anti-patterns:
• Your users cannot reach your site. You are unable to determine if the reason is a DNS issue without
manually performing a check to see if your DNS provider is working.
• Your shopping cart application is unable to complete transactions. You are unable to determine if it's a
problem with your credit card processing provider without contacting them to verify.
Beneﬁts of establishing this best practice:
• Monitoring external dependencies provides advance notice of issues.
• Awareness of the health of your dependencies assists in troubleshooting.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Work with stakeholders to identify external dependencies that your workload depends on. External
dependencies can include external databases, APIs, or network connectivity between your workload and
resources in other environments. Develop a monitoring strategy to provide awareness of the health of
dependencies and proactively alarm if the status changes.
Customer example
AnyCompany Retail’s ecommerce workload relies on a database located in another environment. Every
night, data is populated in the database for use in the ecommerce platform. The network connectivity
and database support are owned by other teams. The ecommerce team conﬁgured several canary alarms
to alert them when the network connectivity drops, the database is unreachable, and when the job fails
to complete.
Implementation steps
1. Identify external dependencies that your workload relies on. Implement telemetry to track the health
or reachability of dependencies.
a. AWS customers can use the AWS Health Dashboard to monitor the health of AWS services and
receive notiﬁcations of health events.
b. Amazon CloudWatch Synthetics can be used to monitor APIs, URLs, and website contents.
2. Set up alerts to notify your organization when a dependency is unhealthy or unreachable.
a. Customers with Enterprise Support can request the Building a Monitoring Strategy Workshop from
their Technical Account Manager. This workshop will help you build an observability strategy for
your workload.
3. Identify contacts for dependencies in cases where the dependency is unhealthy. Document how to
contact the dependency owner, service agreements, and escalation process.
Level of eﬀort for the implementation plan: Medium. Implementing dependency telemetry may require
building custom monitoring solutions.
80

AWS Well-Architected Framework
Prepare
Resources
Related best practices:
• OPS04-BP01 Implement application telemetry (p. 74) - You may build dependency monitoring into
your application telemetry.
Related documents:
• Monitor your private internal endpoints 24x7 using CloudWatch Synthetics
Related videos:
• AWS re:Invent 2018: Monitor All Your Things: Amazon CloudWatch in Action with BBC
• AWS re:Invent 2022 - Developing an observability strategy
• AWS re:Invent 2022 - Observability best practices at Amazon
Related examples:
• One Observability Workshop
• Well-Architected Labs - Dependency Monitoring
Related services:
• Amazon CloudWatch Synthetics
• AWS Health
OPS04-BP05 Implement transaction traceability
Implement your application code and conﬁgure your workload components to emit events, which
are started as a result of single logical operations and consolidated across various boundaries of your
workload. Generate maps to see how traces ﬂow across your workload and services. Gain insight into
the relationships between components, and identify and analyze issues. Use the collected information
to determine when a response is required and to assist you in identifying the factors contributing to an
issue.
Desired outcome:
• Collect transaction traces across your workload to gain insight into the relationship between
components.
• Generate maps to gain a better understanding of how transactions and events ﬂow across your
workload.
Common anti-patterns:
• You have implemented a serverless microservices architecture spanning multiple accounts. Your
customers are experiencing intermittent performance issues. You are unable to discover which
function or component is responsible because you lack transaction traceability.
• There is a performance bottleneck in your workload. Because you lack transaction traceability, you are
unable to see the relationship between your application components and identify the bottleneck.
• The identiﬁer used for traces is not globally unique, resulting in a tracing collision when analyzing
workload behavior.
81

AWS Well-Architected Framework
Prepare
Beneﬁts of establishing this best practice:
• Understanding the ﬂow of transactions across your workload provides insight into the expected
behavior of your workload transactions.
• You can see variations from expected behavior across your workload and you can respond if necessary.
• You can pinpoint transactions by their unique generated identiﬁer independent from where they were
generated.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Design your application and workload to emit information about the ﬂow of transactions across system
components. Data to include in transactions are a globally unique transaction identiﬁer, transaction
stage, active component, and time to complete activity. Use this information to determine what is in
progress, what is complete, and what the results of completed activities are.
Customer example
At AnyCompany Retail, all transactions have a globally unique UUID generated. This UUID is passed
between microservices during transactions. The UUID is used to create transaction traces as users
interact with the workload. A map of the workload topology is generated with the traces and is used to
troubleshoot workload issues and improve performance.
Implementation steps
1. Instrument the applications in your workload to emit transaction traces. This can be done by
generating a unique identiﬁer for each transaction and passing the identiﬁer between applications.
a. You can use auto-instrumentation in the AWS Distro for OpenTelemetry to implement traces in
your existing applications without modifying your application code.
2. Generate maps of your application topology. Use these maps to improve performance, gain insights,
and aid in troubleshooting.
a. AWS X-Ray can generate maps of the applications in your workload.
Level of eﬀort for the implementation plan: Medium. Implementing transaction traces may require
moderate development eﬀort.
Resources
Related best practices:
• OPS04-BP01 Implement application telemetry (p. 74) - Application telemetry covers transaction
traceability and handling and needs to be implementing ﬁrst.
Related documents:
• Discover application issues and get notiﬁcations with AWS X-Ray Insights
• How Wealthfront utilizes AWS X-Ray to analyze and debug distributed applications
• New for AWS Distro for OpenTelemetry – Tracing Support is Now Generally Available
Related videos:
• AWS re:Invent 2018: Deep Dive into AWS X-Ray: Monitor Modern Applications (DEV324)
• AWS re:Invent 2022 - Building observable applications with OpenTelemetry (BOA310)
• AWS re:Invent 2022 - Observability the open-source way (COP301-R)
82

AWS Well-Architected Framework
Prepare
• Capturing Trace Data with the AWS Distro for OpenTelemetry
• Optimize Application Performance with AWS X-Ray
Related examples:
• AWS X-Ray Multi API Gateway Tracing Example
Related services:
• AWS Distro for OpenTelemetry
• AWS X-Ray
OPS 5. How do you reduce defects, ease remediation, and
improve ﬂow into production?
Adopt approaches that improve ﬂow of changes into production, that activate refactoring, fast feedback
on quality, and bug ﬁxing. These accelerate beneﬁcial changes entering production, limit issues
deployed, and achieve rapid identiﬁcation and remediation of issues introduced through deployment
activities.
Best practices
• OPS05-BP01 Use version control (p. 83)
• OPS05-BP02 Test and validate changes (p. 84)
• OPS05-BP03 Use conﬁguration management systems (p. 86)
• OPS05-BP04 Use build and deployment management systems (p. 88)
• OPS05-BP05 Perform patch management (p. 89)
• OPS05-BP06 Share design standards (p. 90)
• OPS05-BP07 Implement practices to improve code quality (p. 92)
• OPS05-BP08 Use multiple environments (p. 93)
• OPS05-BP09 Make frequent, small, reversible changes (p. 94)
• OPS05-BP10 Fully automate integration and deployment (p. 95)
OPS05-BP01 Use version control
Use version control to activate tracking of changes and releases.
Many AWS services oﬀer version control capabilities. Use a revision or source control system such as
AWS CodeCommit to manage code and other artifacts, such as version-controlled AWS CloudFormation
templates of your infrastructure.
Common anti-patterns:
• You have been developing and storing your code on your workstation. You have had an unrecoverable
storage failure on the workstation your code is lost.
• After overwriting the existing code with your changes, you restart your application and it is no longer
operable. You are unable to revert to the change.
• You have a write lock on a report ﬁle that someone else needs to edit. They contact you asking that
you stop work on it so that they can complete their tasks.
• Your research team has been working on a detailed analysis that will shape your future work. Someone
has accidentally saved their shopping list over the ﬁnal report. You are unable to revert the change and
will have to recreate the report.
83

AWS Well-Architected Framework
Prepare
Beneﬁts of establishing this best practice: By using version control capabilities you can easily revert to
known good states, previous versions, and limit the risk of assets being lost.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Use version control: Maintain assets in version controlled repositories. Doing so supports tracking
changes, deploying new versions, detecting changes to existing versions, and reverting to prior
versions (for example, rolling back to a known good state in the event of a failure). Integrate the
version control capabilities of your conﬁguration management systems into your procedures.
• Introduction to AWS CodeCommit
• What is AWS CodeCommit?
Resources
Related documents:
• What is AWS CodeCommit?
Related videos:
• Introduction to AWS CodeCommit
OPS05-BP02 Test and validate changes
Every change deployed must be tested to avoid errors in production. This best practice is focused on
testing changes from version control to artifact build. Besides application code changes, testing should
include infrastructure, conﬁguration, security controls, and operations procedures. Testing takes many
forms, from unit tests to software component analysis (SCA). Move tests further to the left in the
software integration and delivery process results in higher certainty of artifact quality.
Your organization must develop testing standards for all software artifacts. Automated tests reduce toil
and avoid manual test errors. Manual tests may be necessary in some cases. Developers must have access
to automated test results to create feedback loops that improve software quality.
Desired outcome:
• All software changes are tested before they are delivered.
• Developers have access to test results.
• Your organization has a testing standard that applies to all software changes.
Common anti-patterns:
• You deploy a new software change without any tests. It fails to run in production, which leads to an
outage.
• New security groups are deployed with AWS CloudFormation without being tested in a pre-production
environment. The security groups make your app unreachable for your customers.
• A method is modiﬁed but there are no unit tests. The software fails when it is deployed to production.
Beneﬁts of establishing this best practice:
• The change fail rate of software deployments is reduced.
• Software quality is improved.
84

AWS Well-Architected Framework
Prepare
• Developers have increased awareness on the viability of their code.
• Security policies can be rolled out with conﬁdence to support organization's compliance
• Infrastructure changes such as automatic scaling policy updates are tested in advance to meet traﬃc
needs.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Testing is done on all changes, from application code to infrastructure, as part of your continuous
integration practice. Test results are published so that developers have fast feedback. Your organization
has a testing standard that all changes must pass.
Customer example
As part of their continuous integration pipeline, AnyCompany Retail conducts several types of tests on all
software artifacts. They practice test driven development so all software has unit tests. Once the artifact
is built, they run end-to-end tests. After this ﬁrst round of tests is complete, they run a static application
security scan, which looks for known vulnerabilities. Developers receive messages as each testing gate is
passed. Once all tests are complete, the software artifact is stored in an artifact repository.
Implementation steps
1. Work with stakeholders in your organization to develop a testing standard for software artifacts. What
standard tests should all artifacts pass? Are there compliance or governance requirements that must
be included in the test coverage? Do you need to conduct code quality tests? When tests complete,
who needs to know?
a. The AWS Deployment Pipeline Reference Architecture contains an authoritative list of types of tests
that can be conducted on software artifacts as part of an integration pipeline.
2. Instrument your application with the necessary tests based on your software testing standard. Each
set of tests should complete in under ten minutes. Tests should run as part of an integration pipeline.
a. Amazon CodeGuru Reviewer can test your application code for defects.
b. You can use AWS CodeBuild to conduct tests on software artifacts.
c. AWS CodePipeline can orchestrate your software tests into a pipeline.
Resources
Related best practices:
• OPS05-BP01 Use version control (p. 83) - All software artifacts must be backed by a version-
controlled repository.
• OPS05-BP06 Share design standards (p. 90) - Your organizations software testing standards inform
your design standards.
• OPS05-BP10 Fully automate integration and deployment (p. 95) - Software tests should be
automatically run as part of your larger integration and deployment pipeline.
Related documents:
• Adopt a test-driven development approach
• Automated AWS CloudFormation Testing Pipeline with TaskCat and CodePipeline
• Building end-to-end AWS DevSecOps CI/CD pipeline with open source SCA, SAST, and DAST tools
• Getting started with testing serverless applications
• My CI/CD pipeline is my release captain
85

AWS Well-Architected Framework
Prepare
• Practicing Continuous Integration and Continuous Delivery on AWS Whitepaper
Related videos:
• AWS re:Invent 2020: Testable infrastructure: Integration testing on AWS
• AWS Summit ANZ 2021 - Driving a test-ﬁrst strategy with CDK and test driven development
• Testing Your Infrastructure as Code with AWS CDK
Related resources:
• AWS Deployment Pipeline Reference Architecture - Application
• AWS Kubernetes DevSecOps Pipeline
• Policy as Code Workshop – Test Driven Development
• Run unit tests for a Node.js application from GitHub by using AWS CodeBuild
• Use Serverspec for test-driven development of infrastructure code
Related services:
• Amazon CodeGuru Reviewer
• AWS CodeBuild
• AWS CodePipeline
OPS05-BP03 Use conﬁguration management systems
Use conﬁguration management systems to make and track conﬁguration changes. These systems reduce
errors caused by manual processes and reduce the level of eﬀort to deploy changes.
Static conﬁguration management sets values when initializing a resource that are expected to remain
consistent throughout the resource’s lifetime. Some examples include setting the conﬁguration for a
web or application server on an instance, or deﬁning the conﬁguration of an AWS service within the AWS
Management Console or through the AWS CLI.
Dynamic conﬁguration management sets values at initialization that can or are expected to change
during the lifetime of a resource. For example, you could set a feature toggle to activate functionality in
your code through a conﬁguration change, or change the level of log detail during an incident to capture
more data and then change back following the incident eliminating the now unnecessary logs and their
associated expense.
If you have dynamic conﬁgurations in your applications running on instances, containers, serverless
functions, or devices, you can use AWS AppConﬁg to manage and deploy them across your
environments.
On AWS, you can use AWS Conﬁg to continuously monitor your AWS resource conﬁgurations across
accounts and Regions. It helps you to track their conﬁguration history, understand how a conﬁguration
change would aﬀect other resources, and audit them against expected or desired conﬁgurations using
AWS Conﬁg Rules and AWS Conﬁg Conformance Packs.
On AWS, you can build continuous integration/continuous deployment (CI/CD) pipelines using services
such as AWS Developer Tools (for example, AWS CodeCommit, AWS CodeBuild, AWS CodePipeline, AWS
CodeDeploy, and AWS CodeStar).
Have a change calendar and track when signiﬁcant business or operational activities or events are
planned that may be impacted by implementation of change. Adjust activities to manage risk around
86

AWS Well-Architected Framework
Prepare
those plans. AWS Systems Manager Change Calendar provides a mechanism to document blocks of
time as open or closed to changes and why, and share that information with other AWS accounts. AWS
Systems Manager Automation scripts can be conﬁgured to adhere to the change calendar state.
AWS Systems Manager Maintenance Windows can be used to schedule the performance of AWS SSM Run
Command or Automation scripts, AWS Lambda invocations, or AWS Step Functions activities at speciﬁed
times. Mark these activities in your change calendar so that they can be included in your evaluation.
Common anti-patterns:
• You manually update the web server conﬁguration across your ﬂeet and a number of servers become
unresponsive due to update errors.
• You manually update your application server ﬂeet over the course of many hours. The inconsistency in
conﬁguration during the change causes unexpected behaviors.
• Someone has updated your security groups and your web servers are no longer accessible. Without
knowledge of what was changed you spend signiﬁcant time investigating the issue extending your
time to recovery.
Beneﬁts of establishing this best practice: Adopting conﬁguration management systems reduces the
level of eﬀort to make and track changes, and the frequency of errors caused by manual procedures.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Use conﬁguration management systems: Use conﬁguration management systems to track and
implement changes, to reduce errors caused by manual processes, and reduce the level of eﬀort.
• Infrastructure conﬁguration management
• AWS Conﬁg
• What is AWS Conﬁg?
• Introduction to AWS CloudFormation
• What is AWS CloudFormation?
• AWS OpsWorks
• What is AWS OpsWorks?
• Introduction to AWS Elastic Beanstalk
• What is AWS Elastic Beanstalk?
Resources
Related documents:
• AWS AppConﬁg
• AWS Developer Tools
• AWS OpsWorks
• AWS Systems Manager Change Calendar
• AWS Systems Manager Maintenance Windows
• Infrastructure conﬁguration management
• What is AWS CloudFormation?
• What is AWS Conﬁg?
• What is AWS Elastic Beanstalk?
• What is AWS OpsWorks?
87

AWS Well-Architected Framework
Prepare
Related videos:
• Introduction to AWS CloudFormation
• Introduction to AWS Elastic Beanstalk
OPS05-BP04 Use build and deployment management systems
Use build and deployment management systems. These systems reduce errors caused by manual
processes and reduce the level of eﬀort to deploy changes.
In AWS, you can build continuous integration/continuous deployment (CI/CD) pipelines using services
such as AWS Developer Tools (for example, AWS CodeCommit, AWS CodeBuild, AWS CodePipeline, AWS
CodeDeploy, and AWS CodeStar).
Common anti-patterns:
• After compiling your code on your development system you, copy the executable onto your production
systems and it fails to start. The local log ﬁles indicates that it has failed due to missing dependencies.
• You successfully build your application with new features in your development environment and
provide the code to Quality Assurance (QA). It fails QA because it is missing static assets.
• On Friday, after much eﬀort, you successfully built your application manually in your development
environment including your newly coded features. On Monday, you are unable to repeat the steps that
allowed you to successfully build your application.
• You perform the tests you have created for your new release. Then you spend the next week setting
up a test environment and performing all the existing integration tests followed by the performance
tests. The new code has an unacceptable performance impact and must be redeveloped and then
retested.
Beneﬁts of establishing this best practice: By providing mechanisms to manage build and deployment
activities you reduce the level of eﬀort to perform repetitive tasks, free your team members to focus on
their high value creative tasks, and limit the introduction of error from manual procedures.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Use build and deployment management systems: Use build and deployment management systems
to track and implement change, to reduce errors caused by manual processes, and reduce the level
of eﬀort. Fully automate the integration and deployment pipeline from code check-in through build,
testing, deployment, and validation. This reduces lead time, encourages increased frequency of
change, and reduces the level of eﬀort.
• What is AWS CodeBuild?
• Continuous integration best practices for software development
• Slalom: CI/CD for serverless applications on AWS
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
• What is AWS CodeDeploy?
Resources
Related documents:
• AWS Developer Tools
• What is AWS CodeBuild?
• What is AWS CodeDeploy?
88

AWS Well-Architected Framework
Prepare
Related videos:
• Continuous integration best practices for software development
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
• Slalom: CI/CD for serverless applications on AWS
OPS05-BP05 Perform patch management
Perform patch management to gain features, address issues, and remain compliant with governance.
Automate patch management to reduce errors caused by manual processes, and reduce the level of
eﬀort to patch.
Patch and vulnerability management are part of your beneﬁt and risk management activities. It is
preferable to have immutable infrastructures and deploy workloads in veriﬁed known good states.
Where that is not viable, patching in place is the remaining option.
Updating machine images, container images, or Lambda custom runtimes and additional libraries to
remove vulnerabilities are part of patch management. You should manage updates to Amazon Machine
Images (AMIs) for Linux or Windows Server images using EC2 Image Builder. You can use Amazon Elastic
Container Registry with your existing pipeline to manage Amazon ECS images and manage Amazon EKS
images. AWS Lambda includes version management features.
Patching should not be performed on production systems without ﬁrst testing in a safe environment.
Patches should only be applied if they support an operational or business outcome. On AWS, you can
use AWS Systems Manager Patch Manager to automate the process of patching managed systems and
schedule the activity using AWS Systems Manager Maintenance Windows.
Common anti-patterns:
• You are given a mandate to apply all new security patches within two hours resulting in multiple
outages due to application incompatibility with patches.
• An unpatched library results in unintended consequences as unknown parties use vulnerabilities within
it to access your workload.
• You patch the developer environments automatically without notifying the developers. You receive
multiple complaints from the developers that their environment cease to operate as expected.
• You have not patched the commercial oﬀ-the-self software on a persistent instance. When you have an
issue with the software and contact the vendor, they notify you that version is not supported and you
will have to patch to a speciﬁc level to receive any assistance.
• A recently released patch for the encryption software you used has signiﬁcant performance
improvements. Your unpatched system has performance issues that remain in place as a result of not
patching.
Beneﬁts of establishing this best practice: By establishing a patch management process, including
your criteria for patching and methodology for distribution across your environments, you will be able
to realize their beneﬁts and control their impact. This will encourage the adoption of desired features
and capabilities, the removal of issues, and sustained compliance with governance. Implement patch
management systems and automation to reduce the level of eﬀort to deploy patches and limit errors
caused by manual processes.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Patch management: Patch systems to remediate issues, to gain desired features or capabilities, and to
remain compliant with governance policy and vendor support requirements. In immutable systems,
89

AWS Well-Architected Framework
Prepare
deploy with the appropriate patch set to achieve the desired result. Automate the patch management
mechanism to reduce the elapsed time to patch, to reduce errors caused by manual processes, and
reduce the level of eﬀort to patch.
• AWS Systems Manager Patch Manager
Resources
Related documents:
• AWS Developer Tools
• AWS Systems Manager Patch Manager
Related videos:
• CI/CD for Serverless Applications on AWS
• Design with Ops in Mind
Related examples:
• Well-Architected Labs – Inventory and Patch Management
OPS05-BP06 Share design standards
Share best practices across teams to increase awareness and maximize the beneﬁts of development
eﬀorts. Document them and keep them up to date as your architecture evolves. If shared standards
are enforced in your organization, it’s critical that mechanisms exist to request additions, changes, and
exceptions to standards. Without this option, standards become a constraint on innovation.
Desired outcome:
• Design standards are shared across teams in your organizations.
• They are documented and kept up to date as best practices evolve.
Common anti-patterns:
• Two development teams have each created a user authentication service. Your users must maintain a
separate set of credentials for each part of the system they want to access.
• Each team manages their own infrastructure. A new compliance requirement forces a change to your
infrastructure and each team implements it in a diﬀerent way.
Beneﬁts of establishing this best practice:
• Using shared standards supports the adoption of best practices and to maximizes the beneﬁts of
development eﬀorts.
• Documenting and updating design standards keeps your organization up to date with best practices
and security and compliance requirements.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Share existing best practices, design standards, checklists, operating procedures, guidance, and
governance requirements across teams. Have procedures to request changes, additions, and exceptions
90

AWS Well-Architected Framework
Prepare
to design standards to support improvement and innovation. Make teams are aware of published
content. Have a mechanism to keep design standards up to date as new best practices emerge.
Customer example
AnyCompany Retail has a cross-functional architecture team that creates software architecture patterns.
This team builds the architecture with compliance and governance built in. Teams that adopt these
shared standards get the beneﬁts of having compliance and governance built in. They can quickly build
on top of the design standard. The architecture team meets quarterly to evaluate architecture patterns
and update them if necessary.
Implementation steps
1. Identify a cross-functional team that will own developing and updating design standards. This
team will work with stakeholders across your organization to develop design standards, operating
procedures, checklists, guidance, and governance requirements. Document the design standards and
share them within your organization.
a. AWS Service Catalog can be used to create portfolios representing design standards using
infrastructure as code. You can share portfolios across accounts.
2. Have a mechanism in place to keep design standards up to date as new best practices are identiﬁed.
3. If design standards are centrally enforced, have a process to request changes, updates, and
exemptions.
Level of eﬀort for the implementation plan: Medium. Developing a process to create and share design
standards can take coordination and cooperation with stakeholders across your organization.
Resources
Related best practices:
• OPS01-BP03 Evaluate governance requirements (p. 52) - Governance requirements inﬂuence design
standards.
• OPS01-BP04 Evaluate compliance requirements (p. 54) - Compliance is a vital input in creating
design standards.
• OPS07-BP02 Ensure a consistent review of operational readiness (p. 103) - Operational readiness
checklists are a mechanism to implement design standards when designing your workload.
• OPS11-BP01 Have a process for continuous improvement (p. 141) - Updating design standards is a
part of continuous improvement.
• OPS11-BP04 Perform knowledge management (p. 145) - As part of your knowledge management
practice, document and share design standards.
Related documents:
• Automate AWS Backups with AWS Service Catalog
• AWS Service Catalog Account Factory-Enhanced
• How Expedia Group built Database as a Service (DBaaS) oﬀering using AWS Service Catalog
• Maintain visibility over the use of cloud architecture patterns
• Simplify sharing your AWS Service Catalog portfolios in an AWS Organizations setup
Related videos:
• AWS Service Catalog – Getting Started
• AWS re:Invent 2020: Manage your AWS Service Catalog portfolios like an expert
91

AWS Well-Architected Framework
Prepare
Related examples:
• AWS Service Catalog Reference Architecture
• AWS Service Catalog Workshop
Related services:
• AWS Service Catalog
OPS05-BP07 Implement practices to improve code quality
Implement practices to improve code quality and minimize defects. Some examples include test-driven
development, code reviews, standards adoption, and pair programming. Incorporate these practices into
your continuous integration and delivery process.
Desired outcome:
• Your organization uses best practices like code reviews or pair programming to improve code quality.
• Developers and operators adopt code quality best practices as part of the software development
lifecycle.
Common anti-patterns:
• You commit code to the main branch of your application without a code review. The change
automatically deploys to production and causes an outage.
• A new application is developed without any unit, end-to-end, or integration tests. There is no way to
test the application before deployment.
• Your teams make manual changes in production to address defects. Changes do not go through
testing or code reviews and are not captured or logged through continuous integration and delivery
processes.
Beneﬁts of establishing this best practice:
• By adopting practices to improve code quality, you can help minimize issues introduced to production.
• Code quality increases using best practices like pair programming and code reviews.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Implement practices to improve code quality to minimize defects before they are deployed. Use practices
like test-driven development, code reviews, and pair programming to increase the quality of your
development.
Customer example
AnyCompany Retail adopts several practices to improve code quality. They have adopted test-driven
development as the standard for writing applications. For some new features, they will have developers
pair program together during a sprint. Every pull request goes through a code review by a senior
developer before being integrated and deployed.
Implementation steps
1. Adopt code quality practices like test-driven development, code reviews, and pair programming into
your continuous integration and delivery process. Use these techniques to improve software quality.
92

AWS Well-Architected Framework
Prepare
a. Amazon CodeGuru Reviewer can provide programming recommendations for Java and Python code
using machine learning.
b. You can create shared development environments with AWS Cloud9 where you can collaborate on
developing code.
Level of eﬀort for the implementation plan: Medium. There are many ways of implementing this best
practice, but getting organizational adoption may be challenging.
Resources
Related best practices:
• OPS05-BP06 Share design standards (p. 90) - You can share design standards as part of your code
quality practice.
Related documents:
• Agile Software Guide
• My CI/CD pipeline is my release captain
• Automate code reviews with Amazon CodeGuru Reviewer
• Adopt a test-driven development approach
• How DevFactory builds better applications with Amazon CodeGuru
• On Pair Programming
• RENGA Inc. automates code reviews with Amazon CodeGuru
• The Art of Agile Development: Test-Driven Development
• Why code reviews matter (and actually save time!)
Related videos:
• AWS re:Invent 2020: Continuous improvement of code quality with Amazon CodeGuru
• AWS Summit ANZ 2021 - Driving a test-ﬁrst strategy with CDK and test driven development
Related services:
• Amazon CodeGuru Reviewer
• Amazon CodeGuru Proﬁler
• AWS Cloud9
OPS05-BP08 Use multiple environments
Use multiple environments to experiment, develop, and test your workload. Use increasing levels
of controls as environments approach production to gain conﬁdence your workload will operate as
intended when deployed.
Common anti-patterns:
• You are performing development in a shared development environment and another developer
overwrites your code changes.
• The restrictive security controls on your shared development environment are preventing you from
experimenting with new services and features.
• You perform load testing on your production systems and cause an outage for your users.
93

AWS Well-Architected Framework
Prepare
• A critical error resulting in data loss has occurred in production. In your production environment, you
attempt to recreate the conditions that lead to the data loss so that you can identify how it happened
and prevent it from happening again. To prevent further data loss during testing, you are forced to
make the application unavailable to your users.
• You are operating a multi-tenant service and are unable to support a customer request for a dedicated
environment.
• You may not always test, but when you do it’s in production.
• You believe that the simplicity of a single environment overrides the scope of impact of changes within
the environment.
Beneﬁts of establishing this best practice: By deploying multiple environments you can support
multiple simultaneous development, testing, and production environments without creating conﬂicts
between developers or user communities.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Use multiple environments: Provide developers sandbox environments with minimized controls to aid
in experimentation. Provide individual development environments to help work in parallel, increasing
development agility. Implement more rigorous controls in the environments approaching production
to allow developers to innovate. Use infrastructure as code and conﬁguration management systems
to deploy environments that are conﬁgured consistent with the controls present in production to
ensure systems operate as expected when deployed. When environments are not in use, turn them
oﬀ to avoid costs associated with idle resources (for example, development systems on evenings and
weekends). Deploy production equivalent environments when load testing to improve valid results.
• What is AWS CloudFormation?
• How do I stop and start Amazon EC2 instances at regular intervals using AWS Lambda?
Resources
Related documents:
• How do I stop and start Amazon EC2 instances at regular intervals using AWS Lambda?
• What is AWS CloudFormation?
OPS05-BP09 Make frequent, small, reversible changes
Frequent, small, and reversible changes reduce the scope and impact of a change. This eases
troubleshooting, helps with faster remediation, and provides the option to roll back a change.
Common anti-patterns:
• You deploy a new version of your application quarterly.
• You frequently make changes to your database schema.
• You perform manual in-place updates, overwriting existing installations and conﬁgurations.
Beneﬁts of establishing this best practice: You recognize beneﬁts from development eﬀorts faster by
deploying small changes frequently. When the changes are small, it is much easier to identify if they
have unintended consequences. When the changes are reversible, there is less risk to implementing the
change as recovery is simpliﬁed.
Level of risk exposed if this best practice is not established: Low
94

AWS Well-Architected Framework
Prepare
Implementation guidance
• Make frequent, small, reversible changes: Frequent, small, and reversible changes reduce the scope and
impact of a change. This eases troubleshooting, helps with faster remediation, and provides the option
to roll back a change. It also increases the rate at which you can deliver value to the business.
OPS05-BP10 Fully automate integration and deployment
Automate build, deployment, and testing of the workload. This reduces errors caused by manual
processes and reduces the eﬀort to deploy changes.
Apply metadata using Resource Tags and AWS Resource Groups following a consistent tagging strategy
to aid in identiﬁcation of your resources. Tag your resources for organization, cost accounting, access
controls, and targeting the run of automated operations activities.
Common anti-patterns:
• On Friday you, ﬁnish authoring the new code for your feature branch. On Monday, after running your
code quality test scripts and each of your unit tests scripts, you will check in your code for the next
scheduled release.
• You are assigned to code a ﬁx for a critical issue impacting a large number of customers in production.
After testing the ﬁx, you commit your code and email change management to request approval to
deploy it to production.
Beneﬁts of establishing this best practice: By implementing automated build and deployment
management systems, you reduce errors caused by manual processes and reduce the eﬀort to deploy
changes helping your team members to focus on delivering business value.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Use build and deployment management systems: Use build and deployment management systems
to track and implement change, to reduce errors caused by manual processes, and reduce the level
of eﬀort. Fully automate the integration and deployment pipeline from code check-in through build,
testing, deployment, and validation. This reduces lead time, encourages increased frequency of
change, and reduces the level of eﬀort.
• What is AWS CodeBuild?
• Continuous integration best practices for software development
• Slalom: CI/CD for serverless applications on AWS
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
• What is AWS CodeDeploy?
Resources
Related documents:
• What is AWS CodeBuild?
• What is AWS CodeDeploy?
Related videos:
• Continuous integration best practices for software development
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
95

AWS Well-Architected Framework
Prepare
• Slalom: CI/CD for serverless applications on AWS
OPS 6. How do you mitigate deployment risks?
Adopt approaches that provide fast feedback on quality and achieve rapid recovery from changes that do
not have desired outcomes. Using these practices mitigates the impact of issues introduced through the
deployment of changes.
Best practices
• OPS06-BP01 Plan for unsuccessful changes (p. 96)
• OPS06-BP02 Test and validate changes (p. 96)
• OPS06-BP03 Use deployment management systems (p. 97)
• OPS06-BP04 Test using limited deployments (p. 98)
• OPS06-BP05 Deploy using parallel environments (p. 99)
• OPS06-BP06 Deploy frequent, small, reversible changes (p. 100)
• OPS06-BP07 Fully automate integration and deployment (p. 100)
• OPS06-BP08 Automate testing and rollback (p. 101)
OPS06-BP01 Plan for unsuccessful changes
Plan to revert to a known good state, or remediate in the production environment if a change does not
have the desired outcome. This preparation reduces recovery time through faster responses.
Common anti-patterns:
• You performed a deployment and your application has become unstable but there appear to be active
users on the system. You have to decide whether to roll back the change and impact the active users or
wait to roll back the change knowing the users may be impacted regardless.
• After making a routine change, your new environments are accessible but one of your subnets has
become unreachable. You have to decide whether to roll back everything or try to ﬁx the inaccessible
subnet. While you are making that determination, the subnet remains unreachable.
Beneﬁts of establishing this best practice: Having a plan in place reduces the mean time to recover
(MTTR) from unsuccessful changes, reducing the impact to your end users.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Plan for unsuccessful changes: Plan to revert to a known good state (that is, roll back the change), or
remediate in the production environment (that is, roll forward the change) if a change does not have
the desired outcome. When you identify changes that you cannot roll back if unsuccessful, apply due
diligence prior to committing the change.
OPS06-BP02 Test and validate changes
Test changes and validate the results at all lifecycle stages to conﬁrm new features and minimize the risk
and impact of failed deployments.
On AWS, you can create temporary parallel environments to lower the risk, eﬀort, and cost
of experimentation and testing. Automate the deployment of these environments using AWS
CloudFormation to ensure consistent implementations of your temporary environments.
96

AWS Well-Architected Framework
Prepare
Common anti-patterns:
• You deploy a cool new feature to your application. It doesn't work. You don't know.
• You update your certiﬁcates. You accidentally install the certiﬁcates to the wrong components. You
don't know.
Beneﬁts of establishing this best practice: By testing and validating changes following deployment you
are able to identify issues early providing an opportunity to mitigate the impact on your customers.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Test and validate changes: Test changes and validate the results at all lifecycle stages (for example,
development, test, and production), to conﬁrm new features and minimize the risk and impact of
failed deployments.
• AWS Cloud9
• What is AWS Cloud9?
• How to test and debug AWS CodeDeploy locally before you ship your code
Resources
Related documents:
• AWS Cloud9
• AWS Developer Tools
• How to test and debug AWS CodeDeploy locally before you ship your code
• What is AWS Cloud9?
OPS06-BP03 Use deployment management systems
Use deployment management systems to track and implement change. This reduces errors caused by
manual processes and reduces the eﬀort to deploy changes.
In AWS, you can build Continuous Integration/Continuous Deployment (CI/CD) pipelines using services
such as AWS Developer Tools (for example, AWS CodeCommit, AWS CodeBuild, AWS CodePipeline, AWS
CodeDeploy, and AWS CodeStar).
Common anti-patterns:
• You manually deploy updates to the application servers across your ﬂeet and a number of servers
become unresponsive due to update errors.
• You manually deploy to your application server ﬂeet over the course of many hours. The inconsistency
in versions during the change causes unexpected behaviors.
Beneﬁts of establishing this best practice: Adopting deployment management systems reduces the
level of eﬀort to deploy changes, and the frequency of errors caused by manual procedures.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Use deployment management systems: Use deployment management systems to track and implement
change. This will reduce errors caused by manual processes, and reduce the level of eﬀort to deploy
changes. Automate the integration and deployment pipeline from code check-in through testing,
97

AWS Well-Architected Framework
Prepare
deployment, and validation. This reduces lead time, encourages increased frequency of change, and
further reduces the level of eﬀort.
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
• What is AWS CodeDeploy?
• What is AWS Elastic Beanstalk?
• What is Amazon API Gateway?
Resources
Related documents:
• AWS CodeDeploy User Guide
• AWS Developer Tools
• Try a Sample Blue/Green Deployment in AWS CodeDeploy
• What is AWS CodeDeploy?
• What is AWS Elastic Beanstalk?
• What is Amazon API Gateway?
Related videos:
• Deep Dive on Advanced Continuous Delivery Techniques Using AWS
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
OPS06-BP04 Test using limited deployments
Test with limited deployments alongside existing systems to conﬁrm desired outcomes prior to full scale
deployment. For example, use deployment canary testing or one-box deployments.
Common anti-patterns:
• You deploy an unsuccessful change to all of production all at once. You don't know.
Beneﬁts of establishing this best practice: By testing and validating changes following limited
deployment you are able to identify issues early with minimal impact on your customers providing an
opportunity to further mitigate the impact on your customers.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Test using limited deployments: Test with limited deployments alongside existing systems to conﬁrm
desired outcomes prior to full scale deployment. For example, use deployment canary testing or one-
box deployments.
• AWS CodeDeploy User Guide
• Blue/Green deployments with AWS Elastic Beanstalk
• Set up an API Gateway canary release deployment
• Try a Sample Blue/Green Deployment in AWS CodeDeploy
• Working with deployment conﬁgurations in AWS CodeDeploy
Resources
Related documents:
98

AWS Well-Architected Framework
Prepare
• AWS CodeDeploy User Guide
• Blue/Green deployments with AWS Elastic Beanstalk
• Set up an API Gateway canary release deployment
• Try a Sample Blue/Green Deployment in AWS CodeDeploy
• Working with deployment conﬁgurations in AWS CodeDeploy
OPS06-BP05 Deploy using parallel environments
Implement changes onto parallel environments, and then transition over to the new environment.
Maintain the prior environment until there is conﬁrmation of successful deployment. Doing so minimizes
recovery time by permitting rollback to the previous environment.
Common anti-patterns:
• You perform a mutable deployment by modifying your existing systems. After discovering that the
change was unsuccessful, you are forced to modify the systems again to restore the old version
extending your time to recovery.
• During a maintenance window, you decommission the old environment and then start building
your new environment. Many hours into the procedure, you discover unrecoverable issues with the
deployment. While extremely tired, you are forced to ﬁnd the previous deployment procedures and
start rebuilding the old environment.
Beneﬁts of establishing this best practice: By using parallel environments, you can pre-deploy the new
environment and transition over to them when desired. If the new environment is not successful, you can
recover quickly by transitioning back to your original environment.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Deploy using parallel environments: Implement changes onto parallel environments, and transition
or cut over to the new environment. Maintain the prior environment until there is conﬁrmation
of successful deployment. This minimizes recovery time by permitting rollback to the previous
environment. For example, use immutable infrastructures with blue/green deployments.
• Working with deployment conﬁgurations in AWS CodeDeploy
• Blue/Green deployments with AWS Elastic Beanstalk
• Set up an API Gateway canary release deployment
• Try a Sample Blue/Green Deployment in AWS CodeDeploy
Resources
Related documents:
• AWS CodeDeploy User Guide
• Blue/Green deployments with AWS Elastic Beanstalk
• Set up an API Gateway canary release deployment
• Try a Sample Blue/Green Deployment in AWS CodeDeploy
• Working with deployment conﬁgurations in AWS CodeDeploy
Related videos:
• Deep Dive on Advanced Continuous Delivery Techniques Using AWS
99

AWS Well-Architected Framework
Prepare
OPS06-BP06 Deploy frequent, small, reversible changes
Use frequent, small, and reversible changes to reduce the scope of a change. This results in easier
troubleshooting and faster remediation with the option to roll back a change.
Common anti-patterns:
• You deploy a new version of your application quarterly.
• You frequently make changes to your database schema.
• You perform manual in-place updates, overwriting existing installations and conﬁgurations.
Beneﬁts of establishing this best practice: You recognize beneﬁts from development eﬀorts faster by
deploying small changes frequently. When the changes are small it is much easier to identify if they have
unintended consequences. When the changes are reversible there is less risk to implementing the change
as recovery is simpliﬁed.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Deploy frequent, small, reversible changes: Use frequent, small, and reversible changes to reduce the
scope of a change. This results in easier troubleshooting and faster remediation with the option to roll
back a change.
OPS06-BP07 Fully automate integration and deployment
Automate build, deployment, and testing of the workload. This reduces errors cause by manual processes
and reduces the eﬀort to deploy changes.
Apply metadata using Resource Tags and AWS Resource Groups following a consistent tagging strategy
to aid in identiﬁcation of your resources. Tag your resources for organization, cost accounting, access
controls, and targeting the run of automated operations activities.
Common anti-patterns:
• On Friday, you ﬁnish authoring the new code for your feature branch. On Monday, after running your
code quality test scripts and each of your unit tests scripts, you will check in your code for the next
scheduled release.
• You are assigned to code a ﬁx for a critical issue impacting a large number of customers in production.
After testing the ﬁx, you commit your code and email change management to request approval to
deploy it to production.
Beneﬁts of establishing this best practice: By implementing automated build and deployment
management systems you reduce errors caused by manual processes and reduce the eﬀort to deploy
changes helping your team members to focus on delivering business value.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Use build and deployment management systems: Use build and deployment management systems
to track and implement change, to reduce errors caused by manual processes, and reduce the level
of eﬀort. Fully automate the integration and deployment pipeline from code check-in through build,
testing, deployment, and validation. This reduces lead time, encourages increased frequency of
change, and reduces the level of eﬀort.
• What is AWS CodeBuild?
100

AWS Well-Architected Framework
Prepare
• Continuous integration best practices for software development
• Slalom: CI/CD for serverless applications on AWS
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
• What is AWS CodeDeploy?
• Deep Dive on Advanced Continuous Delivery Techniques Using AWS
Resources
Related documents:
• Try a Sample Blue/Green Deployment in AWS CodeDeploy
• What is AWS CodeBuild?
• What is AWS CodeDeploy?
Related videos:
• Continuous integration best practices for software development
• Deep Dive on Advanced Continuous Delivery Techniques Using AWS
• Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services
• Slalom: CI/CD for serverless applications on AWS
OPS06-BP08 Automate testing and rollback
Automate testing of deployed environments to conﬁrm desired outcomes. Automate rollback to a
previous known good state when outcomes are not achieved to minimize recovery time and reduce
errors caused by manual processes.
Common anti-patterns:
• You deploy changes to your workload. After your see that the change is complete, you start post
deployment testing. After you see that they are complete, you realize that your workload is inoperable
and customers are disconnected. You then begin rolling back to the previous version. After an
extended time to detect the issue, the time to recover is extended by your manual redeployment.
Beneﬁts of establishing this best practice: By testing and validating changes following deployment, you
are able to identify issues immediately. By automatically rolling back to the previous version, the impact
on your customers is minimized.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Automate testing and rollback: Automate testing of deployed environments to conﬁrm desired
outcomes. Automate rollback to a previous known good state when outcomes are not achieved to
minimize recovery time and reduce errors caused by manual processes. For example, perform detailed
synthetic user transactions following deployment, verify the results, and roll back on failure.
• Redeploy and roll back a deployment with AWS CodeDeploy
Resources
Related documents:
• Redeploy and roll back a deployment with AWS CodeDeploy
101

AWS Well-Architected Framework
Prepare
OPS 7. How do you know that you are ready to support a
workload?
Evaluate the operational readiness of your workload, processes and procedures, and personnel to
understand the operational risks related to your workload.
Best practices
• OPS07-BP01 Ensure personnel capability (p. 102)
• OPS07-BP02 Ensure a consistent review of operational readiness (p. 103)
• OPS07-BP03 Use runbooks to perform procedures (p. 105)
• OPS07-BP04 Use playbooks to investigate issues (p. 108)
• OPS07-BP05 Make informed decisions to deploy systems and changes (p. 111)
• OPS07-BP06 Create support plans for production workloads (p. 112)
OPS07-BP01 Ensure personnel capability
Have a mechanism to validate that you have the appropriate number of trained personnel to support the
workload. They must be trained on the platform and services that make up your workload. Provide them
with the knowledge necessary to operate the workload. You must have enough trained personnel to
support the normal operation of the workload and troubleshoot any incidents that occur. Have enough
personnel so that you can rotate during on-call and vacations to avoid burnout.
Desired outcome:
• There are enough trained personnel to support the workload at times when the workload is available.
• You provide training for your personnel on the software and services that make up your workload.
Common anti-patterns:
• Deploying a workload without team members trained to operate the platform and services in use.
• Not having enough personnel to support on-call rotations or personnel taking time oﬀ.
Beneﬁts of establishing this best practice:
• Having skilled team members helps eﬀective support of your workload.
• With enough team members, you can support the workload and on-call rotations while decreasing the
risk of burnout.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Validate that there are suﬃcient trained personnel to support the workload. Verify that you have enough
team members to cover normal operational activities, including on-call rotations.
Customer example
AnyCompany Retail makes sure that teams supporting the workload are properly staﬀed and trained.
They have enough engineers to support an on-call rotation. Personnel get training on the software
and platform that the workload is built on and are encouraged to earn certiﬁcations. There are enough
personnel so that people can take time oﬀ while still supporting the workload and the on-call rotation.
Implementation steps
102

AWS Well-Architected Framework
Prepare
1. Assign an adequate number of personnel to operate and support your workload, including on-call
duties.
2. Train your personnel on the software and platforms that compose your workload.
a. AWS Training and Certiﬁcation has a library of courses about AWS. They provide free and paid
courses, online and in-person.
b. AWS hosts events and webinars where you learn from AWS experts.
3. Regularly evaluate team size and skills as operating conditions and the workload change. Adjust team
size and skills to match operational requirements.
Level of eﬀort for the implementation plan: High. Hiring and training a team to support a workload
can take signiﬁcant eﬀort but has substantial long-term beneﬁts.
Resources
Related best practices:
• OPS11-BP04 Perform knowledge management (p. 145) - Team members must have the information
necessary to operate and support the workload. Knowledge management is the key to providing that.
Related documents:
• AWS Events and Webinars
• AWS Training and Certiﬁcation
OPS07-BP02 Ensure a consistent review of operational readiness
Use Operational Readiness Reviews (ORRs) to validate that you can operate your workload. ORR is a
mechanism developed at Amazon to validate that teams can safely operate their workloads. An ORR is a
review and inspection process using a checklist of requirements. An ORR is a self-service experience that
teams use to certify their workloads. ORRs include best practices from lessons learned from our years of
building software.
An ORR checklist is composed of architectural recommendations, operational process, event
management, and release quality. Our Correction of Error (CoE) process is a major driver of these items.
Your own post-incident analysis should drive the evolution of your own ORR. An ORR is not only about
following best practices but preventing the recurrence of events that you’ve seen before. Lastly, security,
governance, and compliance requirements can also be included in an ORR.
Run ORRs before a workload launches to general availability and then throughout the software
development lifecycle. Running the ORR before launch increases your ability to operate the workload
safely. Periodically re-run your ORR on the workload to catch any drift from best practices. You can have
ORR checklists for new services launches and ORRs for periodic reviews. This helps keep you up to date
on new best practices that arise and incorporate lessons learned from post-incident analysis. As your use
of the cloud matures, you can build ORR requirements into your architecture as defaults.
Desired outcome: You have an ORR checklist with best practices for your organization. ORRs are
conducted before workloads launch. ORRs are run periodically over the course of the workload lifecycle.
Common anti-patterns:
• You launch a workload without knowing if you can operate it.
• Governance and security requirements are not included in certifying a workload for launch.
• Workloads are not re-evaluated periodically.
• Workloads launch without required procedures in place.
• You see repetition of the same root cause failures in multiple workloads.
103

AWS Well-Architected Framework
Prepare
Beneﬁts of establishing this best practice:
• Your workloads include architecture, process, and management best practices.
• Lessons learned are incorporated into your ORR process.
• Required procedures are in place when workloads launch.
• ORRs are run throughout the software lifecycle of your workloads.
Level of risk if this best practice is not established: High
Implementation guidance
An ORR is two things: a process and a checklist. Your ORR process should be adopted by your
organization and supported by an executive sponsor. At a minimum, ORRs must be conducted before
a workload launches to general availability. Run the ORR throughout the software development
lifecycle to keep it up to date with best practices or new requirements. The ORR checklist should include
conﬁguration items, security and governance requirements, and best practices from your organization.
Over time, you can use services, such as AWS Conﬁg, AWS Security Hub, and AWS Control Tower
Guardrails, to build best practices from the ORR into guardrails for automatic detection of best practices.
Customer example
After several production incidents, AnyCompany Retail decided to implement an ORR process. They built
a checklist composed of best practices, governance and compliance requirements, and lessons learned
from outages. New workloads conduct ORRs before they launch. Every workload conducts a yearly ORR
with a subset of best practices to incorporate new best practices and requirements that are added to the
ORR checklist. Over time, AnyCompany Retail used AWS Conﬁg to detect some best practices, speeding
up the ORR process.
Implementation steps
To learn more about ORRs, read the Operational Readiness Reviews (ORR) whitepaper. It provides
detailed information on the history of the ORR process, how to build your own ORR practice, and how
to develop your ORR checklist. The following steps are an abbreviated version of that document. For
an in-depth understanding of what ORRs are and how to build your own, we recommend reading that
whitepaper.
1. Gather the key stakeholders together, including representatives from security, operations, and
development.
2. Have each stakeholder provide at least one requirement. For the ﬁrst iteration, try to limit the number
of items to thirty or less.
• Appendix B: Example ORR questions from the Operational Readiness Reviews (ORR) whitepaper
contains sample questions that you can use to get started.
3. Collect your requirements into a spreadsheet.
• You can use custom lenses in the AWS Well-Architected Tool to develop your ORR and share them
across your accounts and AWS Organization.
4. Identify one workload to conduct the ORR on. A pre-launch workload or an internal workload is ideal.
5. Run through the ORR checklist and take note of any discoveries made. Discoveries might not be ok if a
mitigation is in place. For any discovery that lacks a mitigation, add those to your backlog of items and
implement them before launch.
6. Continue to add best practices and requirements to your ORR checklist over time.
AWS Support customers with Enterprise Support can request the Operational Readiness Review
Workshop from their Technical Account Manager. The workshop is an interactive working backwards
session to develop your own ORR checklist.
104

AWS Well-Architected Framework
Prepare
Level of eﬀort for the implementation plan: High. Adopting an ORR practice in your organization
requires executive sponsorship and stakeholder buy-in. Build and update the checklist with inputs from
across your organization.
Resources
Related best practices:
• OPS01-BP03 Evaluate governance requirements (p. 52) – Governance requirements are a natural ﬁt
for an ORR checklist.
• OPS01-BP04 Evaluate compliance requirements (p. 54) – Compliance requirements are sometimes
included in an ORR checklist. Other times they are a separate process.
• OPS03-BP07 Resource teams appropriately (p. 72) – Team capability is a good candidate for an ORR
requirement.
• OPS06-BP01 Plan for unsuccessful changes (p. 96) – A rollback or rollforward plan must be
established before you launch your workload.
• OPS07-BP01 Ensure personnel capability (p. 102) – To support a workload you must have the
required personnel.
• SEC01-BP03 Identify and validate control objectives – Security control objectives make excellent ORR
requirements.
• REL13-BP01 Deﬁne recovery objectives for downtime and data loss – Disaster recovery plans are a
good ORR requirement.
• COST02-BP01 Develop policies based on your organization requirements – Cost management policies
are good to include in your ORR checklist.
Related documents:
• AWS Control Tower - Guardrails in AWS Control Tower
• AWS Well-Architected Tool - Custom Lenses
• Operational Readiness Review Template by Adrian Hornsby
• Operational Readiness Reviews (ORR) Whitepaper
Related videos:
• AWS Supports You | Building an Eﬀective Operational Readiness Review (ORR)
Related examples:
• Sample Operational Readiness Review (ORR) Lens
Related services:
• AWS Conﬁg
• AWS Control Tower
• AWS Security Hub
• AWS Well-Architected Tool
OPS07-BP03 Use runbooks to perform procedures
A runbook is a documented process to achieve a speciﬁc outcome. Runbooks consist of a series of steps
that someone follows to get something done. Runbooks have been used in operations going back to the
105

AWS Well-Architected Framework
Prepare
early days of aviation. In cloud operations, we use runbooks to reduce risk and achieve desired outcomes.
At its simplest, a runbook is a checklist to complete a task.
Runbooks are an essential part of operating your workload. From onboarding a new team member to
deploying a major release, runbooks are the codiﬁed processes that provide consistent outcomes no
matter who uses them. Runbooks should be published in a central location and updated as the process
evolves, as updating runbooks is a key component of a change management process. They should also
include guidance on error handling, tools, permissions, exceptions, and escalations in case a problem
occurs.
As your organization matures, begin automating runbooks. Start with runbooks that are short and
frequently used. Use scripting languages to automate steps or make steps easier to perform. As you
automate the ﬁrst few runbooks, you’ll dedicate time to automating more complex runbooks. Over time,
most of your runbooks should be automated in some way.
Desired outcome: Your team has a collection of step-by-step guides for performing workload tasks.
The runbooks contain the desired outcome, necessary tools and permissions, and instructions for error
handling. They are stored in a central location and updated frequently.
Common anti-patterns:
• Relying on memory to complete each step of a process.
• Manually deploying changes without a checklist.
• Diﬀerent team members performing the same process but with diﬀerent steps or outcomes.
• Letting runbooks drift out of sync with system changes and automation.
Beneﬁts of establishing this best practice:
• Reducing error rates for manual tasks.
• Operations are performed in a consistent manner.
• New team members can start performing tasks sooner.
• Runbooks can be automated to reduce toil.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Runbooks can take several forms depending on the maturity level of your organization. At a minimum,
they should consist of a step-by-step text document. The desired outcome should be clearly indicated.
Clearly document necessary special permissions or tools. Provide detailed guidance on error handling
and escalations in case something goes wrong. List the runbook owner and publish it in a central
location. Once your runbook is documented, validate it by having someone else on your team run it. As
procedures evolve, update your runbooks in accordance with your change management process.
Your text runbooks should be automated as your organization matures. Using services like AWS Systems
Manager automations, you can transform ﬂat text into automations that can be run against your
workload. These automations can be run in response to events, reducing the operational burden to
maintain your workload.
Customer example
AnyCompany Retail must perform database schema updates during software deployments. The Cloud
Operations Team worked with the Database Administration Team to build a runbook for manually
deploying these changes. The runbook listed each step in the process in checklist form. It included a
section on error handling in case something went wrong. They published the runbook on their internal
106

AWS Well-Architected Framework
Prepare
wiki along with their other runbooks. The Cloud Operations Team plans to automate the runbook in a
future sprint.
Implementation steps
If you don’t have an existing document repository, a version control repository is a great place to start
building your runbook library. You can build your runbooks using Markdown. We have provided an
example runbook template that you can use to start building runbooks.
# Runbook Title
## Runbook Info
| Runbook ID | Description | Tools Used | Special Permissions | Runbook Author | Last
Updated | Escalation POC |
|-------|-------|-------|-------|-------|-------|-------|
| RUN001 | What is this runbook for? What is the desired outcome? | Tools | Permissions |
Your Name | 2022-09-21 | Escalation Name |
## Steps
1. Step one
2. Step two
1. If you don’t have an existing documentation repository or wiki, create a new version control repository
in your version control system.
2. Identify a process that does not have a runbook. An ideal process is one that is conducted
semiregularly, short in number of steps, and has low impact failures.
3. In your document repository, create a new draft Markdown document using the template. Fill in
Runbook Title and the required ﬁelds under Runbook Info.
4. Starting with the ﬁrst step, ﬁll in the Steps portion of the runbook.
5. Give the runbook to a team member. Have them use the runbook to validate the steps. If something is
missing or needs clarity, update the runbook.
6. Publish the runbook to your internal documentation store. Once published, tell your team and other
stakeholders.
7. Over time, you’ll build a library of runbooks. As that library grows, start working to automate
runbooks.
Level of eﬀort for the implementation plan: Low. The minimum standard for a runbook is a step-by-
step text guide. Automating runbooks can increase the implementation eﬀort.
Resources
Related best practices:
• OPS02-BP02 Processes and procedures have identiﬁed owners (p. 61): Runbooks should have an
owner in charge of maintaining them.
• OPS07-BP04 Use playbooks to investigate issues (p. 108): Runbooks and playbooks are like each
other with one key diﬀerence: a runbook has a desired outcome. In many cases runbooks are initiated
once a playbook has identiﬁed a root cause.
• OPS10-BP01 Use a process for event, incident, and problem management (p. 131): Runbooks are a
part of a good event, incident, and problem management practice.
• OPS10-BP02 Have a process per alert (p. 134): Runbooks and playbooks should be used to respond
to alerts. Over time these reactions should be automated.
• OPS11-BP04 Perform knowledge management (p. 145): Maintaining runbooks is a key part of
knowledge management.
Related documents:
107

AWS Well-Architected Framework
Prepare
• Achieving Operational Excellence using automated playbook and runbook
• AWS Systems Manager: Working with runbooks
• Migration playbook for AWS large migrations - Task 4: Improving your migration runbooks
• Use AWS Systems Manager Automation runbooks to resolve operational tasks
Related videos:
• AWS re:Invent 2019: DIY guide to runbooks, incident reports, and incident response (SEC318-R1)
• How to automate IT Operations on AWS | Amazon Web Services
• Integrate Scripts into AWS Systems Manager
Related examples:
• AWS Systems Manager: Automation walkthroughs
• AWS Systems Manager: Restore a root volume from the latest snapshot runbook
• Building an AWS incident response runbook using Jupyter notebooks and CloudTrail Lake
• Gitlab - Runbooks
• Rubix - A Python library for building runbooks in Jupyter Notebooks
• Using Document Builder to create a custom runbook
• Well-Architected Labs: Automating operations with Playbooks and Runbooks
Related services:
• AWS Systems Manager Automation
OPS07-BP04 Use playbooks to investigate issues
Playbooks are step-by-step guides used to investigate an incident. When incidents happen, playbooks are
used to investigate, scope impact, and identify a root cause. Playbooks are used for a variety of scenarios,
from failed deployments to security incidents. In many cases, playbooks identify the root cause that
a runbook is used to mitigate. Playbooks are an essential component of your organization's incident
response plans.
A good playbook has several key features. It guides the user, step by step, through the process of
discovery. Thinking outside-in, what steps should someone follow to diagnose an incident? Clearly
deﬁne in the playbook if special tools or elevated permissions are needed in the playbook. Having a
communication plan to update stakeholders on the status of the investigation is a key component. In
situations where a root cause can’t be identiﬁed, the playbook should have an escalation plan. If the root
cause is identiﬁed, the playbook should point to a runbook that describes how to resolve it. Playbooks
should be stored centrally and regularly maintained. If playbooks are used for speciﬁc alerts, provide
your team with pointers to the playbook within the alert.
As your organization matures, automate your playbooks. Start with playbooks that cover low-risk
incidents. Use scripting to automate the discovery steps. Make sure that you have companion runbooks
to mitigate common root causes.
Desired outcome: Your organization has playbooks for common incidents. The playbooks are stored in a
central location and available to your team members. Playbooks are updated frequently. For any known
root causes, companion runbooks are built.
Common anti-patterns:
• There is no standard way to investigate an incident.
108

AWS Well-Architected Framework
Prepare
• Team members rely on muscle memory or institutional knowledge to troubleshoot a failed
deployment.
• New team members learn how to investigate issues through trial and error.
• Best practices for investigating issues are not shared across teams.
Beneﬁts of establishing this best practice:
• Playbooks boost your eﬀorts to mitigate incidents.
• Diﬀerent team members can use the same playbook to identify a root cause in a consistent manner.
• Known root causes can have runbooks developed for them, speeding up recovery time.
• Playbooks help team members to start contributing sooner.
• Teams can scale their processes with repeatable playbooks.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
How you build and use playbooks depends on the maturity of your organization. If you are new to the
cloud, build playbooks in text form in a central document repository. As your organization matures,
playbooks can become semi-automated with scripting languages like Python. These scripts can be
run inside a Jupyter notebook to speed up discovery. Advanced organizations have fully automated
playbooks for common issues that are auto-remediated with runbooks.
Start building your playbooks by listing common incidents that happen to your workload. Choose
playbooks for incidents that are low risk and where the root cause has been narrowed down to a few
issues to start. After you have playbooks for simpler scenarios, move on to the higher risk scenarios or
scenarios where the root cause is not well known.
Your text playbooks should be automated as your organization matures. Using services like AWS Systems
Manager Automations, ﬂat text can be transformed into automations. These automations can be run
against your workload to speed up investigations. These automations can be activated in response to
events, reducing the mean time to discover and resolve incidents.
Customers can use AWS Systems Manager Incident Manager to respond to incidents. This service
provides a single interface to triage incidents, inform stakeholders during discovery and mitigation, and
collaborate throughout the incident. It uses AWS Systems Manager Automations to speed up detection
and recovery.
Customer example
A production incident impacted AnyCompany Retail. The on-call engineer used a playbook to investigate
the issue. As they progressed through the steps, they kept the key stakeholders, identiﬁed in the
playbook, up to date. The engineer identiﬁed the root cause as a race condition in a backend service.
Using a runbook, the engineer relaunched the service, bringing AnyCompany Retail back online.
Implementation steps
If you don’t have an existing document repository, we suggest creating a version control repository for
your playbook library. You can build your playbooks using Markdown, which is compatible with most
playbook automation systems. If you are starting from scratch, use the following example playbook
template.
# Playbook Title
## Playbook Info
| Playbook ID | Description | Tools Used | Special Permissions | Playbook Author | Last
Updated | Escalation POC | Stakeholders | Communication Plan |
109

AWS Well-Architected Framework
Prepare
|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| RUN001 | What is this playbook for? What incident is it used for? | Tools | Permissions
| Your Name | 2022-09-21 | Escalation Name | Stakeholder Name | How will updates be
communicated during the investigation? |
## Steps
1. Step one
2. Step two
1. If you don’t have an existing document repository or wiki, create a new version control repository for
your playbooks in your version control system.
2. Identify a common issue that requires investigation. This should be a scenario where the root cause is
limited to a few issues and resolution is low risk.
3. Using the Markdown template, ﬁll in the Playbook Name section and the ﬁelds under Playbook
Info.
4. Fill in the troubleshooting steps. Be as clear as possible on what actions to perform or what areas you
should investigate.
5. Give a team member the playbook and have them go through it to validate it. If there’s anything
missing or something isn’t clear, update the playbook.
6. Publish your playbook in your document repository and inform your team and any stakeholders.
7. This playbook library will grow as you add more playbooks. Once you have several playbooks, start
automating them using tools like AWS Systems Manager Automations to keep automation and
playbooks in sync.
Level of eﬀort for the implementation plan: Low. Your playbooks should be text documents stored in a
central location. More mature organizations will move towards automating playbooks.
Resources
Related best practices:
• OPS02-BP02 Processes and procedures have identiﬁed owners (p. 61): Playbooks should have an
owner in charge of maintaining them.
• OPS07-BP03 Use runbooks to perform procedures (p. 105): Runbooks and playbooks are similar, but
with one key diﬀerence: a runbook has a desired outcome. In many cases, runbooks are used once a
playbook has identiﬁed a root cause.
• OPS10-BP01 Use a process for event, incident, and problem management (p. 131): Playbooks are a
part of good event, incident, and problem management practice.
• OPS10-BP02 Have a process per alert (p. 134): Runbooks and playbooks should be used to respond
to alerts. Over time, these reactions should be automated.
• OPS11-BP04 Perform knowledge management (p. 145): Maintaining playbooks is a key part of
knowledge management.
Related documents:
• Achieving Operational Excellence using automated playbook and runbook
• AWS Systems Manager: Working with runbooks
• Use AWS Systems Manager Automation runbooks to resolve operational tasks
Related videos:
• AWS re:Invent 2019: DIY guide to runbooks, incident reports, and incident response (SEC318-R1)
• AWS Systems Manager Incident Manager - AWS Virtual Workshops
• Integrate Scripts into AWS Systems Manager
110

AWS Well-Architected Framework
Prepare
Related examples:
• AWS Customer Playbook Framework
• AWS Systems Manager: Automation walkthroughs
• Building an AWS incident response runbook using Jupyter notebooks and CloudTrail Lake
• Rubix – A Python library for building runbooks in Jupyter Notebooks
• Using Document Builder to create a custom runbook
• Well-Architected Labs: Automating operations with Playbooks and Runbooks
• Well-Architected Labs: Incident response playbook with Jupyter
Related services:
• AWS Systems Manager Automation
• AWS Systems Manager Incident Manager
OPS07-BP05 Make informed decisions to deploy systems and changes
Have processes in place for successful and unsuccessful changes to your workload. A pre-mortem is
an exercise where a team simulates a failure to develop mitigation strategies. Use pre-mortems to
anticipate failure and create procedures where appropriate. Evaluate the beneﬁts and risks of deploying
changes to your workload. Verify that all changes comply with governance.
Desired outcome:
• You make informed decisions when deploying changes to your workload.
• Changes comply with governance.
Common anti-patterns:
• Deploying a change to our workload without a process to handle a failed deployment.
• Making changes to your production environment that are out of compliance with governance
requirements.
• Deploying a new version of your workload without establishing a baseline for resource utilization.
Beneﬁts of establishing this best practice:
• You are prepared for unsuccessful changes to your workload.
• Changes to your workload are compliant with governance policies.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Use pre-mortems to develop processes for unsuccessful changes. Document your processes for
unsuccessful changes. Ensure that all changes comply with governance. Evaluate the beneﬁts and risks to
deploying changes to your workload.
Customer example
AnyCompany Retail regularly conducts pre-mortems to validate their processes for unsuccessful changes.
They document their processes in a shared Wiki and update it frequently. All changes comply with
governance requirements.
111

AWS Well-Architected Framework
Prepare
Implementation steps
1. Make informed decisions when deploying changes to your workload. Establish and review criteria for a
successful deployment. Develop scenarios or criteria that would initiate a rollback of a change. Weigh
the beneﬁts of deploying changes against the risks of an unsuccessful change.
2. Verify that all changes comply with governance policies.
3. Use pre-mortems to plan for unsuccessful changes and document mitigation strategies. Run a table-
top exercise to model an unsuccessful change and validate roll-back procedures.
Level of eﬀort for the implementation plan: Moderate. Implementing a practice of pre-mortems
requires coordination and eﬀort from stakeholders across your organization
Resources
Related best practices:
• OPS01-BP03 Evaluate governance requirements (p. 52) - Governance requirements are a key factor
in determining whether to deploy a change.
• OPS06-BP01 Plan for unsuccessful changes (p. 96) - Establish plans to mitigate a failed deployment
and use pre-mortems to validate them.
• OPS06-BP02 Test and validate changes (p. 96) - Every software change should be properly tested
before deployment in order to reduce defects in production.
• OPS07-BP01 Ensure personnel capability (p. 102) - Having enough trained personnel to support the
workload is essential to making an informed decision to deploy a system change.
Related documents:
• Amazon Web Services: Risk and Compliance
• AWS Shared Responsibility Model
• Governance in the AWS Cloud: The Right Balance Between Agility and Safety
OPS07-BP06 Create support plans for production workloads
Enable support for any software and services that your production workload relies on. Select an
appropriate support level to meet your production service-level needs. Support plans for these
dependencies are necessary in case there is a service disruption or software issue. Document support
plans and how to request support for all service and software vendors. Implement mechanisms that
verify that support points of contacts are kept up to date.
Desired outcome:
• Implement support plans for software and services that production workloads rely on.
• Choose an appropriate support plan based on service-level needs.
• Document the support plans, support levels, and how to request support.
Common anti-patterns:
• You have no support plan for a critical software vendor. Your workload is impacted by them and you
can do nothing to expedite a ﬁx or get timely updates from the vendor.
• A developer that was the primary point of contact for a software vendor left the company. You are not
able to reach the vendor support directly. You must spend time researching and navigating generic
contact systems, increasing the time required to respond when needed.
112

AWS Well-Architected Framework
Prepare
• A production outage occurs with a software vendor. There is no documentation on how to ﬁle a
support case.
Beneﬁts of establishing this best practice:
• With the appropriate support level, you are able to get a response in the time frame necessary to meet
service-level needs.
• As a supported customer you can escalate if there are production issues.
• Software and services vendors can assist in troubleshooting during an incident.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Enable support plans for any software and services vendors that your production workload relies on.
Set up appropriate support plans to meet service-level needs. For AWS customers, this means activating
AWS Business Support or greater on any accounts where you have production workloads. Meet with
support vendors on a regular cadence to get updates about support oﬀerings, processes, and contacts.
Document how to request support from software and services vendors, including how to escalate if there
is an outage. Implement mechanisms to keep support contacts up to date.
Customer example
At AnyCompany Retail, all commercial software and services dependencies have support plans. For
example, they have AWS Enterprise Support activated on all accounts with production workloads. Any
developer can raise a support case when there is an issue. There is a wiki page with information on how
to request support, whom to notify, and best practices for expediting a case.
Implementation steps
1. Work with stakeholders in your organization to identify software and services vendors that your
workload relies on. Document these dependencies.
2. Determine service-level needs for your workload. Select a support plan that aligns with them.
3. For commercial software and services, establish a support plan with the vendors.
a. Subscribing to AWS Business Support or greater for all production accounts provides faster
response time from AWS Support and strongly recommended. If you don’t have premium support,
you must have an action plan to handle issues, which require help from AWS Support. AWS Support
provides a mix of tools and technology, people, and programs designed to proactively help you
optimize performance, lower costs, and innovate faster. AWS Business Support provides additional
beneﬁts, including access to AWS Trusted Advisor and AWS Personal Health Dashboard and faster
response times.
4. Document the support plan in your knowledge management tool. Include how to request support,
who to notify if a support case is ﬁled, and how to escalate during an incident. A wiki is a good
mechanism to allow anyone to make necessary updates to documentation when they become aware
of changes to support processes or contacts.
Level of eﬀort for the implementation plan: Low. Most software and services vendors oﬀer opt-in
support plans. Documenting and sharing support best practices on your knowledge management system
veriﬁes that your team knows what to do when there is a production issue.
Resources
Related best practices:
• OPS02-BP02 Processes and procedures have identiﬁed owners (p. 61)
113

AWS Well-Architected Framework
Operate
Related documents:
• AWS Support Plans
Related services:
• AWS Business Support
• AWS Enterprise Support
Operate
Questions
• OPS 8. How do you understand the health of your workload? (p. 114)
• OPS 9. How do you understand the health of your operations? (p. 123)
• OPS 10. How do you manage workload and operations events? (p. 131)
OPS 8. How do you understand the health of your workload?
Deﬁne, capture, and analyze workload metrics to gain visibility to workload events so that you can take
appropriate action.
Best practices
• OPS08-BP01 Identify key performance indicators (p. 114)
• OPS08-BP02 Deﬁne workload metrics (p. 115)
• OPS08-BP03 Collect and analyze workload metrics (p. 117)
• OPS08-BP04 Establish workload metrics baselines (p. 118)
• OPS08-BP05 Learn expected patterns of activity for workload (p. 120)
• OPS08-BP06 Alert when workload outcomes are at risk (p. 121)
• OPS08-BP07 Alert when workload anomalies are detected (p. 122)
• OPS08-BP08 Validate the achievement of outcomes and the eﬀectiveness of KPIs and
metrics (p. 123)
OPS08-BP01 Identify key performance indicators
Identify key performance indicators (KPIs) based on desired business outcomes (for example, order rate,
customer retention rate, and proﬁt versus operating expense) and customer outcomes (for example,
customer satisfaction). Evaluate KPIs to determine workload success.
Common anti-patterns:
• You are asked by business leadership how successful a workload has been serving business needs but
have no frame of reference to determine success.
• You are unable to determine if the commercial oﬀ-the-shelf application you operate for your
organization is cost-eﬀective.
Beneﬁts of establishing this best practice: By identifying key performance indicators you help achieve
business outcomes as the test of the health and success of your workload.
Level of risk exposed if this best practice is not established: High
114

AWS Well-Architected Framework
Operate
Implementation guidance
• Identify key performance indicators: Identify key performance indicators (KPIs) based on desired
business and customer outcomes. Evaluate KPIs to determine workload success.
OPS08-BP02 Deﬁne workload metrics
Deﬁne metrics that measure the health of the workload. Workload health is measured by the
achievement of business outcomes (KPIs) and the state of workload components and applications.
Examples of KPIs are abandoned shopping carts, orders placed, cost, price, and allocated workload
expense. While you may collect telemetry from multiple components, select a subset that provides
insight into the overall workload health. Adjust workload metrics over time as business needs change.
Desired outcome:
• You have identiﬁed metrics that validate the achievement of KPIs that reﬂect business outcomes.
• You have metrics that show a consistent view of workload health.
• Workload metrics are evaluated periodically as business needs change.
Common anti-patterns:
• You are monitoring all the applications in your workload but are unable to determine if your workload
is achieving business outcomes.
• You have deﬁned workload metrics but they are not associated to any business KPIs.
Beneﬁts of establishing this best practice:
• You can measure your workload against the achievement of business outcomes.
• You know if your workload is in a healthy state or needs intervention.
Level of risk exposed if this best practice is not established: High
Implementation guidance
The goal of this best practice is that you can answer the following question: is my workload healthy?
Workload health is determined by the achievement of business outcomes and the state of applications
and components in the workload. Work backwards from business KPIs to identify metrics. Identify key
metrics from components and applications. Periodically review workload metrics as business needs
change.
Customer example
Workload health is determined at AnyCompany Retail by a collection of application and component
metrics. Starting with business KPIs, they identify metrics like order rate that can show they are
achieving business outcomes. They also include key application metrics like page response and
component metrics like open database connections. On a quarterly basis, they re-evaluate workload
metrics to make sure they are still valid in determining workload health.
Implementation steps
1. Starting with business KPIs, identify metrics that show you are achieving business outcomes. If there
are KPIs that do not have metrics, instrument your workload with additional metrics for any missing
business KPIs.
a. You can publish custom metrics from your applications to Amazon CloudWatch.
115

AWS Well-Architected Framework
Operate
b. The AWS Distro for OpenTelemetry can collect metrics from existing applications and be used to
add new metrics.
c. Customers with Enterprise Support can request the Building a Monitoring Strategy Workshop from
their Technical Account Manager. This workshop will help you build an observability strategy for
your workload.
2. Identify metrics for applications and components in the workload. What are key metrics that show
the health of individual components and applications? Applications and components may emit many
diﬀerent metrics, but choose one to three key metrics that show their overall health.
3. Implement a mechanism to evaluate workload metrics periodically. When business KPIs change, work
with stakeholders to update workload metrics. As your workload components and applications evolve,
adjust your workload metrics.
Level of eﬀort for the implementation plan: Medium. Adding metrics for business KPIs to applications
may require moderate eﬀort.
Resources
Related best practices:
• OPS04-BP01 Implement application telemetry (p. 74) - Your application must emit telemetry that
supports business outcomes.
• OPS04-BP02 Implement and conﬁgure workload telemetry (p. 76) - You must instrument your
workload to emit telemetry before you can deﬁne workload metrics that support business outcomes.
• OPS08-BP01 Identify key performance indicators (p. 114) - You must identify key performance
indicators ﬁrst before selecting workload metrics.
Related documents:
• Adding metrics and traces to your application on Amazon EKS with AWS Distro for OpenTelemetry,
AWS X-Ray, and Amazon CloudWatch
• Instrumenting distributed systems for operational visibility
• Implementing health checks
• How to Monitor your Applications Eﬀectively
• How to better monitor your custom application metrics using Amazon CloudWatch Agent
Related videos:
• AWS re:Invent 2020: Monitoring production services at Amazon
• AWS re:Invent 2022 - Building observable applications with OpenTelemetry (BOA310)
• How to Easily Setup Application Monitoring for Your AWS Workloads - AWS Online Tech Talks
• Mastering Observability of Your Serverless Applications - AWS Online Tech Talks
Related examples:
• One Observability Workshop
Related services:
• Amazon CloudWatch
• AWS Distro for OpenTelemetry
116

AWS Well-Architected Framework
Operate
OPS08-BP03 Collect and analyze workload metrics
Perform regular, proactive reviews of workload metrics to identify trends and determine if a response is
necessary and validate the achievement of business outcomes. Aggregate metrics from your workload
applications and components to a central location. Use dashboards and analytics tools to analyze
telemetry and determine workload health. Implement a mechanism to conduct workload health reviews
on periodic basis with stakeholders in your organization.
Desired outcome:
• Workload metrics are collected in a central location.
• Dashboards and analytics tools are used to analyze workload health trends.
• You conduct periodic workload metric reviews with your organization.
Common anti-patterns:
• Your organization collects metrics from the workload in two diﬀerent observability platforms. You are
unable to determine workload health because the platforms are incompatible.
• Error rates for a component of your workload are slowly increasing. You fail to notice this trend
because your organization does not conduct periodic workload metric reviews. The component fails
after a week, impairing your workload.
Beneﬁts of establishing this best practice:
• You have increased awareness of workload health and the achievement of business outcomes.
• Workload health trends can be developed over time.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Collect workload metrics in a central location. Using dashboards and analytics tools, analyze workload
metrics to gain insight into workload health, develop workload health trends, and validate the
achievement of business outcomes. Implement a mechanism to conduct periodic reviews of workload
metrics.
Customer example
AnyCompany Retail conducts workload metric reviews every week on Wednesday. They gather
stakeholders from across the company and go through the previous week’s metrics. During the meeting,
they highlight trends and insights gleaned from analytics tools. Internal dashboards are published with
key workload metrics that any employee can view and search.
Implementation steps
1. Identify the workload metrics that are tied to workload health. Starting with business KPIs, identify
the metrics for applications, components, and platforms that provide an overall view of workload
health.
a. You can publish custom metrics to Amazon CloudWatch. You can leverage the Amazon CloudWatch
agent to collect metrics and logs from Amazon EC2 instances and on-premises servers.
b. The AWS Distro for OpenTelemetry can collect metrics from existing applications and be used to
add new metrics.
c. Customers with Enterprise Support can request the Building a Monitoring Strategy Workshop from
their Technical Account Manager. This workshop helps you build an observability strategy for your
workload.
117

AWS Well-Architected Framework
Operate
2. Collect workload metrics in a central platform. If workload metrics are split between diﬀerent
platform, this can make it diﬃcult to analyze and develop trends. The platform should have
dashboards and analytic capabilities.
a. Amazon CloudWatch can collect and store workload metrics. In multi-account topologies, it is
recommended to have a central logging and monitoring account, referred to as a log archive
account.
3. Build a consolidated dashboard of workload metrics. Use this view for metrics reviews and analysis of
trends.
a. You can create custom CloudWatch dashboards to collect your workload metrics in a consolidated
view.
4. Implement a workload metric review process. On a weekly, bi-weekly, or monthly basis, review your
workload metrics with stakeholders, including technical and non-technical personnel. Use these
review sessions to identify trends and gain insight into workload health.
Level of eﬀort for the implementation plan: High. If workload metrics are not centrally collected, it
could require signiﬁcant investment to consolidate them in one platform.
Resources
Related best practices:
• OPS08-BP01 Identify key performance indicators (p. 114) - You must identify key performance
indicators ﬁrst before selecting workload metrics.
• OPS08-BP02 Deﬁne workload metrics (p. 115) - You must deﬁne workload metrics before collecting
and analyzing them.
Related documents:
• Power operational insights with Amazon QuickSight
• Using Amazon CloudWatch dashboards custom widgets
Related videos:
• Create Cross Account & Cross Region CloudWatch Dashboards
• Monitor AWS Resources Using Amazon CloudWatch Dashboards
Related examples:
• AWS Management and Governance Tools Workshop - CloudWatch Dashboards
• Well-Architected Labs - Level 100: Monitoring with CloudWatch Dashboards
Related services:
• Amazon CloudWatch
• AWS Distro for OpenTelemetry
OPS08-BP04 Establish workload metrics baselines
Establishing a baseline for workload metrics aids in understanding workload health and performance.
Using baselines, you can identify under- and over-performing applications and components. A workload
baseline adds to your ability to mitigate issues before they become incidents. Baselines are foundational
118

AWS Well-Architected Framework
Operate
in developing patterns of activity and implementing anomaly detection when metrics deviate from
expected values.
Desired outcome:
• You have a baseline level of metrics for your workload under normal conditions.
• You can determine if your workload is functioning normally.
Common anti-patterns:
• After deploying a new feature, there is drop in request latency. A baseline was not established for a
composite metric of incoming processed requests and overall latency. You are unable to determine if
the change caused an improvement or caused a defect.
• A sudden spike in user activity occurs, but you have not established a metric baseline. The activity
spike slowly leads to a memory leak in an application. Eventually this takes your workload oﬄine.
Beneﬁts of establishing this best practice:
• You understand the normal pattern of activity for your workload using metrics for key components
and applications.
• You can determine if your workload, its applications, and components, are behaving normally or may
require intervention.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Use historical data to establish a baseline of workload metrics for applications and components in your
workload. Leverage the metric baseline in metric review meetings and troubleshooting. Periodically
review workload performance and adjust the baseline as the architecture evolves.
Customer example
Baselines are established for all components and applications at AnyCompany Retail. Using historical
data, AnyCompany Retail developed their workload metric baselines over a two-month metric window.
Every two months they re-assess baselines and adjust them based on real-world data.
Implementation steps
1. Working backwards from your workload metrics, establish a metric baseline for key components and
applications using historical data. Limit the number of metrics per component or application and
avoid monitor fatigue.
a. You can use Amazon CloudWatch Metrics Insights to query metrics at scale and identify trends and
patterns.
b. Amazon CloudWatch anomaly detection uses machine learning algorithms to identify patterns of
behavior for metrics, determine baselines, and surfaces anomalies.
c. Amazon DevOps Guru provides the ability to detect operational issues with your workload using
machine learning.
d. Customers with Enterprise Support can request the Building a Monitoring Strategy Workshop from
their Technical Account Manager. This workshop will help you build an observability strategy for
your workload.
2. Put in place a mechanism to periodically review workload metric baselines, especially before
signiﬁcant business events. At least once a quarter, evaluate your workload metric baseline using
historical data. Use the baseline in your metric review meetings.
119

AWS Well-Architected Framework
Operate
Level of eﬀort for the implementation plan: Low. Having established workload metrics, establishing
baselines may require you to collect enough data to identify normal patterns of behavior.
Resources
Related best practices:
• OPS08-BP02 Deﬁne workload metrics (p. 115) - Workload metrics must be established ﬁrst before
determining baselines.
• OPS08-BP03 Collect and analyze workload metrics (p. 117) - Collecting and analyzing workload
metrics is necessary to have in place before establishing metric baselines.
• OPS08-BP05 Learn expected patterns of activity for workload (p. 120) - This best practice builds on
top of the baseline to develop usage trends.
• OPS08-BP06 Alert when workload outcomes are at risk (p. 121) - Metric baselines are necessary to
identifying thresholds and developing alerts.
• OPS08-BP07 Alert when workload anomalies are detected (p. 122) - Anomaly detection requires the
establishment of metric baselines.
Related documents:
• AWS Observability Best Practices - Alarms
• How to Monitor your Applications Eﬀectively
• How to set up CloudWatch Anomaly Detection to set dynamic alarms, automate actions, and drive
online sales
• Operationalizing CloudWatch Anomaly Detection
Related videos:
• AWS re:Invent 2020: Monitoring production services at Amazon
• AWS re:Invent 2021- Get insights from operational metrics at scale with CloudWatch Metrics Insights
• AWS re:Invent 2022 - Developing an observability strategy (COP302)
• AWS Summit DC 2022 - Monitoring and observability for modern applications
• AWS Summit SF 2022 - Full-stack observability and application monitoring with AWS (COP310)
Related examples:
• AWS CloudTrail and Amazon CloudWatch Integration Workshop
Related services:
• Amazon CloudWatch
• Amazon DevOps Guru
OPS08-BP05 Learn expected patterns of activity for workload
Establish patterns of workload activity to identify anomalous behavior so that you can respond
appropriately if required.
CloudWatch through the CloudWatch Anomaly Detection feature applies statistical and machine learning
algorithms to generate a range of expected values that represent normal metric behavior.
120

AWS Well-Architected Framework
Operate
Amazon DevOps Guru can be used to identify anomalous behavior through event correlation, log
analysis, and applying machine learning to analyze your workload telemetry. When unexpected
behaviors are detected, it provides the related metrics and events with recommendations to address the
behavior.
Common anti-patterns:
• You are reviewing network utilization logs and see that network utilization increased between
11:30am and 1:30pm and then again at 4:30pm through 6:00pm. You are unaware if this should be
considered normal or not.
• Your web servers reboot every night at 3:00am. You are unaware if this is an expected behavior.
Beneﬁts of establishing this best practice: By learning patterns of behavior you can recognize
unexpected behavior and take action if necessary.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Learn expected patterns of activity for workload: Establish patterns of workload activity to determine
when behavior is outside of the expected values so that you can respond appropriately if required.
Resources
Related documents:
• Amazon DevOps Guru
• CloudWatch Anomaly Detection
OPS08-BP06 Alert when workload outcomes are at risk
Raise an alert when workload outcomes are at risk so that you can respond appropriately if necessary.
Ideally, you have previously identiﬁed a metric threshold that you are able to alarm upon or an event
that you can use to initiate an automated response.
On AWS, you can use Amazon CloudWatch Synthetics to create canary scripts to monitor your endpoints
and APIs by performing the same actions as your customers. The telemetry generated and the insight
gained can help you to identify issues before your customers are impacted.
You can also use CloudWatch Logs Insights to interactively search and analyze your log data using a
purpose-built query language. CloudWatch Logs Insights automatically discovers ﬁelds in logs from AWS
services, and custom log events in JSON. It scales with your log volume and query complexity and gives
you answers in seconds, helping you to search for the contributing factors of an incident.
Common anti-patterns:
• You have no network connectivity. No one is aware. No one is trying to identify why or taking action to
restore connectivity.
• Following a patch, your persistent instances have become unavailable, disrupting users. Your users
have opened support cases. No one has been notiﬁed. No one is taking action.
Beneﬁts of establishing this best practice: By identifying that business outcomes are at risk and alerting
for action to be taken you have the opportunity to prevent or mitigate the impact of an incident.
Level of risk exposed if this best practice is not established: Medium
121

AWS Well-Architected Framework
Operate
Implementation guidance
• Alert when workload outcomes are at risk: Raise an alert when workload outcomes are at risk so that
you can respond appropriately if required.
• What is Amazon CloudWatch Events?
• Creating Amazon CloudWatch Alarms
• Invoking Lambda functions using Amazon SNS notiﬁcations
Resources
Related documents:
• Amazon CloudWatch Synthetics
• CloudWatch Logs Insights
• Creating Amazon CloudWatch Alarms
• Invoking Lambda functions using Amazon SNS notiﬁcations
• What is Amazon CloudWatch Events?
OPS08-BP07 Alert when workload anomalies are detected
Raise an alert when workload anomalies are detected so that you can respond appropriately if necessary.
Your analysis of your workload metrics over time may establish patterns of behavior that you can
quantify suﬃciently to deﬁne an event or raise an alarm in response.
Once trained, the CloudWatch Anomaly Detection feature can be used to alarm on detected anomalies or
can provide overlaid expected values onto a graph of metric data for ongoing comparison.
Common anti-patterns:
• Your retail website sales have increased suddenly and dramatically. No one is aware. No one is trying to
identify what led to this surge. No one is taking action to ensure quality customer experiences under
the additional load.
• Following the application of a patch, your persistent servers are rebooting frequently, disrupting users.
Your servers typically reboot up to three times but not more. No one is aware. No one is trying to
identify why this is happening.
Beneﬁts of establishing this best practice: By understanding patterns of workload behavior, you can
identify unexpected behavior and take action if necessary.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Alert when workload anomalies are detected: Raise an alert when workload anomalies are detected so
that you can respond appropriately if required.
• What is Amazon CloudWatch Events?
• Creating Amazon CloudWatch Alarms
• Invoking Lambda functions using Amazon SNS notiﬁcations
Resources
Related documents:
122

AWS Well-Architected Framework
Operate
• Creating Amazon CloudWatch Alarms
• CloudWatch Anomaly Detection
• Invoking Lambda functions using Amazon SNS notiﬁcations
• What is Amazon CloudWatch Events?
OPS08-BP08 Validate the achievement of outcomes and the eﬀectiveness of
KPIs and metrics
Create a business-level view of your workload operations to help you determine if you are satisfying
needs and to identify areas that need improvement to reach business goals. Validate the eﬀectiveness of
KPIs and metrics and revise them if necessary.
AWS also has support for third-party log analysis systems and business intelligence tools through the
AWS service APIs and SDKs (for example, Grafana, Kibana, and Logstash).
Common anti-patterns:
• Page response time has never been considered a contributor to customer satisfaction. You have never
established a metric or threshold for page response time. Your customers are complaining about
slowness.
• You have not been achieving your minimum response time goals. In an eﬀort to improve response
time, you have scaled up your application servers. You are now exceeding response time goals by a
signiﬁcant margin and also have signiﬁcant unused capacity you are paying for.
Beneﬁts of establishing this best practice: By reviewing and revising KPIs and metrics, you understand
how your workload supports the achievement of your business outcomes and can identify where
improvement is needed to reach business goals.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Validate the achievement of outcomes and the eﬀectiveness of KPIs and metrics: Create a business
level view of your workload operations to help you determine if you are satisfying needs and to
identify areas that need improvement to reach business goals. Validate the eﬀectiveness of KPIs and
metrics and revise them if necessary.
• Using Amazon CloudWatch dashboards
• What is log analytics?
Resources
Related documents:
• Using Amazon CloudWatch dashboards
• What is log analytics?
OPS 9. How do you understand the health of your operations?
Deﬁne, capture, and analyze operations metrics to gain visibility to operations events so that you can
take appropriate action.
Best practices
• OPS09-BP01 Identify key performance indicators (p. 124)
• OPS09-BP02 Deﬁne operations metrics (p. 124)
123

AWS Well-Architected Framework
Operate
• OPS09-BP03 Collect and analyze operations metrics (p. 125)
• OPS09-BP04 Establish operations metrics baselines (p. 126)
• OPS09-BP05 Learn the expected patterns of activity for operations (p. 126)
• OPS09-BP06 Alert when operations outcomes are at risk (p. 127)
• OPS09-BP07 Alert when operations anomalies are detected (p. 129)
• OPS09-BP08 Validate the achievement of outcomes and the eﬀectiveness of KPIs and
metrics (p. 130)
OPS09-BP01 Identify key performance indicators
Identify key performance indicators (KPIs) based on desired business outcomes (for example, new
features delivered) and customer outcomes (for example, customer support cases). Evaluate KPIs to
determine operations success.
Common anti-patterns:
• You are asked by business leadership how successful operations is at accomplishing business goals but
have no frame of reference to determine success.
• You are unable to determine if your maintenance windows have an impact on business outcomes.
Beneﬁts of establishing this best practice: By identifying key performance indicators you help achieve
business outcomes as the test of the health and success of your operations.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Identify key performance indicators: Identify key performance indicators (KPIs) based on desired
business and customer outcomes. Evaluate KPIs to determine operations success.
OPS09-BP02 Deﬁne operations metrics
Deﬁne operations metrics to measure the achievement of KPIs (for example, successful deployments,
and failed deployments). Deﬁne operations metrics to measure the health of operations activities (for
example, mean time to detect an incident (MTTD), and mean time to recovery (MTTR) from an incident).
Evaluate metrics to determine if operations are achieving desired outcomes, and to understand the
health of your operations activities.
Common anti-patterns:
• Your operations metrics are based on what the team thinks is reasonable.
• You have errors in your metrics calculations that will yield incorrect results.
• You don't have any metrics deﬁned for your operations activities.
Beneﬁts of establishing this best practice: By deﬁning and evaluating operations metrics you can
determine the health of your operations activities and measure the achievement of business outcomes.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Deﬁne operations metrics: Deﬁne operations metrics to measure the achievement of KPIs. Deﬁne
operations metrics to measure the health of operations and its activities. Evaluate metrics to
determine if operations are achieving desired outcomes, and to understand the health of the
operations.
124

AWS Well-Architected Framework
Operate
• Publish custom metrics
• Searching and ﬁltering log data
• Amazon CloudWatch metrics and dimensions reference
Resources
Related documents:
• AWS Answers: Centralized Logging
• Amazon CloudWatch metrics and dimensions reference
• Detect and React to Changes in Pipeline State with Amazon CloudWatch Events
• Publish custom metrics
• Searching and ﬁltering log data
Related videos:
• Build a Monitoring Plan
OPS09-BP03 Collect and analyze operations metrics
Perform regular, proactive reviews of metrics to identify trends and determine where appropriate
responses are needed.
You should aggregate log data from the processing of your operations activities and operations API calls,
into a service such as CloudWatch Logs. Generate metrics from observations of necessary log content to
gain insight into the performance of operations activities.
On AWS, you can export your log data to Amazon S3 or send logs directly to Amazon S3 for long-term
storage. Using AWS Glue, you can discover and prepare your log data in Amazon S3 for analytics, storing
associated metadata in the AWSAWS Glue Data Catalog. Amazon Athena, through its native integration
with AWS Glue, can then be used to analyze your log data, querying it using standard SQL. Using a
business intelligence tool like Amazon QuickSight you can visualize, explore, and analyze your data.
Common anti-patterns:
• Consistent delivery of new features is considered a key performance indicator. You have no method to
measure how frequently deployments occur.
• You log deployments, rolled back deployments, patches, and rolled back patches to track you
operations activities, but no one reviews the metrics.
• You have a recovery time objective to restore a lost database within ﬁfteen minutes that was deﬁned
when the system was deployed and had no users. You now have ten thousand users and have been
operating for two years. A recent restore took over two hours. This was not recorded and no one is
aware.
Beneﬁts of establishing this best practice: By collecting and analyzing your operations metrics, you gain
understanding of the health of your operations and can gain insight to trends that have may an impact
on your operations or the achievement of your business outcomes.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Collect and analyze operations metrics: Perform regular proactive reviews of metrics to identify trends
and determine where appropriate responses are needed.
125

AWS Well-Architected Framework
Operate
• Using Amazon CloudWatch metrics
• Amazon CloudWatch metrics and dimensions reference
• Collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch
Agent
Resources
Related documents:
• Amazon Athena
• Amazon CloudWatch metrics and dimensions reference
• Amazon QuickSight
• AWS Glue
• AWSAWS Glue Data Catalog
• Collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch
Agent
• Using Amazon CloudWatch metrics
OPS09-BP04 Establish operations metrics baselines
Establish baselines for metrics to provide expected values as the basis for comparison and identiﬁcation
of under and over performing operations activities.
Common anti-patterns:
• You have been asked what the expected time to deploy is. You have not measured how long it takes to
deploy and can not determine expected times.
• You have been asked what how long it takes to recover from an issue with the application servers. You
have no information about time to recovery from ﬁrst customer contact. You have no information
about time to recovery from ﬁrst identiﬁcation of an issue through monitoring.
• You have been asked how many support personnel are required over the weekend. You have no idea
how many support cases are typical over a weekend and can not provide an estimate.
• You have a recovery time objective to restore lost databases within ﬁfteen minutes that was deﬁned
when the system was deployed and had no users. You now have ten thousand users and have been
operating for two years. You have no information on how the time to restore has changed for your
database.
Beneﬁts of establishing this best practice: By deﬁning baseline metric values you are able to evaluate
current metric values, and metric trends, to determine if action is required.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Learn expected patterns of activity for operations: Establish patterns of operations activity to
determine when behavior is outside of the expected values so that you can respond appropriately if
required.
OPS09-BP05 Learn the expected patterns of activity for operations
Establish patterns of operations activities to identify anomalous activity so that you can respond
appropriately if necessary.
126

AWS Well-Architected Framework
Operate
Common anti-patterns:
• Your deployment failure rate has increased substantially recently. You address each of the failures
independently. You do not realize that the failures correspond to deployments by a new employee who
is unfamiliar with the deployment management system.
Beneﬁts of establishing this best practice: By learning patterns of behavior, you can recognize
unexpected behavior and take action if necessary.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Learn expected patterns of activity for operations: Establish patterns of operations activity to
determine when behavior is outside of the expected values so that you can respond appropriately if
required.
OPS09-BP06 Alert when operations outcomes are at risk
Whenever operations outcomes are at risk, an alert must be raised and acted upon. Operations outcomes
are any activity that supports a workload in production. This includes everything from deploying new
versions of applications to recovering from an outage. Operations outcomes must be treated with the
same importance as business outcomes.
Software teams should identify key operations metrics and activities and build alerts for them. Alerts
must be timely and actionable. If an alert is raised, a reference to a corresponding runbook or playbook
should be included. Alerts without a corresponding action can lead to alert fatigue.
Desired outcome: When operations activities are at risk, alerts are sent to drive action. The alerts contain
context on why an alert is being raised and point to a playbook to investigate or a runbook to mitigate.
Where possible, runbooks are automated and notiﬁcations are sent.
Common anti-patterns:
• You are investigating an incident and support cases are being ﬁled. The support cases are breaching
the service level agreement (SLA) but no alerts are being raised.
• A deployment to production scheduled for midnight is delayed due to last-minute code changes. No
alert is raised and the deployment hangs.
• A production outage occurs but no alerts are sent.
• Your deployment time consistently runs behind estimates. No action is taken to investigate.
Beneﬁts of establishing this best practice:
• Alerting when operations outcomes are at risk boosts your ability to support your workload by staying
ahead of issues.
• Business outcomes are improved due to healthy operations outcomes.
• Detection and remediation of operations issues are improved.
• Overall operational health is increased.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Operations outcomes must be deﬁned before you can alert on them. Start by deﬁning what operations
activities are most important to your organization. Is it deploying to production in under two hours or
127

AWS Well-Architected Framework
Operate
responding to a support case within a set amount of time? Your organization must deﬁne key operations
activities and how they are measured so that they can be monitored, improved, and alerted on. You
need a central location where workload and operations telemetry is stored and analyzed. The same
mechanism should be able to raise an alert when an operations outcome is at risk.
Customer example
A CloudWatch alarm was initiated during a routine deployment at AnyCompany Retail. The lead time
for deployment was breached. Amazon EventBridge created an OpsItem in AWS Systems Manager
OpsCenter. The Cloud Operations team used a playbook to investigate the issue and identiﬁed that a
schema change was taking longer than expected. They alerted the on-call developer and continued
monitoring the deployment. Once the deployment was complete, the Cloud Operations team resolved
the OpsItem. The team will analyze the incident during a postmortem.
Implementation steps
1. If you have not identiﬁed operations KPIs, metrics, and activities, work on implementing the preceding
best practices to this question (OPS09-BP01 to OPS09-BP05).
• AWS Support customers with Enterprise Support can request the Operations KPI Workshop from
their Technical Account Manager. This collaborative workshop helps you deﬁne operations KPIs and
metrics aligned to business goals, provided at no additional cost. Contact your Technical Account
Manager to learn more.
2. Once you have operations activities, KPIs, and metrics established, conﬁgure alerts in your
observability platform. Alerts should have an action associated to them, like a playbook or runbook.
Alerts without an action should be avoided.
3. Over time, you should evaluate your operations metrics, KPIs, and activities to identify areas of
improvement. Capture feedback in runbooks and playbooks from operators to identify areas for
improvement in responding to alerts.
4. Alerts should include a mechanism to ﬂag them as a false-positive. This should lead to a review of the
metric thresholds.
Level of eﬀort for the implementation plan: Medium. There are several best practices that must be
in place before implementing this best practice. Once operations activities have been identiﬁed and
operations KPIs established, alerts should be established.
Resources
Related best practices:
• OPS02-BP03 Operations activities have identiﬁed owners responsible for their performance (p. 62):
Every operation activity and outcome should have an identiﬁed owner that's responsible. This is who
should be alerted when outcomes are at risk.
• OPS03-BP02 Team members are empowered to take action when outcomes are at risk (p. 66):
When alerts are raised, your team should have agency to act to remedy the issue.
• OPS09-BP01 Identify key performance indicators (p. 124): Alerting on operations outcomes starts
with identify operations KPIs.
• OPS09-BP02 Deﬁne operations metrics (p. 124): Establish this best practice before you start
generating alerts.
• OPS09-BP03 Collect and analyze operations metrics (p. 125): Centrally collecting operations metrics
is required to build alerts.
• OPS09-BP04 Establish operations metrics baselines (p. 126): Operations metrics baselines provide
the ability to tune alerts and avoid alert fatigue.
• OPS09-BP05 Learn the expected patterns of activity for operations (p. 126): You can improve the
accuracy of your alerts by understanding the activity patterns for operations events.
128

AWS Well-Architected Framework
Operate
• OPS09-BP08 Validate the achievement of outcomes and the eﬀectiveness of KPIs and
metrics (p. 130): Evaluate the achievement of operations outcomes to ensure that your KPIs and
metrics are valid.
• OPS10-BP02 Have a process per alert (p. 134): Every alert should have an associated runbook or
playbook and provide context for the person being alerted.
• OPS11-BP02 Perform post-incident analysis (p. 142): Conduct a post-incident analysis after the alert
to identify areas for improvement.
Related documents:
• AWS Deployment Pipelines Reference Architecture: Application Pipeline Architecture
• GitLab: Getting Started with Agile / DevOps Metrics
Related videos:
• Aggregate and Resolve Operational Issues Using AWS Systems Manager OpsCenter
• Integrate AWS Systems Manager OpsCenter with Amazon CloudWatch Alarms
• Integrate Your Data Sources into AWS Systems Manager OpsCenter Using Amazon EventBridge
Related examples:
• Automate remediation actions for Amazon EC2 notiﬁcations and beyond using Amazon EC2 Systems
Manager Automation and AWS Health
• AWS Management and Governance Tools Workshop - Operations 2022
• Ingesting, analyzing, and visualizing metrics with DevOps Monitoring Dashboard on AWS
Related services:
• Amazon EventBridge
• AWS Support Proactive Services - Operations KPI Workshop
• AWS Systems Manager OpsCenter
• CloudWatch Events
OPS09-BP07 Alert when operations anomalies are detected
Raise an alert when operations anomalies are detected so that you can respond appropriately if
necessary.
Your analysis of your operations metrics over time may established patterns of behavior that you can
quantify suﬃciently to deﬁne an event or raise an alarm in response.
Once trained, the CloudWatch Anomaly Detection feature can be used to alarm on detected anomalies or
can provide overlaid expected values onto a graph of metric data for ongoing comparison.
Amazon DevOps Guru can be used to identify anomalous behavior through event correlation, log
analysis, and applying machine learning to analyze your workload telemetry. The insights gained are
presented with the relevant data and recommendations.
Common anti-patterns:
• You are applying a patch to your ﬂeet of instances. You tested the patch successfully in the test
environment. The patch is failing for a large percentage of instances in your ﬂeet. You do nothing.
129

AWS Well-Architected Framework
Operate
• You note that there are deployments starting Friday end of day. Your organization has predeﬁned
maintenance windows on Tuesdays and Thursdays. You do nothing.
Beneﬁts of establishing this best practice: By understanding patterns of operations behavior you can
identify unexpected behavior and take action if necessary.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Alert when operations anomalies are detected: Raise an alert when operations anomalies are detected
so that you can respond appropriately if required.
• What is Amazon CloudWatch Events?
• Creating Amazon CloudWatch alarms
• Invoking Lambda functions using Amazon SNS notiﬁcations
Resources
Related documents:
• Amazon DevOps Guru
• CloudWatch Anomaly Detection
• Creating Amazon CloudWatch alarms
• Detect and React to Changes in Pipeline State with Amazon CloudWatch Events
• Invoking Lambda functions using Amazon SNS notiﬁcations
• What is Amazon CloudWatch Events?
OPS09-BP08 Validate the achievement of outcomes and the eﬀectiveness of
KPIs and metrics
Create a business-level view of your operations activities to help you determine if you are satisfying
needs and to identify areas that need improvement to reach business goals. Validate the eﬀectiveness of
KPIs and metrics and revise them if necessary.
AWS also has support for third-party log analysis systems and business intelligence tools through the
AWS service APIs and SDKs (for example, Grafana, Kibana, and Logstash).
Common anti-patterns:
• The frequency of your deployments has increased with the growth in number of development teams.
Your deﬁned expected number of deployments is once per week. You have been regularly deploying
daily. When their is an issue with your deployment system, and deployments are not possible, it goes
undetected for days.
• When your business previously provided support only during core business hours from Monday
to Friday. You established a next business day response time goal for incidents. You have recently
started oﬀering 24x7 support coverage with a two hour response time goal. Your overnight staﬀ are
overwhelmed and customers are unhappy. There is no indication that there are issues with incident
response times because you are reporting against a next business day target.
Beneﬁts of establishing this best practice: By reviewing and revising KPIs and metrics, you understand
how your workload supports the achievement of your business outcomes and can identify where
improvement is needed to reach business goals.
Level of risk exposed if this best practice is not established: Low
130

AWS Well-Architected Framework
Operate
Implementation guidance
• Validate the achievement of outcomes and the eﬀectiveness of KPIs and metrics: Create a business
level view of your operations activities to help you determine if you are satisfying needs and to
identify areas that need improvement to reach business goals. Validate the eﬀectiveness of KPIs and
metrics and revise them if necessary.
• Using Amazon CloudWatch dashboards
• What is log analytics?
Resources
Related documents:
• Using Amazon CloudWatch dashboards
• What is log analytics?
OPS 10. How do you manage workload and operations events?
Prepare and validate procedures for responding to events to minimize their disruption to your workload.
Best practices
• OPS10-BP01 Use a process for event, incident, and problem management (p. 131)
• OPS10-BP02 Have a process per alert (p. 134)
• OPS10-BP03 Prioritize operational events based on business impact (p. 135)
• OPS10-BP04 Deﬁne escalation paths (p. 135)
• OPS10-BP05 Deﬁne a customer communication plan for outages (p. 136)
• OPS10-BP06 Communicate status through dashboards (p. 139)
• OPS10-BP07 Automate responses to events (p. 139)
OPS10-BP01 Use a process for event, incident, and problem management
Your organization has processes to handle events, incidents, and problems. Events are things that occur
in your workload but may not need intervention. Incidents are events that require intervention. Problems
are recurring events that require intervention or cannot be resolved. You need processes to mitigate the
impact of these events on your business and make sure that you respond appropriately.
When incidents and problems happen to your workload, you need processes to handle them. How will
you communicate the status of the event with stakeholders? Who oversees leading the response? What
are the tools that you use to mitigate the event? These are examples of some of the questions you need
answer to have a solid response process.
Processes must be documented in a central location and available to anyone involved in your workload.
If you don’t have a central wiki or document store, a version control repository can be used. You’ll keep
these plans up to date as your processes evolve.
Problems are candidates for automation. These events take time away from your ability to innovate.
Start with building a repeatable process to mitigate the problem. Over time, focus on automating the
mitigation or ﬁxing the underlying issue. This frees up time to devote to making improvements in your
workload.
Desired outcome: Your organization has a process to handle events, incidents, and problems. These
processes are documented and stored in a central location. They are updated as processes change.
Common anti-patterns:
131

AWS Well-Architected Framework
Operate
• An incident happens on the weekend and the on-call engineer doesn’t know what to do.
• A customer sends you an email that the application is down. You reboot the server to ﬁx it. This
happens frequently.
• There is an incident with multiple teams working independently to try to solve it.
• Deployments happen in your workload without being recorded.
Beneﬁts of establishing this best practice:
• You have an audit trail of events in your workload.
• Your time to recover from an incident is decreased.
• Team members can resolve incidents and problems in a consistent manner.
• There is a more consolidated eﬀort when investigating an incident.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Implementing this best practice means you are tracking workload events. You have processes to handle
incidents and problems. The processes are documented, shared, and updated frequently. Problems are
identiﬁed, prioritized, and ﬁxed.
Customer example
AnyCompany Retail has a portion of their internal wiki devoted to processes for event, incident, and
problem management. All events are sent to Amazon EventBridge. Problems are identiﬁed as OpsItems
in AWS Systems Manager OpsCenter and prioritized to ﬁx, reducing undiﬀerentiated labor. As processes
change, they’re updated in their internal wiki. They use AWS Systems Manager Incident Manager to
manage incidents and coordinate mitigation eﬀorts.
Implementation steps
1. Events
• Track events that happen in your workload, even if no human intervention is required.
• Work with workload stakeholders to develop a list of events that should be tracked. Some examples
are completed deployments or successful patching.
• You can use services like Amazon EventBridge or Amazon Simple Notiﬁcation Service to generate
custom events for tracking.
2. Incidents
• Start by deﬁning the communication plan for incidents. What stakeholders must be informed? How
will you keep them in the loop? Who oversees coordinating eﬀorts? We recommend standing up an
internal chat channel for communication and coordination.
• Deﬁne escalation paths for the teams that support your workload, especially if the team doesn’t
have an on-call rotation. Based on your support level, you can also ﬁle a case with AWS Support.
• Create a playbook to investigate the incident. This should include the communication plan and
detailed investigation steps. Include checking the AWS Health Dashboard in your investigation.
• Document your incident response plan. Communicate the incident management plan so internal
and external customers understand the rules of engagement and what is expected of them. Train
your team members on how to use it.
• Customers can use Incident Manager to set up and manage their incident response plan.
• Enterprise Support customers can request the Incident Management Workshop from their Technical
Account Manager. This guided workshop tests your existing incident response plan and helps you
identify areas for improvement.
132

AWS Well-Architected Framework
Operate
3. Problems
• Problems must be identiﬁed and tracked in your ITSM system.
• Identify all known problems and prioritize them by eﬀort to ﬁx and impact to workload.
High Impact
High Effort
High Impact
Low Effort
Low Impact
High Effort
Low Impact
Low Effort
Efffort (High to Low)
• Solve problems that are high impact and low eﬀort ﬁrst. Once those are solved, move on to
problems to that fall into the low impact low eﬀort quadrant.
• You can use Systems Manager OpsCenter to identify these problems, attach runbooks to them, and
track them.
Level of eﬀort for the implementation plan: Medium. You need both a process and tools to implement
this best practice. Document your processes and make them available to anyone associated with the
workload. Update them frequently. You have a process for managing problems and mitigating them or
ﬁxing them.
Resources
Related best practices:
• OPS07-BP03 Use runbooks to perform procedures (p. 105): Known problems need an associated
runbook so that mitigation eﬀorts are consistent.
• OPS07-BP04 Use playbooks to investigate issues (p. 108): Incidents must be investigated using
playbooks.
• OPS11-BP02 Perform post-incident analysis (p. 142): Always conduct a postmortem after you
recover from an incident.
Related documents:
• Atlassian - Incident management in the age of DevOps
• AWS Security Incident Response Guide
• Incident Management in the Age of DevOps and SRE
• PagerDuty - What is Incident Management?
Related videos:
• AWS re:Invent 2020: Incident management in a distributed organization
133
to
(Low
Impact
High

AWS Well-Architected Framework
Operate
• AWS re:Invent 2021 - Building next-gen applications with event-driven architectures
• AWS Supports You | Exploring the Incident Management Tabletop Exercise
• AWS Systems Manager Incident Manager - AWS Virtual Workshops
• AWS What's Next ft. Incident Manager | AWS Events
Related examples:
• AWS Management and Governance Tools Workshop - OpsCenter
• AWS Proactive Services – Incident Management Workshop
• Building an event-driven application with Amazon EventBridge
• Building event-driven architectures on AWS
Related services:
• Amazon EventBridge
• Amazon SNS
• AWS Health Dashboard
• AWS Systems Manager Incident Manager
• AWS Systems Manager OpsCenter
OPS10-BP02 Have a process per alert
Have a well-deﬁned response (runbook or playbook), with a speciﬁcally identiﬁed owner, for any event
for which you raise an alert. This ensures eﬀective and prompt responses to operations events and
prevents actionable events from being obscured by less valuable notiﬁcations.
Common anti-patterns:
• Your monitoring system presents you a stream of approved connections along with other messages.
The volume of messages is so large that you miss periodic error messages that require your
intervention.
• You receive an alert that the website is down. There is no deﬁned process for when this happens. You
are forced to take an ad hoc approach to diagnose and resolve the issue. Developing this process as
you go extends the time to recovery.
Beneﬁts of establishing this best practice: By alerting only when action is required, you prevent low
value alerts from concealing high value alerts. By having a process for every actionable alert, you create
a consistent and prompt response to events in your environment.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Process per alert: Any event for which you raise an alert should have a well-deﬁned response (runbook
or playbook) with a speciﬁcally identiﬁed owner (for example, individual, team, or role) accountable
for successful completion. Performance of the response may be automated or conducted by another
team but the owner is accountable for ensuring the process delivers the expected outcomes. By having
these processes, you ensure eﬀective and prompt responses to operations events and you can prevent
actionable events from being obscured by less valuable notiﬁcations. For example, automatic scaling
might be applied to scale a web front end, but the operations team might be accountable to ensure
that the automatic scaling rules and limits are appropriate for workload needs.
134

AWS Well-Architected Framework
Operate
Resources
Related documents:
• Amazon CloudWatch Features
• What is Amazon CloudWatch Events?
Related videos:
• Build a Monitoring Plan
OPS10-BP03 Prioritize operational events based on business impact
Ensure that when multiple events require intervention, those that are most signiﬁcant to the business
are addressed ﬁrst. Impacts can include loss of life or injury, ﬁnancial loss, or damage to reputation or
trust.
Common anti-patterns:
• You receive a support request to add a printer conﬁguration for a user. While working on the issue,
you receive a support request stating that your retail site is down. After completing the printer
conﬁguration for your user, you start work on the website issue.
• You get notiﬁed that both your retail website and your payroll system are down. You don't know which
one should get priority.
Beneﬁts of establishing this best practice: Prioritizing responses to the incidents with the greatest
impact on the business notiﬁes your management of that impact.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Prioritize operational events based on business impact: Ensure that when multiple events require
intervention, those that are most signiﬁcant to the business are addressed ﬁrst. Impacts can include
loss of life or injury, ﬁnancial loss, regulatory violations, or damage to reputation or trust.
OPS10-BP04 Deﬁne escalation paths
Deﬁne escalation paths in your runbooks and playbooks, including what initiates escalation, and
procedures for escalation. Speciﬁcally identify owners for each action to ensure eﬀective and prompt
responses to operations events.
Identify when a human decision is required before an action is taken. Work with decision makers to have
that decision made in advance, and the action preapproved, so that MTTR is not extended waiting for a
response.
Common anti-patterns:
• Your retail site is down. You don't understand the runbook for recovering the site. You start calling
colleagues hoping that someone will be able to help you.
• You receive a support case for an unreachable application. You don't have permissions to administer
the system. You don't know who does. You attempt to contact the system owner that opened the case
and there is no response. You have no contacts for the system and your colleagues are not familiar
with it.
135

AWS Well-Architected Framework
Operate
Beneﬁts of establishing this best practice: By deﬁning escalations, what initiates the escalation,
and procedures for escalation you provide the systematic addition of resources to an incident at an
appropriate rate for the impact.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Deﬁne escalation paths: Deﬁne escalation paths in your runbooks and playbooks, including what
starts escalation, and procedures for escalation. For example, escalation of an issue from support
engineers to senior support engineers when runbooks cannot resolve the issue, or when a predeﬁned
period of time has elapsed. Another example of an appropriate escalation path is from senior support
engineers to the development team for a workload when the playbooks are unable to identify a path
to remediation, or when a predeﬁned period of time has elapsed. Speciﬁcally identify owners for each
action to ensure eﬀective and prompt responses to operations events. Escalations can include third
parties. For example, a network connectivity provider or a software vendor. Escalations can include
identiﬁed authorized decision makers for impacted systems.
OPS10-BP05 Deﬁne a customer communication plan for outages
Deﬁne and test a communication plan for system outages that you can rely on to keep your customers
and stakeholders informed during outages. Communicate directly with your users both when the services
they use are impacted and when services return to normal.
Desired outcome:
• You have a communication plan for situations ranging from scheduled maintenance to large
unexpected failures, including invocation of disaster recovery plans.
• In your communications, you provide clear and transparent information about systems issues to help
customers avoid second guessing the performance of their systems.
• You use custom error messages and status pages to reduce the spike in help desk requests and keep
users informed.
• The communication plan is regularly tested to verify that it will perform as intended when a real
outage occurs.
Common anti-patterns:
• A workload outage occurs but you have no communication plan. Users overwhelm your trouble ticket
system with requests because they have no information on the outage.
• You send an email notiﬁcation to your users during an outage. It doesn’t contain a timeline for
restoration of service so users cannot plan around the outage.
• There is a communication plan for outages but it has never been tested. An outage occurs and the
communication plan fails because a critical step was missed that could have been caught in testing.
• During an outage, you send a notiﬁcation to users with too many technical details and information
under your AWS NDA.
Beneﬁts of establishing this best practice:
• Maintaining communication during outages ensures that customers are provided with visibility of
progress on issues and estimated time to resolution.
• Developing a well-deﬁned communications plan veriﬁes that your customers and end users are well
informed so they can take required additional steps to mitigate the impact of outages.
• With proper communications and increased awareness of planned and unplanned outages, you can
improve customer satisfaction, limit unintended reactions, and drive customer retention.
136

AWS Well-Architected Framework
Operate
• Timely and transparent system outage communication builds conﬁdence and establishes trust needed
to maintain relationships between you and your customers.
• A proven communication strategy during an outage or crisis reduces speculation and gossip that could
hinder your ability to recover.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Communication plans that keep your customers informed during outages are holistic and cover multiple
interfaces including customer facing error pages, custom API error messages, system status banners,
and health status pages. If your system includes registered users, you can communicate over messaging
channels such as email, SMS or push notiﬁcations to send personalized message content to your
customers.
Customer communication tools
As a ﬁrst line of defense, web and mobile applications should provide friendly and informative error
messages during an outage as well as have the ability to redirect traﬃc to a status page. Amazon
CloudFront is a fully managed content delivery network (CDN) that includes capabilities to deﬁne
and serve custom error content. Custom error pages in CloudFront are a good ﬁrst layer of customer
messaging for component level outages. CloudFront can also simplify managing and activating a status
page to intercept all requests during planned or unplanned outages.
Custom API error messages can help detect and reduce impact when outages are isolated to discrete
services. Amazon API Gateway allows you to conﬁgure custom responses for your REST APIs. This allows
you to provide clear and meaningful messaging to API consumers when API Gateway is not able to reach
backend services. Custom messages can also be used to support outage banner content and notiﬁcations
when a particular system feature is degraded due to service tier outages.
Direct messaging is the most personalized type of customer messaging. Amazon Pinpoint is a managed
service for scalable multichannel communications. Amazon Pinpoint allows you to build campaigns
that can broadcast messages widely across your impacted customer base over SMS, email, voice, push
notiﬁcations, or custom channels you deﬁne. When you manage messaging with Amazon Pinpoint,
message campaigns are well deﬁned, testable, and can be intelligently applied to targeted customer
segments. Once established, campaigns can be scheduled or started by events and they can easily be
tested.
Customer example
When the workload is impaired, AnyCompany Retail sends out an email notiﬁcation to their users. The
email describes what business functionality is impaired and provides a realistic estimate of when service
will be restored. In addition, they have a status page that shows real-time information about the health
of their workload. The communication plan is tested in a development environment twice per year to
validate that it is eﬀective.
Implementation steps
1. Determine the communication channels for your messaging strategy. Consider the architectural
aspects of your application and determine the best strategy for delivering feedback to your
customers. This could include one or more of the guidance strategies outlined including error and
status pages, custom API error responses, or direct messaging.
2. Design status pages for your application. If you’ve determined that status or custom error pages are
suitable for your customers, you’ll need to design your content and messaging for those pages. Error
pages explain to users why an application is not available, when it may become available again, and
what they can do in the meantime. If your application uses Amazon CloudFront you can serve custom
error responses or use Lambda at Edge to translate errors and rewrite page content. CloudFront also
makes it possible to swap destinations from your application content to a static Amazon S3 content
origin containing your maintenance or outage status page .
137

AWS Well-Architected Framework
Operate
3. Design the correct set of API error statuses for your service. Error messages produced by API Gateway
when it can’t reach backend services, as well as service tier exceptions, may not contain friendly
messages suitable for display to end users. Without having to make code changes to your backend
services, you can conﬁgure API Gateway custom error responses to map HTTP response codes to
curated API error messages.
4. Design messaging from a business perspective so that it is relevant to end users for your system and
does not contain technical details. Consider your audience and align your messaging. For example, you
may steer internal users towards a workaround or manual process that leverages alternate systems.
External users may be asked to wait until the system is restored, or subscribe to updates to receive a
notiﬁcation once the system is restored. Deﬁne approved messaging for multiple scenarios including
unexpected outages, planned maintenance, and partial system failures where a particular feature may
be degraded or unavailable.
5. Templatize and automate your customer messaging. Once you have established your message
content, you can use Amazon Pinpoint or other tools to automate your messaging campaign. With
Amazon Pinpoint you can create customer target segments for speciﬁc aﬀected users and transform
messages into templates. Review the Amazon Pinpoint tutorial to get an understanding of how-to
setup a messaging campaign.
6. Avoiding tightly coupling messaging capabilities to your customer facing system. Your messaging
strategy should not have hard dependencies on system data stores or services to verify that you
can successfully send messages when you experience outages. Consider building the ability to send
messages from more than one Availability Zone or Region for messaging availability. If you are using
AWS services to send messages, leverage data plane operations over control plane operation to invoke
your messaging.
Level of eﬀort for the implementation plan: High. Developing a communication plan, and the
mechanisms to send it, can require a signiﬁcant eﬀort.
Resources
Related best practices:
• OPS07-BP03 Use runbooks to perform procedures (p. 105) - Your communication plan should have a
runbook associated with it so that your personnel know how to respond.
• OPS11-BP02 Perform post-incident analysis (p. 142) - After an outage, conduct post-incident
analysis to identify mechanisms to prevent another outage.
Related documents:
• Error Handling Patterns in Amazon API Gateway and AWS Lambda
• Amazon API Gateway responses
Related examples:
• AWS Health Dashboard
• Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region
Related services:
• AWS Support
• AWS Customer Agreement
• Amazon CloudFront
• Amazon API Gateway
• Amazon Pinpoint
138

AWS Well-Architected Framework
Operate
• Amazon S3
OPS10-BP06 Communicate status through dashboards
Provide dashboards tailored to their target audiences (for example, internal technical teams, leadership,
and customers) to communicate the current operating status of the business and provide metrics of
interest.
You can create dashboards using Amazon CloudWatch Dashboards on customizable home pages in the
CloudWatch console. Using business intelligence services such as Amazon QuickSight you can create
and publish interactive dashboards of your workload and operational health (for example, order rates,
connected users, and transaction times). Create Dashboards that present system and business-level views
of your metrics.
Common anti-patterns:
• Upon request, you run a report on the current utilization of your application for management.
• During an incident, you are contacted every twenty minutes by a concerned system owner wanting to
know if it is ﬁxed yet.
Beneﬁts of establishing this best practice: By creating dashboards, you create self-service access to
information helping your customers to informed themselves and determine if they need to take action.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Communicate status through dashboards: Provide dashboards tailored to their target audiences (for
example, internal technical teams, leadership, and customers) to communicate the current operating
status of the business and provide metrics of interest. Providing a self-service option for status
information reduces the disruption of ﬁelding requests for status by the operations team. Examples
include Amazon CloudWatch dashboards, and AWS Health Dashboard.
• CloudWatch dashboards create and use customized metrics views
Resources
Related documents:
• Amazon QuickSight
• CloudWatch dashboards create and use customized metrics views
OPS10-BP07 Automate responses to events
Automate responses to events to reduce errors caused by manual processes, and to ensure prompt and
consistent responses.
There are multiple ways to automate runbook and playbook actions on AWS. To respond to an event
from a state change in your AWS resources, or from your own custom events, you should create
CloudWatch Events rules to initiate responses through CloudWatch targets (for example, Lambda
functions, Amazon Simple Notiﬁcation Service (Amazon SNS) topics, Amazon ECS tasks, and AWS
Systems Manager Automation).
To respond to a metric that crosses a threshold for a resource (for example, wait time), you should create
CloudWatch alarms to perform one or more actions using Amazon EC2 actions, Auto Scaling actions,
or to send a notiﬁcation to an Amazon SNS topic. If you need to perform custom actions in response
139

AWS Well-Architected Framework
Evolve
to an alarm, invoke Lambda through an Amazon SNS notiﬁcation. Use Amazon SNS to publish event
notiﬁcations and escalation messages to keep people informed.
AWS also supports third-party systems through the AWS service APIs and SDKs. There are a number of
monitoring tools provided by AWS Partners and third parties that allow for monitoring, notiﬁcations, and
responses. Some of these tools include New Relic, Splunk, Loggly, SumoLogic, and Datadog.
You should keep critical manual procedures available for use when automated procedures fail
Common anti-patterns:
• A developer checks in their code. This event could have been used to start a build and then perform
testing but instead nothing happens.
• Your application logs a speciﬁc error before it stops working. The procedure to restart the application
is well understood and could be scripted. You could use the log event to invoke a script and restart the
application. Instead, when the error happens at 3am Sunday morning, you are woken up as the on-call
resource responsible to ﬁx the system.
Beneﬁts of establishing this best practice: By using automated responses to events, you reduce the
time to respond and limit the introduction of errors from manual activities.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Automate responses to events: Automate responses to events to reduce errors caused by manual
processes, and to ensure prompt and consistent responses.
• What is Amazon CloudWatch Events?
• Creating a CloudWatch Events rule that starts on an event
• Creating a CloudWatch Events rule that starts on an AWS API call using AWS CloudTrail
• CloudWatch Events event examples from supported services
Resources
Related documents:
• Amazon CloudWatch Features
• CloudWatch Events event examples from supported services
• Creating a CloudWatch Events rule that starts on an AWS API call using AWS CloudTrail
• Creating a CloudWatch Events rule that starts on an event
• What is Amazon CloudWatch Events?
Related videos:
• Build a Monitoring Plan
Related examples:
Evolve
Question
• OPS 11. How do you evolve operations? (p. 141)
140

AWS Well-Architected Framework
Evolve
OPS 11. How do you evolve operations?
Dedicate time and resources for nearly continuous incremental improvement to evolve the eﬀectiveness
and eﬃciency of your operations.
Best practices
• OPS11-BP01 Have a process for continuous improvement (p. 141)
• OPS11-BP02 Perform post-incident analysis (p. 142)
• OPS11-BP03 Implement feedback loops (p. 143)
• OPS11-BP04 Perform knowledge management (p. 145)
• OPS11-BP05 Deﬁne drivers for improvement (p. 147)
• OPS11-BP06 Validate insights (p. 148)
• OPS11-BP07 Perform operations metrics reviews (p. 148)
• OPS11-BP08 Document and share lessons learned (p. 149)
• OPS11-BP09 Allocate time to make improvements (p. 151)
OPS11-BP01 Have a process for continuous improvement
Evaluate your workload against internal and external architecture best practices. Conduct workload
reviews at least once per year. Prioritize improvement opportunities into your software development
cadence.
Desired outcome:
• You analyze your workload against architecture best practices at least yearly.
• Improvement opportunities are given equal priority in your software development process.
Common anti-patterns:
• You have not conducted an architecture review on your workload since it was deployed several years
ago.
• Improvement opportunities are given a lower priority and stay in the backlog.
• There is no standard for implementing modiﬁcations to best practices for the organization.
Beneﬁts of establishing this best practice:
• Your workload is kept up to date on architecture best practices.
• Evolving your workload is done in a deliberate manner.
• You can leverage organization best practices to improve all workloads.
Level of risk exposed if this best practice is not established: High
Implementation guidance
On at least a yearly basis, you conduct an architectural review of your workload. Using internal and
external best practices, evaluate your workload and identify improvement opportunities. Prioritize
improvement opportunities into your software development cadence.
Customer example
All workloads at AnyCompany Retail go through a yearly architecture review process. They developed
their own checklist of best practices that apply to all workloads. Using the AWS Well-Architected Tool’s
141

AWS Well-Architected Framework
Evolve
Custom Lens feature, they conduct reviews using the tool and their custom lens of best practices.
Improvement opportunities generated from the reviews are given priority in their software sprints.
Implementation steps
1. Conduct periodic architecture reviews of your production workload at least yearly. Use a documented
architectural standard that includes AWS-speciﬁc best practices.
a. We recommend you use your own internally deﬁned standards it for these reviews. If you do not
have an internal standard, we recommend you use the AWS Well-Architected Framework.
b. You can use the AWS Well-Architected Tool to create a Custom Lens of your internal best practices
and conduct your architecture review.
c. Customers can contact their AWS Solutions Architect to conduct a guided Well-Architected
Framework Review of their workload.
2. Prioritize improvement opportunities identiﬁed during the review into your software development
process.
Level of eﬀort for the implementation plan: Low. You can use the AWS Well-Architected Framework to
conduct your yearly architecture review.
Resources
Related best practices:
• OPS11-BP02 Perform post-incident analysis (p. 142) - Post-incident analysis is another generator for
improvement items. Feed lessons learned into your internal list of architecture best practices.
• OPS11-BP08 Document and share lessons learned (p. 149) - As you develop your own architecture
best practices, share those across your organization.
Related documents:
• AWS Well-Architected Tool - Custom lenses
• AWS Well-Architected Whitepaper - The review process
• Customize Well-Architected Reviews using Custom Lenses and the AWS Well-Architected Tool
• Implementing the AWS Well-Architected Custom Lens lifecycle in your organization
Related videos:
• Well-Architected Labs - Level 100: Custom Lenses on AWS Well-Architected Tool
Related examples:
• The AWS Well-Architected Tool
OPS11-BP02 Perform post-incident analysis
Review customer-impacting events, and identify the contributing factors and preventative actions.
Use this information to develop mitigations to limit or prevent recurrence. Develop procedures for
prompt and eﬀective responses. Communicate contributing factors and corrective actions as appropriate,
tailored to target audiences.
Common anti-patterns:
• You administer an application server. Approximately every 23 hours and 55 minutes all your active
sessions are terminated. You have tried to identify what is going wrong on your application server. You
142

AWS Well-Architected Framework
Evolve
suspect it could instead be a network issue but are unable to get cooperation from the network team
as they are too busy to support you. You lack a predeﬁned process to follow to get support and collect
the information necessary to determine what is going on.
• You have had data loss within your workload. This is the ﬁrst time it has happened and the cause is not
obvious. You decide it is not important because you can recreate the data. Data loss starts occurring
with greater frequency impacting your customers. This also places addition operational burden on you
as you restore the missing data.
Beneﬁts of establishing this best practice: Having a predeﬁned processes to determine the
components, conditions, actions, and events that contributed to an incident helps you to identify
opportunities for improvement.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Use a process to determine contributing factors: Review all customer impacting incidents. Have a
process to identify and document the contributing factors of an incident so that you can develop
mitigations to limit or prevent recurrence and you can develop procedures for prompt and eﬀective
responses. Communicate root cause as appropriate, tailored to target audiences.
OPS11-BP03 Implement feedback loops
Feedback loops provide actionable insights that drive decision making. Build feedback loops into your
procedures and workloads. This helps you identify issues and areas that need improvement. They also
validate investments made in improvements. These feedback loops are the foundation for continuously
improving your workload.
Feedback loops fall into two categories: immediate feedback and retrospective analysis. Immediate
feedback is gathered through review of the performance and outcomes from operations activities. This
feedback comes from team members, customers, or the automated output of the activity. Immediate
feedback is received from things like A/B testing and shipping new features, and it is essential to failing
fast.
Retrospective analysis is performed regularly to capture feedback from the review of operational
outcomes and metrics over time. These retrospectives happen at the end of a sprint, on a cadence, or
after major releases or events. This type of feedback loop validates investments in operations or your
workload. It helps you measure success and validates your strategy.
Desired outcome: You use immediate feedback and retrospective analysis to drive improvements. There
is a mechanism to capture user and team member feedback. Retrospective analysis is used to identify
trends that drive improvements.
Common anti-patterns:
• You launch a new feature but have no way of receiving customer feedback on it.
• After investing in operations improvements, you don’t conduct a retrospective to validate them.
• You collect customer feedback but don’t regularly review it.
• Feedback loops lead to proposed action items but they aren’t included in the software development
process.
• Customers don’t receive feedback on improvements they’ve proposed.
Beneﬁts of establishing this best practice:
• You can work backwards from the customer to drive new features.
• Your organization culture can react to changes faster.
143

AWS Well-Architected Framework
Evolve
• Trends are used to identify improvement opportunities.
• Retrospectives validate investments made to your workload and operations.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Implementing this best practice means that you use both immediate feedback and retrospective analysis.
These feedback loops drive improvements. There are many mechanisms for immediate feedback,
including surveys, customer polls, or feedback forms. Your organization also uses retrospectives to
identify improvement opportunities and validate initiatives.
Customer example
AnyCompany Retail created a web form where customers can give feedback or report issues. During the
weekly scrum, user feedback is evaluated by the software development team. Feedback is regularly used
to steer the evolution of their platform. They conduct a retrospective at the end of each sprint to identify
items they want to improve.
Implementation steps
1. Immediate feedback
• You need a mechanism to receive feedback from customers and team members. Your operations
activities can also be conﬁgured to deliver automated feedback.
• Your organization needs a process to review this feedback, determine what to improve, and
schedule the improvement.
• Feedback must be added into your software development process.
• As you make improvements, follow up with the feedback submitter.
• You can use AWS Systems Manager OpsCenter to create and track these improvements as
OpsItems.
2. Retrospective analysis
• Conduct retrospectives at the end of a development cycle, on a set cadence, or after a major release.
• Gather stakeholders involved in the workload for a retrospective meeting.
• Create three columns on a whiteboard or spreadsheet: Stop, Start, and Keep.
• Stop is for anything that you want your team to stop doing.
• Start is for ideas that you want to start doing.
• Keep is for items that you want to keep doing.
• Go around the room and gather feedback from the stakeholders.
• Prioritize the feedback. Assign actions and stakeholders to any Start or Keep items.
• Add the actions to your software development process and communicate status updates to
stakeholders as you make the improvements.
Level of eﬀort for the implementation plan: Medium. To implement this best practice, you need a
way to take in immediate feedback and analyze it. Also, you need to establish a retrospective analysis
process.
Resources
Related best practices:
• OPS01-BP01 Evaluate external customer needs (p. 51): Feedback loops are a mechanism to gather
external customer needs.
• OPS01-BP02 Evaluate internal customer needs (p. 51): Internal stakeholders can use feedback loops
to communicate needs and requirements.
144

AWS Well-Architected Framework
Evolve
• OPS11-BP02 Perform post-incident analysis (p. 142): Post-incident analyses are an important form
of retrospective analysis conducted after incidents.
• OPS11-BP07 Perform operations metrics reviews (p. 148): Operations metrics reviews identify trends
and areas for improvement.
Related documents:
• 7 Pitfalls to Avoid When Building a CCOE
• Atlassian Team Playbook - Retrospectives
• Email Deﬁnitions: Feedback Loops
• Establishing Feedback Loops Based on the AWS Well-Architected Framework Review
• IBM Garage Methodology - Hold a retrospective
• Investopedia – The PDCS Cycle
• Maximizing Developer Eﬀectiveness by Tim Cochran
• Operations Readiness Reviews (ORR) Whitepaper - Iteration
• ITIL CSI - Continual Service Improvement
• When Toyota met e-commerce: Lean at Amazon
Related videos:
• Building Eﬀective Customer Feedback Loops
Related examples:
• Astuto - Open source customer feedback tool
• AWS Solutions - QnABot on AWS
• Fider - A platform to organize customer feedback
Related services:
• AWS Systems Manager OpsCenter
OPS11-BP04 Perform knowledge management
Knowledge management helps team members ﬁnd the information to perform their job. In learning
organizations, information is freely shared which empowers individuals. The information can be
discovered or searched. Information is accurate and up to date. Mechanisms exist to create new
information, update existing information, and archive outdated information. The most common example
of a knowledge management platform is a content management system like a wiki.
Desired outcome:
• Team members have access to timely, accurate information.
• Information is searchable.
• Mechanisms exist to add, update, and archive information.
Common anti-patterns:
• There is no centralized knowledge storage. Team members manage their own notes on their local
machines.
145

AWS Well-Architected Framework
Evolve
• You have a self-hosted wiki but no mechanisms to manage information, resulting in outdated
information.
• Someone identiﬁes missing information but there’s no process to request adding it the team wiki. They
add it themselves but they miss a key step, leading to an outage.
Beneﬁts of establishing this best practice:
• Team members are empowered because information is shared freely.
• New team members are onboarded faster because documentation is up to date and searchable.
• Information is timely, accurate, and actionable.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Knowledge management is an important facet of learning organizations. To begin, you need a central
repository to store your knowledge (as a common example, a self-hosted wiki). You must develop
processes for adding, updating, and archiving knowledge. Develop standards for what should be
documented and let everyone contribute.
Customer example
AnyCompany Retail hosts an internal Wiki where all knowledge is stored. Team members are encouraged
to add to the knowledge base as they go about their daily duties. On a quarterly basis, a cross-functional
team evaluates which pages are least updated and determines if they should be archived or updated.
Implementation steps
1. Start with identifying the content management system where knowledge will be stored. Get
agreement from stakeholders across your organization.
a. If you don’t have an existing content management system, consider running a self-hosted wiki or
using a version control repository as a starting point.
2. Develop runbooks for adding, updating, and archiving information. Educate your team on these
processes.
3. Identify what knowledge should be stored in the content management system. Start with daily
activities (runbooks and playbooks) that team members perform. Work with stakeholders to prioritize
what knowledge is added.
4. On a periodic basis, work with stakeholders to identify out-of-date information and archive it or bring
it up to date.
Level of eﬀort for the implementation plan: Medium. If you don’t have an existing content
management system, you can set up a self-hosted wiki or a version-controlled document repository.
Resources
Related best practices:
• OPS11-BP08 Document and share lessons learned (p. 149) - Knowledge management facilitates
information sharing about lessons learned.
Related documents:
• Atlassian - Knowledge Management
Related examples:
146

AWS Well-Architected Framework
Evolve
• DokuWiki
• Gollum
• MediaWiki
• Wiki.js
OPS11-BP05 Deﬁne drivers for improvement
Identify drivers for improvement to help you evaluate and prioritize opportunities.
On AWS, you can aggregate the logs of all your operations activities, workloads, and infrastructure to
create a detailed activity history. You can then use AWS tools to analyze your operations and workload
health over time (for example, identify trends, correlate events and activities to outcomes, and compare
and contrast between environments and across systems) to reveal opportunities for improvement based
on your drivers.
You should use CloudTrail to track API activity (through the AWS Management Console, CLI, SDKs, and
APIs) to know what is happening across your accounts. Track your AWS developer Tools deployment
activities with CloudTrail and CloudWatch. This will add a detailed activity history of your deployments
and their outcomes to your CloudWatch Logs log data.
Export your log data to Amazon S3 for long-term storage. Using AWS Glue, you discover and prepare
your log data in Amazon S3 for analytics. Use Amazon Athena, through its native integration with AWS
Glue, to analyze your log data. Use a business intelligence tool like Amazon QuickSight to visualize,
explore, and analyze your data
Common anti-patterns:
• You have a script that works but is not elegant. You invest time in rewriting it. It is now a work of art.
• Your start-up is trying to get another set of funding from a venture capitalist. They want you to
demonstrate compliance with PCI DSS. You want to make them happy so you document your
compliance and miss a delivery date for a customer, losing that customer. It wasn't a wrong thing to do
but now you wonder if it was the right thing to do.
Beneﬁts of establishing this best practice: By determining the criteria you want to use for
improvement, you can minimize the impact of event based motivations or emotional investment.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Understand drivers for improvement: You should only make changes to a system when a desired
outcome is supported.
• Desired capabilities: Evaluate desired features and capabilities when evaluating opportunities for
improvement.
• What's New with AWS
• Unacceptable issues: Evaluate unacceptable issues, bugs, and vulnerabilities when evaluating
opportunities for improvement.
• AWS Latest Security Bulletins
• AWS Trusted Advisor
• Compliance requirements: Evaluate updates and changes required to maintain compliance with
regulation, policy, or to remain under support from a third party, when reviewing opportunities for
improvement.
• AWS Compliance
• AWS Compliance Programs
• AWS Compliance Latest News
147

AWS Well-Architected Framework
Evolve
Resources
Related documents:
• Amazon Athena
• Amazon QuickSight
• AWS Compliance
• AWS Compliance Latest News
• AWS Compliance Programs
• AWS Glue
• AWS Latest Security Bulletins
• AWS Trusted Advisor
• Export your log data to Amazon S3
• What's New with AWS
OPS11-BP06 Validate insights
Review your analysis results and responses with cross-functional teams and business owners. Use these
reviews to establish common understanding, identify additional impacts, and determine courses of
action. Adjust responses as appropriate.
Common anti-patterns:
• You see that CPU utilization is at 95% on a system and make it a priority to ﬁnd a way to reduce load
on the system. You determine the best course of action is to scale up. The system is a transcoder and
the system is scaled to run at 95% CPU utilization all the time. The system owner could have explained
the situation to you had you contacted them. Your time has been wasted.
• A system owner maintains that their system is mission critical. The system was not placed in a high
security environment. To improve security, you implement the additional detective and preventative
controls that are required for mission critical systems. You notify the system owner that the work is
complete and that he will be charged for the additional resources. In the discussion following this
notiﬁcation, the system owner learns there is a formal deﬁnition for mission critical systems that this
system does not meet.
Beneﬁts of establishing this best practice: By validating insights with business owners and subject
matter experts, you can establish common understanding and more eﬀectively guide improvement.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Validate insights: Engage with business owners and subject matter experts to ensure there is common
understanding and agreement of the meaning of the data you have collected. Identify additional
concerns, potential impacts, and determine a courses of action.
OPS11-BP07 Perform operations metrics reviews
Regularly perform retrospective analysis of operations metrics with cross-team participants from
diﬀerent areas of the business. Use these reviews to identify opportunities for improvement, potential
courses of action, and to share lessons learned.
Look for opportunities to improve in all of your environments (for example, development, test, and
production).
148

AWS Well-Architected Framework
Evolve
Common anti-patterns:
• There was a signiﬁcant retail promotion that was interrupted by your maintenance window. The
business remains unaware that there is a standard maintenance window that could be delayed if there
are other business impacting events.
• You suﬀered an extended outage because of your use of a buggy library commonly used in your
organization. You have since migrated to a reliable library. The other teams in your organization do not
know that they are at risk. If you met regularly and reviewed this incident, they would be aware of the
risk.
• Performance of your transcoder has been falling oﬀ steadily and impacting the media team. It isn't
terrible yet. You will not have an opportunity to ﬁnd out until it is bad enough to cause an incident.
Were you to review your operations metrics with the media team, there would be an opportunity for
the change in metrics and their experience to be recognized and the issue addressed.
• You are not reviewing your satisfaction of customer SLAs. You are trending to not meet your customer
SLAs. There are ﬁnancial penalties related to not meeting your customer SLAs. If you meet regularly to
review the metrics for these SLAs, you would have the opportunity to recognize and address the issue.
Beneﬁts of establishing this best practice: By meeting regularly to review operations metrics, events,
and incidents, you maintain common understanding across teams, share lessons learned, and can
prioritize and target improvements.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Operations metrics reviews: Regularly perform retrospective analysis of operations metrics with
cross-team participants from diﬀerent areas of the business. Engage stakeholders, including the
business, development, and operations teams, to validate your ﬁndings from immediate feedback and
retrospective analysis, and to share lessons learned. Use their insights to identify opportunities for
improvement and potential courses of action.
• Amazon CloudWatch
• Using Amazon CloudWatch metrics
• Publish custom metrics
• Amazon CloudWatch metrics and dimensions reference
Resources
Related documents:
• Amazon CloudWatch
• Amazon CloudWatch metrics and dimensions reference
• Publish custom metrics
• Using Amazon CloudWatch metrics
OPS11-BP08 Document and share lessons learned
Document and share lessons learned from the operations activities so that you can use them internally
and across teams.
You should share what your teams learn to increase the beneﬁt across your organization. You will want
to share information and resources to prevent avoidable errors and ease development eﬀorts. This will
allow you to focus on delivering desired features.
149

AWS Well-Architected Framework
Evolve
Use AWS Identity and Access Management (IAM) to deﬁne permissions permitting controlled access to
the resources you wish to share within and across accounts. You should then use version-controlled AWS
CodeCommit repositories to share application libraries, scripted procedures, procedure documentation,
and other system documentation. Share your compute standards by sharing access to your AMIs and by
authorizing the use of your Lambda functions across accounts. You should also share your infrastructure
standards as AWS CloudFormation templates.
Through the AWS APIs and SDKs, you can integrate external and third-party tools and repositories (for
example, GitHub, BitBucket, and SourceForge). When sharing what you have learned and developed, be
careful to structure permissions to ensure the integrity of shared repositories.
Common anti-patterns:
• You suﬀered an extended outage because of your use of a buggy library commonly used in your
organization. You have since migrated to a reliable library. The other teams in your organization do not
know they are at risk. Were you to document and share your experience with this library, they would
be aware of the risk.
• You have identiﬁed an edge case in an internally shared microservice that causes sessions to drop. You
have updated your calls to the service to avoid this edge case. The other teams in your organization do
not know that they are at risk. Were you to document and share your experience with this library, they
would be aware of the risk.
• You have found a way to signiﬁcantly reduce the CPU utilization requirements for one of your
microservices. You do not know if any other teams could take advantage of this technique. Were you to
document and share your experience with this library, they would have the opportunity to do so.
Beneﬁts of establishing this best practice: Share lessons learned to support improvement and to
maximize the beneﬁts of experience.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Document and share lessons learned: Have procedures to document the lessons learned from the
running of operations activities and retrospective analysis so that they can be used by other teams.
• Share learnings: Have procedures to share lessons learned and associated artifacts across teams. For
example, share updated procedures, guidance, governance, and best practices through an accessible
wiki. Share scripts, code, and libraries through a common repository.
• Delegating access to your AWS environment
• Share an AWS CodeCommit repository
• Easy authorization of AWS Lambda functions
• Sharing an AMI with speciﬁc AWS Accounts
• Speed template sharing with an AWS CloudFormation designer URL
• Using AWS Lambda with Amazon SNS
Resources
Related documents:
• Easy authorization of AWS Lambda functions
• Share an AWS CodeCommit repository
• Sharing an AMI with speciﬁc AWS Accounts
• Speed template sharing with an AWS CloudFormation designer URL
• Using AWS Lambda with Amazon SNS
150

AWS Well-Architected Framework
Security
Related videos:
• Delegating access to your AWS environment
OPS11-BP09 Allocate time to make improvements
Dedicate time and resources within your processes to make continuous incremental improvements
possible.
On AWS, you can create temporary duplicates of environments, lowering the risk, eﬀort, and cost of
experimentation and testing. These duplicated environments can be used to test the conclusions from
your analysis, experiment, and develop and test planned improvements.
Common anti-patterns:
• There is a known performance issue in your application server. It is added to the backlog behind every
planned feature implementation. If the rate of planned features being added remains constant, the
performance issue will never be addressed.
• To support continual improvement you approve administrators and developers using all their extra
time to select and implement improvements. No improvements are ever completed.
Beneﬁts of establishing this best practice: By dedicating time and resources within your processes you
make continuous incremental improvements possible.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Allocate time to make improvements: Dedicate time and resources within your processes to make
continuous incremental improvements possible. Implement changes to improve and evaluate the
results to determine success. If the results do not satisfy the goals, and the improvement is still a
priority, pursue alternative courses of action.
Security
The Security pillar encompasses the ability to protect data, systems, and assets to take advantage of
cloud technologies to improve your security. You can ﬁnd prescriptive guidance on implementation in the
Security Pillar whitepaper.
Best practice areas
• Security foundations (p. 151)
• Identity and access management (p. 163)
• Detection (p. 191)
• Infrastructure protection (p. 197)
• Data protection (p. 209)
• Incident response (p. 223)
• Application security (p. 233)
Security foundations
Question
151

AWS Well-Architected Framework
Security foundations
• SEC 1. How do you securely operate your workload? (p. 152)
SEC 1. How do you securely operate your workload?
To operate your workload securely, you must apply overarching best practices to every area of
security. Take requirements and processes that you have deﬁned in operational excellence at an
organizational and workload level, and apply them to all areas. Staying up to date with AWS and
industry recommendations and threat intelligence helps you evolve your threat model and control
objectives. Automating security processes, testing, and validation permit you to scale your security
operations.
Best practices
• SEC01-BP01 Separate workloads using accounts (p. 152)
• SEC01-BP02 Secure account root user and properties (p. 154)
• SEC01-BP03 Identify and validate control objectives (p. 157)
• SEC01-BP04 Keep up-to-date with security threats (p. 158)
• SEC01-BP05 Keep up-to-date with security recommendations (p. 158)
• SEC01-BP06 Automate testing and validation of security controls in pipelines (p. 159)
• SEC01-BP07 Identify threats and prioritize mitigations using a threat model (p. 160)
• SEC01-BP08 Evaluate and implement new security services and features regularly (p. 162)
SEC01-BP01 Separate workloads using accounts
Establish common guardrails and isolation between environments (such as production, development,
and test) and workloads through a multi-account strategy. Account-level separation is strongly
recommended, as it provides a strong isolation boundary for security, billing, and access.
Desired outcome: An account structure that isolates cloud operations, unrelated workloads, and
environments into separate accounts, increasing security across the cloud infrastructure.
Common anti-patterns:
• Placing multiple unrelated workloads with diﬀerent data sensitivity levels into the same account.
• Poorly deﬁned organizational unit (OU) structure.
Beneﬁts of establishing this best practice:
• Decreased scope of impact if a workload is inadvertently accessed.
• Central governance of access to AWS services, resources, and Regions.
• Maintain security of the cloud infrastructure with policies and centralized administration of security
services.
• Automated account creation and maintenance process.
• Centralized auditing of your infrastructure for compliance and regulatory requirements.
Level of risk exposed if this best practice is not established: High
Implementation guidance
AWS accounts provide a security isolation boundary between workloads or resources that operate at
diﬀerent sensitivity levels. AWS provides tools to manage your cloud workloads at scale through a
multi-account strategy to leverage this isolation boundary. For guidance on the concepts, patterns,
152

AWS Well-Architected Framework
Security foundations
and implementation of a multi-account strategy on AWS, see Organizing Your AWS Environment Using
Multiple Accounts.
When you have multiple AWS accounts under central management, your accounts should be organized
into a hierarchy deﬁned by layers of organizational units (OUs). Security controls can then be organized
and applied to the OUs and member accounts, establishing consistent preventative controls on member
accounts in the organization. The security controls are inherited, allowing you to ﬁlter permissions
available to member accounts located at lower levels of an OU hierarchy. A good design takes advantage
of this inheritance to reduce the number and complexity of security policies required to achieve the
desired security controls for each member account.
AWS Organizations and AWS Control Tower are two services that you can use to implement and manage
this multi-account structure in your AWS environment. AWS Organizations allows you to organize
accounts into a hierarchy deﬁned by one or more layers of OUs, with each OU containing a number
of member accounts. Service control policies (SCPs) allow the organization administrator to establish
granular preventative controls on member accounts, and AWS Conﬁg can be used to establish proactive
and detective controls on member accounts. Many AWS services integrate with AWS Organizations
to provide delegated administrative controls and performing service-speciﬁc tasks across all member
accounts in the organization.
Layered on top of AWS Organizations, AWS Control Tower provides a one-click best practices setup for
a multi-account AWS environment with a landing zone. The landing zone is the entry point to the multi-
account environment established by Control Tower. Control Tower provides several beneﬁts over AWS
Organizations. Three beneﬁts that provide improved account governance are:
• Integrated mandatory security controls that are automatically applied to accounts admitted into the
organization.
• Optional controls that can be turned on or oﬀ for a given set of OUs.
• AWS Control Tower Account Factory provides automated deployment of accounts containing pre-
approved baselines and conﬁguration options inside your organization.
Implementation steps
1. Design an organizational unit structure: A properly designed organizational unit structure reduces
the management burden required to create and maintain service control policies and other security
controls. Your organizational unit structure should be aligned with your business needs, data
sensitivity, and workload structure.
2. Create a landing zone for your multi-account environment: A landing zone provides a consistent
security and infrastructure foundation from which your organization can quickly develop, launch, and
deploy workloads. You can use a custom-built landing zone or AWS Control Tower to orchestrate your
environment.
3. Establish guardrails: Implement consistent security guardrails for your environment through your
landing zone. AWS Control Tower provides a list of mandatory and optional controls that can be
deployed. Mandatory controls are automatically deployed when implementing Control Tower. Review
the list of highly recommended and optional controls, and implement controls that are appropriate to
your needs.
4. Restrict access to newly added Regions: For new AWS Regions, IAM resources such as users and
roles are only propagated to the Regions that you specify. This action can be performed through the
console when using Control Tower, or by adjusting IAM permission policies in AWS Organizations.
5. Consider AWS CloudFormation StackSets: StackSets help you deploy resources including IAM
policies, roles, and groups into diﬀerent AWS accounts and Regions from an approved template.
Resources
Related best practices:
153

AWS Well-Architected Framework
Security foundations
• SEC02-BP04 Rely on a centralized identity provider (p. 171)
Related documents:
• AWS Control Tower
• AWS Security Audit Guidelines
• IAM Best Practices
• Use CloudFormation StackSets to provision resources across multiple AWS accounts and regions
• Organizations FAQ
• AWS Organizations terminology and concepts
• Best Practices for Service Control Policies in an AWS Organizations Multi-Account Environment
• AWS Account Management Reference Guide
• Organizing Your AWS Environment Using Multiple Accounts
Related videos:
• Enable AWS adoption at scale with automation and governance
• Security Best Practices the Well-Architected Way
• Building and Governing Multiple Accounts using AWS Control Tower
• Enable Control Tower for Existing Organizations
Related workshops:
• Control Tower Immersion Day
SEC01-BP02 Secure account root user and properties
The root user is the most privileged user in an AWS account, with full administrative access to all
resources within the account, and in some cases cannot be constrained by security policies. Deactivating
programmatic access to the root user, establishing appropriate controls for the root user, and avoiding
routine use of the root user helps reduce the risk of inadvertent exposure of the root credentials and
subsequent compromise of the cloud environment.
Desired outcome: Securing the root user helps reduce the chance that accidental or intentional damage
can occur through the misuse of root user credentials. Establishing detective controls can also alert the
appropriate personnel when actions are taken using the root user.
Common anti-patterns:
• Using the root user for tasks other than the few that require root user credentials.
• Neglecting to test contingency plans on a regular basis to verify the functioning of critical
infrastructure, processes, and personnel during an emergency.
• Only considering the typical account login ﬂow and neglecting to consider or test alternate account
recovery methods.
• Not handling DNS, email servers, and telephone providers as part of the critical security perimeter, as
these are used in the account recovery ﬂow.
Beneﬁts of establishing this best practice: Securing access to the root user builds conﬁdence that
actions in your account are controlled and audited.
Level of risk exposed if this best practice is not established: High
154

AWS Well-Architected Framework
Security foundations
Implementation guidance
AWS oﬀers many tools to help secure your account. However, because some of these measures are not
turned on by default, you must take direct action to implement them. Consider these recommendations
as foundational steps to securing your AWS account. As you implement these steps it’s important that
you build a process to continuously assess and monitor the security controls.
When you ﬁrst create an AWS account, you begin with one identity that has complete access to all AWS
services and resources in the account. This identity is called the AWS account root user. You can sign in
as the root user using the email address and password that you used to create the account. Due to the
elevated access granted to the AWS root user, you must limit use of the AWS root user to perform tasks
that speciﬁcally require it. The root user login credentials must be closely guarded, and multi-factor
authentication (MFA) should always be used for the AWS account root user.
In addition to the normal authentication ﬂow to log into your root user using a username, password, and
multi-factor authentication (MFA) device, there are account recovery ﬂows to log into your AWS account
root user given access to the email address and phone number associated with your account. Therefore,
it is equally important to secure the root user email account where the recovery email is sent and the
phone number associated with the account. Also consider potential circular dependencies where the
email address associated with the root user is hosted on email servers or domain name service (DNS)
resources from the same AWS account.
When using AWS Organizations, there are multiple AWS accounts each of which have a root user. One
account is designated as the management account and several layers of member accounts can then be
added underneath the management account. Prioritize securing your management account’s root user,
then address your member account root users. The strategy for securing your management account’s
root user can diﬀer from your member account root users, and you can place preventative security
controls on your member account root users.
Implementation steps
The following implementation steps are recommended to establish controls for the root user. Where
applicable, recommendations are cross-referenced to CIS AWS Foundations benchmark version 1.4.0.
In addition to these steps, consult AWS best practice guidelines for securing your AWS account and
resources.
Preventative controls
1. Set up accurate contact information for the account.
a. This information is used for the lost password recovery ﬂow, lost MFA device account recovery ﬂow,
and for critical security-related communications with your team.
b. Use an email address hosted by your corporate domain, preferably a distribution list, as the root
user’s email address. Using a distribution list rather than an individual’s email account provides
additional redundancy and continuity for access to the root account over long periods of time.
c. The phone number listed on the contact information should be a dedicated, secure phone for this
purpose. The phone number should not be listed or shared with anyone.
2. Do not create access keys for the root user. If access keys exist, remove them (CIS 1.4).
a. Eliminate any long-lived programmatic credentials (access and secret keys) for the root user.
b. If root user access keys already exist, you should transition processes using those keys to use
temporary access keys from an AWS Identity and Access Management (IAM) role, then delete the
root user access keys.
3. Determine if you need to store credentials for the root user.
a. If you are using AWS Organizations to create new member accounts, the initial password for the
root user on new member accounts is set to a random value that is not exposed to you. Consider
using the password reset ﬂow from your AWS Organization management account to gain access to
the member account if needed.
155

AWS Well-Architected Framework
Security foundations
b. For standalone AWS accounts or the management AWS Organization account, consider creating and
securely storing credentials for the root user. Use MFA for the root user.
4. Use preventative controls for member account root users in AWS multi-account environments.
a. Consider using the Disallow Creation of Root Access Keys for the Root User preventative guard rail
for member accounts.
b. Consider using the Disallow Actions as a Root User preventative guard rail for member accounts.
5. If you need credentials for the root user:
a. Use a complex password.
b. Turn on multi-factor authentication (MFA) for the root user, especially for AWS Organizations
management (payer) accounts (CIS 1.5).
c. Consider hardware MFA devices for resiliency and security, as single use devices can reduce the
chances that the devices containing your MFA codes might be reused for other purposes. Verify that
hardware MFA devices powered by a battery are replaced regularly. (CIS 1.6)
• To conﬁgure MFA for the root user, follow the instructions for creating either a virtual
MFA or hardware MFA device.
d. Consider enrolling multiple MFA devices for backup. Up to 8 MFA devices are allowed per account.
• Note that enrolling more than one MFA device for the root user automatically turns oﬀ the ﬂow
for recovering your account if the MFA device is lost.
e. Store the password securely, and consider circular dependencies if storing the password
electronically. Don’t store the password in such a way that would require access to the same AWS
account to obtain it.
6. Optional: Consider establishing a periodic password rotation schedule for the root user.
• Credential management best practices depend on your regulatory and policy requirements. Root
users protected by MFA are not reliant on the password as a single factor of authentication.
• Changing the root user password on a periodic basis reduces the risk that an inadvertently exposed
password can be misused.
Detective controls
• Create alarms to detect use of the root credentials (CIS 1.7). Amazon GuardDuty can monitor and alert
on root user API credential usage through the RootCredentialUsage ﬁnding.
• Evaluate and implement the detective controls included in the AWS Well-Architected Security Pillar
conformance pack for AWS Conﬁg, or if using AWS Control Tower, the strongly recommended controls
available inside Control Tower.
Operational guidance
• Determine who in the organization should have access to the root user credentials.
• Use a two-person rule so that no one individual has access to all necessary credentials and MFA to
obtain root user access.
• Verify that the organization, and not a single individual, maintains control over the phone number
and email alias associated with the account (which are used for password reset and MFA reset ﬂow).
• Use root user only by exception (CIS 1.7).
• The AWS root user must not be used for everyday tasks, even administrative ones. Only log in as
the root user to perform AWS tasks that require root user. All other actions should be performed by
other users assuming appropriate roles.
• Periodically check that access to the root user is functioning so that procedures are tested prior to an
emergency situation requiring the use of the root user credentials.
156

AWS Well-Architected Framework
Security foundations
• Periodically check that the email address associated with the account and those listed under Alternate
Contacts work. Monitor these email inboxes for security notiﬁcations you might receive from
<abuse@amazon.com>. Also ensure any phone numbers associated with the account are working.
• Prepare incident response procedures to respond to root account misuse. Refer to the AWS Security
Incident Response Guide and the best practices in the Incident Response section of the Security Pillar
whitepaper for more information on building an incident response strategy for your AWS account.
Resources
Related best practices:
• SEC01-BP01 Separate workloads using accounts (p. 152)
• SEC02-BP01 Use strong sign-in mechanisms (p. 163)
• SEC03-BP02 Grant least privilege access (p. 177)
• SEC03-BP03 Establish emergency access process (p. 179)
• SEC10-BP05 Pre-provision access (p. 228)
Related documents:
• AWS Control Tower
• AWS Security Audit Guidelines
• IAM Best Practices
• Amazon GuardDuty – root credential usage alert
• Step-by-step guidance on monitoring for root credential use through CloudTrail
• MFA tokens approved for use with AWS
• Implementing break glass access on AWS
• Top 10 security items to improve in your AWS account
• What do I do if I notice unauthorized activity in my AWS account?
Related videos:
• Enable AWS adoption at scale with automation and governance
• Security Best Practices the Well-Architected Way
• Limiting use of AWS root credentials from AWS re:inforce 2022 – Security best practices with AWS IAM
Related examples and labs:
• Lab: AWS account setup and root user
SEC01-BP03 Identify and validate control objectives
Based on your compliance requirements and risks identiﬁed from your threat model, derive and validate
the control objectives and controls that you need to apply to your workload. Ongoing validation of
control objectives and controls help you measure the eﬀectiveness of risk mitigation.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Identify compliance requirements: Discover the organizational, legal, and compliance requirements
that your workload must comply with.
157

AWS Well-Architected Framework
Security foundations
• Identify AWS compliance resources: Identify resources that AWS has available to assist you with
compliance.
• https://aws.amazon.com/compliance/
• https://aws.amazon.com/artifact/
Resources
Related documents:
• AWS Security Audit Guidelines
• Security Bulletins
Related videos:
• AWS Security Hub: Manage Security Alerts and Automate Compliance
• Security Best Practices the Well-Architected Way
SEC01-BP04 Keep up-to-date with security threats
To help you deﬁne and implement appropriate controls, recognize attack vectors by staying up to date
with the latest security threats. Consume AWS Managed Services to make it easier to receive notiﬁcation
of unexpected or unusual behavior in your AWS accounts. Investigate using AWS Partner tools or third-
party threat information feeds as part of your security information ﬂow. The Common Vulnerabilities
and Exposures (CVE) List list contains publicly disclosed cyber security vulnerabilities that you can use to
stay up to date.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Subscribe to threat intelligence sources: Regularly review threat intelligence information from multiple
sources that are relevant to the technologies used in your workload.
• Common Vulnerabilities and Exposures List
• Consider AWS Shield Advanced service: It provides near real-time visibility into intelligence sources, if
your workload is internet accessible.
Resources
Related documents:
• AWS Security Audit Guidelines
• AWS Shield
• Security Bulletins
Related videos:
• Security Best Practices the Well-Architected Way
SEC01-BP05 Keep up-to-date with security recommendations
Stay up-to-date with both AWS and industry security recommendations to evolve the security posture
of your workload. AWS Security Bulletins contain important information about security and privacy
notiﬁcations.
158

AWS Well-Architected Framework
Security foundations
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Follow AWS updates: Subscribe or regularly check for new recommendations, tips and tricks.
• AWS Well-Architected Labs
• AWS security blog
• AWS service documentation
• Subscribe to industry news: Regularly review news feeds from multiple sources that are relevant to the
technologies that are used in your workload.
• Example: Common Vulnerabilities and Exposures List
Resources
Related documents:
• Security Bulletins
Related videos:
• Security Best Practices the Well-Architected Way
SEC01-BP06 Automate testing and validation of security controls in pipelines
Establish secure baselines and templates for security mechanisms that are tested and validated as part
of your build, pipelines, and processes. Use tools and automation to test and validate all security controls
continuously. For example, scan items such as machine images and infrastructure-as-code templates
for security vulnerabilities, irregularities, and drift from an established baseline at each stage. AWS
CloudFormation Guard can help you verify that CloudFormation templates are safe, save you time, and
reduce the risk of conﬁguration error.
Reducing the number of security misconﬁgurations introduced into a production environment is critical
—the more quality control and reduction of defects you can perform in the build process, the better.
Design continuous integration and continuous deployment (CI/CD) pipelines to test for security issues
whenever possible. CI/CD pipelines oﬀer the opportunity to enhance security at each stage of build and
delivery. CI/CD security tooling must also be kept updated to mitigate evolving threats.
Track changes to your workload conﬁguration to help with compliance auditing, change management,
and investigations that may apply to you. You can use AWS Conﬁg to record and evaluate your AWS and
third-party resources. It allows you to continuously audit and assess the overall compliance with rules
and conformance packs, which are collections of rules with remediation actions.
Change tracking should include planned changes, which are part of your organization’s change control
process (sometimes referred to as MACD—Move, Add, Change, Delete), unplanned changes, and
unexpected changes, such as incidents. Changes might occur on the infrastructure, but they might also
be related to other categories, such as changes in code repositories, machine images and application
inventory changes, process and policy changes, or documentation changes.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Automate conﬁguration management: Enforce and validate secure conﬁgurations automatically by
using a conﬁguration management service or tool.
• AWS Systems Manager
• AWS CloudFormation
159

AWS Well-Architected Framework
Security foundations
• Set Up a CI/CD Pipeline on AWS
Resources
Related documents:
• How to use service control policies to set permission guardrails across accounts in your AWS
Organization
Related videos:
• Managing Multi-Account AWS Environments Using AWS Organizations
• Security Best Practices the Well-Architected Way
SEC01-BP07 Identify threats and prioritize mitigations using a threat model
Perform threat modeling to identify and maintain an up-to-date register of potential threats and
associated mitigations for your workload. Prioritize your threats and adapt your security control
mitigations to prevent, detect, and respond. Revisit and maintain this in the context of your workload,
and the evolving security landscape.
Level of risk exposed if this best practice is not established: High
Implementation guidance
What is threat modeling?
As a deﬁnition, “Threat modeling works to identify, communicate, and understand threats and
mitigations within the context of protecting something of value.” – The Open Web Application Security
Project (OWASP) Application Threat Modeling
Why should you threat model?
Systems are complex, and are becoming increasingly more complex and capable over time, delivering
more business value and increased customer satisfaction and engagement. This means that IT design
decisions need to account for an ever-increasing number of use cases. This complexity and number of
use-case permutations typically makes unstructured approaches ineﬀective for ﬁnding and mitigating
threats. Instead, you need a systematic approach to enumerate the potential threats to the system, and
to devise mitigations and prioritize these mitigations to make sure that the limited resources of your
organization have the maximum impact in improving the overall security posture of the system.
Threat modeling is designed to provide this systematic approach, with the aim of ﬁnding and addressing
issues early in the design process, when the mitigations have a low relative cost and eﬀort compared to
later in the lifecycle. This approach aligns with the industry principle of “shift-left” security. Ultimately,
threat modeling integrates with an organization’s risk management process and helps drive decisions on
which controls to implement by using a threat driven approach.
When should threat modeling be performed?
Start threat modeling as early as possible in the lifecycle of your workload, this gives you better
ﬂexibility on what to do with the threats you have identiﬁed. Much like software bugs, the earlier you
identify threats, the more cost eﬀective it is to address them. A threat model is a living document and
should continue to evolve as your workloads change. Revisit your threat models over time, including
when there is a major change, a change in the threat landscape, or when you adopt a new feature or
service.
Implementation steps
How can we perform threat modeling?
160

AWS Well-Architected Framework
Security foundations
There are many diﬀerent ways to perform threat modeling. Much like programming languages, there
are advantages and disadvantages to each, and you should choose the way that works best for you. One
approach is to start with Shostack’s 4 Question Frame for Threat Modeling, which poses open-ended
questions to provide structure to your threat modeling exercise:
1. What are working on?
The purpose of this question is to help you understand and agree upon the system you are building
and the details about that system that are relevant to security. Creating a model or diagram is the
most popular way to answer this question, as it helps you to visualize what you are building, for
example, using a data ﬂow diagram. Writing down assumptions and important details about your
system also helps you deﬁne what is in scope. This allows everyone contributing to the threat model
to focus on the same thing, and avoid time-consuming detours into out-of-scope topics (including out
of date versions of your system). For example, if you are building a web application, it is probably not
worth your time threat modeling the operating system trusted boot sequence for browser clients, as
you have no ability to aﬀect this with your design.
2. What can go wrong?
This is where you identify threats to your system. Threats are accidental or intentional actions or
events that have unwanted impacts and could aﬀect the security of your system. Without a clear
understanding of what could go wrong, you have no way of doing anything about it.
There is no canonical list of what can go wrong. Creating this list requires brainstorming and
collaboration between all of the individuals within your team and relevant personas involved in the
threat modeling exercise. You can aid your brainstorming by using a model for identifying threats,
such as STRIDE, which suggests diﬀerent categories to evaluate: Spooﬁng, Tampering, Repudiation,
Information Disclosure, Denial of Service, and Elevation of privilege. In addition, you might want to aid
the brainstorming by reviewing existing lists and research for inspiration, including the OWASP Top
10, HiTrust Threat Catalog, and your organization’s own threat catalog.
3. What are we going to do about it?
As was the case with the previous question, there is no canonical list of all possible mitigations. The
inputs into this step are the identiﬁed threats, actors, and areas of improvement from the previous
step.
Security and compliance is a shared responsibility between you and AWS. It’s important to understand
that when you ask “What are we going to do about it?”, that you are also asking “Who is responsible
for doing something about it?”. Understanding the balance of responsibilities between you and
AWS helps you scope your threat modeling exercise to the mitigations that are under your control,
which are typically a combination of AWS service conﬁguration options and your own system-speciﬁc
mitigations.
For the AWS portion of the shared responsibility, you will ﬁnd that AWS services are in-scope of many
compliance programs. These programs help you to understand the robust controls in place at AWS to
maintain security and compliance of the cloud. The audit reports from these programs are available
for download for AWS customers from AWS Artifact.
Regardless of which AWS services you are using, there’s always an element of customer responsibility,
and mitigations aligned to these responsibilities should be included in your threat model. For security
control mitigations for the AWS services themselves, you want to consider implementing security
controls across domains, including domains such as identity and access management (authentication
and authorization), data protection (at rest and in transit), infrastructure security, logging, and
monitoring. The documentation for each AWS service has a dedicated security chapter that provides
guidance on the security controls to consider as mitigations. Importantly, consider the code that you
are writing and its code dependencies, and think about the controls that you could put in place to
address those threats. These controls could be things such as input validation, session handling, and
bounds handling. Often, the majority of vulnerabilities are introduced in custom code, so focus on this
area.
161

AWS Well-Architected Framework
Security foundations
4. Did we do a good job?
The aim is for your team and organization to improve both the quality of threat models and the
velocity at which you are performing threat modeling over time. These improvements come from
a combination of practice, learning, teaching, and reviewing. To go deeper and get hands on, it’s
recommended that you and your team complete the Threat modeling the right way for builders
training course or workshop. In addition, if you are looking for guidance on how to integrate threat
modeling into your organization’s application development lifecycle, see How to approach threat
modeling post on the AWS Security Blog.
Resources
Related best practices:
• SEC01-BP03 Identify and validate control objectives (p. 157)
• SEC01-BP04 Keep up-to-date with security threats (p. 158)
• SEC01-BP05 Keep up-to-date with security recommendations (p. 158)
• SEC01-BP08 Evaluate and implement new security services and features regularly (p. 162)
Related documents:
• How to approach threat modeling (AWS Security Blog)
• NIST: Guide to Data-Centric System Threat Modelling
Related videos:
• AWS Summit ANZ 2021 - How to approach threat modelling
• AWS Summit ANZ 2022 - Scaling security – Optimise for fast and secure delivery
Related training:
• Threat modeling the right way for builders – AWS Skill Builder virtual self-paced training
• Threat modeling the right way for builders – AWS Workshop
SEC01-BP08 Evaluate and implement new security services and features
regularly
Evaluate and implement security services and features from AWS and AWS Partners that allow you to
evolve the security posture of your workload. The AWS Security Blog highlights new AWS services and
features, implementation guides, and general security guidance. What's New with AWS? is a great way to
stay up to date with all new AWS features, services, and announcements.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Plan regular reviews: Create a calendar of review activities that includes compliance requirements,
evaluation of new AWS security features and services, and staying up-to-date with industry news.
• Discover AWS services and features: Discover the security features that are available for the services
that you are using, and review new features as they are released.
• AWS security blog
• AWS security bulletins
• AWS service documentation
162

AWS Well-Architected Framework
Identity and access management
• Deﬁne AWS service on-boarding process: Deﬁne processes for onboarding of new AWS services.
Include how you evaluate new AWS services for functionality, and the compliance requirements for
your workload.
• Test new services and features: Test new services and features as they are released in a non-production
environment that closely replicates your production one.
• Implement other defense mechanisms: Implement automated mechanisms to defend your workload,
explore the options available.
• Remediating non-compliant AWS resources by AWS Conﬁg Rules
Resources
Related videos:
• Security Best Practices the Well-Architected Way
Identity and access management
Questions
• SEC 2. How do you manage authentication for people and machines? (p. 163)
• SEC 3. How do you manage permissions for people and machines? (p. 175)
SEC 2. How do you manage authentication for people and
machines?
There are two types of identities that you must manage when approaching operating secure AWS
workloads. Understanding the type of identity you must manage and grant access helps you verify the
right identities have access to the right resources under the right conditions.
Human Identities: Your administrators, developers, operators, and end users require an identity to
access your AWS environments and applications. These are members of your organization, or external
users with whom you collaborate, and who interact with your AWS resources via a web browser, client
application, or interactive command line tools.
Machine Identities: Your service applications, operational tools, and workloads require an identity to
make requests to AWS services, for example, to read data. These identities include machines running in
your AWS environment such as Amazon EC2 instances or AWS Lambda functions. You may also manage
machine identities for external parties who need access. Additionally, you may also have machines
outside of AWS that need access to your AWS environment.
Best practices
• SEC02-BP01 Use strong sign-in mechanisms (p. 163)
• SEC02-BP02 Use temporary credentials (p. 165)
• SEC02-BP03 Store and use secrets securely (p. 167)
• SEC02-BP04 Rely on a centralized identity provider (p. 171)
• SEC02-BP05 Audit and rotate credentials periodically (p. 172)
• SEC02-BP06 Leverage user groups and attributes (p. 174)
SEC02-BP01 Use strong sign-in mechanisms
Sign-ins (authentication using sign-in credentials) can present risks when not using mechanisms
like multi-factor authentication (MFA), especially in situations where sign-in credentials have been
163

AWS Well-Architected Framework
Identity and access management
inadvertently disclosed or are easily guessed. Use strong sign-in mechanisms to reduce these risks by
requiring MFA and strong password policies.
Desired outcome: Reduce the risks of unintended access to credentials in AWS by using strong sign-
in mechanisms for AWS Identity and Access Management (IAM) users, the AWS account root user, AWS
IAM Identity Center (successor to AWS Single Sign-On) (successor to AWS Single Sign-On), and third-
party identity providers. This means requiring MFA, enforcing strong password policies, and detecting
anomalous login behavior.
Common anti-patterns:
• Not enforcing a strong password policy for your identities including complex passwords and MFA.
• Sharing the same credentials among diﬀerent users.
• Not using detective controls for suspicious sign-ins.
Level of risk exposed if this best practice is not established: High
Implementation guidance
There are many ways for human identities to sign-in to AWS. It is an AWS best practice to rely on a
centralized identity provider using federation (direct federation or using AWS IAM Identity Center
(successor to AWS Single Sign-On)) when authenticating to AWS. In that case, you should establish a
secure sign-in process with your identity provider or Microsoft Active Directory.
When you ﬁrst open an AWS account, you begin with an AWS account root user. You should only use the
account root user to set up access for your users (and for tasks that require the root user). It’s important
to turn on MFA for the account root user immediately after opening your AWS account and to secure the
root user using the AWS best practice guide.
If you create users in AWS IAM Identity Center (successor to AWS Single Sign-On), then secure the sign-
in process in that service. For consumer identities, you can use Amazon Cognito user pools and secure the
sign-in process in that service, or by using one of the identity providers that Amazon Cognito user pools
supports.
If you are using AWS Identity and Access Management (IAM) users, you would secure the sign-in process
using IAM.
Regardless of the sign-in method, it’s critical to enforce a strong sign-in policy.
Implementation steps
The following are general strong sign-in recommendations. The actual settings you conﬁgure should be
set by your company policy or use a standard like NIST 800-63.
• Require MFA. It’s an IAM best practice to require MFA for human identities and workloads. Turning on
MFA provides an additional layer of security requiring that users provide sign-in credentials and a one-
time password (OTP) or a cryptographically veriﬁed and generated string from a hardware device.
• Enforce a minimum password length, which is a primary factor in password strength.
• Enforce password complexity to make passwords more diﬃcult to guess.
• Allow users to change their own passwords.
• Create individual identities instead of shared credentials. By creating individual identities, you can give
each user a unique set of security credentials. Individual users provide the ability to audit each user’s
activity.
IAM Identity Center recommendations:
164

AWS Well-Architected Framework
Identity and access management
• IAM Identity Center provides a predeﬁned password policy when using the default directory that
establishes password length, complexity, and reuse requirements.
• Turn on MFA and conﬁgure the context-aware or always-on setting for MFA when the identity source is
the default directory, AWS Managed Microsoft AD, or AD Connector.
• Allow users to register their own MFA devices.
Amazon Cognito user pools directory recommendations:
• Conﬁgure the Password strength settings.
• Require MFA for users.
• Use the Amazon Cognito user pools advanced security settings for features like adaptive
authentication which can block suspicious sign-ins.
IAM user recommendations:
• Ideally you are using IAM Identity Center or direct federation. However, you might have the need for
IAM users. In that case, set a password policy for IAM users. You can use the password policy to deﬁne
requirements such as minimum length or whether the password requires non-alphabetic characters.
• Create an IAM policy to enforce MFA sign-in so that users are allowed to manage their own passwords
and MFA devices.
Resources
Related best practices:
• SEC02-BP03 Store and use secrets securely (p. 167)
• SEC02-BP04 Rely on a centralized identity provider (p. 171)
• SEC03-BP08 Share resources securely within your organization (p. 185)
Related documents:
• AWS IAM Identity Center (successor to AWS Single Sign-On) Password Policy
• IAM user password policy
• Setting the AWS account root user password
• Amazon Cognito password policy
• AWS credentials
• IAM security best practices
Related videos:
• Managing user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)
• Mastering identity at every layer of the cake
SEC02-BP02 Use temporary credentials
When doing any type of authentication, it’s best to use temporary credentials instead of long-term
credentials to reduce or eliminate risks, such as credentials being inadvertently disclosed, shared, or
stolen.
Desired outcome: To reduce the risk of long-term credentials, use temporary credentials wherever
possible for both human and machine identities. Long-term credentials create many risks, for example,
165

AWS Well-Architected Framework
Identity and access management
they can be uploaded in code to public GitHub repositories. By using temporary credentials, you
signiﬁcantly reduce the chances of credentials becoming compromised.
Common anti-patterns:
• Developers using long-term access keys from IAM users rather than obtaining temporary credentials
from the CLI using federation.
• Developers embedding long-term access keys in their code and uploading that code to public Git
repositories.
• Developers embedding long-term access keys in mobile apps that are then made available in app
stores.
• Users sharing long-term access keys with other users, or employees leaving the company with long-
term access keys still in their possession.
• Using long-term access keys for machine identities when temporary credentials could be used.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Use temporary security credentials instead of long-term credentials for all AWS API and CLI requests.
API and CLI requests to AWS services must, in nearly every case, be signed using AWS access keys.
These requests can be signed with either temporary or long-term credentials. The only time you should
use long-term credentials, also known as long-term access keys, is if you are using an IAM user or the
AWS account root user. When you federate to AWS or assume an IAM role through other methods,
temporary credentials are generated. Even when you access the AWS Management Console using sign-
in credentials, temporary credentials are generated for you to make calls to AWS services. There are
few situations where you need long-term credentials and you can accomplish nearly all tasks using
temporary credentials.
Avoiding the use of long-term credentials in favor of temporary credentials should go hand in hand with
a strategy of reducing the usage of IAM users in favor of federation and IAM roles. While IAM users have
been used for both human and machine identities in the past, we now recommend not using them to
avoid the risks in using long-term access keys.
Implementation steps
For human identities like employees, administrators, developers, operators, and customers:
• You should rely on a centralized identity provider and require human users to use federation with an
identity provider to access AWS using temporary credentials. Federation for your users can be done
either with direct federation to each AWS account or using AWS IAM Identity Center (successor to
AWS IAM Identity Center (successor to AWS Single Sign-On)) and the identity provider of your choice.
Federation provides a number of advantages over using IAM users in addition to eliminating long-
term credentials. Your users can also request temporary credentials from the command line for direct
federation or by using IAM Identity Center. This means that there are few uses cases that require IAM
users or long-term credentials for your users.
• When granting third parties, such as software as a service (SaaS) providers, access to resources in your
AWS account, you can use cross-account roles and resource-based policies.
• If you need to grant applications for consumers or customers access to your AWS resources, you can
use Amazon Cognito identity pools or Amazon Cognito user pools to provide temporary credentials.
The permissions for the credentials are conﬁgured through IAM roles. You can also deﬁne a separate
IAM role with limited permissions for guest users who are not authenticated.
For machine identities, you might need to use long-term credentials. In these cases, you should require
workloads to use temporary credentials with IAM roles to access AWS.
166

AWS Well-Architected Framework
Identity and access management
• For Amazon Elastic Compute Cloud (Amazon EC2), you can use roles for Amazon EC2.
• AWS Lambda allows you to conﬁgure a Lambda execution role to grant the service permissions to
perform AWS actions using temporary credentials. There are many other similar models for AWS
services to grant temporary credentials using IAM roles.
• For IoT devices, you can use the AWS IoT Core credential provider to request temporary credentials.
• For on-premises systems or systems that run outside of AWS that need access to AWS resources, you
can use IAM Roles Anywhere.
There are scenarios where temporary credentials are not an option and you might need to use long-
term credentials. In these situations, audit and rotate credentials periodically and rotate access keys
regularly for use cases that require long-term credentials. Some examples that might require long-
term credentials include WordPress plugins and third-party AWS clients. In situations where you must
use long-term credentials, or for credentials other than AWS access keys, such as database logins,
you can use a service that is designed to handle the management of secrets, such as AWS Secrets
Manager. Secrets Manager makes it simple to manage, rotate, and securely store encrypted secrets using
supported services. For more information about rotating long-term credentials, see rotating access keys.
Resources
Related best practices:
• SEC02-BP03 Store and use secrets securely (p. 167)
• SEC02-BP04 Rely on a centralized identity provider (p. 171)
• SEC03-BP08 Share resources securely within your organization (p. 185)
Related documents:
• Temporary Security Credentials
• AWS Credentials
• IAM Security Best Practices
• IAM Roles
• IAM Identity Center
• Identity Providers and Federation
• Rotating Access Keys
• Security Partner Solutions: Access and Access Control
• The AWS Account Root User
Related videos:
• Managing user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)
• Mastering identity at every layer of the cake
SEC02-BP03 Store and use secrets securely
A workload requires an automated capability to prove its identity to databases, resources, and third-
party services. This is accomplished using secret access credentials, such as API access keys, passwords,
and OAuth tokens. Using a purpose-built service to store, manage, and rotate these credentials helps
reduce the likelihood that those credentials become compromised.
167

AWS Well-Architected Framework
Identity and access management
Desired outcome: Implementing a mechanism for securely managing application credentials that
achieves the following goals:
• Identifying what secrets are required for the workload.
• Reducing the number of long-term credentials required by replacing them with short-term credentials
when possible.
• Establishing secure storage and automated rotation of remaining long-term credentials.
• Auditing access to secrets that exist in the workload.
• Continual monitoring to verify that no secrets are embedded in source code during the development
process.
• Reduce the likelihood of credentials being inadvertently disclosed.
Common anti-patterns:
• Not rotating credentials.
• Storing long-term credentials in source code or conﬁguration ﬁles.
• Storing credentials at rest unencrypted.
Beneﬁts of establishing this best practice:
• Secrets are stored encrypted at rest and in transit.
• Access to credentials is gated through an API (think of it as a credential vending machine).
• Access to a credential (both read and write) is audited and logged.
• Separation of concerns: credential rotation is performed by a separate component, which can be
segregated from the rest of the architecture.
• Secrets are automatically distributed on-demand to software components and rotation occurs in a
central location.
• Access to credentials can be controlled in a ﬁne-grained manner.
Level of risk exposed if this best practice is not established: High
Implementation guidance
In the past, credentials used to authenticate to databases, third-party APIs, tokens, and other secrets
might have been embedded in source code or in environment ﬁles. AWS provides several mechanisms to
store these credentials securely, automatically rotate them, and audit their usage.
The best way to approach secrets management is to follow the guidance of remove, replace, and rotate.
The most secure credential is one that you do not have to store, manage, or handle. There might be
credentials that are no longer necessary to the functioning of the workload that can be safely removed.
For credentials that are still required for the proper functioning of the workload, there might be an
opportunity to replace a long-term credential with a temporary or short-term credential. For example,
instead of hard-coding an AWS secret access key, consider replacing that long-term credential with a
temporary credential using IAM roles.
Some long-lived secrets might not be able to be removed or replaced. These secrets can be stored in a
service such as AWS Secrets Manager, where they can be centrally stored, managed, and rotated on a
regular basis.
An audit of the workload’s source code and conﬁguration ﬁles can reveal many types of credentials. The
following table summarizes strategies for handling common types of credentials:
168

AWS Well-Architected Framework
Identity and access management
Credential type
IAM access keys
Description
AWS IAM access and secret keys
used to assume IAM roles inside
of a workload
Suggested strategy
Replace: Use IAM roles assigned
to the compute instances (such
as Amazon EC2 or AWS Lambda)
instead. For interoperability
with third parties that require
access to resources in your AWS
account, ask if they support
AWS cross-account access. For
mobile apps, consider using
temporary credentials through
Amazon Cognito identity pools
(federated identities). For
workloads running outside
of AWS, consider IAM Roles
Anywhere or AWS Systems
Manager Hybrid Activations.
SSH keys
Secure Shell private keys used
to log into Linux EC2 instances,
manually or as part of an
automated process
Application and database
credentials
Amazon RDS and Aurora Admin
Database credentials
Passwords – plain text string
Passwords – plain text string
Replace: Use AWS Systems
Manager or EC2 Instance
Connect to provide
programmatic and human access
to EC2 instances using IAM roles.
Rotate: Store credentials in AWS
Secrets Manager and establish
automated rotation if possible.
Replace: Use the Secrets
Manager integration with
Amazon RDS or Amazon Aurora.
In addition, some RDS database
types can use IAM roles instead
of passwords for some use
cases (for more detail, see IAM
database authentication).
OAuth tokens
API tokens and keys
Secret tokens – plain text string Rotate: Store tokens in AWS
Secrets Manager and conﬁgure
automated rotation.
Secret tokens – plain text string Rotate: Store in AWS Secrets
Manager and establish
automated rotation if possible.
A common anti-pattern is embedding IAM access keys inside source code, conﬁguration ﬁles, or mobile
apps. When an IAM access key is required to communicate with an AWS service, use temporary (short-
term) security credentials. These short-term credentials can be provided through IAM roles for EC2
instances, execution roles for Lambda functions, Cognito IAM roles for mobile user access, and IoT Core
policies for IoT devices. When interfacing with third parties, prefer delegating access to an IAM role with
the necessary access to your account’s resources rather than conﬁguring an IAM user and sending the
third party the secret access key for that user.
There are many cases where the workload requires the storage of secrets necessary to interoperate with
other services and resources. AWS Secrets Manager is purpose built to securely manage these credentials,
as well as the storage, use, and rotation of API tokens, passwords, and other credentials.
169

AWS Well-Architected Framework
Identity and access management
AWS Secrets Manager provides ﬁve key capabilities to ensure the secure storage and handling of
sensitive credentials: encryption at rest, encryption in transit, comprehensive auditing, ﬁne-grained
access control, and extensible credential rotation. Other secret management services from AWS Partners
or locally developed solutions that provide similar capabilities and assurances are also acceptable.
Implementation steps
1. Identify code paths containing hard-coded credentials using automated tools such as Amazon
CodeGuru.
a. Use Amazon CodeGuru to scan your code repositories. Once the review is complete, ﬁlter on
Type=Secrets in CodeGuru to ﬁnd problematic lines of code.
2. Identify credentials that can be removed or replaced.
a. Identify credentials no longer needed and mark for removal.
b. For AWS Secret Keys that are embedded in source code, replace them with IAM roles associated
with the necessary resources. If part of your workload is outside AWS but requires IAM credentials
to access AWS resources, consider IAM Roles Anywhere or AWS Systems Manager Hybrid
Activations.
3. For other third-party, long-lived secrets that require the use of the rotate strategy, integrate Secrets
Manager into your code to retrieve third-party secrets at runtime.
a. The CodeGuru console can automatically create a secret in Secrets Manager using the discovered
credentials.
b. Integrate secret retrieval from Secrets Manager into your application code.
i. Serverless Lambda functions can use a language-agnostic Lambda extension.
ii. For EC2 instances or containers, AWS provides example client-side code for retrieving secrets
from Secrets Manager in several popular programming languages.
4. Periodically review your code base and re-scan to verify no new secrets have been added to the code.
a. Consider using a tool such as git-secrets to prevent committing new secrets to your source code
repository.
5. Monitor Secrets Manager activity for indications of unexpected usage, inappropriate secret access, or
attempts to delete secrets.
6. Reduce human exposure to credentials. Restrict access to read, write, and modify credentials to an
IAM role dedicated for this purpose, and only provide access to assume the role to a small subset of
operational users.
Resources
Related best practices:
• SEC02-BP02 Use temporary credentials (p. 165)
• SEC02-BP05 Audit and rotate credentials periodically (p. 172)
Related documents:
• Getting Started with AWS Secrets Manager
• Identity Providers and Federation
• Amazon CodeGuru Introduces Secrets Detector
• How AWS Secrets Manager uses AWS Key Management Service
• Secret encryption and decryption in Secrets Manager
• Secrets Manager blog entries
• Amazon RDS announces integration with AWS Secrets Manager
170

AWS Well-Architected Framework
Identity and access management
Related videos:
• Best Practices for Managing, Retrieving, and Rotating Secrets at Scale
• Find Hard-Coded Secrets Using Amazon CodeGuru Secrets Detector
• Securing Secrets for Hybrid Workloads Using AWS Secrets Manager
Related workshops:
• Store, retrieve, and manage sensitive credentials in AWS Secrets Manager
• AWS Systems Manager Hybrid Activations
SEC02-BP04 Rely on a centralized identity provider
For workforce identities, rely on an identity provider that allows you to manage identities in a centralized
place. This makes it easier to manage access across multiple applications and services, because you are
creating, managing, and revoking access from a single location. For example, if someone leaves your
organization, you can revoke access for all applications and services (including AWS) from one location.
This reduces the need for multiple credentials and provides an opportunity to integrate with existing
human resources (HR) processes.
For federation with individual AWS accounts, you can use centralized identities for AWS with a SAML
2.0-based provider with AWS Identity and Access Management. You can use any provider— whether
hosted by you in AWS, external to AWS, or supplied by the AWS Partner—that is compatible with the
SAML 2.0 protocol. You can use federation between your AWS account and your chosen provider to
grant a user or application access to call AWS API operations by using a SAML assertion to get temporary
security credentials. Web-based single sign-on is also supported, allowing users to sign in to the AWS
Management Console from your sign in website.
For federation to multiple accounts in your AWS Organizations, you can conﬁgure your identity source
in AWS IAM Identity Center (successor to AWS Single Sign-On) (IAM Identity Center), and specify where
your users and groups are stored. Once conﬁgured, your identity provider is your source of truth, and
information can be synchronized using the System for Cross-domain Identity Management (SCIM) v2.0
protocol. You can then look up users or groups and grant them IAM Identity Center access to AWS
accounts, cloud applications, or both.
IAM Identity Center integrates with AWS Organizations, which allows you to conﬁgure your identity
provider once and then grant access to existing and new accounts managed in your organization. IAM
Identity Center provides you with a default store, which you can use to manage your users and groups.
If you choose to use the IAM Identity Center store, create your users and groups and assign their level
of access to your AWS accounts and applications, keeping in mind the best practice of least privilege.
Alternatively, you can choose to Connect to Your External Identity Provider using SAML 2.0, or Connect
to Your Microsoft AD Directory using AWS Directory Service. Once conﬁgured, you can sign into the AWS
Management Console, or the AWS mobile app, by authenticating through your central identity provider.
For managing end-users or consumers of your workloads, such as a mobile app, you can use Amazon
Cognito. It provides authentication, authorization, and user management for your web and mobile apps.
Your users can sign in directly with sign-in credentials, or through a third party, such as Amazon, Apple,
Facebook, or Google.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Centralize administrative access: Create an Identity and Access Management (IAM) identity provider
entity to establish a trusted relationship between your AWS account and your identity provider (IdP).
IAM supports IdPs that are compatible with OpenID Connect (OIDC) or SAML 2.0 (Security Assertion
Markup Language 2.0).
171

AWS Well-Architected Framework
Identity and access management
• Identity Providers and Federation
• Centralize application access: Consider Amazon Cognito for centralizing application access. It lets
you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily.
Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as
Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.
• Remove old users and groups: After you start using an identity provider (IdP), remove users and groups
that are no longer required.
• Finding unused credentials
• Deleting an IAM group
Resources
Related documents:
• IAM Best Practices
• Security Partner Solutions: Access and Access Control
• Temporary Security Credentials
• The AWS Account Root User
Related videos:
• Best Practices for Managing, Retrieving, and Rotating Secrets at Scale
• Managing user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)
• Mastering identity at every layer of the cake
SEC02-BP05 Audit and rotate credentials periodically
Audit and rotate credentials periodically to limit how long the credentials can be used to access your
resources. Long-term credentials create many risks, and these risks can be reduced by rotating long-term
credentials regularly.
Desired outcome: Implement credential rotation to help reduce the risks associated with long-term
credential usage. Regularly audit and remediate non-compliance with credential rotation policies.
Common anti-patterns:
• Not auditing credential use.
• Using long-term credentials unnecessarily.
• Using long-term credentials and not rotating them regularly.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
When you cannot rely on temporary credentials and require long-term credentials, audit credentials to
verify that deﬁned controls like multi-factor authentication (MFA) are enforced, rotated regularly, and
have the appropriate access level.
Periodic validation, preferably through an automated tool, is necessary to verify that the correct controls
are enforced. For human identities, you should require users to change their passwords periodically
and retire access keys in favor of temporary credentials. As you move from AWS Identity and Access
172

AWS Well-Architected Framework
Identity and access management
Management (IAM) users to centralized identities, you can generate a credential report to audit your
users.
We also recommend that you enforce and monitor MFA in your identity provider. You can set up AWS
Conﬁg Rules, or use AWS Security Hub Security Standards, to monitor if users have conﬁgured MFA.
Consider using IAM Roles Anywhere to provide temporary credentials for machine identities. In situations
when using IAM roles and temporary credentials is not possible, frequent auditing and rotating access
keys is necessary.
Implementation steps
• Regularly audit credentials: Auditing the identities that are conﬁgured in your identity provider and
IAM helps verify that only authorized identities have access to your workload. Such identities can
include, but are not limited to, IAM users, AWS IAM Identity Center (successor to AWS Single Sign-On)
users, Active Directory users, or users in a diﬀerent upstream identity provider. For example, remove
people that leave the organization, and remove cross-account roles that are no longer required. Have
a process in place to periodically audit permissions to the services accessed by an IAM entity. This
helps you identify the policies you need to modify to remove any unused permissions. Use credential
reports and AWS Identity and Access Management Access Analyzer to audit IAM credentials and
permissions. You can use Amazon CloudWatch to set up alarms for speciﬁc API calls called within your
AWS environment. Amazon GuardDuty can also alert you to unexpected activity, which might indicate
overly permissive access or unintended access to IAM credentials.
• Rotate credentials regularly: When you are unable to use temporary credentials, rotate long-term
IAM access keys regularly (maximum every 90 days). If an access key is unintentionally disclosed
without your knowledge, this limits how long the credentials can be used to access your resources. For
information about rotating access keys for IAM users, see Rotating access keys.
• Review IAM permissions: To improve the security of your AWS account, regularly review and monitor
each of your IAM policies. Verify that policies adhere to the principle of least privilege.
• Consider automating IAM resource creation and updates: IAM Identity Center automates many
IAM tasks, such as role and policy management. Alternatively, AWS CloudFormation can be used to
automate the deployment of IAM resources, including roles and policies, to reduce the chance of
human error because the templates can be veriﬁed and version controlled.
• Use IAM Roles Anywhere to replace IAM users for machine identities: IAM Roles Anywhere allows
you to use roles in areas that you traditionally could not, such as on-premise servers. IAM Roles
Anywhere uses a trusted X.509 certiﬁcate to authenticate to AWS and receive temporary credentials.
Using IAM Roles Anywhere avoids the need to rotate these credentials, as long-term credentials are no
longer stored in your on-premises environment. Please note that you will need to monitor and rotate
the X.509 certiﬁcate as it approaches expiration.
Resources
Related best practices:
• SEC02-BP02 Use temporary credentials (p. 165)
• SEC02-BP03 Store and use secrets securely (p. 167)
Related documents:
• Getting Started with AWS Secrets Manager
• IAM Best Practices
• Identity Providers and Federation
• Security Partner Solutions: Access and Access Control
• Temporary Security Credentials
• Getting credential reports for your AWS account
173

AWS Well-Architected Framework
Identity and access management
Related videos:
• Best Practices for Managing, Retrieving, and Rotating Secrets at Scale
• Managing user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)
• Mastering identity at every layer of the cake
Related examples:
• Well-Architected Lab - Automated IAM User Cleanup
• Well-Architected Lab - Automated Deployment of IAM Groups and Roles
SEC02-BP06 Leverage user groups and attributes
As the number of users you manage grows, you will need to determine ways to organize them so that
you can manage them at scale. Place users with common security requirements in groups deﬁned by
your identity provider, and put mechanisms in place to ensure that user attributes that may be used
for access control (for example, department or location) are correct and updated. Use these groups and
attributes to control access, rather than individual users. This allows you to manage access centrally by
changing a user’s group membership or attributes once with a permission set, rather than updating many
individual policies when a user’s access needs change.
You can use AWS IAM Identity Center (successor to AWS Single Sign-On) (IAM Identity Center) to manage
user groups and attributes. IAM Identity Center supports most commonly used attributes whether they
are entered manually during user creation or automatically provisioned using a synchronization engine,
such as deﬁned in the System for Cross-Domain Identity Management (SCIM) speciﬁcation.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• If you are using AWS IAM Identity Center (successor to AWS Single Sign-On) (IAM Identity Center),
conﬁgure groups: IAM Identity Center provides you with the ability to conﬁgure groups of users, and
assign groups the desired level of permission.
• AWS Single Sign-On - Manage Identities
• Learn about attribute-based access control (ABAC): ABAC is an authorization strategy that deﬁnes
permissions based on attributes.
• What Is ABAC for AWS?
• Lab: IAM Tag Based Access Control for EC2
Resources
Related documents:
• Getting Started with AWS Secrets Manager
• IAM Best Practices
• Identity Providers and Federation
• The AWS Account Root User
Related videos:
• Best Practices for Managing, Retrieving, and Rotating Secrets at Scale
• Managing user permissions at scale with AWS IAM Identity Center (successor to AWS Single Sign-On)
• Mastering identity at every layer of the cake
174

AWS Well-Architected Framework
Identity and access management
Related examples:
• Lab: IAM Tag Based Access Control for EC2
SEC 3. How do you manage permissions for people and
machines?
Manage permissions to control access to people and machine identities that require access to AWS and
your workload. Permissions control who can access what, and under what conditions.
Best practices
• SEC03-BP01 Deﬁne access requirements (p. 175)
• SEC03-BP02 Grant least privilege access (p. 177)
• SEC03-BP03 Establish emergency access process (p. 179)
• SEC03-BP04 Reduce permissions continuously (p. 180)
• SEC03-BP05 Deﬁne permission guardrails for your organization (p. 182)
• SEC03-BP06 Manage access based on lifecycle (p. 183)
• SEC03-BP07 Analyze public and cross-account access (p. 183)
• SEC03-BP08 Share resources securely within your organization (p. 185)
• SEC03-BP09 Share resources securely with a third party (p. 188)
SEC03-BP01 Deﬁne access requirements
Each component or resource of your workload needs to be accessed by administrators, end users, or
other components. Have a clear deﬁnition of who or what should have access to each component,
choose the appropriate identity type and method of authentication and authorization.
Common anti-patterns:
• Hard-coding or storing secrets in your application.
• Granting custom permissions for each user.
• Using long-lived credentials.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Each component or resource of your workload needs to be accessed by administrators, end users, or
other components. Have a clear deﬁnition of who or what should have access to each component,
choose the appropriate identity type and method of authentication and authorization.
Regular access to AWS accounts within the organization should be provided using federated access or
a centralized identity provider. You should also centralize your identity management and ensure that
there is an established practice to integrate AWS access to your employee access lifecycle. For example,
when an employee changes to a job role with a diﬀerent access level, their group membership should
also change to reﬂect their new access requirements.
When deﬁning access requirements for non-human identities, determine which applications and
components need access and how permissions are granted. Using IAM roles built with the least privilege
access model is a recommended approach. AWS Managed policies provide predeﬁned IAM policies that
cover most common use cases.
AWS services, such as AWS Secrets Manager and AWS Systems Manager Parameter Store, can help
decouple secrets from the application or workload securely in cases where it's not feasible to use IAM
175

AWS Well-Architected Framework
Identity and access management
roles. In Secrets Manager, you can establish automatic rotation for your credentials. You can use Systems
Manager to reference parameters in your scripts, commands, SSM documents, conﬁguration, and
automation workﬂows by using the unique name that you speciﬁed when you created the parameter.
You can use AWS Identity and Access Management Roles Anywhere to obtain temporary security
credentials in IAM for workloads that run outside of AWS. Your workloads can use the same IAM
policies and IAM roles that you use with AWS applications to access AWS resources.
Where possible, prefer short-term temporary credentials over long-term static credentials. For scenarios
in which you need users with programmatic access and long-term credentials, use access key last used
information to rotate and remove access keys.
Users need programmatic access if they want to interact with AWS outside of the AWS Management
Console. The way to grant programmatic access depends on the type of user that's accessing AWS.
To grant users programmatic access, choose one of the following options.
Which user needs
programmatic access?
Workforce identity
(Users managed in IAM Identity
Center)
To
Use temporary credentials to
sign programmatic requests to
the AWS CLI, AWS SDKs, or AWS
APIs.
By
Following the instructions for
the interface that you want to
use.
• For the AWS CLI, see
Conﬁguring the AWS CLI to
use AWS IAM Identity Center
(successor to AWS Single Sign-
On) in the AWS Command Line
Interface User Guide.
• For AWS SDKs, tools, and
AWS APIs, see IAM Identity
Center authentication in the
AWS SDKs and Tools Reference
Guide.
IAM
Use temporary credentials to
sign programmatic requests to
the AWS CLI, AWS SDKs, or AWS
APIs.
IAM
(Not recommended)
Use long-term credentials to
sign programmatic requests to
the AWS CLI, AWS SDKs, or AWS
APIs.
Following the instructions in
Using temporary credentials
with AWS resources in the IAM
User Guide.
Following the instructions for
the interface that you want to
use.
• For the AWS CLI, see
Authenticating using IAM
user credentials in the AWS
Command Line Interface User
Guide.
• For AWS SDKs and tools, see
Authenticate using long-term
credentials in the AWS SDKs
and Tools Reference Guide.
• For AWS APIs, see Managing
access keys for IAM users in
the IAM User Guide.
176

AWS Well-Architected Framework
Identity and access management
Resources
Related documents:
• Attribute-based access control (ABAC)
• AWS IAM Identity Center (successor to AWS Single Sign-On)
• IAM Roles Anywhere
• AWS Managed policies for IAM Identity Center
• AWS IAM policy conditions
• IAM use cases
• Remove unnecessary credentials
• Working with Policies
• How to control access to AWS resources based on AWS account, OU, or organization
• Identify, arrange, and manage secrets easily using enhanced search in AWS Secrets Manager
Related videos:
• Become an IAM Policy Master in 60 Minutes or Less
• Separation of Duties, Least Privilege, Delegation, and CI/CD
• Streamlining identity and access management for innovation
SEC03-BP02 Grant least privilege access
It's a best practice to grant only the access that identities require to perform speciﬁc actions on speciﬁc
resources under speciﬁc conditions. Use group and identity attributes to dynamically set permissions
at scale, rather than deﬁning permissions for individual users. For example, you can allow a group of
developers access to manage only resources for their project. This way, if a developer leaves the project,
the developer’s access is automatically revoked without changing the underlying access policies.
Desired outcome: Users should only have the permissions required to do their job. Users should only
be given access to production environments to perform a speciﬁc task within a limited time period, and
access should be revoked once that task is complete. Permissions should be revoked when no longer
needed, including when a user moves onto a diﬀerent project or job function. Administrator privileges
should be given only to a small group of trusted administrators. Permissions should be reviewed
regularly to avoid permission creep. Machine or system accounts should be given the smallest set of
permissions needed to complete their tasks.
Common anti-patterns:
• Defaulting to granting users administrator permissions.
• Using the root user for day-to-day activities.
• Creating policies that are overly permissive, but without full administrator privileges.
• Not reviewing permissions to understand whether they permit least privilege access.
Level of risk exposed if this best practice is not established: High
Implementation guidance
The principle of least privilege states that identities should only be permitted to perform the smallest
set of actions necessary to fulﬁll a speciﬁc task. This balances usability, eﬃciency, and security. Operating
under this principle helps limit unintended access and helps track who has access to what resources. IAM
177

AWS Well-Architected Framework
Identity and access management
users and roles have no permissions by default. The root user has full access by default and should be
tightly controlled, monitored, and used only for tasks that require root access.
IAM policies are used to explicitly grant permissions to IAM roles or speciﬁc resources. For example,
identity-based policies can be attached to IAM groups, while S3 buckets can be controlled by resource-
based policies.
When creating an IAM policy, you can specify the service actions, resources, and conditions that must
be true for AWS to allow or deny access. AWS supports a variety of conditions to help you scope down
access. For example, by using the PrincipalOrgID condition key, you can deny actions if the requestor
isn’t a part of your AWS Organization.
You can also control requests that AWS services make on your behalf, such as AWS CloudFormation
creating an AWS Lambda function, using the CalledVia condition key. You should layer diﬀerent
policy types to establish defense-in-depth and limit the overall permissions of your users. You can
also restrict what permissions can be granted and under what conditions. For example, you can allow
your application teams to create their own IAM policies for systems they build, but must also apply a
Permission Boundary to limit the maximum permissions the system can receive.
Implementation steps
• Implement least privilege policies: Assign access policies with least privilege to IAM groups and roles
to reﬂect the user’s role or function that you have deﬁned.
• Base policies on API usage: One way to determine the needed permissions is to review AWS
CloudTrail logs. This review allows you to create permissions tailored to the actions that the user
actually performs within AWS. IAM Access Analyzer can automatically generate an IAM policy based
on activity. You can use IAM Access Advisor at the organization or account level to track the last
accessed information for a particular policy.
• Consider using AWS managed policies for job functions. When starting to create ﬁne-grained
permissions policies, it can be diﬃcult to know where to start. AWS has managed policies for common
job roles, for example billing, database administrators, and data scientists. These policies can help
narrow the access that users have while determining how to implement the least privilege policies.
• Remove unnecessary permissions: Remove permissions that are not needed and trim back overly
permissive policies. IAM Access Analyzer policy generation can help ﬁne-tune permissions policies.
• Ensure that users have limited access to production environments: Users should only have access
to production environments with a valid use case. After the user performs the speciﬁc tasks that
required production access, access should be revoked. Limiting access to production environments
helps prevent unintended production-impacting events and lowers the scope of impact of unintended
access.
• Consider permissions boundaries: A permissions boundary is a feature for using a managed policy
that sets the maximum permissions that an identity-based policy can grant to an IAM entity. An
entity's permissions boundary allows it to perform only the actions that are allowed by both its
identity-based policies and its permissions boundaries.
• Consider resource tags for permissions: An attribute-based access control model using resource
tags allows you to grant access based on resource purpose, owner, environment, or other criteria.
For example, you can use resource tags to diﬀerentiate between development and production
environments. Using these tags, you can restrict developers to the development environment. By
combining tagging and permissions policies, you can achieve ﬁne-grained resource access without
needing to deﬁne complicated, custom policies for every job function.
• Use service control policies for AWS Organizations. Service control policies centrally control the
maximum available permissions for member accounts in your organization. Importantly, service
control policies allow you to restrict root user permissions in member accounts. Also consider using
AWS Control Tower, which provides prescriptive managed controls that enrich AWS Organizations. You
can also deﬁne your own controls within Control Tower.
• Establish a user lifecycle policy for your organization: User lifecycle policies deﬁne tasks to perform
when users are onboarded onto AWS, change job role or scope, or no longer need access to AWS.
178

AWS Well-Architected Framework
Identity and access management
Permission reviews should be done during each step of a user’s lifecycle to verify that permissions are
properly restrictive and to avoid permissions creep.
• Establish a regular schedule to review permissions and remove any unneeded permissions: You
should regularly review user access to verify that users do not have overly permissive access. AWS
Conﬁg and IAM Access Analyzer can help when auditing user permissions.
• Establish a job role matrix: A job role matrix visualizes the various roles and access levels required
within your AWS footprint. Using a job role matrix, you can deﬁne and separate permissions based on
user responsibilities within your organization. Use groups instead of applying permissions directly to
individual users or roles.
Resources
Related documents:
• Grant least privilege
• Permissions boundaries for IAM entities
• Techniques for writing least privilege IAM policies
• IAM Access Analyzer makes it easier to implement least privilege permissions by generating IAM
policies based on access activity
• Delegate permission management to developers by using IAM permissions boundaries
• Reﬁning Permissions using last accessed information
• IAM policy types and when to use them
• Testing IAM policies with the IAM policy simulator
• Guardrails in AWS Control Tower
• Zero Trust architectures: An AWS perspective
• How to implement the principle of least privilege with CloudFormation StackSets
• Attribute-based access control (ABAC)
• Reducing policy scope by viewing user activity
• View role access
• Use Tagging to Organize Your Environment and Drive Accountability
• AWS Tagging Strategies
• Tagging AWS resources
Related videos:
• Next-generation permissions management
• Zero Trust: An AWS perspective
Related examples:
• Lab: IAM permissions boundaries delegating role creation
• Lab: IAM tag based access control for EC2
SEC03-BP03 Establish emergency access process
A process that allows emergency access to your workload in the unlikely event of an automated process
or pipeline issue. This will help you rely on least privilege access, but ensure users can obtain the right
179

AWS Well-Architected Framework
Identity and access management
level of access when they require it. For example, establish a process for administrators to verify and
approve their request, such as an emergency AWS cross-account role for access, or a speciﬁc process for
administrators to follow to validate and approve an emergency request.
Common anti-patterns:
• Not having an emergency process in place to recover from an outage with your existing identity
conﬁguration.
• Granting long term elevated permissions for troubleshooting or recovery purposes.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Establishing emergency access can take several forms for which you should be prepared. The ﬁrst is
a failure of your primary identity provider. In this case, you should rely on a second method of access
with the required permissions to recover. This method could be a backup identity provider or a user.
This second method should be tightly controlled, monitored, and notify in the event it is used. The
emergency access identity should source from an account speciﬁc for this purpose and only have
permissions to assume a role speciﬁcally designed for recovery.
You should also be prepared for emergency access where temporary elevated administrative access is
needed. A common scenario is to limit mutating permissions to an automated process used for deploying
changes. In the event that this process has an issue, users might need to request elevated permissions
to restore functionality. In this case, establish a process where users can request elevated access and
administrators can validate and approve it. The implementation plans detailing the best practice
guidance for pre-provisioning access and setting up emergency, break-glass, roles are provided as part of
SEC10-BP05 Pre-provision access (p. 228).
Resources
Related documents:
• Monitor and Notify on AWS
• Managing temporary elevated access
Related video:
• Become an IAM Policy Master in 60 Minutes or Less
SEC03-BP04 Reduce permissions continuously
As your teams determine what access is required, remove unneeded permissions and establish review
processes to achieve least privilege permissions. Continually monitor and remove unused identities and
permissions for both human and machine access.
Desired outcome: Permission policies should adhere to the least privilege principle. As job duties and
roles become better deﬁned, your permission policies need to be reviewed to remove unnecessary
permissions. This approach lessens the scope of impact should credentials be inadvertently exposed or
otherwise accessed without authorization.
Common anti-patterns:
• Defaulting to granting users administrator permissions.
• Creating policies that are overly permissive, but without full administrator privileges.
180

AWS Well-Architected Framework
Identity and access management
• Keeping permission policies after they are no longer needed.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
As teams and projects are just getting started, permissive permission policies might be used to inspire
innovation and agility. For example, in a development or test environment, developers can be given
access to a broad set of AWS services. We recommend that you evaluate access continuously and restrict
access to only those services and service actions that are necessary to complete the current job. We
recommend this evaluation for both human and machine identities. Machine identities, sometimes called
system or service accounts, are identities that give AWS access to applications or servers. This access is
especially important in a production environment, where overly permissive permissions can have a broad
impact and potentially expose customer data.
AWS provides multiple methods to help identify unused users, roles, permissions, and credentials. AWS
can also help analyze access activity of IAM users and roles, including associated access keys, and access
to AWS resources such as objects in Amazon S3 buckets. AWS Identity and Access Management Access
Analyzer policy generation can assist you in creating restrictive permission policies based on the actual
services and actions a principal interacts with. Attribute-based access control (ABAC) can help simplify
permissions management, as you can provide permissions to users using their attributes instead of
attaching permissions policies directly to each user.
Implementation steps
• Use AWS Identity and Access Management Access Analyzer: IAM Access Analyzer helps identify
resources in your organization and accounts, such as Amazon Simple Storage Service (Amazon S3)
buckets or IAM roles that are shared with an external entity.
• Use IAM Access Analyzer policy generation: IAM Access Analyzer policy generation helps you create
ﬁne-grained permission policies based on an IAM user or role’s access activity.
• Determine an acceptable timeframe and usage policy for IAM users and roles: Use the last accessed
timestamp to identify unused users and roles and remove them. Review service and action last
accessed information to identify and scope permissions for speciﬁc users and roles. For example, you
can use last accessed information to identify the speciﬁc Amazon S3 actions that your application role
requires and restrict the role’s access to only those actions. Last accessed information features are
available in the AWS Management Console and programmatically allow you to incorporate them into
your infrastructure workﬂows and automated tools.
• Consider logging data events in AWS CloudTrail: By default, CloudTrail does not log data events
such as Amazon S3 object-level activity (for example, GetObject and DeleteObject) or Amazon
DynamoDB table activities (for example, PutItem and DeleteItem). Consider using logging for these
events to determine what users and roles need access to speciﬁc Amazon S3 objects or DynamoDB
table items.
Resources
Related documents:
• Grant least privilege
• Remove unnecessary credentials
• What is AWS CloudTrail?
• Working with Policies
• Logging and monitoring DynamoDB
• Using CloudTrail event logging for Amazon S3 buckets and objects
181

AWS Well-Architected Framework
Identity and access management
• Getting credential reports for your AWS account
Related videos:
• Become an IAM Policy Master in 60 Minutes or Less
• Separation of Duties, Least Privilege, Delegation, and CI/CD
• AWS re:Inforce 2022 - AWS Identity and Access Management (IAM) deep dive
SEC03-BP05 Deﬁne permission guardrails for your organization
Establish common controls that restrict access to all identities in your organization. For example, you can
restrict access to speciﬁc AWS Regions, or prevent your operators from deleting common resources, such
as an IAM role used for your central security team.
Common anti-patterns:
• Running workloads in your Organizational administrator account.
• Running production and non-production workloads in the same account.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
As you grow and manage additional workloads in AWS, you should separate these workloads using
accounts and manage those accounts using AWS Organizations. We recommend that you establish
common permission guardrails that restrict access to all identities in your organization. For example, you
can restrict access to speciﬁc AWS Regions, or prevent your team from deleting common resources, such
as an IAM role used by your central security team.
You can get started by implementing example service control policies, such as preventing users from
turning oﬀ key services. SCPs use the IAM policy language and allow you to establish controls that all
IAM principals (users and roles) adhere to. You can restrict access to speciﬁc service actions, resources
and based on speciﬁc condition to meet the access control needs of your organization. If necessary, you
can deﬁne exceptions to your guardrails. For example, you can restrict service actions for all IAM entities
in the account except for a speciﬁc administrator role.
We recommend you avoid running workloads in your management account. The management account
should be used to govern and deploy security guardrails that will aﬀect member accounts. Some
AWS services support the use of a delegated administrator account. When available, you should use
this delegated account instead of the management account. You should strongly limit access to the
Organizational administrator account.
Using a multi-account strategy allows you to have greater ﬂexibility in applying guardrails to your
workloads. The AWS Security Reference Architecture gives prescriptive guidance on how to design your
account structure. AWS services such as AWS Control Tower provide capabilities to centrally manage both
preventative and detective controls across your organization. Deﬁne a clear purpose for each account or
OU within your organization and limit controls in line with that purpose.
Resources
Related documents:
• AWS Organizations
• Service control policies (SCPs)
• Get more out of service control policies in a multi-account environment
182

AWS Well-Architected Framework
Identity and access management
• AWS Security Reference Architecture (AWS SRA)
Related videos:
• Enforce Preventive Guardrails using Service Control Policies
• Building governance at scale with AWS Control Tower
• AWS Identity and Access Management deep dive
SEC03-BP06 Manage access based on lifecycle
Integrate access controls with operator and application lifecycle and your centralized federation provider.
For example, remove a user’s access when they leave the organization or change roles.
As you manage workloads using separate accounts, there will be cases where you need to share resources
between those accounts. We recommend that you share resources using AWS Resource Access Manager
(AWS RAM). This service allows you to easily and securely share AWS resources within your AWS
Organizations and Organizational Units. Using AWS RAM, access to shared resources is automatically
granted or revoked as accounts are moved in and out of the Organization or Organization Unit with
which they are shared. This helps ensure that resources are only shared with the accounts that you
intend.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Implement a user access lifecycle policy for new users joining, job function changes, and users leaving so
that only current users have access.
Resources
Related documents:
• Attribute-based access control (ABAC)
• Grant least privilege
• IAM Access Analyzer
• Remove unnecessary credentials
• Working with Policies
Related videos:
• Become an IAM Policy Master in 60 Minutes or Less
• Separation of Duties, Least Privilege, Delegation, and CI/CD
SEC03-BP07 Analyze public and cross-account access
Continually monitor ﬁndings that highlight public and cross-account access. Reduce public access and
cross-account access to only the speciﬁc resources that require this access.
Desired outcome: Know which of your AWS resources are shared and with whom. Continually monitor
and audit your shared resources to verify they are shared with only authorized principals.
Common anti-patterns:
• Not keeping an inventory of shared resources.
183

AWS Well-Architected Framework
Identity and access management
• Not following a process for approval of cross-account or public access to resources.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
If your account is in AWS Organizations, you can grant access to resources to the entire organization,
speciﬁc organizational units, or individual accounts. If your account is not a member of an organization,
you can share resources with individual accounts. You can grant direct cross-account access using
resource-based policies — for example, Amazon Simple Storage Service (Amazon S3) bucket policies
— or by allowing a principal in another account to assume an IAM role in your account. When using
resource policies, verify that access is only granted to authorized principals. Deﬁne a process to approve
all resources which are required to be publicly available.
AWS Identity and Access Management Access Analyzer uses provable security to identify all access
paths to a resource from outside of its account. It reviews resource policies continuously, and reports
ﬁndings of public and cross-account access to make it simple for you to analyze potentially broad
access. Consider conﬁguring IAM Access Analyzer with AWS Organizations to verify that you have
visibility to all your accounts. IAM Access Analyzer also allows you to preview ﬁndings before deploying
resource permissions. This allows you to validate that your policy changes grant only the intended
public and cross-account access to your resources. When designing for multi-account access, you
can use trust policies to control in what cases a role can be assumed. For example, you could use
the PrincipalOrgId condition key to deny an attempt to assume a role from outside your AWS
Organizations.
AWS Conﬁg can report resources that are misconﬁgured, and through AWS Conﬁg policy checks, can
detect resources that have public access conﬁgured. Services such as AWS Control Tower and AWS
Security Hub simplify deploying detective controls and guardrails across AWS Organizations to identify
and remediate publicly exposed resources. For example, AWS Control Tower has a managed guardrail
which can detect if any Amazon EBS snapshots are restorable by AWS accounts.
Implementation steps
• Consider using AWS Conﬁg for AWS Organizations: AWS Conﬁg allows you to aggregate ﬁndings
from multiple accounts within an AWS Organizations to a delegated administrator account. This
provides a comprehensive view, and allows you to deploy AWS Conﬁg Rules across accounts to detect
publicly accessible resources.
• Conﬁgure AWS Identity and Access Management Access Analyzer IAM Access Analyzer helps you
identify resources in your organization and accounts, such as Amazon S3 buckets or IAM roles that are
shared with an external entity.
• Use auto-remediation in AWS Conﬁg to respond to changes in public access conﬁguration of
Amazon S3 buckets: You can automatically turn on the block public access settings for Amazon S3
buckets.
• Implement monitoring and alerting to identify if Amazon S3 buckets have become public: You
must have monitoring and alerting in place to identify when Amazon S3 Block Public Access is turned
oﬀ, and if Amazon S3 buckets become public. Additionally, if you are using AWS Organizations, you
can create a service control policy that prevents changes to Amazon S3 public access policies. AWS
Trusted Advisor checks for Amazon S3 buckets that have open access permissions. Bucket permissions
that grant, upload, or delete access to everyone create potential security issues by allowing anyone
to add, modify, or remove items in a bucket. The Trusted Advisor check examines explicit bucket
permissions and associated bucket policies that might override the bucket permissions. You also can
use AWS Conﬁg to monitor your Amazon S3 buckets for public access. For more information, see How
to Use AWS Conﬁg to Monitor for and Respond to Amazon S3 Buckets Allowing Public Access. While
reviewing access, it’s important to consider what types of data are contained in Amazon S3 buckets.
Amazon Macie helps discover and protect sensitive data, such as PII, PHI, and credentials, such as
private or AWS keys.
184

AWS Well-Architected Framework
Identity and access management
Resources
Related documents:
• Using AWS Identity and Access Management Access Analyzer
• AWS Control Tower controls library
• AWS Foundational Security Best Practices standard
• AWS Conﬁg Managed Rules
• AWS Trusted Advisor check reference
• Monitoring AWS Trusted Advisor check results with Amazon EventBridge
• Managing AWS Conﬁg Rules Across All Accounts in Your Organization
• AWS Conﬁg and AWS Organizations
Related videos:
• Best Practices for securing your multi-account environment
• Dive Deep into IAM Access Analyzer
SEC03-BP08 Share resources securely within your organization
As the number of workloads grows, you might need to share access to resources in those workloads
or provision the resources multiple times across multiple accounts. You might have constructs
to compartmentalize your environment, such as having development, testing, and production
environments. However, having separation constructs does not limit you from being able to share
securely. By sharing components that overlap, you can reduce operational overhead and allow for a
consistent experience without guessing what you might have missed while creating the same resource
multiple times.
Desired outcome: Minimize unintended access by using secure methods to share resources within your
organization, and help with your data loss prevention initiative. Reduce your operational overhead
compared to managing individual components, reduce errors from manually creating the same
component multiple times, and increase your workloads’ scalability. You can beneﬁt from decreased
time to resolution in multi-point failure scenarios, and increase your conﬁdence in determining when a
component is no longer needed. For prescriptive guidance on analyzing externally shared resources, see
SEC03-BP07 Analyze public and cross-account access (p. 183).
Common anti-patterns:
• Lack of process to continually monitor and automatically alert on unexpected external share.
• Lack of baseline on what should be shared and what should not.
• Defaulting to a broadly open policy rather than sharing explicitly when required.
• Manually creating foundational resources that overlap when required.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Architect your access controls and patterns to govern the consumption of shared resources securely and
only with trusted entities. Monitor shared resources and review shared resource access continuously,
and be alerted on inappropriate or unexpected sharing. Review Analyze public and cross-account access
to help you establish governance to reduce the external access to only resources that require it, and to
establish a process to monitor continuously and alert automatically.
185

AWS Well-Architected Framework
Identity and access management
Cross-account sharing within AWS Organizations is supported by a number of AWS services, such as
AWS Security Hub, Amazon GuardDuty, and AWS Backup. These services allow for data to be shared to
a central account, be accessible from a central account, or manage resources and data from a central
account. For example, AWS Security Hub can transfer ﬁndings from individual accounts to a central
account where you can view all the ﬁndings. AWS Backup can take a backup for a resource and share
it across accounts. You can use AWS Resource Access Manager (AWS RAM) to share other common
resources, such as VPC subnets and Transit Gateway attachments, AWS Network Firewall, or Amazon
SageMaker pipelines.
To restrict your account to only share resources within your organization, use service control policies
(SCPs) to prevent access to external principals. When sharing resources, combine identity-based
controls and network controls to create a data perimeter for your organization to help protect against
unintended access. A data perimeter is a set of preventive guardrails to help verify that only your trusted
identities are accessing trusted resources from expected networks. These controls place appropriate
limits on what resources can be shared and prevent sharing or exposing resources that should not be
allowed. For example, as a part of your data perimeter, you can use VPC endpoint policies and the
AWS:PrincipalOrgId condition to ensure the identities accessing your Amazon S3 buckets belong to
your organization. It is important to note that SCPs do not apply to service-linked roles or AWS service
principals.
When using Amazon S3, turn oﬀ ACLs for your Amazon S3 bucket and use IAM policies to deﬁne access
control. For restricting access to an Amazon S3 origin from Amazon CloudFront, migrate from origin
access identity (OAI) to origin access control (OAC) which supports additional features including server-
side encryption with AWS Key Management Service.
In some cases, you might want to allow sharing resources outside of your organization or grant a third
party access to your resources. For prescriptive guidance on managing permissions to share resources
externally, see Permissions management.
Implementation steps
1. Use AWS Organizations.
AWS Organizations is an account management service that allows you to consolidate multiple AWS
accounts into an organization that you create and centrally manage. You can group your accounts into
organizational units (OUs) and attach diﬀerent policies to each OU to help you meet your budgetary,
security, and compliance needs. You can also control how AWS artiﬁcial intelligence (AI) and machine
learning (ML) services can collect and store data, and use the multi-account management of the AWS
services integrated with Organizations.
2. Integrate AWS Organizations with AWS services.
When you use an AWS service to perform tasks on your behalf in the member accounts of your
organization, AWS Organizations creates an IAM service-linked role (SLR) for that service in each
member account. You should manage trusted access using the AWS Management Console, the
AWS APIs, or the AWS CLI. For prescriptive guidance on turning on trusted access, see Using AWS
Organizations with other AWS services and AWS services that you can use with Organizations.
3. Establish a data perimeter.
The AWS perimeter is typically represented as an organization managed by AWS Organizations.
Along with on-premises networks and systems, accessing AWS resources is what many consider as
the perimeter of My AWS. The goal of the perimeter is to verify that access is allowed if the identity is
trusted, the resource is trusted, and the network is expected.
a. Deﬁne and implement the perimeters.
Follow the steps described in Perimeter implementation in the Building a Perimeter on AWS
whitepaper for each authorization condition. For prescriptive guidance on protecting network layer,
see Protecting networks.
b. Monitor and alert continually.
186

AWS Well-Architected Framework
Identity and access management
AWS Identity and Access Management Access Analyzer helps identify resources in your organization
and accounts that are shared with external entities. You can integrate IAM Access Analyzer with
AWS Security Hub to send and aggregate ﬁndings for a resource from IAM Access Analyzer to
Security Hub to help analyze the security posture of your environment. To integrate, turn on both
IAM Access Analyzer and Security Hub in each Region in each account. You can also use AWS Conﬁg
Rules to audit conﬁguration and alert the appropriate party using AWS Chatbot with AWS Security
Hub. You can then use AWS Systems Manager Automation documents to remediate noncompliant
resources.
c. For prescriptive guidance on monitoring and alerting continuously on resources shared externally,
see Analyze public and cross-account access.
4. Use resource sharing in AWS services and restrict accordingly.
Many AWS services allow you to share resources with another account, or target a resource in another
account, such as Amazon Machine Images (AMIs) and AWS Resource Access Manager (AWS RAM).
Restrict the ModifyImageAttribute API to specify the trusted accounts to share the AMI with.
Specify the ram:RequestedAllowsExternalPrincipals condition when using AWS RAM to
constrain sharing to your organization only, to help prevent access from untrusted identities. For
prescriptive guidance and considerations, see Resource sharing and external targets.
5. Use AWS RAM to share securely in an account or with other AWS accounts.
AWS RAM helps you securely share the resources that you have created with roles and users in your
account and with other AWS accounts. In a multi-account environment, AWS RAM allows you to
create a resource once and share it with other accounts. This approach helps reduce your operational
overhead while providing consistency, visibility, and auditability through integrations with Amazon
CloudWatch and AWS CloudTrail, which you do not receive when using cross-account access.
If you have resources that you shared previously using a resource-based policy, you can use the
PromoteResourceShareCreatedFromPolicy API or an equivalent to promote the resource share
to a full AWS RAM resource share.
In some cases, you might need to take additional steps to share resources. For example, to share an
encrypted snapshot, you need to share a AWS KMS key.
Resources
Related best practices:
• SEC03-BP07 Analyze public and cross-account access (p. 183)
• SEC03-BP09 Share resources securely with a third party (p. 188)
• SEC05-BP01 Create network layers (p. 197)
Related documents:
• Bucket owner granting cross-account permission to objects it does not own
• How to use Trust Policies with IAM
• Building Data Perimeter on AWS
• How to use an external ID when granting a third party access to your AWS resources
• AWS services you can use with AWS Organizations
• Establishing a data perimeter on AWS: Allow only trusted identities to access company data
Related videos:
• Granular Access with AWS Resource Access Manager
187

AWS Well-Architected Framework
Identity and access management
• Securing your data perimeter with VPC endpoints
• Establishing a data perimeter on AWS
Related tools:
• Data Perimeter Policy Examples
SEC03-BP09 Share resources securely with a third party
The security of your cloud environment doesn’t stop at your organization. Your organization might rely
on a third party to manage a portion of your data. The permission management for the third-party
managed system should follow the practice of just-in-time access using the principle of least privilege
with temporary credentials. By working closely with a third party, you can reduce the scope of impact
and risk of unintended access together.
Desired outcome: Long-term AWS Identity and Access Management (IAM) credentials, IAM access keys,
and secret keys that are associated with a user can be used by anyone as long as the credentials are valid
and active. Using an IAM role and temporary credentials helps you improve your overall security stance
by reducing the eﬀort to maintain long-term credentials, including the management and operational
overhead of those sensitive details. By using a universally unique identiﬁer (UUID) for the external ID
in the IAM trust policy, and keeping the IAM policies attached to the IAM role under your control, you
can audit and verify that the access granted to the third party is not too permissive. For prescriptive
guidance on analyzing externally shared resources, see SEC03-BP07 Analyze public and cross-account
access (p. 183).
Common anti-patterns:
• Using the default IAM trust policy without any conditions.
• Using long-term IAM credentials and access keys.
• Reusing external IDs.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
You might want to allow sharing resources outside of AWS Organizations or grant a third party access
to your account. For example, a third party might provide a monitoring solution that needs to access
resources within your account. In those cases, create an IAM cross-account role with only the privileges
needed by the third party. Additionally, deﬁne a trust policy using the external ID condition. When using
an external ID, you or the third party can generate a unique ID for each customer, third party, or tenancy.
The unique ID should not be controlled by anyone but you after it’s created. The third party must
implement a process to relate the external ID to the customer in a secure, auditable, and reproduceable
manner.
You can also use IAM Roles Anywhere to manage IAM roles for applications outside of AWS that use AWS
APIs.
If the third party no longer requires access to your environment, remove the role. Avoid providing long-
term credentials to a third party. Maintain awareness of other AWS services that support sharing. For
example, the AWS Well-Architected Tool allows sharing a workload with other AWS accounts, and AWS
Resource Access Manager helps you securely share an AWS resource you own with other accounts.
Implementation steps
1. Use cross-account roles to provide access to external accounts.
188

AWS Well-Architected Framework
Identity and access management
Cross-account roles reduce the amount of sensitive information that is stored by external accounts
and third parties for servicing their customers. Cross-account roles allow you to grant access to AWS
resources in your account securely to a third party, such as AWS Partners or other accounts in your
organization, while maintaining the ability to manage and audit that access.
The third party might be providing service to you from a hybrid infrastructure or alternatively pulling
data into an oﬀsite location. IAM Roles Anywhere helps you allow third party workloads to securely
interact with your AWS workloads and further reduce the need for long-term credentials.
You should not use long-term credentials, or access keys associated with users, to provide external
account access. Instead, use cross-account roles to provide the cross-account access.
2. Use an external ID with third parties.
Using an external ID allows you to designate who can assume a role in an IAM trust policy. The trust
policy can require that the user assuming the role assert the condition and target in which they are
operating. It also provides a way for the account owner to permit the role to be assumed only under
speciﬁc circumstances. The primary function of the external ID is to address and prevent the confused
deputy problem.
Use an external ID if you are an AWS account owner and you have conﬁgured a role for a third party
that accesses other AWS accounts in addition to yours, or when you are in the position of assuming
roles on behalf of diﬀerent customers. Work with your third party or AWS Partner to establish an
external ID condition to include in IAM trust policy.
3. Use universally unique external IDs.
Implement a process that generates random unique value for an external ID, such as a universally
unique identiﬁer (UUID). A third party reusing external IDs across diﬀerent customers does not address
the confused deputy problem, because customer A might be able to view data of customer B by using
the role ARN of customer B along with the duplicated external ID. In a multi-tenant environment,
where a third party supports multiple customers with diﬀerent AWS accounts, the third party must
use a diﬀerent unique ID as the external ID for each AWS account. The third party is responsible for
detecting duplicate external IDs and securely mapping each customer to their respective external ID.
The third party should test to verify that they can only assume the role when specifying the external
ID. The third party should refrain from storing the customer role ARN and the external ID until the
external ID is required.
The external ID is not treated as a secret, but the external ID must not be an easily guessable value,
such as a phone number, name, or account ID. Make the external ID a read-only ﬁeld so that the
external ID cannot be changed for the purpose of impersonating the setup.
You or the third party can generate the external ID. Deﬁne a process to determine who is responsible
for generating the ID. Regardless of the entity creating the external ID, the third party enforces
uniqueness and formats consistently across customers.
4. Deprecate customer-provided long-term credentials.
Deprecate the use of long-term credentials and use cross-account roles or IAM Roles Anywhere. If
you must use long-term credentials, establish a plan to migrate to role-based access. For details on
managing keys, see Identity Management. Also work with your AWS account team and the third party
to establish risk mitigation runbook. For prescriptive guidance on responding to and mitigating the
potential impact of security incident, see Incident response.
5. Verify that setup has prescriptive guidance or is automated.
The policy created for cross-account access in your accounts must follow the least-privilege principle.
The third party must provide a role policy document or automated setup mechanism that uses an
AWS CloudFormation template or an equivalent for you. This reduces the chance of errors associated
with manual policy creation and oﬀers an auditable trail. For more information on using a AWS
CloudFormation template to create cross-account roles, see Cross-Account Roles.
189

AWS Well-Architected Framework
Identity and access management
The third party should provide an automated, auditable setup mechanism. However, by using the role
policy document outlining the access needed, you should automate the setup of the role. Using a AWS
CloudFormation template or equivalent, you should monitor for changes with drift detection as part
of the audit practice.
6. Account for changes.
Your account structure, your need for the third party, or their service oﬀering being provided might
change. You should anticipate changes and failures, and plan accordingly with the right people,
process, and technology. Audit the level of access you provide on a periodic basis, and implement
detection methods to alert you to unexpected changes. Monitor and audit the use of the role and the
datastore of the external IDs. You should be prepared to revoke third-party access, either temporarily
or permanently, as a result of unexpected changes or access patterns. Also, measure the impact to
your revocation operation, including the time it takes to perform, the people involved, the cost, and
the impact to other resources.
For prescriptive guidance on detection methods, see the Detection best practices.
Resources
Related best practices:
• SEC02-BP02 Use temporary credentials (p. 165)
• SEC03-BP05 Deﬁne permission guardrails for your organization (p. 182)
• SEC03-BP06 Manage access based on lifecycle (p. 183)
• SEC03-BP07 Analyze public and cross-account access (p. 183)
• SEC04 Detection
Related documents:
• Bucket owner granting cross-account permission to objects it does not own
• How to use trust policies with IAM roles
• Delegate access across AWS accounts using IAM roles
• How do I access resources in another AWS account using IAM?
• Security best practices in IAM
• Cross-account policy evaluation logic
• How to use an external ID when granting access to your AWS resources to a third party
• Collecting Information from AWS CloudFormation Resources Created in External Accounts with
Custom Resources
• Securely Using External ID for Accessing AWS Accounts Owned by Others
• Extend IAM roles to workloads outside of IAM with IAM Roles Anywhere
Related videos:
• How do I allow users or roles in a separate AWS account access to my AWS account?
• AWS re:Invent 2018: Become an IAM Policy Master in 60 Minutes or Less
• AWS Knowledge Center Live: IAM Best Practices and Design Decisions
Related examples:
• Well-Architected Lab - Lambda cross account IAM role assumption (Level 300)
190

AWS Well-Architected Framework
Detection
• Conﬁgure cross-account access to Amazon DynamoDB
• AWS STS Network Query Tool
Detection
Question
• SEC 4. How do you detect and investigate security events? (p. 191)
SEC 4. How do you detect and investigate security events?
Capture and analyze events from logs and metrics to gain visibility. Take action on security events and
potential threats to help secure your workload.
Best practices
• SEC04-BP01 Conﬁgure service and application logging (p. 191)
• SEC04-BP02 Analyze logs, ﬁndings, and metrics centrally (p. 194)
• SEC04-BP03 Automate response to events (p. 195)
• SEC04-BP04 Implement actionable security events (p. 196)
SEC04-BP01 Conﬁgure service and application logging
Retain security event logs from services and applications. This is a fundamental principle of security
for audit, investigations, and operational use cases, and a common security requirement driven by
governance, risk, and compliance (GRC) standards, policies, and procedures.
Desired outcome: An organization should be able to reliably and consistently retrieve security event logs
from AWS services and applications in a timely manner when required to fulﬁll an internal process or
obligation, such as a security incident response. Consider centralizing logs for better operational results.
Common anti-patterns:
• Logs are stored in perpetuity or deleted too soon.
• Everybody can access logs.
• Relying entirely on manual processes for log governance and use.
• Storing every single type of log just in case it is needed.
• Checking log integrity only when necessary.
Beneﬁts of establishing this best practice: Implement a root cause analysis (RCA) mechanism for
security incidents and a source of evidence for your governance, risk, and compliance obligations.
Level of risk exposed if this best practice is not established: High
Implementation guidance
During a security investigation or other use cases based on your requirements, you need to be able to
review relevant logs to record and understand the full scope and timeline of the incident. Logs are also
required for alert generation, indicating that certain actions of interest have happened. It is critical to
select, turn on, store, and set up querying and retrieval mechanisms and alerting.
Implementation steps
191

AWS Well-Architected Framework
Detection
• Select and use log sources. Ahead of a security investigation, you need to capture relevant logs to
retroactively reconstruct activity in an AWS account. Select log sources relevant to your workloads.
The log source selection criteria should be based on the use cases required by your business. Establish
a trail for each AWS account using AWS CloudTrail or an AWS Organizations trail, and conﬁgure an
Amazon S3 bucket for it.
AWS CloudTrail is a logging service that tracks API calls made against an AWS account capturing AWS
service activity. It’s turned on by default with a 90-day retention of management events that can be
retrieved through CloudTrail Event history using the AWS Management Console, the AWS CLI, or an
AWS SDK. For longer retention and visibility of data events, create a CloudTrail trail and associate it
with an Amazon S3 bucket, and optionally with a Amazon CloudWatch log group. Alternatively, you
can create a CloudTrail Lake, which retains CloudTrail logs for up to seven years and provides a SQL-
based querying facility
AWS recommends that customers using a VPC turn on network traﬃc and DNS logs using VPC Flow
Logs and Amazon Route 53 resolver query logs, respectively, and streaming them to either an Amazon
S3 bucket or a CloudWatch log group. You can create a VPC ﬂow log for a VPC, a subnet, or a network
interface. For VPC Flow Logs, you can be selective on how and where you use Flow Logs to reduce cost.
AWS CloudTrail Logs, VPC Flow Logs, and Route 53 resolver query logs are the basic logging sources to
support security investigations in AWS. You can also use Amazon Security Lake to collect, normalize,
and store this log data in Apache Parquet format and Open Cybersecurity Schema Framework (OCSF),
which is ready for querying. Security Lake also supports other AWS logs and logs from third-party
sources.
AWS services can generate logs not captured by the basic log sources, such as Elastic Load Balancing
logs, AWS WAF logs, AWS Conﬁg recorder logs, Amazon GuardDuty ﬁndings, Amazon Elastic
Kubernetes Service (Amazon EKS) audit logs, and Amazon EC2 instance operating system and
application logs. For a full list of logging and monitoring options, see Appendix A: Cloud capability
deﬁnitions – Logging and Events of the AWS Security Incident Response Guide.
• Research logging capabilities for each AWS service and application: Each AWS service and
application provides you with options for log storage, each of which with its own retention and life-
cycle capabilities. The two most common log storage services are Amazon Simple Storage Service
(Amazon S3) and Amazon CloudWatch. For long retention periods, it is recommended to use Amazon
S3 for its cost eﬀectiveness and ﬂexible lifecycle capabilities. If the primary logging option is Amazon
CloudWatch Logs, as an option, you should consider archiving less frequently accessed logs to Amazon
S3.
• Select log storage: The choice of log storage is generally related to which querying tool you use,
retention capabilities, familiarity, and cost. The main options for log storage are an Amazon S3 bucket
or a CloudWatch Log group.
An Amazon S3 bucket provides cost-eﬀective, durable storage with an optional lifecycle policy. Logs
stored in Amazon S3 buckets can be queried using services such as Amazon Athena.
A CloudWatch log group provides durable storage and a built-in query facility through CloudWatch
Logs Insights.
• Identify appropriate log retention: When you use an Amazon S3 bucket or CloudWatch log group
to store logs, you must establish adequate lifecycles for each log source to optimize storage and
retrieval costs. Customers generally have between three months to one year of logs readily available
for querying, with retention of up to seven years. The choice of availability and retention should align
with your security requirements and a composite of statutory, regulatory, and business mandates.
• Use logging for each AWS service and application with proper retention and lifecycle policies:
For each AWS service or application in your organization, look for the speciﬁc logging conﬁguration
guidance:
• Conﬁgure AWS CloudTrail Trail
• Conﬁgure VPC Flow Logs
192

AWS Well-Architected Framework
Detection
• Conﬁgure Amazon GuardDuty Finding Export
• Conﬁgure AWS Conﬁg recording
• Conﬁgure AWS WAF web ACL traﬃc
• Conﬁgure AWS Network Firewall network traﬃc logs
• Conﬁgure Elastic Load Balancing access logs
• Conﬁgure Amazon Route 53 resolver query logs
• Conﬁgure Amazon RDS logs
• Conﬁgure Amazon EKS Control Plane logs
• Conﬁgure Amazon CloudWatch agent for Amazon EC2 instances and on-premises servers
• Select and implement querying mechanisms for logs: For log queries, you can use CloudWatch Logs
Insights for data stored in CloudWatch log groups, and Amazon Athena and Amazon OpenSearch
Service for data stored in Amazon S3. You can also use third-party querying tools such as a security
information and event management (SIEM) service.
The process for selecting a log querying tool should consider the people, process, and technology
aspects of your security operations. Select a tool that fulﬁlls operational, business, and security
requirements, and is both accessible and maintainable in the long term. Keep in mind that log
querying tools work optimally when the number of logs to be scanned is kept within the tool’s limits. It
is not uncommon to have multiple querying tools because of cost or technical constraints.
For example, you might use a third-party security information and event management (SIEM) tool
to perform queries for the last 90 days of data, but use Athena to perform queries beyond 90 days
because of the log ingestion cost of a SIEM. Regardless of the implementation, verify that your
approach minimizes the number of tools required to maximize operational eﬃciency, especially during
a security event investigation.
• Use logs for alerting: AWS provides alerting through several security services:
• AWS Conﬁg monitors and records your AWS resource conﬁgurations and allows you to automate the
evaluation and remediation against desired conﬁgurations.
• Amazon GuardDuty is a threat detection service that continually monitors for malicious activity
and unauthorized behavior to protect your AWS accounts and workloads. GuardDuty ingests,
aggregates, and analyzes information from sources, such as AWS CloudTrail management and
data events, DNS logs, VPC Flow Logs, and Amazon EKS Audit logs. GuardDuty pulls independent
data streams directly from CloudTrail, VPC Flow Logs, DNS query logs, and Amazon EKS. You don’t
have to manage Amazon S3 bucket policies or modify the way you collect and store logs. It is still
recommended to retain these logs for your own investigation and compliance purposes.
• AWS Security Hub provides a single place that aggregates, organizes, and prioritizes your security
alerts or ﬁndings from multiple AWS services and optional third-party products to give you a
comprehensive view of security alerts and compliance status.
You can also use custom alert generation engines for security alerts not covered by these services
or for speciﬁc alerts relevant to your environment. For information on building these alerts and
detections, see Detection in the AWS Security Incident Response Guide.
Resources
Related best practices:
• SEC04-BP02 Analyze logs, ﬁndings, and metrics centrally (p. 194)
• SEC07-BP04 Deﬁne data lifecycle management (p. 214)
• SEC10-BP06 Pre-deploy tools (p. 231)
Related documents:
193

AWS Well-Architected Framework
Detection
• AWS Security Incident Response Guide
• Getting Started with Amazon Security Lake
• Getting started: Amazon CloudWatch Logs
• Security Partner Solutions: Logging and Monitoring
Related videos:
• AWS re:Invent 2022 - Introducing Amazon Security Lake
Related examples:
• Assisted Log Enabler for AWS
• AWS Security Hub Findings Historical Export
Related tools:
• Snowﬂake for Cybersecurity
SEC04-BP02 Analyze logs, ﬁndings, and metrics centrally
Security operations teams rely on the collection of logs and the use of search tools to discover potential
events of interest, which might indicate unauthorized activity or unintentional change. However, simply
analyzing collected data and manually processing information is insuﬃcient to keep up with the volume
of information ﬂowing from complex architectures. Analysis and reporting alone don’t facilitate the
assignment of the right resources to work an event in a timely fashion.
A best practice for building a mature security operations team is to deeply integrate the ﬂow of security
events and ﬁndings into a notiﬁcation and workﬂow system such as a ticketing system, a bug or issue
system, or other security information and event management (SIEM) system. This takes the workﬂow
out of email and static reports, and allows you to route, escalate, and manage events or ﬁndings.
Many organizations are also integrating security alerts into their chat or collaboration, and developer
productivity platforms. For organizations embarking on automation, an API-driven, low-latency ticketing
system oﬀers considerable ﬂexibility when planning what to automate ﬁrst.
This best practice applies not only to security events generated from log messages depicting user activity
or network events, but also from changes detected in the infrastructure itself. The ability to detect
change, determine whether a change was appropriate, and then route that information to the correct
remediation workﬂow is essential in maintaining and validating a secure architecture, in the context
of changes where the nature of their undesirability is suﬃciently subtle that they cannot currently be
prevented with a combination of AWS Identity and Access Management (IAM) and AWS Organizations
conﬁguration.
Amazon GuardDuty and AWS Security Hub provide aggregation, deduplication, and analysis mechanisms
for log records that are also made available to you via other AWS services. GuardDuty ingests,
aggregates, and analyzes information from sources such as AWS CloudTrail management and data
events, VPC DNS logs, and VPC Flow Logs. Security Hub can ingest, aggregate, and analyze output from
GuardDuty, AWS Conﬁg, Amazon Inspector, Amazon Macie, AWS Firewall Manager, and a signiﬁcant
number of third-party security products available in the AWS Marketplace, and if built accordingly, your
own code. Both GuardDuty and Security Hub have an Administrator-Member model that can aggregate
ﬁndings and insights across multiple accounts, and Security Hub is often used by customers who have an
on- premises SIEM as an AWS-side log and alert preprocessor and aggregator from which they can then
ingest Amazon EventBridge through a AWS Lambda-based processor and forwarder.
Level of risk exposed if this best practice is not established: High
194
