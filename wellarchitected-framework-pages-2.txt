AWS Well-Architected Framework
Detection
Implementation guidance
• Evaluate log processing capabilities: Evaluate the options that are available for processing logs.
• Find an AWS Partner that specializes in logging and monitoring solutions
• As a start for analyzing CloudTrail logs, test Amazon Athena.
• Configuring Athena to analyze CloudTrail logs
• Implement centralize logging in AWS: See the following AWS example solution to centralize logging
from multiple sources.
• Centralize logging solution
• Implement centralize logging with partner: APN Partners have solutions to help you analyze logs
centrally.
• Logging and Monitoring
Resources
Related documents:
• AWS Answers: Centralized Logging
• AWS Security Hub
• Amazon CloudWatch
• Amazon EventBridge
• Getting started: Amazon CloudWatch Logs
• Security Partner Solutions: Logging and Monitoring
Related videos:
• Centrally Monitoring Resource Configuration and Compliance
• Remediating Amazon GuardDuty and AWS Security Hub Findings
• Threat management in the cloud: Amazon GuardDuty and AWS Security Hub
SEC04-BP03 Automate response to events
Using automation to investigate and remediate events reduces human effort and error, and allows you
to scale investigation capabilities. Regular reviews will help you tune automation tools, and continuously
iterate.
In AWS, investigating events of interest and information on potentially unexpected changes into an
automated workflow can be achieved using Amazon EventBridge. This service provides a scalable rules
engine designed to broker both native AWS event formats (such as AWS CloudTrail events), as well as
custom events you can generate from your application. Amazon GuardDuty also allows you to route
events to a workflow system for those building incident response systems (AWS Step Functions), or to a
central Security Account, or to a bucket for further analysis.
Detecting change and routing this information to the correct workflow can also be accomplished using
AWS Config Rules and Conformance Packs. AWS Config detects changes to in-scope services (though
with higher latency than EventBridge) and generates events that can be parsed using AWS Config Rules
for rollback, enforcement of compliance policy, and forwarding of information to systems, such as
change management platforms and operational ticketing systems. As well as writing your own Lambda
functions to respond to AWS Config events, you can also take advantage of the AWS Config Rules
Development Kit, and a library of open source AWS Config Rules. Conformance packs are a collection of
AWS Config Rules and remediation actions you deploy as a single entity authored as a YAML template. A
sample conformance pack template is available for the Well-Architected Security Pillar.
195

AWS Well-Architected Framework
Detection
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Implement automated alerting with GuardDuty: GuardDuty is a threat detection service that
continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts
and workloads. Turn on GuardDuty and configure automated alerts.
• Automate investigation processes: Develop automated processes that investigate an event and report
information to an administrator to save time.
• Lab: Amazon GuardDuty hands on
Resources
Related documents:
• AWS Answers: Centralized Logging
• AWS Security Hub
• Amazon CloudWatch
• Amazon EventBridge
• Getting started: Amazon CloudWatch Logs
• Security Partner Solutions: Logging and Monitoring
• Setting up Amazon GuardDuty
Related videos:
• Centrally Monitoring Resource Configuration and Compliance
• Remediating Amazon GuardDuty and AWS Security Hub Findings
• Threat management in the cloud: Amazon GuardDuty and AWS Security Hub
Related examples:
• Lab: Automated Deployment of Detective Controls
SEC04-BP04 Implement actionable security events
Create alerts that are sent to and can be actioned by your team. Ensure that alerts include relevant
information for the team to take action. For each detective mechanism you have, you should also have
a process, in the form of a runbook or playbook, to investigate. For example, when you use Amazon
GuardDuty, it generates different findings. You should have a runbook entry for each finding type,
for example, if a trojan is discovered, your runbook has simple instructions that instruct someone to
investigate and remediate.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Discover metrics available for AWS services: Discover the metrics that are available through Amazon
CloudWatch for the services that you are using.
• AWS service documentation
• Using Amazon CloudWatch Metrics
• Configure Amazon CloudWatch alarms.
196

AWS Well-Architected Framework
Infrastructure protection
• Using Amazon CloudWatch Alarms
Resources
Related documents:
• Amazon CloudWatch
• Amazon EventBridge
• Security Partner Solutions: Logging and Monitoring
Related videos:
• Centrally Monitoring Resource Configuration and Compliance
• Remediating Amazon GuardDuty and AWS Security Hub Findings
• Threat management in the cloud: Amazon GuardDuty and AWS Security Hub
Infrastructure protection
Questions
• SEC 5. How do you protect your network resources?  (p. 197)
• SEC 6. How do you protect your compute resources? (p. 202)
SEC 5. How do you protect your network resources?
Any workload that has some form of network connectivity, whether it’s the internet or a private network,
requires multiple layers of defense to help protect from external and internal network-based threats.
Best practices
• SEC05-BP01 Create network layers (p. 197)
• SEC05-BP02 Control traffic at all layers (p. 199)
• SEC05-BP03 Automate network protection (p. 200)
• SEC05-BP04 Implement inspection and protection (p. 201)
SEC05-BP01 Create network layers
Group components that share sensitivity requirements into layers to minimize the potential scope of
impact of unauthorized access. For example, a database cluster in a virtual private cloud (VPC) with no
need for internet access should be placed in subnets with no route to or from the internet. Traffic should
only flow from the adjacent next least sensitive resource. Consider a web application sitting behind a
load balancer. Your database should not be accessible directly from the load balancer. Only the business
logic or web server should have direct access to your database.
Desired outcome: Create a layered network. Layered networks help logically group similar networking
components. They also shrink the potential scope of impact of unauthorized network access. A properly
layered network makes it harder for unauthorized users to pivot to additional resources within your AWS
environment. In addition to securing internal network paths, you should also protect your network edge,
such as web applications and API endpoints.
Common anti-patterns:
• Creating all resources in a single VPC or subnet.
197

AWS Well-Architected Framework
Infrastructure protection
• Using overly permissive security groups.
• Failing to use subnets.
• Allowing direct access to data stores such as databases.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Components such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Relational
Database Service (Amazon RDS) database clusters, and AWS Lambda functions that share reachability
requirements can be segmented into layers formed by subnets. Consider deploying serverless workloads,
such as Lambda functions, within a VPC or behind an Amazon API Gateway. AWS Fargate (Fargate)
tasks that have no need for internet access should be placed in subnets with no route to or from the
internet. This layered approach mitigates the impact of a single layer misconfiguration, which could
allow unintended access. For AWS Lambda, you can run your functions in your VPC to take advantage of
VPC-based controls.
For network connectivity that can include thousands of VPCs, AWS accounts, and on-premises networks,
you should use AWS Transit Gateway. Transit Gateway acts as a hub that controls how traffic is routed
among all the connected networks, which act like spokes. Traffic between Amazon Virtual Private
Cloud (Amazon VPC) and Transit Gateway remains on the AWS private network, which reduces external
exposure to unauthorized users and potential security issues. Transit Gateway Inter-Region peering also
encrypts inter-Region traffic with no single point of failure or bandwidth bottleneck.
Implementation steps
• Use Reachability Analyzer to analyze the path between a source and destination based on
configuration: Reachability Analyzer allows you to automate verification of connectivity to and from
VPC connected resources. Note that this analysis is done by reviewing configuration (no network
packets are sent in conducting the analysis).
• Use Amazon VPC Network Access Analyzer to identify unintended network access to resources:
Amazon VPC Network Access Analyzer allows you to specify your network access requirements and
identify potential network paths.
• Consider whether resources need to be in a public subnet: Do not place resources in public subnets
of your VPC unless they absolutely must receive inbound network traffic from public sources.
• Create subnets in your VPCs: Create subnets for each network layer (in groups that include multiple
Availability Zones) to enhance micro-segmentation. Also verify that you have associated the correct
route tables with your subnets to control routing and internet connectivity.
• Use AWS Firewall Manager to manage your VPC security groups: AWS Firewall Manager helps lessen
the management burden of using multiple security groups.
• Use AWS WAF to protect against common web vulnerabilities: AWS WAF can help enhance edge
security by inspecting traffic for common web vulnerabilities, such as SQL injection. It also allows you
to restrict traffic from IP addresses originating from certain countries or geographical locations.
• Use Amazon CloudFront as a content distribution network (CDN): Amazon CloudFront can help
speed up your web application by storing data closer to your users. It can also improve edge security
by enforcing HTTPS, restricting access to geographic areas, and ensuring that network traffic can only
access resources when routed through CloudFront.
• Use Amazon API Gateway when creating application programming interfaces (APIs): Amazon API
Gateway helps publish, monitor, and secure REST, HTTPS, and WebSocket APIs.
Resources
Related documents:
• AWS Firewall Manager
198

AWS Well-Architected Framework
Infrastructure protection
• Amazon Inspector
• Amazon VPC Security
• Reachability Analyzer
• Amazon VPC Network Access Analyzer
Related videos:
• AWS Transit Gateway reference architectures for many VPCs
• Application Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS Shield
• AWS re:Inforce 2022 - Validate effective network access controls on AWS
• AWS re:Inforce 2022 - Advanced protections against bots using AWS WAF
Related examples:
• Well-Architected Lab - Automated Deployment of VPC
• Workshop: Amazon VPC Network Access Analyzer
SEC05-BP02 Control traffic at all layers
When architecting your network topology, you should examine the connectivity requirements of
each component. For example, if a component requires internet accessibility (inbound and outbound),
connectivity to VPCs, edge services, and external data centers.
A VPC allows you to define your network topology that spans an AWS Region with a private IPv4 address
range that you set, or an IPv6 address range AWS selects. You should apply multiple controls with a
defense in depth approach for both inbound and outbound traffic, including the use of security groups
(stateful inspection firewall), Network ACLs, subnets, and route tables. Within a VPC, you can create
subnets in an Availability Zone. Each subnet can have an associated route table that defines routing rules
for managing the paths that traffic takes within the subnet. You can define an internet routable subnet
by having a route that goes to an internet or NAT gateway attached to the VPC, or through another VPC.
When an instance, Amazon Relational Database Service(Amazon RDS) database, or other service is
launched within a VPC, it has its own security group per network interface. This firewall is outside the
operating system layer and can be used to define rules for allowed inbound and outbound traffic.
You can also define relationships between security groups. For example, instances within a database
tier security group only accept traffic from instances within the application tier, by reference to the
security groups applied to the instances involved. Unless you are using non-TCP protocols, it shouldn’t
be necessary to have an Amazon Elastic Compute Cloud(Amazon EC2) instance directly accessible by the
internet (even with ports restricted by security groups) without a load balancer, or CloudFront. This helps
protect it from unintended access through an operating system or application issue. A subnet can also
have a network ACL attached to it, which acts as a stateless firewall. You should configure the network
ACL to narrow the scope of traffic allowed between layers, note that you need to define both inbound
and outbound rules.
Some AWS services require components to access the internet for making API calls, where AWS API
endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS
services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology
has been generalized in AWS PrivateLink. We recommend you use this approach to access AWS services,
third-party services, and your own services hosted in other VPCs securely. All network traffic on AWS
PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be
initiated by the consumer of the service, and not by the provider of the service. Using AWS PrivateLink
for external service access allows you to create air-gapped VPCs with no internet access and helps
protect your VPCs from external threat vectors. Third-party services can use AWS PrivateLink to allow
their customers to connect to the services from their VPCs over private IP addresses. For VPC assets
199

AWS Well-Architected Framework
Infrastructure protection
that need to make outbound connections to the internet, these can be made outbound only (one-way)
through an AWS managed NAT gateway, outbound only internet gateway, or web proxies that you create
and manage.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Control network traffic in a VPC: Implement VPC best practices to control traffic.
• Amazon VPC security
• VPC endpoints
• Amazon VPC security group
• Network ACLs
• Control traffic at the edge: Implement edge services, such as Amazon CloudFront, to provide an
additional layer of protection and other features.
• Amazon CloudFront use cases
• AWS Global Accelerator
• AWS Web Application Firewall (AWS WAF)
• Amazon Route 53
• Amazon VPC Ingress Routing
• Control private network traffic: Implement services that protect your private traffic for your workload.
• Amazon VPC Peering
• Amazon VPC Endpoint Services (AWS PrivateLink)
• Amazon VPC Transit Gateway
• AWS Direct Connect
• AWS Site-to-Site VPN
• AWS Client VPN
• Amazon S3 Access Points
Resources
Related documents:
• AWS Firewall Manager
• Amazon Inspector
• Getting started with AWS WAF
Related videos:
• AWS Transit Gateway reference architectures for many VPCs
• Application Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS Shield
Related examples:
• Lab: Automated Deployment of VPC
SEC05-BP03 Automate network protection
Automate protection mechanisms to provide a self-defending network based on threat intelligence and
anomaly detection. For example, intrusion detection and prevention tools that can adapt to current
200

AWS Well-Architected Framework
Infrastructure protection
threats and reduce their impact. A web application firewall is an example of where you can automate
network protection, for example, by using the AWS WAF Security Automations solution (https://
github.com/awslabs/aws-waf-security-automations) to automatically block requests originating from IP
addresses associated with known threat actors.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Automate protection for web-based traffic: AWS offers a solution that uses AWS CloudFormation to
automatically deploy a set of AWS WAF rules designed to filter common web-based attacks. Users can
select from preconfigured protective features that define the rules included in an AWS WAF web access
control list (web ACL).
• AWS WAF security automations
• Consider AWS Partner solutions: AWS Partners offer hundreds of industry-leading products that are
equivalent, identical to, or integrate with existing controls in your on-premises environments. These
products complement the existing AWS services to allow you to deploy a comprehensive security
architecture and a more seamless experience across your cloud and on-premises environments.
• Infrastructure security
Resources
Related documents:
• AWS Firewall Manager
• Amazon Inspector
• Amazon VPC Security
• Getting started with AWS WAF
Related videos:
• AWS Transit Gateway reference architectures for many VPCs
• Application Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS Shield
Related examples:
• Lab: Automated Deployment of VPC
SEC05-BP04 Implement inspection and protection
Inspect and filter your traffic at each layer. You can inspect your VPC configurations for potential
unintended access using VPC Network Access Analyzer. You can specify your network access
requirements and identify potential network paths that do not meet them. For components transacting
over HTTP-based protocols, a web application firewall can help protect from common attacks. AWS
WAF is a web application firewall that lets you monitor and block HTTP(s) requests that match
your configurable rules that are forwarded to an Amazon API Gateway API, Amazon CloudFront,
or an Application Load Balancer. To get started with AWS WAF, you can use AWS Managed Rules in
combination with your own, or use existing partner integrations.
For managing AWS WAF, AWS Shield Advanced protections, and Amazon VPC security groups across
AWS Organizations, you can use AWS Firewall Manager. It allows you to centrally configure and manage
firewall rules across your accounts and applications, making it easier to scale enforcement of common
rules. It also allows you to rapidly respond to attacks, using AWS Shield Advanced, or solutions that can
201

AWS Well-Architected Framework
Infrastructure protection
automatically block unwanted requests to your web applications. Firewall Manager also works with AWS
Network Firewall. AWS Network Firewall is a managed service that uses a rules engine to give you fine-
grained control over both stateful and stateless network traffic. It supports the Suricata compatible open
source intrusion prevention system (IPS) specifications for rules to help protect your workload.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Configure Amazon GuardDuty: GuardDuty is a threat detection service that continuously monitors
for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. Use
GuardDuty and configure automated alerts.
• Amazon GuardDuty
• Lab: Automated Deployment of Detective Controls
• Configure virtual private cloud (VPC) Flow Logs: VPC Flow Logs is a feature that allows you to capture
information about the IP traffic going to and from network interfaces in your VPC. Flow log data can
be published to Amazon CloudWatch Logs and Amazon Simple Storage Service (Amazon S3). After
you've created a flow log, you can retrieve and view its data in the chosen destination.
• Consider VPC traffic mirroring: Traffic mirroring is an Amazon VPC feature that you can use to copy
network traffic from an elastic network interface of Amazon Elastic Compute Cloud (Amazon EC2)
instances and then send it to out-of-band security and monitoring appliances for content inspection,
threat monitoring, and troubleshooting.
• VPC traffic mirroring
Resources
Related documents:
• AWS Firewall Manager
• Amazon Inspector
• Amazon VPC Security
• Getting started with AWS WAF
Related videos:
• AWS Transit Gateway reference architectures for many VPCs
• Application Acceleration and Protection with Amazon CloudFront, AWS WAF, and AWS Shield
Related examples:
• Lab: Automated Deployment of VPC
SEC 6. How do you protect your compute resources?
Compute resources in your workload require multiple layers of defense to help protect from external and
internal threats. Compute resources include EC2 instances, containers, AWS Lambda functions, database
services, IoT devices, and more.
Best practices
• SEC06-BP01 Perform vulnerability management (p. 203)
• SEC06-BP02 Reduce attack surface (p. 205)
202

AWS Well-Architected Framework
Infrastructure protection
• SEC06-BP03 Implement managed services (p. 206)
• SEC06-BP04 Automate compute protection (p. 207)
• SEC06-BP05 Enable people to perform actions at a distance (p. 208)
• SEC06-BP06 Validate software integrity (p. 208)
SEC06-BP01 Perform vulnerability management
Frequently scan and patch for vulnerabilities in your code, dependencies, and in your infrastructure to
help protect against new threats.
Desired outcome: Create and maintain a vulnerability management program. Regularly scan and patch
resources such as Amazon EC2 instances, Amazon Elastic Container Service (Amazon ECS) containers,
and Amazon Elastic Kubernetes Service (Amazon EKS) workloads. Configure maintenance windows for
AWS managed resources, such as Amazon Relational Database Service (Amazon RDS) databases. Use
static code scanning to inspect application source code for common issues. Consider web application
penetration testing if your organization has the requisite skills or can hire outside assistance.
Common anti-patterns:
• Not having a vulnerability management program.
• Performing system patching without considering severity or risk avoidance.
• Using software that has passed its vendor-provided end of life (EOL) date.
• Deploying code into production before analyzing it for security issues.
Level of risk exposed if this best practice is not established: High
Implementation guidance
A vulnerability management program includes security assessment, identifying issues, prioritizing,
and performing patch operations as part of resolving the issues. Automation is the key to continually
scanning workloads for issues and unintended network exposure and performing remediation.
Automating the creation and updating of resources saves time and reduces the risk of configuration
errors creating further issues. A well-designed vulnerability management program should also consider
vulnerability testing during the development and deployment stages of the software life cycle.
Implementing vulnerability management during development and deployment helps lessen the chance
that a vulnerability can make its way into your production environment.
Implementing a vulnerability management program requires a good understanding of the AWS Shared
Responsibly model and how it relates to your specific workloads. Under the Shared Responsibility Model,
AWS is responsible for protecting the infrastructure of the AWS Cloud. This infrastructure is composed
of the hardware, software, networking, and facilities that run AWS Cloud services. You are responsible
for security in the cloud, for example, the actual data, security configuration, and management tasks of
Amazon EC2 instances, and verifying that your Amazon S3 objects are properly classified and configured.
Your approach to vulnerability management also can vary depending on the services you consume. For
example, AWS manages the patching for our managed relational database service, Amazon RDS, but you
would be responsible for patching self-hosted databases.
AWS has a range of services to help with your vulnerability management program. Amazon Inspector
continually scans AWS workloads for software issues and unintended network access. AWS Systems
Manager Patch Manager helps manage patching across your Amazon EC2 instances. Amazon Inspector
and Systems Manager can be viewed in AWS Security Hub, a cloud security posture management service
that helps automate AWS security checks and centralize security alerts.
Amazon CodeGuru can help identify potential issues in Java and Python applications using static code
analysis.
203

AWS Well-Architected Framework
Infrastructure protection
Implementation steps
•                                                                                                       Configure Amazon Inspector: Amazon Inspector automatically detects newly launched Amazon EC2
instances, Lambda functions, and eligible container images pushed to Amazon ECR and immediately
scans them for software issues, potential defects, and unintended network exposure.
•                                                                                                       Scan source code: Scan libraries and dependencies for issues and defects. Amazon CodeGuru can
scan and provide recommendations to remediating common security issues for both Java and Python
applications. The OWASP Foundation publishes a list of Source Code Analysis Tools (also known as
SAST tools).
•                                                                                                       Implement a mechanism to scan and patch your existing environment, as well as scanning as
part of a CI/CD pipeline build process: Implement a mechanism to scan and patch for issues in your
dependencies and operating systems to help protect against new threats. Have that mechanism run
on a regular basis. Software vulnerability management is essential to understanding where you need
to apply patches or address software issues. Prioritize remediation of potential security issues by
embedding vulnerability assessments early into your continuous integration/continuous delivery (CI/
CD) pipeline. Your approach can vary based on the AWS services that you are consuming. To check for
potential issues in software running in Amazon EC2 instances, add Amazon Inspector to your pipeline
to alert you and stop the build process if issues or potential defects are detected. Amazon Inspector
continually monitors resources. You can also use open source products such as OWASP Dependency-
Check, Snyk, OpenVAS, package managers, and AWS Partner tools for vulnerability management.
•                                                                                                       Use AWS Systems Manager: You are responsible for patch management for your AWS resources,
including Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Machine Images (AMIs),
and other compute resources. AWS Systems Manager Patch Manager automates the process of
patching managed instances with both security related and other types of updates. Patch Manager
can be used to apply patches on Amazon EC2 instances for both operating systems and applications,
including Microsoft applications, Windows service packs, and minor version upgrades for Linux based
instances. In addition to Amazon EC2, Patch Manager can also be used to patch on-premises servers.
For a list of supported operating systems, see Supported operating systems in the Systems Manager
User Guide. You can scan instances to see only a report of missing patches, or you can scan and
automatically install all missing patches.
•                                                                                                       Use AWS Security Hub: Security Hub provides a comprehensive view of your security state in AWS.
It collects security data across multiple AWS services and provides those findings in a standardized
format, allowing you to prioritize security findings across AWS services.
•                                                                                                       Use AWS CloudFormation: AWS CloudFormation is an infrastructure as code (IaC) service that can
help with vulnerability management by automating resource deployment and standardizing resource
architecture across multiple accounts and environments.
Resources
Related documents:
• AWS Systems Manager
• Security Overview of AWS Lambda
• Amazon CodeGuru
• Improved, Automated Vulnerability Management for Cloud Workloads with a New Amazon Inspector
• Automate vulnerability management and remediation in AWS using Amazon Inspector and AWS
Systems Manager - Part 1
Related videos:
• Securing Serverless and Container Services
• Security best practices for the Amazon EC2 instance metadata service
204

AWS Well-Architected Framework
Infrastructure protection
SEC06-BP02 Reduce attack surface
Reduce your exposure to unintended access by hardening operating systems and minimizing the
components, libraries, and externally consumable services in use. Start by reducing unused components,
whether they are operating system packages or applications, for Amazon Elastic Compute Cloud
(Amazon EC2)-based workloads, or external software modules in your code, for all workloads. You
can find many hardening and security configuration guides for common operating systems and server
software. For example, you can start with the Center for Internet Security and iterate.
In Amazon EC2, you can create your own Amazon Machine Images (AMIs), which you have patched and
hardened, to help you meet the specific security requirements for your organization. The patches and
other security controls you apply on the AMI are effective at the point in time in which they were created
—they are not dynamic unless you modify after launching, for example, with AWS Systems Manager.
You can simplify the process of building secure AMIs with EC2 Image Builder. EC2 Image Builder
significantly reduces the effort required to create and maintain golden images without writing and
maintaining automation. When software updates become available, Image Builder automatically
produces a new image without requiring users to manually initiate image builds. EC2 Image Builder
allows you to easily validate the functionality and security of your images before using them in
production with AWS-provided tests and your own tests. You can also apply AWS-provided security
settings to further secure your images to meet internal security criteria. For example, you can produce
images that conform to the Security Technical Implementation Guide (STIG) standard using AWS-
provided templates.
Using third-party static code analysis tools, you can identify common security issues such as unchecked
function input bounds, as well as applicable common vulnerabilities and exposures (CVEs). You can
use Amazon CodeGuru for supported languages. Dependency checking tools can also be used to
determine whether libraries your code links against are the latest versions, are themselves free of CVEs,
and have licensing conditions that meet your software policy requirements.
Using Amazon Inspector, you can perform configuration assessments against your instances for known
CVEs, assess against security benchmarks, and automate the notification of defects. Amazon Inspector
runs on production instances or in a build pipeline, and it notifies developers and engineers when
findings are present. You can access findings programmatically and direct your team to backlogs and
bug-tracking systems. EC2 Image Builder can be used to maintain server images (AMIs) with automated
patching, AWS-provided security policy enforcement, and other customizations. When using containers
implement ECR Image Scanning in your build pipeline and on a regular basis against your image
repository to look for CVEs in your containers.
While Amazon Inspector and other tools are effective at identifying configurations and any CVEs that
are present, other methods are required to test your workload at the application level. Fuzzing is a well-
known method of finding bugs using automation to inject malformed data into input fields and other
areas of your application.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Harden operating system: Configure operating systems to meet best practices.
• Securing Amazon Linux
• Securing Microsoft Windows Server
• Harden containerized resources: Configure containerized resources to meet security best practices.
• Implement AWS Lambda best practices.
• AWS Lambda best practices
Resources
Related documents:
205

AWS Well-Architected Framework
Infrastructure protection
• AWS Systems Manager
• Replacing a Bastion Host with Amazon EC2 Systems Manager
• Security Overview of AWS Lambda
Related videos:
• Running high-security workloads on Amazon EKS
• Securing Serverless and Container Services
• Security best practices for the Amazon EC2 instance metadata service
Related examples:
• Lab: Automated Deployment of Web Application Firewall
SEC06-BP03 Implement managed services
Implement services that manage resources, such as Amazon Relational Database Service (Amazon RDS),
AWS Lambda, and Amazon Elastic Container Service (Amazon ECS), to reduce your security maintenance
tasks as part of the shared responsibility model. For example, Amazon RDS helps you set up, operate,
and scale a relational database, automates administration tasks such as hardware provisioning, database
setup, patching, and backups. This means you have more free time to focus on securing your application
in other ways described in the AWS Well-Architected Framework. Lambda lets you run code without
provisioning or managing servers, so you only need to focus on the connectivity, invocation, and security
at the code level-not the infrastructure or operating system.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Explore available services: Explore, test, and implement services that manage resources, such as
Amazon RDS, AWS Lambda, and Amazon ECS.
Resources
Related documents:
• AWS Website
• AWS Systems Manager
• Replacing a Bastion Host with Amazon EC2 Systems Manager
• Security Overview of AWS Lambda
Related videos:
• Running high-security workloads on Amazon EKS
• Securing Serverless and Container Services
• Security best practices for the Amazon EC2 instance metadata service
Related examples:
• Lab: AWS Certificate Manager Request Public Certificate
206

AWS Well-Architected Framework
Infrastructure protection
SEC06-BP04 Automate compute protection
Automate your protective compute mechanisms including vulnerability management, reduction in attack
surface, and management of resources. The automation will help you invest time in securing other
aspects of your workload, and reduce the risk of human error.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Automate configuration management: Enforce and validate secure configurations automatically by
using a configuration management service or tool.
• AWS Systems Manager
• AWS CloudFormation
• Lab: Automated deployment of VPC
• Lab: Automated deployment of EC2 web application
• Automate patching of Amazon Elastic Compute Cloud (Amazon EC2) instances: AWS Systems Manager
Patch Manager automates the process of patching managed instances with both security-related and
other types of updates. You can use Patch Manager to apply patches for both operating systems and
applications.
• AWS Systems Manager Patch Manager
• Centralized multi-account and multi-Region patching with AWS Systems Manager Automation
• Implement intrusion detection and prevention: Implement an intrusion detection and prevention tool
to monitor and stop malicious activity on instances.
• Consider AWS Partner solutions: AWS Partners offer hundreds of industry-leading products that are
equivalent, identical to, or integrate with existing controls in your on-premises environments. These
products complement the existing AWS services to allow you to deploy a comprehensive security
architecture and a more seamless experience across your cloud and on-premises environments.
• Infrastructure security
Resources
Related documents:
• AWS CloudFormation
• AWS Systems Manager
• AWS Systems Manager Patch Manager
• Centralized multi-account and multi-region patching with AWS Systems Manager Automation
• Infrastructure security
• Replacing a Bastion Host with Amazon EC2 Systems Manager
• Security Overview of AWS Lambda
Related videos:
• Running high-security workloads on Amazon EKS
• Securing Serverless and Container Services
• Security best practices for the Amazon EC2 instance metadata service
Related examples:
207

AWS Well-Architected Framework
Infrastructure protection
• Lab: Automated Deployment of Web Application Firewall
• Lab: Automated deployment of Amazon EC2 web application
SEC06-BP05 Enable people to perform actions at a distance
Removing the ability for interactive access reduces the risk of human error, and the potential for manual
configuration or management. For example, use a change management workflow to deploy Amazon
Elastic Compute Cloud (Amazon EC2) instances using infrastructure-as-code, then manage Amazon
EC2 instances using tools such as AWS Systems Manager instead of allowing direct access or through
a bastion host. AWS Systems Manager can automate a variety of maintenance and deployment tasks,
using features including automation workflows, documents (playbooks), and the run command. AWS
CloudFormation stacks build from pipelines and can automate your infrastructure deployment and
management tasks without using the AWS Management Console or APIs directly.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Replace console access: Replace console access (SSH or RDP) to instances with AWS Systems Manager
Run Command to automate management tasks.
• AWS Systems Manager Run Command
Resources
Related documents:
• AWS Systems Manager
• AWS Systems Manager Run Command
• Replacing a Bastion Host with Amazon EC2 Systems Manager
• Security Overview of AWS Lambda
Related videos:
• Running high-security workloads on Amazon EKS
• Securing Serverless and Container Services
• Security best practices for the Amazon EC2 instance metadata service
Related examples:
• Lab: Automated Deployment of Web Application Firewall
SEC06-BP06 Validate software integrity
Implement mechanisms (for example, code signing) to validate that the software, code and libraries
used in the workload are from trusted sources and have not been tampered with. For example, you
should verify the code signing certificate of binaries and scripts to confirm the author, and ensure it
has not been tampered with since created by the author. AWS Signer can help ensure the trust and
integrity of your code by centrally managing the code- signing lifecycle, including signing certification
and public and private keys. You can learn how to use advanced patterns and best practices for code
signing with AWS Lambda. Additionally, a checksum of software that you download, compared to that of
the checksum from the provider, can help ensure it has not been tampered with.
208

AWS Well-Architected Framework
Data protection
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Investigate mechanisms: Code signing is one mechanism that can be used to validate software
integrity.
• NIST: Security Considerations for Code Signing
Resources
Related documents:
• AWS Signer
• New - Code Signing, a Trust and Integrity Control for AWS Lambda
Data protection
Questions
• SEC 7. How do you classify your data? (p. 209)
• SEC 8. How do you protect your data at rest?  (p. 214)
• SEC 9. How do you protect your data in transit?  (p. 220)
SEC 7. How do you classify your data?
Classification provides a way to categorize data, based on criticality and sensitivity in order to help you
determine appropriate protection and retention controls.
Best practices
• SEC07-BP01 Identify the data within your workload (p. 209)
• SEC07-BP02 Define data protection controls (p. 212)
• SEC07-BP03 Automate identification and classification (p. 213)
• SEC07-BP04 Define data lifecycle management (p. 214)
SEC07-BP01 Identify the data within your workload
It’s critical to understand the type and classification of data your workload is processing, the associated
business processes, where the data is stored, and who is the data owner. You should also have an
understanding of the applicable legal and compliance requirements of your workload, and what data
controls need to be enforced. Identifying data is the first step in the data classification journey.
Benefits of establishing this best practice:
Data classification allows workload owners to identify locations that store sensitive data and determine
how that data should be accessed and shared.
Data classification aims to answer the following questions:
• What type of data do you have?
This could be data such as:
• Intellectual property (IP) such as trade secrets, patents, or contract agreements.
209

AWS Well-Architected Framework
Data protection
• Protected health information (PHI) such as medical records that contain medical history information
connected to an individual.
• Personally identifiable information (PII), such as name, address, date of birth, and national ID or
registration number.
• Credit card data, such as the Primary Account Number (PAN), cardholder name, expiration date, and
service code number.
• Where is the sensitive data is stored?
• Who can access, modify, and delete data?
• Understanding user permissions is essential in guarding against potential data mishandling.
• Who can perform create, read, update, and delete (CRUD) operations?
• Account for potential escalation of privileges by understanding who can manage permissions to the
data.
• What business impact might occur if the data is disclosed unintentionally, altered, or deleted?
• Understand the risk consequence if data is modified, deleted, or inadvertently disclosed.
By knowing the answers to these questions, you can take the following actions:
• Decrease sensitive data scope (such as the number of sensitive data locations) and limit access to
sensitive data to only approved users.
• Gain an understanding of different data types so that you can implement appropriate data protection
mechanisms and techniques, such as encryption, data loss prevention, and identity and access
management.
• Optimize costs by delivering the right control objectives for the data.
• Confidently answer questions from regulators and auditors regarding the types and amount of data,
and how data of different sensitivities are isolated from each other.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Data classification is the act of identifying the sensitivity of data. It might involve tagging to make the
data easily searchable and trackable. Data classification also reduces the duplication of data, which can
help reduce storage and backup costs while speeding up the search process.
Use services such as Amazon Macie to automate at scale both the discovery and classification of sensitive
data. Other services, such as Amazon EventBridge and AWS Config, can be used to automate remediation
for data security issues such as unencrypted Amazon Simple Storage Service (Amazon S3) buckets and
Amazon EC2 EBS volumes or untagged data resources. For a complete list of AWS service integrations,
see the EventBridge documentation.
Detecting PII in unstructured data such as customer emails, support tickets, product reviews, and social
media, is possible by using Amazon Comprehend, which is a natural language processing (NLP) service
that uses machine learning (ML) to find insights and relationships like people, places, sentiments,
and topics in unstructured text. For a list of AWS services that can assist with data identification, see
Common techniques to detect PHI and PII data using AWS services.
Another method that supports data classification and protection is AWS resource tagging. Tagging
allows you to assign metadata to your AWS resources that you can use to manage, identify, organize,
search for, and filter resources.
In some cases, you might choose to tag entire resources (such as an S3 bucket), especially when a specific
workload or service is expected to store processes or transmissions of already known data classification.
Where appropriate, you can tag an S3 bucket instead of individual objects for ease of administration and
security maintenance.
210

AWS Well-Architected Framework
Data protection
Implementation steps
Detect sensitive data within Amazon S3:
1. Before starting, make sure you have the appropriate permissions to access the Amazon Macie console
and API operations. For additional details, see Getting started with Amazon Macie.
2. Use Amazon Macie to perform automated data discovery when your sensitive data resides in Amazon
S3.
• Use the Getting Started with Amazon Macie guide to configure a repository for sensitive data
discovery results and create a discovery job for sensitive data.
• How to use Amazon Macie to preview sensitive data in S3 buckets.
By default, Macie analyzes objects by using the set of managed data identifiers that we recommend
for automated sensitive data discovery. You can tailor the analysis by configuring Macie to use
specific managed data identifiers, custom data identifiers, and allow lists when it performs
automated sensitive data discovery for your account or organization. You can adjust the scope of
the analysis by excluding specific buckets (for example, S3 buckets that typically store AWS logging
data).
3. To configure and use automated sensitive data discovery, see Performing automated sensitive data
discovery with Amazon Macie.
4. You might also consider Automated Data Discovery for Amazon Macie.
Detect sensitive data within Amazon RDS:
For more information on data discovery in Amazon Relational Database Service (Amazon RDS) databases,
see Enabling data classification for Amazon RDS database with Macie.
Detect sensitive data within DynamoDB:
• Detecting sensitive data in DynamoDB with Macie explains how to use Amazon Macie to detect
sensitive data in Amazon DynamoDB tables by exporting the data to Amazon S3 for scanning.
AWS Partner solutions:
• Consider using our extensive AWS Partner Network. AWS Partners have extensive tools and compliance
frameworks that directly integrate with AWS services. Partners can provide you with a tailored
governance and compliance solution to help you meet your organizational needs.
• For customized solutions in data classification, see Data governance in the age of regulation and
compliance requirements.
You can automatically enforce the tagging standards that your organization adopts by creating and
deploying policies using AWS Organizations. Tag policies let you specify rules that define valid key
names and what values are valid for each key. You can choose to monitor only, which gives you an
opportunity to evaluate and clean up your existing tags. After your tags are in compliance with your
chosen standards, you can turn on enforcement in the tag policies to prevent non-compliant tags from
being created. For more details, see Securing resource tags used for authorization using a service control
policy in AWS Organizations and the example policy on preventing tags from being modified except by
authorized principals.
• To begin using tag policies in AWS Organizations, it’s strongly recommended that you follow the
workflow in Getting started with tag policies before moving on to more advanced tag policies.
Understanding the effects of attaching a simple tag policy to a single account before expanding to
an entire organizational unit (OU) or organization allows you to see a tag policy’s effects before you
enforce compliance with the tag policy. Getting started with tag policies provides links to instructions
for more advanced policy-related tasks.
211

AWS Well-Architected Framework
Data protection
• Consider evaluating other AWS services and features that support data classification, which are listed
in the Data Classification whitepaper.
Resources
Related documents:
• Getting Started with Amazon Macie
• Automated data discovery with Amazon Macie
• Getting started with tag policies
• Detecting PII entities
Related blogs:
• How to use Amazon Macie to preview sensitive data in S3 buckets.
• Performing automated sensitive data discovery with Amazon Macie.
• Common techniques to detect PHI and PII data using AWS Services
• Detecting and redacting PII using Amazon Comprehend
• Securing resource tags used for authorization using a service control policy in AWS Organizations
• Enabling data classification for Amazon RDS database with Macie
• Detecting sensitive data in DynamoDB with Macie
•
Related videos:
• Event-driven data security using Amazon Macie
• Amazon Macie for data protection and governance
• Fine-tune sensitive data findings with allow lists
SEC07-BP02 Define data protection controls
Protect data according to its classification level. For example, secure data classified as public by using
relevant recommendations while protecting sensitive data with additional controls.
By using resource tags, separate AWS accounts per sensitivity (and potentially also for each caveat,
enclave, or community of interest), IAM policies, AWS Organizations SCPs, AWS Key Management Service
(AWS KMS), and AWS CloudHSM, you can define and implement your policies for data classification
and protection with encryption. For example, if you have a project with S3 buckets that contain highly
critical data or Amazon Elastic Compute Cloud (Amazon EC2) instances that process confidential data,
they can be tagged with a Project=ABC tag. Only your immediate team knows what the project code
means, and it provides a way to use attribute-based access control. You can define levels of access to the
AWS KMS encryption keys through key policies and grants to ensure that only appropriate services have
access to the sensitive content through a secure mechanism. If you are making authorization decisions
based on tags you should make sure that the permissions on the tags are defined appropriately using tag
policies in AWS Organizations.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Define your data identification and classification schema: Identification and classification of your data
is performed to assess the potential impact and type of data you store, and who can access it.
212

AWS Well-Architected Framework
Data protection
• AWS Documentation
• Discover available AWS controls: For the AWS services you are or plan to use, discover the security
controls. Many services have a security section in their documentation.
• AWS Documentation
• Identify AWS compliance resources: Identify resources that AWS has available to assist.
• https://aws.amazon.com/compliance/
Resources
Related documents:
• AWS Documentation
• Data Classification whitepaper
• Getting started with Amazon Macie
• AWS Compliance
Related videos:
• Introducing the New Amazon Macie
SEC07-BP03 Automate identification and classification
Automating the identification and classification of data can help you implement the correct controls.
Using automation for this instead of direct access from a person reduces the risk of human error and
exposure. You should evaluate using a tool, such as Amazon Macie, that uses machine learning to
automatically discover, classify, and protect sensitive data in AWS. Amazon Macie recognizes sensitive
data, such as personally identifiable information (PII) or intellectual property, and provides you with
dashboards and alerts that give visibility into how this data is being accessed or moved.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Use Amazon Simple Storage Service (Amazon S3) Inventory: Amazon S3 inventory is one of the tools
you can use to audit and report on the replication and encryption status of your objects.
• Amazon S3 Inventory
• Consider Amazon Macie: Amazon Macie uses machine learning to automatically discover and classify
data stored in Amazon S3.
• Amazon Macie
Resources
Related documents:
• Amazon Macie
• Amazon S3 Inventory
• Data Classification Whitepaper
• Getting started with Amazon Macie
Related videos:
213

AWS Well-Architected Framework
Data protection
• Introducing the New Amazon Macie
SEC07-BP04 Define data lifecycle management
Your defined lifecycle strategy should be based on sensitivity level as well as legal and organization
requirements. Aspects including the duration for which you retain data, data destruction processes, data
access management, data transformation, and data sharing should be considered. When choosing a data
classification methodology, balance usability versus access. You should also accommodate the multiple
levels of access and nuances for implementing a secure, but still usable, approach for each level. Always
use a defense in depth approach and reduce human access to data and mechanisms for transforming,
deleting, or copying data. For example, require users to strongly authenticate to an application, and give
the application, rather than the users, the requisite access permission to perform action at a distance. In
addition, ensure that users come from a trusted network path and require access to the decryption keys.
Use tools, such as dashboards and automated reporting, to give users information from the data rather
than giving them direct access to the data.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Identify data types: Identify the types of data that you are storing or processing in your workload. That
data could be text, images, binary databases, and so forth.
Resources
Related documents:
• Data Classification Whitepaper
• Getting started with Amazon Macie
Related videos:
• Introducing the New Amazon Macie
SEC 8. How do you protect your data at rest?
Protect your data at rest by implementing multiple controls, to reduce the risk of unauthorized access or
mishandling.
Best practices
• SEC08-BP01 Implement secure key management (p. 214)
• SEC08-BP02 Enforce encryption at rest (p. 215)
• SEC08-BP03 Automate data at rest protection (p. 217)
• SEC08-BP04 Enforce access control (p. 218)
• SEC08-BP05 Use mechanisms to keep people away from data (p. 219)
SEC08-BP01 Implement secure key management
By defining an encryption approach that includes the storage, rotation, and access control of keys,
you can help provide protection for your content against unauthorized users and against unnecessary
exposure to authorized users. AWS Key Management Service (AWS KMS) helps you manage encryption
keys and integrates with many AWS services. This service provides durable, secure, and redundant
214

AWS Well-Architected Framework
Data protection
storage for your AWS KMS keys. You can define your key aliases as well as key-level policies. The policies
help you define key administrators as well as key users. Additionally, AWS CloudHSM is a cloud-based
hardware security module (HSM) that allows you to easily generate and use your own encryption keys
in the AWS Cloud. It helps you meet corporate, contractual, and regulatory compliance requirements for
data security by using FIPS 140-2 Level 3 validated HSMs.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Implement AWS KMS: AWS KMS makes it easy for you to create and manage keys and control the use
of encryption across a wide range of AWS services and in your applications. AWS KMS is a secure and
resilient service that uses FIPS 140-2 validated hardware security modules to protect your keys.
• Getting started: AWS Key Management Service (AWS KMS)
• Consider AWS Encryption SDK: Use the AWS Encryption SDK with AWS KMS integration when your
application needs to encrypt data client-side.
• AWS Encryption SDK
Resources
Related documents:
• AWS Key Management Service
• AWS cryptographic services and tools
• Getting started: AWS Key Management Service (AWS KMS)
• Protecting Amazon S3 Data Using Encryption
Related videos:
• How Encryption Works in AWS
• Securing Your Block Storage on AWS
SEC08-BP02 Enforce encryption at rest
You should enforce the use of encryption for data at rest. Encryption maintains the confidentiality of
sensitive data in the event of unauthorized access or accidental disclosure.
Desired outcome: Private data should be encrypted by default when at rest. Encryption helps maintain
confidentiality of the data and provides an additional layer of protection against intentional or
inadvertent data disclosure or exfiltration. Data that is encrypted cannot be read or accessed without
first unencrypting the data. Any data stored unencrypted should be inventoried and controlled.
Common anti-patterns:
• Not using encrypt-by-default configurations.
• Providing overly permissive access to decryption keys.
• Not monitoring the use of encryption and decryption keys.
• Storing data unencrypted.
• Using the same encryption key for all data regardless of data usage, types, and classification.
Level of risk exposed if this best practice is not established: High
215

AWS Well-Architected Framework
Data protection
Implementation guidance
Map encryption keys to data classifications within your workloads. This approach helps protect against
overly permissive access when using either a single, or very small number of encryption keys for your
data (see SEC07-BP01 Identify the data within your workload (p. 209)).
AWS Key Management Service (AWS KMS) integrates with many AWS services to make it easier to
encrypt your data at rest. For example, in Amazon Simple Storage Service (Amazon S3), you can set
default encryption on a bucket so that new objects are automatically encrypted. When using AWS KMS,
consider how tightly the data needs to be restricted. Default and service-controlled AWS KMS keys are
managed and used on your behalf by AWS. For sensitive data that requires fine-grained access to the
underlying encryption key, consider customer managed keys (CMKs). You have full control over CMKs,
including rotation and access management through the use of key policies.
Additionally, Amazon Elastic Compute Cloud (Amazon EC2) and Amazon S3 support the enforcement
of encryption by setting default encryption. You can use AWS Config Rules to check automatically that
you are using encryption, for example, for Amazon Elastic Block Store (Amazon EBS) volumes, Amazon
Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets.
AWS also provides options for client-side encryption, allowing you to encrypt data prior to uploading it
to the cloud. The AWS Encryption SDK provides a way to encrypt your data using envelope encryption.
You provide the wrapping key, and the AWS Encryption SDK generates a unique data key for each data
object it encrypts. Consider AWS CloudHSM if you need a managed single-tenant hardware security
module (HSM). AWS CloudHSM allows you to generate, import, and manage cryptographic keys on a
FIPS 140-2 level 3 validated HSM. Some use cases for AWS CloudHSM include protecting private keys
for issuing a certificate authority (CA), and turning on transparent data encryption (TDE) for Oracle
databases. The AWS CloudHSM Client SDK provides software that allows you to encrypt data client side
using keys stored inside AWS CloudHSM prior to uploading your data into AWS. The Amazon DynamoDB
Encryption Client also allows you to encrypt and sign items prior to upload into a DynamoDB table.
Implementation steps
• Enforce encryption at rest for Amazon S3: Implement Amazon S3 bucket default encryption.
Configure default encryption for new Amazon EBS volumes: Specify that you want all newly created
Amazon EBS volumes to be created in encrypted form, with the option of using the default key
provided by AWS or a key that you create.
Configure encrypted Amazon Machine Images (AMIs): Copying an existing AMI with encryption
configured will automatically encrypt root volumes and snapshots.
Configure Amazon RDS encryption: Configure encryption for your Amazon RDS database clusters and
snapshots at rest by using the encryption option.
Create and configure AWS KMS keys with policies that limit access to the appropriate principals for
each classification of data: For example, create one AWS KMS key for encrypting production data and
a different key for encrypting development or test data. You can also provide key access to other AWS
accounts. Consider having different accounts for your development and production environments.
If your production environment needs to decrypt artifacts in the development account, you can edit
the CMK policy used to encrypt the development artifacts to give the production account the ability
to decrypt those artifacts. The production environment can then ingest the decrypted data for use in
production.
Configure encryption in additional AWS services: For other AWS services you use, review the security
documentation for that service to determine the service’s encryption options.
Resources
Related documents:
216

AWS Well-Architected Framework
Data protection
• AWS Crypto Tools
• AWS Encryption SDK
• AWS KMS Cryptographic Details Whitepaper
• AWS Key Management Service
• AWS cryptographic services and tools
• Amazon EBS Encryption
• Default encryption for Amazon EBS volumes
• Encrypting Amazon RDS Resources
• How do I enable default encryption for an Amazon S3 bucket?
• Protecting Amazon S3 Data Using Encryption
Related videos:
• How Encryption Works in AWS
• Securing Your Block Storage on AWS
SEC08-BP03 Automate data at rest protection
Use automated tools to validate and enforce data at rest controls continuously, for example, verify
that there are only encrypted storage resources. You can automate validation that all EBS volumes are
encrypted using AWS Config Rules. AWS Security Hub can also verify several different controls through
automated checks against security standards. Additionally, your AWS Config Rules can automatically
remediate noncompliant resources.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Data at rest represents any data that you persist in non-volatile storage for any duration in your
workload. This includes block storage, object storage, databases, archives, IoT devices, and any other
storage medium on which data is persisted. Protecting your data at rest reduces the risk of unauthorized
access, when encryption and appropriate access controls are implemented.
Enforce encryption at rest: You should ensure that the only way to store data is by using encryption. AWS
KMS integrates seamlessly with many AWS services to make it easier for you to encrypt all your data at
rest. For example, in Amazon Simple Storage Service (Amazon S3) you can set default encryption on a
bucket so that all new objects are automatically encrypted. Additionally, Amazon EC2 and Amazon S3
support the enforcement of encryption by setting default encryption. You can use AWS Managed Config
Rules to check automatically that you are using encryption, for example, for EBS volumes, Amazon
Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets.
Resources
Related documents:
• AWS Crypto Tools
• AWS Encryption SDK
Related videos:
• How Encryption Works in AWS
• Securing Your Block Storage on AWS
217

AWS Well-Architected Framework
Data protection
SEC08-BP04 Enforce access control
To help protect your data at rest, enforce access control using mechanisms, such as isolation and
versioning, and apply the principle of least privilege. Prevent the granting of public access to your data.
Desired outcome: Verify that only authorized users can access data on a need-to-know basis. Protect
your data with regular backups and versioning to prevent against intentional or inadvertent modification
or deletion of data. Isolate critical data from other data to protect its confidentiality and data integrity.
Common anti-patterns:
• Storing data with different sensitivity requirements or classification together.
• Using overly permissive permissions on decryption keys.
• Improperly classifying data.
• Not retaining detailed backups of important data.
• Providing persistent access to production data.
• Not auditing data access or regularly reviewing permissions.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Multiple controls can help protect your data at rest, including access (using least privilege), isolation, and
versioning. Access to your data should be audited using detective mechanisms, such as AWS CloudTrail,
and service level logs, such as Amazon Simple Storage Service (Amazon S3) access logs. You should
inventory what data is publicly accessible, and create a plan to reduce the amount of publicly available
data over time.
Amazon S3 Glacier Vault Lock and Amazon S3 Object Lock provide mandatory access control for objects
in Amazon S3—once a vault policy is locked with the compliance option, not even the root user can
change it until the lock expires.
Implementation steps
• Enforce access control: Enforce access control with least privileges, including access to encryption
keys.
• Separate data based on different classification levels: Use different AWS accounts for data
classification levels, and manage those accounts using AWS Organizations.
• Review AWS Key Management Service (AWS KMS) policies: Review the level of access granted in
AWS KMS policies.
• Review Amazon S3 bucket and object permissions: Regularly review the level of access granted in S3
bucket policies. Best practice is to avoid using publicly readable or writeable buckets. Consider using
AWS Config to detect buckets that are publicly available, and Amazon CloudFront to serve content
from Amazon S3. Verify that buckets that should not allow public access are properly configured to
prevent public access. By default, all S3 buckets are private, and can only be accessed by users that
have been explicitly granted access.
• Use AWS IAM Access Analyzer: IAM Access Analyzer analyzes Amazon S3 buckets and generates a
finding when an S3 policy grants access to an external entity.
• Use Amazon S3 versioning and object lock when appropriate.
• Use Amazon S3 Inventory: Amazon S3 Inventory can be used to audit and report on the replication
and encryption status of your S3 objects.
• Review Amazon EBS and AMI sharing permissions: Sharing permissions can allow images and
volumes to be shared with AWS accounts that are external to your workload.
• Review AWS Resource Access Manager Shares periodically to determine whether resources should
continue to be shared. Resource Access Manager allows you to share resources, such as AWS Network
218

AWS Well-Architected Framework
Data protection
Firewall policies, Amazon Route 53 resolver rules, and subnets, within your Amazon VPCs. Audit shared
resources regularly and stop sharing resources which no longer need to be shared.
Resources
Related best practices:
• SEC03-BP01 Define access requirements (p. 175)
• SEC03-BP02 Grant least privilege access (p. 177)
Related documents:
• AWS KMS Cryptographic Details Whitepaper
• Introduction to Managing Access Permissions to Your Amazon S3 Resources
• Overview of managing access to your AWS KMS resources
• AWS Config Rules
• Amazon S3 + Amazon CloudFront: A Match Made in the Cloud
• Using versioning
• Locking Objects Using Amazon S3 Object Lock
• Sharing an Amazon EBS Snapshot
• Shared AMIs
• Hosting a single-page application on Amazon S3
Related videos:
• Securing Your Block Storage on AWS
SEC08-BP05 Use mechanisms to keep people away from data
Keep all users away from directly accessing sensitive data and systems under normal operational
circumstances. For example, use a change management workflow to manage Amazon Elastic Compute
Cloud (Amazon EC2) instances using tools instead of allowing direct access or a bastion host. This can
be achieved using AWS Systems Manager Automation, which uses automation documents that contain
steps you use to perform tasks. These documents can be stored in source control, be peer reviewed
before running, and tested thoroughly to minimize risk compared to shell access. Business users could
have a dashboard instead of direct access to a data store to run queries. Where CI/CD pipelines are not
used, determine which controls and processes are required to adequately provide a normally deactivated
break-glass access mechanism.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Implement mechanisms to keep people away from data: Mechanisms include using dashboards, such
as Amazon QuickSight, to display data to users instead of directly querying.
• Amazon QuickSight
• Automate configuration management: Perform actions at a distance, enforce and validate secure
configurations automatically by using a configuration management service or tool. Avoid use of
bastion hosts or directly accessing EC2 instances.
• AWS Systems Manager
• AWS CloudFormation
219

AWS Well-Architected Framework
Data protection
• CI/CD Pipeline for AWS CloudFormation templates on AWS
Resources
Related documents:
• AWS KMS Cryptographic Details Whitepaper
Related videos:
• How Encryption Works in AWS
• Securing Your Block Storage on AWS
SEC 9. How do you protect your data in transit?
Protect your data in transit by implementing multiple controls to reduce the risk of unauthorized access
or loss.
Best practices
• SEC09-BP01 Implement secure key and certificate management (p. 220)
• SEC09-BP02 Enforce encryption in transit (p. 221)
• SEC09-BP03 Automate detection of unintended data access (p. 222)
• SEC09-BP04 Authenticate network communications (p. 222)
SEC09-BP01 Implement secure key and certificate management
Store encryption keys and certificates securely and rotate them at appropriate time intervals with strict
access control. The best way to accomplish this is to use a managed service, such as AWS Certificate
Manager (ACM). It lets you easily provision, manage, and deploy public and private Transport Layer
Security (TLS) certificates for use with AWS services and your internal connected resources. TLS
certificates are used to secure network communications and establish the identity of websites over the
internet as well as resources on private networks. ACM integrates with AWS resources, such as Elastic
Load Balancers (ELBs), AWS distributions, and APIs on API Gateway, also handling automatic certificate
renewals. If you use ACM to deploy a private root CA, both certificates and private keys can be provided
by it for use in Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, and so on.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Implement secure key and certificate management: Implement your defined secure key and certificate
management solution.
• AWS Certificate Manager
• How to host and manage an entire private certificate infrastructure in AWS
• Implement secure protocols: Use secure protocols that offer authentication and confidentiality, such
as Transport Layer Security (TLS) or IPsec, to reduce the risk of data tampering or loss. Check the AWS
documentation for the protocols and security relevant to the services that you are using.
Resources
Related documents:
• AWS Documentation
220

AWS Well-Architected Framework
Data protection
SEC09-BP02 Enforce encryption in transit
Enforce your defined encryption requirements based on your organization’s policies, regulatory
obligations and standards to help meet organizational, legal, and compliance requirements. Only use
protocols with encryption when transmitting sensitive data outside of your virtual private cloud (VPC).
Encryption helps maintain data confidentiality even when the data transits untrusted networks.
Desired outcome: All data should be encrypted in transit using secure TLS protocols and cipher suites.
Network traffic between your resources and the internet must be encrypted to mitigate unauthorized
access to the data. Network traffic solely within your internal AWS environment should be encrypted
using TLS wherever possible. The AWS internal network is encrypted by default and network traffic
within a VPC cannot be spoofed or sniffed unless an unauthorized party has gained access to whatever
resource is generating traffic (such as Amazon EC2 instances, and Amazon ECS containers). Consider
protecting network-to-network traffic with an IPsec virtual private network (VPN).
Common anti-patterns:
• Using deprecated versions of SSL, TLS, and cipher suite components (for example, SSL v3.0, 1024-bit
RSA keys, and RC4 cipher).
• Allowing unencrypted (HTTP) traffic to or from public-facing resources.
• Not monitoring and replacing X.509 certificates prior to expiration.
• Using self-signed X.509 certificates for TLS.
Level of risk exposed if this best practice is not established: High
Implementation guidance
AWS services provide HTTPS endpoints using TLS for communication, providing encryption in transit
when communicating with the AWS APIs. Insecure protocols like HTTP can be audited and blocked in a
VPC through the use of security groups. HTTP requests can also be automatically redirected to HTTPS
in Amazon CloudFront or on an Application Load Balancer. You have full control over your computing
resources to implement encryption in transit across your services. Additionally, you can use VPN
connectivity into your VPC from an external network or AWS Direct Connect to facilitate encryption of
traffic. Verify that your clients are making calls to AWS APIs using at least TLS 1.2, as AWS is deprecating
the use of earlier versions of TLS in June 2023. AWS recommends using TLS 1.3. Third-party solutions are
available in the AWS Marketplace if you have special requirements.
Implementation steps
• Enforce encryption in transit: Your defined encryption requirements should be based on the latest
standards and best practices and only allow secure protocols. For example, configure a security group
to only allow the HTTPS protocol to an application load balancer or Amazon EC2 instance.
• Configure secure protocols in edge services: Configure HTTPS with Amazon CloudFront and use a
security profile appropriate for your security posture and use case.
• Use a VPN for external connectivity: Consider using an IPsec VPN for securing point-to-point or
network-to-network connections to help provide both data privacy and integrity.
• Configure secure protocols in load balancers: Select a security policy that provides the strongest
cipher suites supported by the clients that will be connecting to the listener. Create an HTTPS listener
for your Application Load Balancer.
• Configure secure protocols in Amazon Redshift: Configure your cluster to require a secure socket
layer (SSL) or transport layer security (TLS) connection.
• Configure secure protocols: Review AWS service documentation to determine encryption-in-transit
capabilities.
• Configure secure access when uploading to Amazon S3 buckets: Use Amazon S3 bucket policy
controls to enforce secure access to data.
221

AWS Well-Architected Framework
Data protection
• Consider using AWS Certificate Manager: ACM allows you to provision, manage, and deploy public
TLS certificates for use with AWS services.
• Consider using AWS Private Certificate Authority for private PKI needs: AWS Private CA allows you
to create private certificate authority (CA) hierarchies to issue end-entity X.509 certificates that can be
used to create encrypted TLS channels.
Resources
Related documents:
• Using HTTPS with CloudFront
• Connect your VPC to remote networks using AWS Virtual Private Network
• Create an HTTPS listener for your Application Load Balancer
• Tutorial: Configure SSL/TLS on Amazon Linux 2
• Using SSL/TLS to encrypt a connection to a DB instance
• Configuring security options for connections
SEC09-BP03 Automate detection of unintended data access
Use tools such as Amazon GuardDuty to automatically detect suspicious activity or attempts to move
data outside of defined boundaries. For example, GuardDuty can detect Amazon Simple Storage Service
(Amazon S3) read activity that is unusual with the Exfiltration:S3/AnomalousBehavior finding. In
addition to GuardDuty, Amazon VPC Flow Logs, which capture network traffic information, can be used
with Amazon EventBridge to detect connections, both successful and denied. Amazon S3 Access Analyzer
can help assess what data is accessible to who in your Amazon S3 buckets.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Automate detection of unintended data access: Use a tool or detection mechanism to automatically
detect attempts to move data outside of defined boundaries, for example, to detect a database system
that is copying data to an unrecognized host.
• VPC Flow Logs
• Consider Amazon Macie: Amazon Macie is a fully managed data security and data privacy service that
uses machine learning and pattern matching to discover and protect your sensitive data in AWS.
• Amazon Macie
Resources
Related documents:
• VPC Flow Logs
• Amazon Macie
SEC09-BP04 Authenticate network communications
Verify the identity of communications by using protocols that support authentication, such as Transport
Layer Security (TLS) or IPsec.
Using network protocols that support authentication, allows for trust to be established between the
parties. This adds to the encryption used in the protocol to reduce the risk of communications being
altered or intercepted. Common protocols that implement authentication include Transport Layer
222

AWS Well-Architected Framework
Incident response
Security (TLS), which is used in many AWS services, and IPsec, which is used in AWS Virtual Private
Network (AWS VPN).
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Implement secure protocols: Use secure protocols that offer authentication and confidentiality, such
as TLS or IPsec, to reduce the risk of data tampering or loss. Check the AWS documentation for the
protocols and security relevant to the services you are using.
Resources
Related documents:
• AWS Documentation
Incident response
Question
• SEC 10. How do you anticipate, respond to, and recover from incidents?  (p. 223)
SEC 10. How do you anticipate, respond to, and recover from
incidents?
Preparation is critical to timely and effective investigation, response to, and recovery from security
incidents to help minimize disruption to your organization.
Best practices
• SEC10-BP01 Identify key personnel and external resources (p. 223)
• SEC10-BP02 Develop incident management plans (p. 224)
• SEC10-BP03 Prepare forensic capabilities (p. 226)
• SEC10-BP04 Automate containment capability (p. 227)
• SEC10-BP05 Pre-provision access (p. 228)
• SEC10-BP06 Pre-deploy tools (p. 231)
• SEC10-BP07 Run game days (p. 232)
SEC10-BP01 Identify key personnel and external resources
Identify internal and external personnel, resources, and legal obligations that would help your
organization respond to an incident.
When you define your approach to incident response in the cloud, in unison with other teams (such
as your legal counsel, leadership, business stakeholders, AWS Support Services, and others), you must
identify key personnel, stakeholders, and relevant contacts. To reduce dependency and decrease
response time, make sure that your team, specialist security teams, and responders are educated about
the services that you use and have opportunities to practice hands-on.
We encourage you to identify external AWS security partners that can provide you with outside expertise
and a different perspective to augment your response capabilities. Your trusted security partners can
help you identify potential risks or threats that you might not be familiar with.
Level of risk exposed if this best practice is not established: High
223

AWS Well-Architected Framework
Incident response
Implementation guidance
• Identify key personnel in your organization: Maintain a contact list of personnel within your
organization that you would need to involve to respond to and recover from an incident.
• Identify external partners: Engage with external partners if necessary that can help you respond to and
recover from an incident.
Resources
Related documents:
• AWS Incident Response Guide
Related videos:
• Prepare for and respond to security incidents in your AWS environment
Related examples:
SEC10-BP02 Develop incident management plans
This best practice was updated with new guidance on July 13th, 2023.
The first document to develop for incident response is the incident response plan. The incident response
plan is designed to be the foundation for your incident response program and strategy.
Benefits of establishing this best practice: Developing thorough and clearly defined incident response
processes is key to a successful and scalable incident response program. When a security event occurs,
clear steps and workflows will help you to respond in a timely manner. You might already have existing
incident response processes. Regardless of your current state, it’s important to update, iterate, and test
your incident response processes regularly.
Level of risk exposed if this best practice is not established: High
Implementation guidance
An incident management plan is critical to respond, mitigate, and recover from the potential impact of
security incidents. An incident management plan is a structured process for identifying, remediating, and
responding in a timely matter to security incidents.
The cloud has many of the same operational roles and requirements found in an on-premises
environment. When creating an incident management plan, it is important to factor response and
recovery strategies that best align with your business outcome and compliance requirements. For
example, if you are operating workloads in AWS that are FedRAMP compliant in the United States,
it’s useful to adhere to NIST SP 800-61 Computer Security Handling Guide. Similarly, when operating
workloads with European personally identifiable information (PII) data, consider scenarios like how
you might protect and respond to issues related to data residency as mandated by EU General Data
Protection Regulation (GDPR) Regulations.
When building an incident management plan for your workloads operating in AWS, start with the AWS
Shared Responsibility Model for building a defense-in-depth approach towards incident response. In this
model, AWS manages security of the cloud, and you are responsible for security in the cloud. This means
that you retain control and are responsible for the security controls you choose to implement. The AWS
Security Incident Response Guide details key concepts and foundational guidance for building a cloud-
centric incident management plan.
224

AWS Well-Architected Framework
Incident response
An effective incident management plan must be continually iterated upon, remaining current with your
cloud operations goal. Consider using the implementation plans detailed below as you create and evolve
your incident management plan.
Implementation steps
Define roles and responsibilities
Handling security events requires cross-organizational discipline and an inclination for action. Within
your organizational structure, there should be many people who are responsible, accountable, consulted,
or kept informed during an incident, such as representatives from human resources (HR), the executive
team, and legal. Consider these roles and responsibilities, and whether any third parties must be
involved. Note that many geographies have local laws that govern what should and should not be done.
Although it might seem bureaucratic to build a responsible, accountable, consulted, and informed (RACI)
chart for your security response plans, doing so facilitates quick and direct communication and clearly
outlines the leadership across different stages of the event.
During an incident, including the owners and developers of impacted applications and resources is
key because they are subject matter experts (SMEs) that can provide information and context to aid in
measuring impact. Make sure to practice and build relationships with the developers and application
owners before you rely on their expertise for incident response. Application owners or SMEs, such as your
cloud administrators or engineers, might need to act in situations where the environment is unfamiliar or
has complexity, or where the responders don’t have access.
Lastly, trusted partners might be involved in the investigation or response because they can provide
additional expertise and valuable scrutiny. When you don’t have these skills on your own team, you
might want to hire an external party for assistance.
Understand AWS response teams and support
•                                                                                                           AWS Support
• AWS Support offers a range of plans that provide access to tools and expertise that support the
success and operational health of your AWS solutions. If you need technical support and more
resources to help plan, deploy, and optimize your AWS environment, you can select a support plan
that best aligns with your AWS use case.
• Consider the Support Center in AWS Management Console (sign-in required) as the central point
of contact to get support for issues that affect your AWS resources. Access to AWS Support is
controlled by AWS Identity and Access Management. For more information about getting access to
AWS Support features, see Getting started with AWS Support.
•                                                                                                           AWS Customer Incident Response Team (CIRT)
• The AWS Customer Incident Response Team (CIRT) is a specialized 24/7 global AWS team that
provides support to customers during active security events on the customer side of the AWS Shared
Responsibility Model.
• When the AWS CIRT supports you, they provide assistance with triage and recovery for an
active security event on AWS. They can assist in root cause analysis through the use of AWS
service logs and provide you with recommendations for recovery. They can also provide security
recommendations and best practices to help you avoid security events in the future.
• AWS customers can engage the AWS CIRT through an AWS Support case.
•                                                                                                           DDoS response support
• AWS offers AWS Shield, which provides a managed distributed denial of service (DDoS) protection
service that safeguards web applications running on AWS. Shield provides always-on detection and
automatic inline mitigations that can minimize application downtime and latency, so there is no
need to engage AWS Support to benefit from DDoS protection. There are two tiers of Shield: AWS
Shield Standard and AWS Shield Advanced. To learn about the differences between these two tiers,
see Shield features documentation.
•                                                                                                           AWS Managed Services (AMS)
225

AWS Well-Architected Framework
Incident response
• AWS Managed Services (AMS) provides ongoing management of your AWS infrastructure so you can
focus on your applications. By implementing best practices to maintain your infrastructure, AMS
helps reduce your operational overhead and risk. AMS automates common activities such as change
requests, monitoring, patch management, security, and backup services, and provides full-lifecycle
services to provision, run, and support your infrastructure.
• AMS takes responsibility for deploying a suite of security detective controls and provides a 24/7 first
line of response to alerts. When an alert is initiated, AMS follows a standard set of automated and
manual playbooks to verify a consistent response. These playbooks are shared with AMS customers
during onboarding so that they can develop and coordinate a response with AMS.
Develop the incident response plan
The incident response plan is designed to be the foundation for your incident response program and
strategy. The incident response plan should be in a formal document. An incident response plan typically
includes these sections:
• An incident response team overview: Outlines the goals and functions of the incident response team.
• Roles and responsibilities: Lists the incident response stakeholders and details their roles when an
incident occurs.
• A communication plan: Details contact information and how you will communicate during an
incident.
• Backup communication methods: It’s a best practice to have out-of-band communication as a
backup for incident communication. An example of an application that provides a secure out-of-band
communications channel is AWS Wickr.
• Phases of incident response and actions to take: Enumerates the phases of incident response (for
example, detect, analyze, eradicate, contain, and recover), including high-level actions to take within
those phases.
• Incident severity and prioritization definitions: Details how to classify the severity of an incident,
how to prioritize the incident, and then how the severity definitions affect escalation procedures.
While these sections are common throughout companies of different sizes and industries, each
organization’s incident response plan is unique. You will need to build an incident response plan that
works best for your organization.
Resources
Related best practices:
• SEC04 (How do you detect and investigate security events?)
Related documents:
• AWS Security Incident Response Guide
• NIST: Computer Security Incident Handling Guide
SEC10-BP03 Prepare forensic capabilities
It’s important for your incident responders to understand when and how the forensic investigation fits
into your response plan. Your organization should define what evidence is collected and what tools are
used in the process. Identify and prepare forensic investigation capabilities that are suitable, including
external specialists, tools, and automation. A key decision that you should make upfront is if you will
collect data from a live system. Some data, such as the contents of volatile memory or active network
connections, will be lost if the system is powered off or rebooted.
226

AWS Well-Architected Framework
Incident response
Your response team can combine tools, such as AWS Systems Manager, Amazon EventBridge, and AWS
Lambda, to automatically run forensic tools within an operating system and VPC traffic mirroring to
obtain a network packet capture, to gather non-persistent evidence. Conduct other activities, such as log
analysis or analyzing disk images, in a dedicated security account with customized forensic workstations
and tools accessible to your responders.
Routinely ship relevant logs to a data store that provides high durability and integrity. Responders
should have access to those logs. AWS offers several tools that can make log investigation easier, such
as Amazon Athena, Amazon OpenSearch Service (OpenSearch Service), and Amazon CloudWatch Logs
Insights. Additionally, preserve evidence securely using Amazon Simple Storage Service (Amazon S3)
Object Lock. This service follows the WORM (write-once- read-many) model and prevents objects from
being deleted or overwritten for a defined period. As forensic investigation techniques require specialist
training, you might need to engage external specialists.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Identify forensic capabilities: Research your organization's forensic investigation capabilities, available
tools, and external specialists.
• Automating Incident Response and Forensics
Resources
Related documents:
• How to automate forensic disk collection in AWS
SEC10-BP04 Automate containment capability
Automate containment and recovery of an incident to reduce response times and organizational impact.
Once you create and practice the processes and tools from your playbooks, you can deconstruct the
logic into a code-based solution, which can be used as a tool by many responders to automate the
response and remove variance or guess-work by your responders. This can speed up the lifecycle of
a response. The next goal is to allow this code to be fully automated by being invoked by the alerts
or events themselves, rather than by a human responder, to create an event-driven response. These
processes should also automatically add relevant data to your security systems. For example, an incident
involving traffic from an unwanted IP address can automatically populate an AWS WAF block list or
Network Firewall rule group to prevent further activity.
227

AWS Well-Architected Framework
Incident response
Figure 3: AWS WAF automate blocking of known malicious IP addresses.
With an event-driven response system, a detective mechanism initiates a responsive mechanism to
automatically remediate the event. You can use event-driven response capabilities to reduce the time-
to-value between detective mechanisms and responsive mechanisms. To create this event-driven
architecture, you can use AWS Lambda, which is a serverless compute service that runs your code
in response to events and automatically manages the underlying compute resources for you. For
example, assume that you have an AWS account using the AWS CloudTrail service. If CloudTrail is ever
turned off (through the cloudtrail:StopLogging API call), you can use Amazon EventBridge to
monitor for the specific cloudtrail:StopLogging event, and invoke a Lambda function to call
cloudtrail:StartLogging to restart logging.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Automate containment capability.
Resources
Related documents:
• AWS Incident Response Guide
Related videos:
• Prepare for and respond to security incidents in your AWS environment
SEC10-BP05 Pre-provision access
Verify that incident responders have the correct access pre-provisioned in AWS to reduce the time
needed for investigation through to recovery.
Common anti-patterns:
• Using the root account for incident response.
• Altering existing accounts.
• Manipulating IAM permissions directly when providing just-in-time privilege elevation.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
AWS recommends reducing or eliminating reliance on long-lived credentials wherever possible, in favor
of temporary credentials and just-in-time privilege escalation mechanisms. Long-lived credentials are
prone to security risk and increase operational overhead. For most management tasks, as well as incident
response tasks, we recommend you implement identity federation alongside temporary escalation for
administrative access. In this model, a user requests elevation to a higher level of privilege (such as an
incident response role) and, provided the user is eligible for elevation, a request is sent to an approver.
If the request is approved, the user receives a set of temporary AWS credentials which can be used to
complete their tasks. After these credentials expire, the user must submit a new elevation request.
We recommend the use of temporary privilege escalation in the majority of incident response scenarios.
The correct way to do this is to use the AWS Security Token Service and session policies to scope access.
There are scenarios where federated identities are unavailable, such as:
228

AWS Well-Architected Framework
Incident response
• Outage related to a compromised identity provider (IdP).
• Misconfiguration or human error causing broken federated access management system.
• Malicious activity such as a distributed denial of service (DDoS) event or rendering unavailability of the
system.
In the preceding cases, there should be emergency break glass access configured to allow investigation
and timely remediation of incidents. We recommend that you use a user, group, or role with appropriate
permissions to perform tasks and access AWS resources. Use the root user only for tasks that require
root user credentials. To verify that incident responders have the correct level of access to AWS and
other relevant systems, we recommend the pre-provisioning of dedicated accounts. The accounts require
privileged access, and must be tightly controlled and monitored. The accounts must be built with the
fewest privileges required to perform the necessary tasks, and the level of access should be based on the
playbooks created as part of the incident management plan.
Use purpose-built and dedicated users and roles as a best practice. Temporarily escalating user or role
access through the addition of IAM policies both makes it unclear what access users had during the
incident, and risks the escalated privileges not being revoked.
It is important to remove as many dependencies as possible to verify that access can be gained under
the widest possible number of failure scenarios. To support this, create a playbook to verify that incident
response users are created as users in a dedicated security account, and not managed through any
existing Federation or single sign-on (SSO) solution. Each individual responder must have their own
named account. The account configuration must enforce strong password policy and multi-factor
authentication (MFA). If the incident response playbooks only require access to the AWS Management
Console, the user should not have access keys configured and should be explicitly disallowed from
creating access keys. This can be configured with IAM policies or service control policies (SCPs) as
mentioned in the AWS Security Best Practices for AWS Organizations SCPs. The users should have no
privileges other than the ability to assume incident response roles in other accounts.
During an incident it might be necessary to grant access to other internal or external individuals to
support investigation, remediation, or recovery activities. In this case, use the playbook mechanism
mentioned previously, and there must be a process to verify that any additional access is revoked
immediately after the incident is complete.
To verify that the use of incident response roles can be properly monitored and audited, it is essential
that the IAM accounts created for this purpose are not shared between individuals, and that the AWS
account root user is not used unless required for a specific task. If the root user is required (for example,
IAM access to a specific account is unavailable), use a separate process with a playbook available to verify
availability of the root user sign-in credentials and MFA token.
To configure the IAM policies for the incident response roles, consider using IAM Access Analyzer to
generate policies based on AWS CloudTrail logs. To do this, grant administrator access to the incident
response role on a non-production account and run through your playbooks. Once complete, a policy can
be created that allows only the actions taken. This policy can then be applied to all the incident response
roles across all accounts. You might wish to create a separate IAM policy for each playbook to allow
easier management and auditing. Example playbooks could include response plans for ransomware, data
breaches, loss of production access, and other scenarios.
Use the incident response accounts to assume dedicated incident response IAM roles in other AWS
accounts. These roles must be configured to only be assumable by users in the security account, and the
trust relationship must require that the calling principal has authenticated using MFA. The roles must use
tightly-scoped IAM policies to control access. Ensure that all AssumeRole requests for these roles are
logged in CloudTrail and alerted on, and that any actions taken using these roles are logged.
It is strongly recommended that both the IAM accounts and the IAM roles are clearly named to allow
them to be easily found in CloudTrail logs. An example of this would be to name the IAM accounts
<USER_ID>-BREAK-GLASS and the IAM roles BREAK-GLASS-ROLE.
229

AWS Well-Architected Framework
Incident response
CloudTrail is used to log API activity in your AWS accounts and should be used to configure alerts on
usage of the incident response roles. Refer to the blog post on configuring alerts when root keys are
used. The instructions can be modified to configure the Amazon CloudWatch metric filter-to-filter on
AssumeRole events related to the incident response IAM role:
{ $.eventName = "AssumeRole" && $.requestParameters.roleArn =
"<INCIDENT_RESPONSE_ROLE_ARN>" && $.userIdentity.invokedBy NOT EXISTS && $.eventType !=
"AwsServiceEvent" }
As the incident response roles are likely to have a high level of access, it is important that these alerts go
to a wide group and are acted upon promptly.
During an incident, it is possible that a responder might require access to systems which are not directly
secured by IAM. These could include Amazon Elastic Compute Cloud instances, Amazon Relational
Database Service databases, or software-as-a-service (SaaS) platforms. It is strongly recommended
that rather than using native protocols such as SSH or RDP, AWS Systems Manager Session Manager
is used for all administrative access to Amazon EC2 instances. This access can be controlled using IAM,
which is secure and audited. It might also be possible to automate parts of your playbooks using AWS
Systems Manager Run Command documents, which can reduce user error and improve time to recovery.
For access to databases and third-party tools, we recommend storing access credentials in AWS Secrets
Manager and granting access to the incident responder roles.
Finally, the management of the incident response IAM accounts should be added to your Joiners, Movers,
and Leavers processes and reviewed and tested periodically to verify that only the intended access is
allowed.
Resources
Related documents:
• Managing temporary elevated access to your AWS environment
• AWS Security Incident Response Guide
• AWS Elastic Disaster Recovery
• AWS Systems Manager Incident Manager
• Setting an account password policy for IAM users
• Using multi-factor authentication (MFA) in AWS
• Configuring Cross-Account Access with MFA
• Using IAM Access Analyzer to generate IAM policies
• Best Practices for AWS Organizations Service Control Policies in a Multi-Account Environment
• How to Receive Notifications When Your AWS Account’s Root Access Keys Are Used
• Create fine-grained session permissions using IAM managed policies
Related videos:
• Automating Incident Response and Forensics in AWS
• DIY guide to runbooks, incident reports, and incident response
• Prepare for and respond to security incidents in your AWS environment
Related examples:
• Lab: AWS Account Setup and Root User
230

AWS Well-Architected Framework
Incident response
• Lab: Incident Response with AWS Console and CLI
SEC10-BP06 Pre-deploy tools
Ensure that security personnel have the right tools pre-deployed into AWS to reduce the time for
investigation through to recovery.
To automate security engineering and operations functions, you can use a comprehensive set of APIs
and tools from AWS. You can fully automate identity management, network security, data protection,
and monitoring capabilities and deliver them using popular software development methods that you
already have in place. When you build security automation, your system can monitor, review, and initiate
a response, rather than having people monitor your security position and manually react to events. An
effective way to automatically provide searchable and relevant log data across AWS services to your
incident responders is to turn on Amazon Detective.
If your incident response teams continue to respond to alerts in the same way, they risk alert fatigue.
Over time, the team can become desensitized to alerts and can either make mistakes handling ordinary
situations or miss unusual alerts. Automation helps avoid alert fatigue by using functions that process
the repetitive and ordinary alerts, leaving humans to handle the sensitive and unique incidents.
Integrating anomaly detection systems, such as Amazon GuardDuty, AWS CloudTrail Insights, and
Amazon CloudWatch Anomaly Detection, can reduce the burden of common threshold-based alerts.
You can improve manual processes by programmatically automating steps in the process. After you
define the remediation pattern to an event, you can decompose that pattern into actionable logic, and
write the code to perform that logic. Responders can then run that code to remediate the issue. Over
time, you can automate more and more steps, and ultimately automatically handle whole classes of
common incidents.
For tools that run within the operating system of your Amazon Elastic Compute Cloud (Amazon EC2)
instance, you should evaluate using the AWS Systems Manager Run Command, which allows you to
remotely and securely administrate instances using an agent that you install on your Amazon EC2
instance operating system. It requires the Systems Manager Agent (SSM Agent), which is installed by
default on many Amazon Machine Images (AMIs). Be aware, though, that once an instance has been
compromised, no responses from tools or agents running on it should be considered trustworthy.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Pre-deploy tools: Ensure that security personnel have the right tools pre-deployed in AWS so that an
appropriate response can be made to an incident.
• Lab: Incident response with AWS Management Console and CLI
• Incident Response Playbook with Jupyter - AWS IAM
• AWS Security Automation
• Implement resource tagging: Tag resources with information, such as a code for the resource under
investigation, so that you can identify resources during an incident.
• AWS Tagging Strategies
Resources
Related documents:
• AWS Incident Response Guide
Related videos:
231

AWS Well-Architected Framework
Incident response
• DIY guide to runbooks, incident reports, and incident response
SEC10-BP07 Run game days
This best practice was updated with new guidance on July 13th, 2023.
As organizations grow and evolve over time, so does the threat landscape, making it important to
continually review your incident response capabilities. Running game days, or simulations, is one method
that can be used to perform this assessment. Simulations use real-world security event scenarios
designed to mimic a threat actor’s tactics, techniques, and procedures (TTPs) and allow an organization
to exercise and evaluate their incident response capabilities by responding to these mock cyber events as
they might occur in reality.
Benefits of establishing this best practice: Simulations have a variety of benefits:
• Validating cyber readiness and developing the confidence of your incident responders.
• Testing the accuracy and efficiency of tools and workflows.
• Refining communication and escalation methods aligned with your incident response plan.
• Providing an opportunity to respond to less common vectors.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
There are three main types of simulations:
• Tabletop exercises: The tabletop approach to simulations is a discussion-based session involving
the various incident response stakeholders to practice roles and responsibilities and use established
communication tools and playbooks. Exercise facilitation can typically be accomplished in a full day in
a virtual venue, physical venue, or a combination. Because it is discussion-based, the tabletop exercise
focuses on processes, people, and collaboration. Technology is an integral part of the discussion, but
the actual use of incident response tools or scripts is generally not a part of the tabletop exercise.
• Purple team exercises: Purple team exercises increase the level of collaboration between the incident
responders (blue team) and simulated threat actors (red team). The blue team is comprised of
members of the security operations center (SOC), but can also include other stakeholders that would
be involved during an actual cyber event. The red team is comprised of a penetration testing team or
key stakeholders that are trained in offensive security. The red team works collaboratively with the
exercise facilitators when designing a scenario so that the scenario is accurate and feasible. During
purple team exercises, the primary focus is on the detection mechanisms, the tools, and the standard
operating procedures (SOPs) supporting the incident response efforts.
• Red team exercises: During a red team exercise, the offense (red team) conducts a simulation to
achieve a certain objective or set of objectives from a predetermined scope. The defenders (blue team)
will not necessarily have knowledge of the scope and duration of the exercise, which provides a more
realistic assessment of how they would respond to an actual incident. Because red team exercises can
be invasive tests, be cautious and implement controls to verify that the exercise does not cause actual
harm to your environment.
Consider facilitating cyber simulations at a regular interval. Each exercise type can provide unique
benefits to the participants and the organization as a whole, so you might choose to start with less
complex simulation types (such as tabletop exercises) and progress to more complex simulation types
(red team exercises). You should select a simulation type based on your security maturity, resources,
and your desired outcomes. Some customers might not choose to perform red team exercises due to
complexity and cost.
232

AWS Well-Architected Framework
Application security
Implementation steps
Regardless of the type of simulation you choose, simulations generally follow these implementation
steps:
1. Define core exercise elements: Define the simulation scenario and the objectives of the simulation.
Both of these should have leadership acceptance.
2. Identify key stakeholders: At a minimum, an exercise needs exercise facilitators and participants.
Depending on the scenario, additional stakeholders such as legal, communications, or executive
leadership might be involved.
3. Build and test the scenario: The scenario might need to be redefined as it is being built if specific
elements aren’t feasible. A finalized scenario is expected as the output of this stage.
4. Facilitate the simulation: The type of simulation determines the facilitation used (a paper-based
scenario compared to a highly technical, simulated scenario). The facilitators should align their
facilitation tactics to the exercise objects and they should engage all exercise participants wherever
possible to provide the most benefit.
5. Develop the after-action report (AAR): Identify areas that went well, those that can use
improvement, and potential gaps. The AAR should measure the effectiveness of the simulation as well
as the team’s response to the simulated event so that progress can be tracked over time with future
simulations.
Resources
Related documents:
• AWS Incident Response Guide
Related videos:
• AWS GameDay - Security Edition
Application security
Question
• SEC 11. How do you incorporate and validate the security properties of applications throughout the
design, development, and deployment lifecycle? (p. 233)
SEC 11. How do you incorporate and validate the security
properties of applications throughout the design, development,
and deployment lifecycle?
Training people, testing using automation, understanding dependencies, and validating the security
properties of tools and applications help to reduce the likelihood of security issues in production
workloads.
Best practices
• SEC11-BP01 Train for application security (p. 234)
• SEC11-BP02 Automate testing throughout the development and release lifecycle (p. 235)
• SEC11-BP03 Perform regular penetration testing (p. 237)
• SEC11-BP04 Manual code reviews (p. 239)
233

AWS Well-Architected Framework
Application security
• SEC11-BP05 Centralize services for packages and dependencies (p. 240)
• SEC11-BP06 Deploy software programmatically (p. 242)
• SEC11-BP07 Regularly assess security properties of the pipelines (p. 243)
• SEC11-BP08 Build a program that embeds security ownership in workload teams (p. 244)
SEC11-BP01 Train for application security
Provide training to the builders in your organization on common practices for the secure development
and operation of applications. Adopting security focused development practices helps reduce the
likelihood of issues that are only detected at the security review stage.
Desired outcome: Software should be designed and built with security in mind. When the builders in
an organization are trained on secure development practices that start with a threat model, it improves
the overall quality and security of the software produced. This approach can reduce the time to ship
software or features because less rework is needed after the security review stage.
For the purposes of this best practice, secure development refers to the software that is being written and
the tools or systems that support the software development lifecycle (SDLC).
Common anti-patterns:
• Waiting until a security review, and then considering the security properties of a system.
• Leaving all security decisions to the security team.
• Failing to communicate how the decisions taken in the SDLC relate to the overall security expectations
or policies of the organization.
• Engaging in the security review process too late.
Benefits of establishing this best practice:
• Better knowledge of the organizational requirements for security early in the development cycle.
• Being able to identify and remediate potential security issues faster, resulting in a quicker delivery of
features.
• Improved quality of software and systems.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Provide training to the builders in your organization. Starting off with a course on threat modeling is
a good foundation for helping train for security. Ideally, builders should be able to self-serve access to
information relevant to their workloads. This access helps them make informed decisions about the
security properties of the systems they build without needing to ask another team. The process for
engaging the security team for reviews should be clearly defined and simple to follow. The steps in the
review process should be included in the security training. Where known implementation patterns or
templates are available, they should be simple to find and link to the overall security requirements.
Consider using AWS CloudFormation, AWS Cloud Development Kit (AWS CDK) Constructs, Service
Catalog, or other templating tools to reduce the need for custom configuration.
Implementation steps
• Start builders with a course on threat modeling to build a good foundation, and help train them on
how to think about security.
• Provide access to AWS Training and Certification, industry, or AWS Partner training.
234

AWS Well-Architected Framework
Application security
• Provide training on your organization's security review process, which clarifies the division of
responsibilities between the security team, workload teams, and other stakeholders.
• Publish self-service guidance on how to meet your security requirements, including code examples and
templates, if available.
• Regularly obtain feedback from builder teams on their experience with the security review process and
training, and use that feedback to improve.
• Use game days or bug bash campaigns to help reduce the number of issues, and increase the skills of
your builders.
Resources
Related best practices:
• SEC11-BP08 Build a program that embeds security ownership in workload teams (p. 244)
Related documents:
• AWS Training and Certification
• How to think about cloud security governance
• How to approach threat modeling
• Accelerating training - The AWS Skills Guild
Related videos:
• Proactive security: Considerations and approaches
Related examples:
• Workshop on threat modeling
• Industry awareness for developers
Related services:
• AWS CloudFormation
• AWS Cloud Development Kit (AWS CDK) (AWS CDK) Constructs
• Service Catalog
• AWS BugBust
SEC11-BP02 Automate testing throughout the development and release
lifecycle
Automate the testing for security properties throughout the development and release lifecycle.
Automation makes it easier to consistently and repeatably identify potential issues in software prior to
release, which reduces the risk of security issues in the software being provided.
Desired outcome:  The goal of automated testing is to provide a programmatic way of detecting
potential issues early and often throughout the development lifecycle. When you automate regression
testing, you can rerun functional and non-functional tests to verify that previously tested software
still performs as expected after a change. When you define security unit tests to check for common
misconfigurations, such as broken or missing authentication, you can identify and fix these issues early in
the development process.
235

AWS Well-Architected Framework
Application security
Test automation uses purpose-built test cases for application validation, based on the application’s
requirements and desired functionality. The result of the automated testing is based on comparing the
generated test output to its respective expected output, which expedites the overall testing lifecycle.
Testing methodologies such as regression testing and unit test suites are best suited for automation.
Automating the testing of security properties allows builders to receive automated feedback without
having to wait for a security review. Automated tests in the form of static or dynamic code analysis can
increase code quality and help detect potential software issues early in the development lifecycle.
Common anti-patterns:
• Not communicating the test cases and test results of the automated testing.
• Performing the automated testing only immediately prior to a release.
• Automating test cases with frequently changing requirements.
• Failing to provide guidance on how to address the results of security tests.
Benefits of establishing this best practice:
• Reduced dependency on people evaluating the security properties of systems.
• Having consistent findings across multiple workstreams improves consistency.
• Reduced likelihood of introducing security issues into production software.
• Shorter window of time between detection and remediation due to catching software issues earlier.
• Increased visibility of systemic or repeated behavior across multiple workstreams, which can be used to
drive organization-wide improvements.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
As you build your software, adopt various mechanisms for software testing to ensure that you are testing
your application for both functional requirements, based on your application’s business logic, and non-
functional requirements, which are focused on application reliability, performance, and security.
Static application security testing (SAST) analyzes your source code for anomalous security patterns,
and provides indications for defect prone code. SAST relies on static inputs, such as documentation
(requirements specification, design documentation, and design specifications) and application source
code to test for a range of known security issues. Static code analyzers can help expedite the analysis of
large volumes of code. The NIST Quality Group provides a comparison of Source Code Security Analyzers,
which includes open source tools for Byte Code Scanners and Binary Code Scanners.
Complement your static testing with dynamic analysis security testing (DAST) methodologies, which
performs tests against the running application to identify potentially unexpected behavior. Dynamic
testing can be used to detect potential issues that are not detectable via static analysis. Testing at the
code repository, build, and pipeline stages allows you to check for different types of potential issues
from entering into your code. Amazon CodeWhisperer provides code recommendations, including
security scanning, in the builder’s IDE. Amazon CodeGuru Reviewer can identify critical issues, security
issues, and hard-to-find bugs during application development, and provides recommendations to
improve code quality.
The Security for Developers workshop uses AWS developer tools, such as AWS CodeBuild, AWS
CodeCommit, and AWS CodePipeline, for release pipeline automation that includes SAST and DAST
testing methodologies.
As you progress through your SDLC, establish an iterative process that includes periodic application
reviews with your security team. Feedback gathered from these security reviews should be addressed and
236

AWS Well-Architected Framework
Application security
validated as part of your release readiness review. These reviews establish a robust application security
posture, and provide builders with actionable feedback to address potential issues.
Implementation steps
• Implement consistent IDE, code review, and CI/CD tools that include security testing.
• Consider where in the SDLC it is appropriate to block pipelines instead of just notifying builders that
issues need to be remediated.
• The Security for Developers workshop provides an example of integrating static and dynamic testing
into a release pipeline.
• Performing testing or code analysis using automated tools, such as Amazon CodeWhisperer integrated
with developer IDEs, and Amazon CodeGuru Reviewer for scanning code on commit, helps builders get
feedback at the right time.
• When building using AWS Lambda, you can use Amazon Inspector to scan the application code in your
functions.
• When automated testing is included in CI/CD pipelines, you should use a ticketing system to track the
notification and remediation of software issues.
• For security tests that might generate findings, linking to guidance for remediation helps builders
improve code quality.
• Regularly analyze the findings from automated tools to prioritize the next automation, builder
training, or awareness campaign.
Resources
Related documents:
• Continuous Delivery and Continuous Deployment
• AWS DevOps Competency Partners
• AWS Security Competency Partners for Application Security
• Choosing a Well-Architected CI/CD approach
• Monitoring CodeCommit events in Amazon EventBridge and Amazon CloudWatch Events
• Secrets detection in Amazon CodeGuru Review
• Accelerate deployments on AWS with effective governance
• How AWS approaches automating safe, hands-off deployments
Related videos:
• Hands-off: Automating continuous delivery pipelines at Amazon
• Automating cross-account CI/CD pipelines
Related examples:
• Industry awareness for developers
• AWS CodePipeline Governance (GitHub)
• Security for Developers workshop
SEC11-BP03 Perform regular penetration testing
Perform regular penetration testing of your software. This mechanism helps identify potential software
issues that cannot be detected by automated testing or a manual code review. It can also help you
237

AWS Well-Architected Framework
Application security
understand the efficacy of your detective controls. Penetration testing should try to determine if the
software can be made to perform in unexpected ways, such as exposing data that should be protected,
or granting broader permissions than expected.
Desired outcome: Penetration testing is used to detect, remediate, and validate your application’s
security properties. Regular and scheduled penetration testing should be performed as part of the
software development lifecycle (SDLC). The findings from penetration tests should be addressed prior to
the software being released. You should analyze the findings from penetration tests to identify if there
are issues that could be found using automation. Having a regular and repeatable penetration testing
process that includes an active feedback mechanism helps inform the guidance to builders and improves
software quality.
Common anti-patterns:
• Only penetration testing for known or prevalent security issues.
• Penetration testing applications without dependent third-party tools and libraries.
• Only penetration testing for package security issues, and not evaluating implemented business logic.
Benefits of establishing this best practice:
• Increased confidence in the security properties of the software prior to release.
• Opportunity to identify preferred application patterns, which leads to greater software quality.
• A feedback loop that identifies earlier in the development cycle where automation or additional
training can improve the security properties of software.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Penetration testing is a structured security testing exercise where you run planned security
breach scenarios to detect, remediate, and validate security controls. Penetration tests start with
reconnaissance, during which data is gathered based on the current design of the application and its
dependencies. A curated list of security-specific testing scenarios are built and run. The key purpose
of these tests is to uncover security issues in your application, which could be exploited for gaining
unintended access to your environment, or unauthorized access to data. You should perform penetration
testing when you launch new features, or whenever your application has undergone major changes in
function or technical implementation.
You should identify the most appropriate stage in the development lifecycle to perform penetration
testing. This testing should happen late enough that the functionality of the system is close to the
intended release state, but with enough time remaining for any issues to be remediated.
Implementation steps
• Have a structured process for how penetration testing is scoped, basing this process on the threat
model is a good way of maintaining context.
• Identify the appropriate place in the development cycle to perform penetration testing. This should
be when there is minimal change expected in the application, but with enough time to perform
remediation.
• Train your builders on what to expect from penetration testing findings, and how to get information
on remediation.
• Use tools to speed up the penetration testing process by automating common or repeatable tests.
• Analyze penetration testing findings to identify systemic security issues, and use this data to inform
additional automated testing and ongoing builder education.
238

AWS Well-Architected Framework
Application security
Resources
Related best practices:
• SEC11-BP01 Train for application security (p. 234)
• SEC11-BP02 Automate testing throughout the development and release lifecycle (p. 235)
Related documents:
• AWS Penetration Testing provides detailed guidance for penetration testing on AWS
• Accelerate deployments on AWS with effective governance
• AWS Security Competency Partners
• Modernize your penetration testing architecture on AWS Fargate
• AWS Fault injection Simulator
Related examples:
• Automate API testing with AWS CodePipeline (GitHub)
• Automated security helper (GitHub)
SEC11-BP04 Manual code reviews
Perform a manual code review of the software that you produce. This process helps verify that the
person who wrote the code is not the only one checking the code quality.
Desired outcome: Including a manual code review step during development increases the quality
of the software being written, helps upskill less experienced members of the team, and provides an
opportunity to identify places where automation can be used. Manual code reviews can be supported by
automated tools and testing.
Common anti-patterns:
• Not performing reviews of code before deployment.
• Having the same person write and review the code.
• Not using automation to assist or orchestrate code reviews.
• Not training builders on application security before they review code.
Benefits of establishing this best practice:
• Increased code quality.
• Increased consistency of code development through reuse of common approaches.
• Reduction in the number of issues discovered during penetration testing and later stages.
• Improved knowledge transfer within the team.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
The review step should be implemented as part of the overall code management flow. The specifics
depend on the approach used for branching, pull-requests, and merging. You might be using AWS
CodeCommit or third-party solutions such as GitHub, GitLab, or Bitbucket. Whatever method you use, it’s
239

AWS Well-Architected Framework
Application security
important to verify that your processes require the review of code before it’s deployed in a production
environment. Using tools such as Amazon CodeGuru Reviewer can make it easier to orchestrate the code
review process.
Implementation steps
• Implement a manual review step as part of your code management flow and perform this review
before proceeding.
• Consider Amazon CodeGuru Reviewer for managing and assisting in code reviews.
• Implement an approval flow that requires a code review being completed before code can progress to
the next stage.
• Verify there is a process to identify issues being found during manual code reviews that could be
detected automatically.
• Integrate the manual code review step in a way that aligns with your code development practices.
Resources
Related best practices:
• SEC11-BP02 Automate testing throughout the development and release lifecycle (p. 235)
Related documents:
• Working with pull requests in AWS CodeCommit repositories
• Working with approval rule templates in AWS CodeCommit
• About pull requests in GitHub
• Automate code reviews with Amazon CodeGuru Reviewer
• Automating detection of security vulnerabilities and bugs in CI/CD pipelines using Amazon CodeGuru
Reviewer CLI
Related videos:
• Continuous improvement of code quality with Amazon CodeGuru
Related examples:
• Security for Developers workshop
SEC11-BP05 Centralize services for packages and dependencies
Provide centralized services for builder teams to obtain software packages and other dependencies. This
allows the validation of packages before they are included in the software that you write, and provides a
source of data for the analysis of the software being used in your organization.
Desired outcome: Software is comprised of a set of other software packages in addition to the code
that is being written. This makes it simple to consume implementations of functionality that are
repeatedly used, such as a JSON parser or an encryption library. Logically centralizing the sources for
these packages and dependencies provides a mechanism for security teams to validate the properties
of the packages before they are used. This approach also reduces the risk of an unexpected issue being
caused by a change in an existing package, or by builder teams including arbitrary packages directly from
the internet. Use this approach in conjunction with the manual and automated testing flows to increase
the confidence in the quality of the software that is being developed.
240

AWS Well-Architected Framework
Application security
Common anti-patterns:
• Pulling packages from arbitrary repositories on the internet.
• Not testing new packages before making them available to builders.
Benefits of establishing this best practice:
• Better understanding of what packages are being used in the software being built.
• Being able to notify workload teams when a package needs to be updated based on the understanding
of who is using what.
• Reducing the risk of a package with issues being included in your software.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Provide centralized services for packages and dependencies in a way that is simple for builders to
consume. Centralized services can be logically central rather than implemented as a monolithic system.
This approach allows you to provide services in a way that meets the needs of your builders. You
should implement an efficient way of adding packages to the repository when updates happen or new
requirements emerge. AWS services such as AWS CodeArtifact or similar AWS partner solutions provide a
way of delivering this capability.
Implementation steps:
• Implement a logically centralized repository service that is available in all of the environments where
software is developed.
• Include access to the repository as part of the AWS account vending process.
• Build automation to test packages before they are published in a repository.
• Maintain metrics of the most commonly used packages, languages, and teams with the highest
amount of change.
• Provide an automated mechanism for builder teams to request new packages and provide feedback.
• Regularly scan packages in your repository to identify the potential impact of newly discovered issues.
Resources
Related best practices:
• SEC11-BP02 Automate testing throughout the development and release lifecycle (p. 235)
Related documents:
• Accelerate deployments on AWS with effective governance
• Tighten your package security with CodeArtifact Package Origin Control toolkit
• Detecting security issues in logging with Amazon CodeGuru Reviewer
• Supply chain Levels for Software Artifacts (SLSA)
Related videos:
• Proactive security: Considerations and approaches
• The AWS Philosophy of Security (re:Invent 2017)
241

AWS Well-Architected Framework
Application security
• When security, safety, and urgency all matter: Handling Log4Shell
Related examples:
• Multi Region Package Publishing Pipeline (GitHub)
• Publishing Node.js Modules on AWS CodeArtifact using AWS CodePipeline (GitHub)
• AWS CDK Java CodeArtifact Pipeline Sample (GitHub)
• Distribute private .NET NuGet packages with AWS CodeArtifact (GitHub)
SEC11-BP06 Deploy software programmatically
Perform software deployments programmatically where possible. This approach reduces the likelihood
that a deployment fails or an unexpected issue is introduced due to human error.
Desired outcome: Keeping people away from data is a key principle of building securely in the AWS
Cloud. This principle includes how you deploy your software.
The benefits of not relying on people to deploy software is the greater confidence that what you tested
is what gets deployed, and that the deployment is performed consistently every time. The software
should not need to be changed to function in different environments. Using the principles of twelve-
factor application development, specifically the externalizing of configuration, allows you to deploy
the same code to multiple environments without requiring changes. Cryptographically signing software
packages is a good way to verify that nothing has changed between environments. The overall outcome
of this approach is to reduce risk in your change process and improve the consistency of software
releases.
Common anti-patterns:
• Manually deploying software into production.
• Manually performing changes to software to cater to different environments.
Benefits of establishing this best practice:
• Increased confidence in the software release process.
• Reduced risk of a failed change impacting business functionality.
• Increased release cadence due to lower change risk.
• Automatic rollback capability for unexpected events during deployment.
• Ability to cryptographically prove that the software that was tested is the software deployed.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Build your AWS account structure to remove persistent human access from environments and use CI/CD
tools to perform deployments. Architect your applications so that environment-specific configuration
data is obtained from an external source, such as AWS Systems Manager Parameter Store. Sign packages
after they have been tested, and validate these signatures during deployment. Configure your CI/CD
pipelines to push application code and use canaries to confirm successful deployment. Use tools such
as AWS CloudFormation or AWS CDK to define your infrastructure, then use AWS CodeBuild and AWS
CodePipeline to perform CI/CD operations.
Implementation steps
• Build well-defined CI/CD pipelines to streamline the deployment process.
242

AWS Well-Architected Framework
Application security
• Using AWS CodeBuild and AWS Code Pipeline to provide CI/CD capability makes it simple to integrate
security testing into your pipelines.
• Follow the guidance on separation of environments in the Organizing Your AWS Environment Using
Multiple Accounts whitepaper.
• Verify no persistent human access to environments where production workloads are running.
• Architect your applications to support the externalization of configuration data.
• Consider deploying using a blue/green deployment model.
• Implement canaries to validate the successful deployment of software.
• Use cryptographic tools such as AWS Signer or AWS Key Management Service (AWS KMS) to sign and
verify the software packages that you are deploying.
Resources
Related best practices:
• SEC11-BP02 Automate testing throughout the development and release lifecycle (p. 235)
Related documents:
• AWS CI/CD Workshop
• Accelerate deployments on AWS with effective governance
• Automating safe, hands-off deployments
• Code signing using AWS Certificate Manager Private CA and AWS Key Management Service asymmetric
keys
• Code Signing, a Trust and Integrity Control for AWS Lambda
Related videos:
• Hands-off: Automating continuous delivery pipelines at Amazon
Related examples:
• Blue/Green deployments with AWS Fargate
SEC11-BP07 Regularly assess security properties of the pipelines
Apply the principles of the Well-Architected Security Pillar to your pipelines, with particular attention
to the separation of permissions. Regularly assess the security properties of your pipeline infrastructure.
Effectively managing the security of the pipelines allows you to deliver the security of the software that
passes through the pipelines.
Desired outcome: The pipelines used to build and deploy your software should follow the same
recommended practices as any other workload in your environment. The tests that are implemented in
the pipelines should not be editable by the builders who are using them. The pipelines should only have
the permissions needed for the deployments they are doing and should implement safeguards to avoid
deploying to the wrong environments. Pipelines should not rely on long-term credentials, and should be
configured to emit state so that the integrity of the build environments can be validated.
Common anti-patterns:
• Security tests that can be bypassed by builders.
243

AWS Well-Architected Framework
Application security
• Overly broad permissions for deployment pipelines.
• Pipelines not being configured to validate inputs.
• Not regularly reviewing the permissions associated with your CI/CD infrastructure.
• Use of long-term or hardcoded credentials.
Benefits of establishing this best practice:
• Greater confidence in the integrity of the software that is built and deployed through the pipelines.
• Ability to stop a deployment when there is suspicious activity.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Starting with managed CI/CD services that support IAM roles reduces the risk of credential leakage.
Applying the Security Pillar principles to your CI/CD pipeline infrastructure can help you determine
where security improvements can be made. Following the AWS Deployment Pipelines Reference
Architecture is a good starting point for building your CI/CD environments. Regularly reviewing the
pipeline implementation and analyzing logs for unexpected behavior can help you understand the usage
patterns of the pipelines being used to deploy software.
Implementation steps
• Start with the AWS Deployment Pipelines Reference Architecture.
• Consider using AWS IAM Access Analyzer to programmatically generate least privilege IAM policies for
the pipelines.
• Integrate your pipelines with monitoring and alerting so that you are notified of unexpected or
abnormal activity, for AWS managed services Amazon EventBridge allows you to route data to targets
such as AWS Lambda or Amazon Simple Notification Service (Amazon SNS).
Resources
Related documents:
• AWS Deployment Pipelines Reference Architecture
• Monitoring AWS CodePipeline
• Security best practices for AWS CodePipeline
Related examples:
• DevOps monitoring dashboard (GitHub)
SEC11-BP08 Build a program that embeds security ownership in workload
teams
Build a program or mechanism that empowers builder teams to make security decisions about the
software that they create. Your security team still needs to validate these decisions during a review, but
embedding security ownership in builder teams allows for faster, more secure workloads to be built. This
mechanism also promotes a culture of ownership that positively impacts the operation of the systems
you build.
244

AWS Well-Architected Framework
Application security
Desired outcome: To embed security ownership and decision making in builder teams, you can either
train builders on how to think about security or you can augment their training with security people
embedded or associated with the builder teams. Either approach is valid and allows the team to make
higher quality security decisions earlier in the development cycle. This ownership model is predicated
on training for application security. Starting with the threat model for the particular workload helps
focus the design thinking on the appropriate context. Another benefit of having a community of security
focused builders, or a group of security engineers working with builder teams, is that you can more
deeply understand how software is written. This understanding helps you determine the next areas for
improvement in your automation capability.
Common anti-patterns:
• Leaving all security design decisions to a security team.
• Not addressing security requirements early enough in the development process.
• Not obtaining feedback from builders and security people on the operation of the program.
Benefits of establishing this best practice:
• Reduced time to complete security reviews.
• Reduction in security issues that are only detected at the security review stage.
• Improvement in the overall quality of the software being written.
• Opportunity to identify and understand systemic issues or areas of high value improvement.
• Reduction in the amount of rework required due to security review findings.
• Improvement in the perception of the security function.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Start with the guidance in SEC11-BP01 Train for application security (p. 234). Then identify the
operational model for the program that you think might work best for your organization. The two main
patterns are to train builders or to embed security people in builder teams. After you have decided on
the initial approach, you should pilot with a single or small group of workload teams to prove the model
works for your organization. Leadership support from the builder and security parts of the organization
helps with the delivery and success of the program. As you build this program, it’s important to choose
metrics that can be used to show the value of the program. Learning from how AWS has approached this
problem is a good learning experience. This best practice is very much focused on organizational change
and culture. The tools that you use should support the collaboration between the builder and security
communities.
Implementation steps
• Start by training your builders for application security.
• Create a community and an onboarding program to educate builders.
• Pick a name for the program. Guardians, Champions, or Advocates are commonly used.
• Identify the model to use: train builders, embed security engineers, or have affinity security roles.
• Identify project sponsors from security, builders, and potentially other relevant groups.
• Track metrics for the number of people involved in the program, the time taken for reviews, and the
feedback from builders and security people. Use these metrics to make improvements.
Resources
Related best practices:
245

AWS Well-Architected Framework
Reliability
• SEC11-BP01 Train for application security (p. 234)
• SEC11-BP02 Automate testing throughout the development and release lifecycle (p. 235)
Related documents:
• How to approach threat modeling
• How to think about cloud security governance
Related videos:
• Proactive security: Considerations and approaches
Reliability
The Reliability pillar encompasses the ability of a workload to perform its intended function correctly
and consistently when it’s expected to. You can find prescriptive guidance on implementation in the
Reliability Pillar whitepaper.
Best practice areas
• Foundations (p. 246)
• Workload architecture (p. 272)
• Change management (p. 299)
• Failure management (p. 319)
Foundations
Questions
• REL 1. How do you manage Service Quotas and constraints? (p. 246)
• REL 2. How do you plan your network topology?  (p. 262)
REL 1. How do you manage Service Quotas and constraints?
For cloud-based workload architectures, there are Service Quotas (which are also referred to as service
limits). These quotas exist to prevent accidentally provisioning more resources than you need and to limit
request rates on API operations so as to protect services from abuse. There are also resource constraints,
for example, the rate that you can push bits down a fiber-optic cable, or the amount of storage on a
physical disk.
Best practices
• REL01-BP01 Aware of service quotas and constraints (p. 247)
• REL01-BP02 Manage service quotas across accounts and regions (p. 250)
• REL01-BP03 Accommodate fixed service quotas and constraints through architecture (p. 253)
• REL01-BP04 Monitor and manage quotas (p. 255)
• REL01-BP05 Automate quota management (p. 258)
• REL01-BP06 Ensure that a sufficient gap exists between the current quotas and the maximum usage
to accommodate failover (p. 259)
246

AWS Well-Architected Framework
Foundations
REL01-BP01 Aware of service quotas and constraints
This best practice was updated with new guidance on July 13th, 2023.
Be aware of your default quotas and manage your quota increase requests for your workload
architecture. Know which cloud resource constraints, such as disk or network, are potentially impactful.
Desired outcome: Customers can prevent service degradation or disruption in their AWS accounts by
implementing proper guidelines for monitoring key metrics, infrastructure reviews, and automation
remediation steps to verify that services quotas and constraints are not reached that could cause service
degradation or disruption.
Common anti-patterns:
• Deploying a workload without understanding the hard or soft quotas and their limits for the services
used.
• Deploying a replacement workload without analyzing and reconfiguring the necessary quotas or
contacting Support in advance.
• Assuming that cloud services have no limits and the services can be used without consideration to
rates, limits, counts, quantities.
• Assuming that quotas will automatically be increased.
• Not knowing the process and timeline of quota requests.
• Assuming that the default cloud service quota is the identical for every service compared across
regions.
• Assuming that service constraints can be breached and the systems will auto-scale or add increase the
limit beyond the resource’s constraints
• Not testing the application at peak traffic in order to stress the utilization of its resources.
• Provisioning the resource without analysis of the required resource size.
• Overprovisioning capacity by choosing resource types that go well beyond actual need or expected
peaks.
• Not assessing capacity requirements for new levels of traffic in advance of a new customer event or
deploying a new technology.
Benefits of establishing this best practice: Monitoring and automated management of service quotas
and resource constraints can proactively reduce failures. Changes in traffic patterns for a customer’s
service can cause a disruption or degradation if best practices are not followed. By monitoring and
managing these values across all regions and all accounts, applications can have improved resiliency
under adverse or unplanned events.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Service Quotas is an AWS service that helps you manage your quotas for over 250 AWS services from one
location. Along with looking up the quota values, you can also request and track quota increases from
the Service Quotas console or using the AWS SDK. AWS Trusted Advisor offers a service quotas check
that displays your usage and quotas for some aspects of some services. The default service quotas per
service are also in the AWS documentation per respective service (for example, see Amazon VPC Quotas).
Some service limits, like rate limits on throttled APIs are set within the Amazon API Gateway itself by
configuring a usage plan. Some limits that are set as configuration on their respective services include
Provisioned IOPS, Amazon RDS storage allocated, and Amazon EBS volume allocations. Amazon Elastic
247

AWS Well-Architected Framework
Foundations
Compute Cloud has its own service limits dashboard that can help you manage your instance, Amazon
Elastic Block Store, and Elastic IP address limits. If you have a use case where service quotas impact your
application’s performance and they are not adjustable to your needs, then contact AWS Support to see if
there are mitigations.
Service quotas can be Region specific or can also be global in nature. Using an AWS service that reaches
its quota will not act as expected in normal usage and may cause service disruption or degradation. For
example, a service quota limits the number of DL Amazon EC2 that be used in an Region and that limit
may be reached during a traffic scaling event using Auto Scaling groups (ASG).
Service quotas for each account should be assessed for usage on a regular basis to determine what the
appropriate service limits might be for that account. These service quotas exist as operational guardrails,
to prevent accidentally provisioning more resources than you need. They also serve to limit request rates
on API operations to protect services from abuse.
Service constraints are different from service quotas. Service constraints represent a particular resource’s
limits as defined by that resource type. These might be storage capacity (for example, gp2 has a size
limit of 1 GB - 16 TB) or disk throughput (10,0000 iops). It is essential that a resource type’s constraint
be engineered and constantly assessed for usage that might reach its limit. If a constraint is reached
unexpectedly, the account’s applications or services may be degraded or disrupted.
If there is a use case where service quotas impact an application’s performance and they cannot be
adjusted to required needs, contact AWS Support to see if there are mitigations. For more detail on
adjusting fixed quotas, see REL01-BP03 Accommodate fixed service quotas and constraints through
architecture (p. 253).
There are a number of AWS services and tools to help monitor and manage Service Quotas. The service
and tools should be leveraged to provide automated or manual checks of quota levels.
• AWS Trusted Advisor offers a service quota check that displays your usage and quotas for some aspects
of some services. It can aid in identifying services that are near quota.
• AWS Management Console provides methods to display services quota values, manage, request new
quotas, monitor status of quota requests, and display history of quotas.
• AWS CLI and CDKs offer programmatic methods to automatically manage and monitor service quota
levels and usage.
Implementation steps
For Service Quotas:
• Review AWS Service Quotas.
• To be aware of your existing service quotas, determine the services (like IAM Access Analyzer) that are
used. There are approximately 250 AWS services controlled by service quotas. Then, determine the
specific service quota name that might be used within each account and region. There are approximate
3000 service quota names per region.
• Augment this quota analysis with AWS Config to find all AWS resources used in your AWS accounts.
• Use AWS CloudFormation data to determine your AWS resources used. Look at the resources that
were created either in the AWS Management Console or with the list-stack-resources AWS CLI
command. You can also see resources configured to be deployed in the template itself.
• Determine all the services your workload requires by looking at the deployment code.
• Determine the service quotas that apply. Use the programmatically accessible information from
Trusted Advisor and Service Quotas.
• Establish an automated monitoring method (see REL01-BP02 Manage service quotas across accounts
and regions (p. 250) and REL01-BP04 Monitor and manage quotas (p. 255)) to alert and inform if
services quotas are near or have reached their limit.
248

AWS Well-Architected Framework
Foundations
• Establish an automated and programmatic method to check if a service quota has been changed in
one region but not in other regions in the same account (see REL01-BP02 Manage service quotas
across accounts and regions (p. 250) and REL01-BP04 Monitor and manage quotas (p. 255)).
• Automate scanning application logs and metrics to determine if there are any quota or service
constraint errors. If these errors are present, send alerts to the monitoring system.
• Establish engineering procedures to calculate the required change in quota (see REL01-BP05 Automate
quota management (p. 258)) once it has been identified that larger quotas are required for specific
services.
• Create a provisioning and approval workflow to request changes in service quota. This should include
an exception workflow in case of request deny or partial approval.
• Create an engineering method to review service quotas prior to provisioning and using new AWS
services before rolling out to production or loaded environments. (for example, load testing account).
For service constraints:
• Establish monitoring and metrics methods to alert for resources reading close to their resource
constraints. Leverage CloudWatch as appropriate for metrics or log monitoring.
• Establish alert thresholds for each resource that has a constraint that is meaningful to the application
or system.
• Create workflow and infrastructure management procedures to change the resource type if the
constraint is near utilization. This workflow should include load testing as a best practice to verify that
new type is the correct resource type with the new constraints.
• Migrate identified resource to the recommended new resource type, using existing procedures and
processes.
Resources
Related best practices:
• REL01-BP02 Manage service quotas across accounts and regions (p. 250)
• REL01-BP03 Accommodate fixed service quotas and constraints through architecture (p. 253)
• REL01-BP04 Monitor and manage quotas (p. 255)
• REL01-BP05 Automate quota management (p. 258)
• REL01-BP06 Ensure that a sufficient gap exists between the current quotas and the maximum usage to
accommodate failover (p. 259)
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
• REL11-BP03 Automate healing on all layers (p. 342)
• REL12-BP05 Test resiliency using chaos engineering (p. 353)
Related documents:
• AWS Well-Architected Framework’s Reliability Pillar: Availability
• AWS Service Quotas (formerly referred to as service limits)
• AWS Trusted Advisor Best Practice Checks (see the Service Limits section)
• AWS limit monitor on AWS answers
• Amazon EC2 Service Limits
• What is Service Quotas?
• How to Request Quota Increase
249

AWS Well-Architected Framework
Foundations
• Service endpoints and quotas
• Service Quotas User Guide
• Quota Monitor for AWS
• AWS Fault Isolation Boundaries
• Availability with redundancy
• AWS for Data
• What is Continuous Integration?
• What is Continuous Delivery?
• APN Partner: partners that can help with configuration management
• Managing the account lifecycle in account-per-tenant SaaS environments on AWS
• Managing and monitoring API throttling in your workloads
• View AWS Trusted Advisor recommendations at scale with AWS Organizations
• Automating Service Limit Increases and Enterprise Support with AWS Control Tower
Related videos:
• AWS Live re:Inforce 2019 - Service Quotas
• View and Manage Quotas for AWS Services Using Service Quotas
• AWS IAM Quotas Demo
Related tools:
• Amazon CodeGuru Reviewer
• AWS CodeDeploy
• AWS CloudTrail
• Amazon CloudWatch
• Amazon EventBridge
• Amazon DevOps Guru
• AWS Config
• AWS Trusted Advisor
• AWS CDK
• AWS Systems Manager
• AWS Marketplace
REL01-BP02 Manage service quotas across accounts and regions
If you are using multiple accounts or Regions, request the appropriate quotas in all environments in
which your production workloads run.
Desired outcome: Services and applications should not be affected by service quota exhaustion for
configurations that span accounts or Regions or that have resilience designs using zone, Region, or
account failover.
Common anti-patterns:
• Allowing resource usage in one isolation Region to grow with no mechanism to maintain capacity in
the other ones.
• Manually setting all quotas independently in isolation Regions.
250

AWS Well-Architected Framework
Foundations
• Not considering the effect of resiliency architectures (like active or passive) in future quota needs
during a degradation in the non-primary Region.
• Not evaluating quotas regularly and making necessary changes in every Region and account the
workload runs.
• Not leveraging quota request templates to request increases across multiple Regions and accounts.
• Not updating service quotas due to incorrectly thinking that increasing quotas has cost implications
like compute reservation requests.
Benefits of establishing this best practice: Verifying that you can handle your current load in secondary
regions or accounts if regional services become unavailable. This can help reduce the number of errors or
levels of degradations that occur during region loss.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Service quotas are tracked per account. Unless otherwise noted, each quota is AWS Region-specific.
In addition to the production environments, also manage quotas in all applicable non-production
environments so that testing and development are not hindered. Maintaining a high degree of resiliency
requires that service quotas are assessed continually (whether automated or manual).
With more workloads spanning Regions due to the implementation of designs using Active/Active,
Active/Passive - Hot, Active/Passive-Cold, and Active/Passive-Pilot Light approaches, it is essential to
understand all Region and account quota levels. Past traffic patterns are not always a good indicator if
the service quota is set correctly.
Equally important, the service quota name limit is not always the same for every Region. In one Region,
the value could be five, and in another region the value could be ten. Management of these quotas must
span all the same services, accounts, and Regions to provide consistent resilience under load.
Reconcile all the service quota differences across different Regions (Active Region or Passive Region) and
create processes to continually reconcile these differences. The testing plans of passive Region failovers
are rarely scaled to peak active capacity, meaning that game day or table top exercises can fail to find
differences in service quotas between Regions and also then maintain the correct limits.
Service quota drift, the condition where service quota limits for a specific named quota is changed in one
Region and not all Regions, is very important to track and assess. Changing the quota in Regions with
traffic or potentially could carry traffic should be considered.
• Select relevant accounts and Regions based on your service requirements, latency, regulatory, and
disaster recovery (DR) requirements.
• Identify service quotas across all relevant accounts, Regions, and Availability Zones. The limits are
scoped to account and Region. These values should be compared for differences.
Implementation steps
• Review Service Quotas values that might have breached beyond the a risk level of usage. AWS Trusted
Advisor provides alerts for 80% and 90% threshold breaches.
• Review values for service quotas in any Passive Regions (in an Active/Passive design). Verify that load
will successfully run in secondary Regions in the event of a failure in the primary Region.
• Automate assessing if any service quota drift has occurred between Regions in the same account and
act accordingly to change the limits.
• If the customer Organizational Units (OU) are structured in the supported manner, service quota
templates should be updated to reflect changes in any quotas that should be applied to multiple
Regions and accounts.
251

AWS Well-Architected Framework
Foundations
• Create a template and associate Regions to the quota change.
• Review all existing service quota templates for any changes required (Region, limits, and accounts).
Resources
Related best practices:
• REL01-BP01 Aware of service quotas and constraints (p. 247)
• REL01-BP03 Accommodate fixed service quotas and constraints through architecture (p. 253)
• REL01-BP04 Monitor and manage quotas (p. 255)
• REL01-BP05 Automate quota management (p. 258)
• REL01-BP06 Ensure that a sufficient gap exists between the current quotas and the maximum usage to
accommodate failover (p. 259)
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
• REL11-BP03 Automate healing on all layers (p. 342)
• REL12-BP05 Test resiliency using chaos engineering (p. 353)
Related documents:
• AWS Well-Architected Framework’s Reliability Pillar: Availability
• AWS Service Quotas (formerly referred to as service limits)
• AWS Trusted Advisor Best Practice Checks (see the Service Limits section)
• AWS limit monitor on AWS answers
• Amazon EC2 Service Limits
• What is Service Quotas?
• How to Request Quota Increase
• Service endpoints and quotas
• Service Quotas User Guide
• Quota Monitor for AWS
• AWS Fault Isolation Boundaries
• Availability with redundancy
• AWS for Data
• What is Continuous Integration?
• What is Continuous Delivery?
• APN Partner: partners that can help with configuration management
• Managing the account lifecycle in account-per-tenant SaaS environments on AWS
• Managing and monitoring API throttling in your workloads
• View AWS Trusted Advisor recommendations at scale with AWS Organizations
• Automating Service Limit Increases and Enterprise Support with AWS Control Tower
Related videos:
• AWS Live re:Inforce 2019 - Service Quotas
• View and Manage Quotas for AWS Services Using Service Quotas
• AWS IAM Quotas Demo
252

AWS Well-Architected Framework
Foundations
Related services:
• Amazon CodeGuru Reviewer
• AWS CodeDeploy
• AWS CloudTrail
• Amazon CloudWatch
• Amazon EventBridge
• Amazon DevOps Guru
• AWS Config
• AWS Trusted Advisor
• AWS CDK
• AWS Systems Manager
• AWS Marketplace
REL01-BP03 Accommodate fixed service quotas and constraints through
architecture
Be aware of unchangeable service quotas, service constraints, and physical resource limits. Design
architectures for applications and services to prevent these limits from impacting reliability.
Examples include network bandwidth, serverless function invocation payload size, throttle burst rate for
of an API gateway, and concurrent user connections to a database.
Desired outcome: The application or service performs as expected under normal and high traffic
conditions. They have been designed to work within the limitations for that resource’s fixed constraints
or service quotas.
Common anti-patterns:
• Choosing a design that uses a resource of a service, unaware that there are design constraints that will
cause this design to fail as you scale.
• Performing benchmarking that is unrealistic and will reach service fixed quotas during the testing. For
example, running tests at a burst limit but for an extended amount of time.
• Choosing a design that cannot scale or be modified if fixed service quotas are to be exceeded. For
example, an SQS payload size of 256KB.
• Observability has not been designed and implemented to monitor and alert on thresholds for service
quotas that might be at risk during high traffic events
Benefits of establishing this best practice: Verifying that the application will run under all projected
services load levels without disruption or degradation.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Unlike soft service quotas or resources that be replaced with higher capacity units, AWS services’ fixed
quotas cannot be changed. This means that all these type of AWS services must be evaluated for
potential hard capacity limits when used in an application design.
Hard limits are show in the Service Quotas console. If the columns shows ADJUSTABLE = No, the service
has a hard limit. Hard limits are also shown in some resources configuration pages. For example, Lambda
has specific hard limits that cannot be adjusted.
253

AWS Well-Architected Framework
Foundations
As an example, when designing a python application to run in a Lambda function, the application should
be evaluated to determine if there is any chance of Lambda running longer than 15 minutes. If the code
may run more than this service quota limit, alternate technologies or designs must be considered. If
this limit is reached after production deployment, the application will suffer degradation and disruption
until it can be remediated. Unlike soft quotas, there is no method to change to these limits even under
emergency Severity 1 events.
Once the application has been deployed to a testing environment, strategies should be used to find
if any hard limits can be reached. Stress testing, load testing, and chaos testing should be part of the
introduction test plan.
Implementation steps
• Review the complete list of AWS services that could be used in the application design phase.
• Review the soft quota limits and hard quota limits for all these services. Not all limits are shown in the
Service Quotas console. Some services describe these limits in alternate locations.
• As you design your application, review your workload’s business and technology drivers, such as
business outcomes, use case, dependent systems, availability targets, and disaster recovery objects. Let
your business and technology drivers guide the process to identify the distributed system that is right
for your workload.
• Analyze service load across Regions and accounts. Many hard limits are regionally based for services.
However, some limits are account based.
• Analyze resilience architectures for resource usage during a zonal failure and Regional failure. In the
progression of multi-Region designs using active/active, active/passive - hot, active/passive - cold,
and active/passive - pilot light approaches, these failure cases will cause higher usage. This creates a
potential use case for hitting hard limits.
Resources
Related best practices:
• REL01-BP01 Aware of service quotas and constraints (p. 247)
• REL01-BP02 Manage service quotas across accounts and regions (p. 250)
• REL01-BP04 Monitor and manage quotas (p. 255)
• REL01-BP05 Automate quota management (p. 258)
• REL01-BP06 Ensure that a sufficient gap exists between the current quotas and the maximum usage to
accommodate failover (p. 259)
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
• REL11-BP03 Automate healing on all layers (p. 342)
• REL12-BP05 Test resiliency using chaos engineering (p. 353)
Related documents:
• AWS Well-Architected Framework’s Reliability Pillar: Availability
• AWS Service Quotas (formerly referred to as service limits)
• AWS Trusted Advisor Best Practice Checks (see the Service Limits section)
• AWS limit monitor on AWS answers
• Amazon EC2 Service Limits
• What is Service Quotas?
254

AWS Well-Architected Framework
Foundations
• How to Request Quota Increase
• Service endpoints and quotas
• Service Quotas User Guide
• Quota Monitor for AWS
• AWS Fault Isolation Boundaries
• Availability with redundancy
• AWS for Data
• What is Continuous Integration?
• What is Continuous Delivery?
• APN Partner: partners that can help with configuration management
• Managing the account lifecycle in account-per-tenant SaaS environments on AWS
• Managing and monitoring API throttling in your workloads
• View AWS Trusted Advisor recommendations at scale with AWS Organizations
• Automating Service Limit Increases and Enterprise Support with AWS Control Tower
• Actions, resources, and condition keys for Service Quotas
Related videos:
• AWS Live re:Inforce 2019 - Service Quotas
• View and Manage Quotas for AWS Services Using Service Quotas
• AWS IAM Quotas Demo
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
Related tools:
• AWS CodeDeploy
• AWS CloudTrail
• Amazon CloudWatch
• Amazon EventBridge
• Amazon DevOps Guru
• AWS Config
• AWS Trusted Advisor
• AWS CDK
• AWS Systems Manager
• AWS Marketplace
REL01-BP04 Monitor and manage quotas
Evaluate your potential usage and increase your quotas appropriately, allowing for planned growth in
usage.
Desired outcome: Active and automated systems that manage and monitor have been deployed. These
operations solutions ensure that quota usage thresholds are nearing being reached. These would be
proactively remediated by requested quota changes.
Common anti-patterns:
• Not configuring monitoring to check for service quota thresholds
255

AWS Well-Architected Framework
Foundations
• Not configuring monitoring for hard limits, even though those values cannot be changed.
• Assuming that amount of time required to request and secure a soft quota change is immediate or a
short period.
• Configuring alarms for when service quotas are being approached, but having no process on how to
respond to an alert.
• Only configuring alarms for services supported by AWS Service Quotas and not monitoring other AWS
services.
• Not considering quota management for multiple Region resiliency designs, like active/active, active/
passive - hot, active/passive - cold, and active/passive - pilot light approaches.
• Not assessing quota differences between Regions.
• Not assessing the needs in every Region for a specific quota increase request.
• Not leveraging templates for multi-Region quota management.
Benefits of establishing this best practice: Automatic tracking of the AWS Service Quotas and
monitoring your usage against those quotas will allow you to see when you are approaching a quota
limit. You can also use this monitoring data to help limit any degradations due to quota exhaustion.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
For supported services, you can monitor your quotas by configuring various different services that can
assess and then send alerts or alarms. This can aid in monitoring usage and can alert you to approaching
quotas. These alarms can be invoked from AWS Config, Lambda functions, Amazon CloudWatch, or from
AWS Trusted Advisor. You can also use metric filters on CloudWatch Logs to search and extract patterns
in logs to determine if usage is approaching quota thresholds.
Implementation steps
For monitoring:
• Capture current resource consumption (for example, buckets or instances). Use service API operations,
such as the Amazon EC2 DescribeInstances API, to collect current resource consumption.
• Capture your current quotas that are essential and applicable to the services using:
• AWS Service Quotas
• AWS Trusted Advisor
• AWS documentation
• AWS service-specific pages
• AWS Command Line Interface (AWS CLI)
• AWS Cloud Development Kit (AWS CDK)
• Use AWS Service Quotas, an AWS service that helps you manage your quotas for over 250 AWS
services from one location.
• Use Trusted Advisor service limits to monitor your current service limits at various thresholds.
• Use the service quota history (console or AWS CLI) to check on regional increases.
• Compare service quota changes in each Region and each account to create equivalency, if required.
For management:
• Automated: Set up an AWS Config custom rule to scan service quotas across Regions and compare for
differences.
• Automated: Set up a scheduled Lambda function to scan service quotas across Regions and compare
for differences.
256

AWS Well-Architected Framework
Foundations
• Manual: Scan services quota through AWS CLI, API, or AWS Console to scan service quotas across
Regions and compare for differences. Report the differences.
• If differences in quotas are identified between Regions, request a quota change, if required.
• Review the result of all requests.
Resources
Related best practices:
• REL01-BP01 Aware of service quotas and constraints (p. 247)
• REL01-BP02 Manage service quotas across accounts and regions (p. 250)
• REL01-BP03 Accommodate fixed service quotas and constraints through architecture (p. 253)
• REL01-BP05 Automate quota management (p. 258)
• REL01-BP06 Ensure that a sufficient gap exists between the current quotas and the maximum usage to
accommodate failover (p. 259)
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
• REL11-BP03 Automate healing on all layers (p. 342)
• REL12-BP05 Test resiliency using chaos engineering (p. 353)
Related documents:
•                                                                                                      AWS Well-Architected Framework’s Reliability Pillar: Availability
•                                                                                                      AWS Service Quotas (formerly referred to as service limits)
•                                                                                                      AWS Trusted Advisor Best Practice Checks (see the Service Limits section)
•                                                                                                      AWS limit monitor on AWS answers
•                                                                                                      Amazon EC2 Service Limits
•                                                                                                      What is Service Quotas?
•                                                                                                      How to Request Quota Increase
•                                                                                                      Service endpoints and quotas
•                                                                                                      Service Quotas User Guide
•                                                                                                      Quota Monitor for AWS
•                                                                                                      AWS Fault Isolation Boundaries
•                                                                                                      Availability with redundancy
•                                                                                                      AWS for Data
•                                                                                                      What is Continuous Integration?
•                                                                                                      What is Continuous Delivery?
•                                                                                                      APN Partner: partners that can help with configuration management
•                                                                                                      Managing the account lifecycle in account-per-tenant SaaS environments on AWS
•                                                                                                      Managing and monitoring API throttling in your workloads
•                                                                                                      View AWS Trusted Advisor recommendations at scale with AWS Organizations
•                                                                                                      Automating Service Limit Increases and Enterprise Support with AWS Control Tower
•                                                                                                      Actions, resources, and condition keys for Service Quotas
Related videos:
257

AWS Well-Architected Framework
Foundations
• AWS Live re:Inforce 2019 - Service Quotas
• View and Manage Quotas for AWS Services Using Service Quotas
• AWS IAM Quotas Demo
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
Related tools:
• AWS CodeDeploy
• AWS CloudTrail
• Amazon CloudWatch
• Amazon EventBridge
• Amazon DevOps Guru
• AWS Config
• AWS Trusted Advisor
• AWS CDK
• AWS Systems Manager
• AWS Marketplace
REL01-BP05 Automate quota management
Implement tools to alert you when thresholds are being approached. You can automate quota increase
requests by using AWS Service Quotas APIs.
If you integrate your Configuration Management Database (CMDB) or ticketing system with Service
Quotas, you can automate the tracking of quota increase requests and current quotas. In addition to the
AWS SDK, Service Quotas offers automation using the AWS Command Line Interface (AWS CLI).
Common anti-patterns:
• Tracking the quotas and usage in spreadsheets.
• Running reports on usage daily, weekly, or monthly, and then comparing usage to the quotas.
Benefits of establishing this best practice: Automated tracking of the AWS service quotas and
monitoring of your usage against that quota allows you to see when you are approaching a quota. You
can set up automation to assist you in requesting a quota increase when needed. You might want to
consider lowering some quotas when your usage trends in the opposite direction to realize the benefits
of lowered risk (in case of compromised credentials) and cost savings.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Set up automated monitoring Implement tools using SDKs to alert you when thresholds are being
approached.
• Use Service Quotas and augment the service with an automated quota monitoring solution, such as
AWS Limit Monitor or an offering from AWS Marketplace.
• What is Service Quotas?
• Quota Monitor on AWS - AWS Solution
• Set up automated responses based on quota thresholds, using Amazon SNS and AWS Service Quotas
APIs.
• Test automation.
258

AWS Well-Architected Framework
Foundations
• Configure limit thresholds.
• Integrate with change events from AWS Config, deployment pipelines, Amazon EventBridge, or
third parties.
• Artificially set low quota thresholds to test responses.
• Set up automated operations to take appropriate action on notifications and contact AWS Support
when necessary.
• Manually start change events.
• Run a game day to test the quota increase change process.
Resources
Related documents:
• APN Partner: partners that can help with configuration management
• AWS Marketplace: CMDB products that help track limits
• AWS Service Quotas (formerly referred to as service limits)
• AWS Trusted Advisor Best Practice Checks (see the Service Limits section)
• Quota Monitor on AWS - AWS Solution
• Amazon EC2 Service Limits
• What is Service Quotas?
Related videos:
• AWS Live re:Inforce 2019 - Service Quotas
REL01-BP06 Ensure that a sufficient gap exists between the current quotas and
the maximum usage to accommodate failover
When a resource fails or is inaccessible, that resource might still be counted against a quota until it’s
successfully terminated. Verify that your quotas cover the overlap of failed or inaccessible resources
and their replacements. You should consider use cases like network failure, Availability Zone failure, or
Regional failures when calculating this gap.
Desired outcome: Small or large failures in resources or resource accessibility can be covered within
the current service thresholds. Zone failures, network failures, or even Regional failures have been
considered in the resource planning.
Common anti-patterns:
• Setting service quotas based on current needs without accounting for failover scenarios.
• Not considering the principals of static stability when calculating the peak quota for a service.
• Not considering the potential of inaccessible resources in calculating total quota needed for each
Region.
• Not considering AWS service fault isolation boundaries for some services and their potential abnormal
usage patterns.
Benefits of establishing this best practice: When a service disruption events impact application
availability, the cloud allows you to implement strategies to mitigate or recover from these events. Such
strategies often include creating additional resources to replace failed or inaccessible ones. Your quota
strategy would accommodate these failover conditions and not layer in additional degradations due to
service limit exhaustion.
259

AWS Well-Architected Framework
Foundations
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
When evaluating quota limits, consider failover cases that might occur due to some degradation. The
following types of failover cases should be considered:
• A VPC that is disrupted or inaccessible.
• A Subnet that is inaccessible.
• An Availability Zone has been degraded sufficiently to impact the accessibility of many resources.
• Various networking routes or ingress and egress points are blocked or changed.
• A Region has been degraded sufficiently to impact the accessibility of many resources.
• There are multiple resources but not all are affected by a failure in a Region or an Availability Zone.
Failures like the ones listed could be the reason to initiate a failover event. The decision to failover is
unique for each situation and customer, as the business impact can vary dramatically. However, when
operationally deciding to failover application or services, the capacity planning of resources in the
failover location and their related quotas must be addressed before the event.
Review the service quotas for each service considering the high than normal peaks that might occur.
These peaks might be related to resources that can be reached due to networking or permissions but are
still active. Unterminated active resources will still be counted against the service quota limit.
Implementation steps
• Verify that there is enough gap between your service quota and your maximum usage to
accommodate for a failover or loss of accessibility.
• Determine your service quotas, accounting for your deployment patterns, availability requirements,
and consumption growth.
• Request quota increases if necessary. Plan for necessary time for quota increase requests to be
fulfilled.
• Determine your reliability requirements (also known as your number of nines).
• Establish your fault scenarios (for example, loss of a component, an Availability Zone, or a Region).
• Establish your deployment methodology (for example, canary, blue/green, red/black, or rolling).
• Include an appropriate buffer (for example, 15%) to the current limit.
• Include calculations for static stability (Zonal and Regional) where appropriate.
• Plan consumption growth (for example, monitor your trends in consumption).
• Consider the impact of static stability for your most critical workloads. Assess resources conforming to
a statically stable system in all Regions and Availability Zones.
• Consider the use of On-Demand Capacity Reservations to schedule capacity ahead of any failover. This
can a useful strategy during the most critical business schedules to reduce potential risks of obtaining
the correct quantity and type of resources during failover.
Resources
Related best practices:
• REL01-BP01 Aware of service quotas and constraints (p. 247)
• REL01-BP02 Manage service quotas across accounts and regions (p. 250)
• REL01-BP03 Accommodate fixed service quotas and constraints through architecture (p. 253)
• REL01-BP04 Monitor and manage quotas (p. 255)
260

AWS Well-Architected Framework
Foundations
• REL01-BP05 Automate quota management (p. 258)
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
• REL11-BP03 Automate healing on all layers (p. 342)
• REL12-BP05 Test resiliency using chaos engineering (p. 353)
Related documents:
• AWS Well-Architected Framework’s Reliability Pillar: Availability
• AWS Service Quotas (formerly referred to as service limits)
• AWS Trusted Advisor Best Practice Checks (see the Service Limits section)
• AWS limit monitor on AWS answers
• Amazon EC2 Service Limits
• What is Service Quotas?
• How to Request Quota Increase
• Service endpoints and quotas
• Service Quotas User Guide
• Quota Monitor for AWS
• AWS Fault Isolation Boundaries
• Availability with redundancy
• AWS for Data
• What is Continuous Integration?
• What is Continuous Delivery?
• APN Partner: partners that can help with configuration management
• Managing the account lifecycle in account-per-tenant SaaS environments on AWS
• Managing and monitoring API throttling in your workloads
• View AWS Trusted Advisor recommendations at scale with AWS Organizations
• Automating Service Limit Increases and Enterprise Support with AWS Control Tower
• Actions, resources, and condition keys for Service Quotas
Related videos:
• AWS Live re:Inforce 2019 - Service Quotas
• View and Manage Quotas for AWS Services Using Service Quotas
• AWS IAM Quotas Demo
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
Related tools:
• AWS CodeDeploy
• AWS CloudTrail
• Amazon CloudWatch
• Amazon EventBridge
• Amazon DevOps Guru
• AWS Config
261

AWS Well-Architected Framework
Foundations
• AWS Trusted Advisor
• AWS CDK
• AWS Systems Manager
• AWS Marketplace
REL 2. How do you plan your network topology?
Workloads often exist in multiple environments. These include multiple cloud environments (both
publicly accessible and private) and possibly your existing data center infrastructure. Plans must include
network considerations such as intra- and intersystem connectivity, public IP address management,
private IP address management, and domain name resolution.
Best practices
• REL02-BP01 Use highly available network connectivity for your workload public endpoints (p. 262)
• REL02-BP02 Provision redundant connectivity between private networks in the cloud and on-
premises environments (p. 265)
• REL02-BP03 Ensure IP subnet allocation accounts for expansion and availability (p. 268)
• REL02-BP04 Prefer hub-and-spoke topologies over many-to-many mesh (p. 269)
• REL02-BP05 Enforce non-overlapping private IP address ranges in all private address spaces where
they are connected (p. 271)
REL02-BP01 Use highly available network connectivity for your workload public
endpoints
Building highly available network connectivity to public endpoints of your workloads can help you
reduce downtime due to loss of connectivity and improve the availability and SLA of your workload. To
achieve this, use highly available DNS, content delivery networks (CDNs), API gateways, load balancing,
or reverse proxies.
Desired outcome: It is critical to plan, build, and operationalize highly available network connectivity for
your public endpoints. If your workload becomes unreachable due to a loss in connectivity, even if your
workload is running and available, your customers will see your system as down. By combining the highly
available and resilient network connectivity for your workload’s public endpoints, along with a resilient
architecture for your workload itself, you can provide the best possible availability and service level for
your customers.
AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, AWS Lambda Function URLs, AWS
AppSync APIs, and Elastic Load Balancing (ELB) all provide highly available public endpoints. Amazon
Route 53 provides a highly available DNS service for domain name resolution to verify that your public
endpoint addresses can be resolved.
You can also evaluate AWS Marketplace software appliances for load balancing and proxying.
Common anti-patterns:
• Designing a highly available workload without planning out DNS and network connectivity for high
availability.
• Using public internet addresses on individual instances or containers and managing the connectivity to
them with DNS.
• Using IP addresses instead of domain names for locating services.
• Not testing out scenarios where connectivity to your public endpoints is lost.
• Not analyzing network throughput needs and distribution patterns.
262

AWS Well-Architected Framework
Foundations
• Not testing and planning for scenarios where internet network connectivity to your public endpoints of
your workload might be interrupted.
• Providing content (like web pages, static assets, or media files) to a large geographic area and not
using a content delivery network.
• Not planning for distributed denial of service (DDoS) attacks. DDoS attacks risk shutting out legitimate
traffic and lowering availability for your users.
Benefits of establishing this best practice: Designing for highly available and resilient network
connectivity ensures that your workload is accessible and available to your users.
Level of risk exposed if this best practice is not established: High
Implementation guidance
At the core of building highly available network connectivity to your public endpoints is the routing
of the traffic. To verify your traffic is able to reach the endpoints, the DNS must be able to resolve the
domain names to their corresponding IP addresses. Use a highly available and scalable Domain Name
System (DNS) such as Amazon Route 53 to manage your domain’s DNS records. You can also use health
checks provided by Amazon Route 53. The health checks verify that your application is reachable,
available, and functional, and they can be set up in a way that they mimic your user’s behavior, such as
requesting a web page or a specific URL. In case of failure, Amazon Route 53 responds to DNS resolution
requests and directs the traffic to only health endpoints. You can also consider using Geo DNS and
Latency Based Routing capabilities offered by Amazon Route 53.
To verify that your workload itself is highly available, use Elastic Load Balancing (ELB). Amazon Route 53
can be used to target traffic to ELB, which distributes the traffic to the target compute instances. You
can also use Amazon API Gateway along with AWS Lambda for a serverless solution. Customers can also
run workloads in multiple AWS Regions. With multi-site active/active pattern, the workload can serve
traffic from multiple Regions. With a multi-site active/passive pattern, the workload serves traffic from
the active region while data is replicated to the secondary region and becomes active in the event of
a failure in the primary region. Route 53 health checks can then be used to control DNS failover from
any endpoint in a primary Region to an endpoint in a secondary Region, verifying that your workload is
reachable and available to your users.
Amazon CloudFront provides a simple API for distributing content with low latency and high data
transfer rates by serving requests using a network of edge locations around the world. Content delivery
networks (CDNs) serve customers by serving content located or cached at a location near to the user.
This also improves availability of your application as the load for content is shifted away from your
servers over to CloudFront’s edge locations. The edge locations and regional edge caches hold cached
copies of your content close to your viewers resulting in quick retrieval and increasing reachability and
availability of your workload.
For workloads with users spread out geographically, AWS Global Accelerator helps you improve the
availability and performance of the applications. AWS Global Accelerator provides Anycast static IP
addresses that serve as a fixed entry point to your application hosted in one or more AWS Regions.
This allows traffic to ingress onto the AWS global network as close to your users as possible, improving
reachability and availability of your workload. AWS Global Accelerator also monitors the health of
your application endpoints by using TCP, HTTP, and HTTPS health checks. Any changes in the health or
configuration of your endpoints permit redirection of user traffic to healthy endpoints that deliver the
best performance and availability to your users. In addition, AWS Global Accelerator has a fault-isolating
design that uses two static IPv4 addresses that are serviced by independent network zones increasing the
availability of your applications.
To help protect customers from DDoS attacks, AWS provides AWS Shield Standard. Shield Standard
comes automatically turned on and protects from common infrastructure (layer 3 and 4) attacks like
SYN/UDP floods and reflection attacks to support high availability of your applications on AWS. For
additional protections against more sophisticated and larger attacks (like UDP floods), state exhaustion
attacks (like TCP SYN floods), and to help protect your applications running on Amazon Elastic Compute
263

AWS Well-Architected Framework
Foundations
Cloud (Amazon EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and
Route 53, you can consider using AWS Shield Advanced. For protection against Application layer attacks
like HTTP POST or GET floods, use AWS WAF. AWS WAF can use IP addresses, HTTP headers, HTTP body,
URI strings, SQL injection, and cross-site scripting conditions to determine if a request should be blocked
or allowed.
Implementation steps
1. Set up highly available DNS: Amazon Route 53 is a highly available and scalable domain name system
(DNS) web service. Route 53 connects user requests to internet applications running on AWS or on-
premises. For more information, see configuring Amazon Route 53 as your DNS service.
2. Setup health checks: When using Route 53, verify that only healthy targets are resolvable. Start by
creating Route 53 health checks and configuring DNS failover. The following aspects are important to
consider when setting up health checks:
a. How Amazon Route 53 determines whether a health check is healthy
b. Creating, updating, and deleting health checks
c. Monitoring health check status and getting notifications
d. Best practices for Amazon Route 53 DNS
3. Connect your DNS service to your endpoints.
a. When using Elastic Load Balancing as a target for your traffic, create an alias record using Amazon
Route 53 that points to your load balancer’s regional endpoint. During the creation of the alias
record, set the Evaluate target health option to Yes.
b. For serverless workloads or private APIs when API Gateway is used, use Route 53 to direct traffic to
API Gateway.
4. Decide on a content delivery network.
a. For delivering content using edge locations closer to the user, start by understanding how
CloudFront delivers content.
b. Get started with a simple CloudFront distribution. CloudFront then knows where you want the
content to be delivered from, and the details about how to track and manage content delivery.
The following aspects are important to understand and consider when setting up CloudFront
distribution:
i.  How caching works with CloudFront edge locations
ii. Increasing the proportion of requests that are served directly from the CloudFront caches (cache
hit ratio)
iii. Using Amazon CloudFront Origin Shield
iv. Optimizing high availability with CloudFront origin failover
5. Set up application layer protection: AWS WAF helps you protect against common web exploits and
bots that can affect availability, compromise security, or consume excessive resources. To get a deeper
understanding, review how AWS WAF works and when you are ready to implement protections from
application layer HTTP POST AND GET floods, review Getting started with AWS WAF. You can also use
AWS WAF with CloudFront see the documentation on how AWS WAF works with Amazon CloudFront
features.
6. Set up additional DDoS protection: By default, all AWS customers receive protection from common,
most frequently occurring network and transport layer DDoS attacks that target your web site or
application with AWS Shield Standard at no additional charge. For additional protection of internet-
facing applications running on Amazon EC2, Elastic Load Balancing, Amazon CloudFront, AWS Global
Accelerator, and Amazon Route 53 you can consider AWS Shield Advanced and review examples of
DDoS resilient architectures. To protect your workload and your public endpoints from DDoS attacks
review Getting started with AWS Shield Advanced.
Resources
Related best practices:
264

AWS Well-Architected Framework
Foundations
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL10-BP02 Select the appropriate locations for your multi-location deployment (p. 332)
• REL11-BP04 Rely on the data plane and not the control plane during recovery (p. 344)
• REL11-BP06 Send notifications when events impact availability (p. 347)
Related documents:
• APN Partner: partners that can help plan your networking
• AWS Marketplace for Network Infrastructure
• What Is AWS Global Accelerator?
• What is Amazon CloudFront?
• What is Amazon Route 53?
• What is Elastic Load Balancing?
• Network Connectivity capability - Establishing Your Cloud Foundations
• What is Amazon API Gateway?
• What are AWS WAF, AWS Shield, and AWS Firewall Manager?
• What is Amazon Route 53 Application Recovery Controller?
• Configure custom health checks for DNS failover
Related videos:
• AWS re:Invent 2022 - Improve performance and availability with AWS Global Accelerator
• AWS re:Invent 2020: Global traffic management with Amazon Route 53
• AWS re:Invent 2022 - Operating highly available Multi-AZ applications
• AWS re:Invent 2022 - Dive deep on AWS networking infrastructure
• AWS re:Invent 2022 - Building resilient networks
Related examples:
• Disaster Recovery with Amazon Route 53 Application Recovery Controller (ARC)
• Reliability Workshops
• AWS Global Accelerator Workshop
REL02-BP02 Provision redundant connectivity between private networks in the
cloud and on-premises environments
Use multiple AWS Direct Connect connections or VPN tunnels between separately deployed private
networks. Use multiple Direct Connect locations for high availability. If using multiple AWS Regions,
ensure redundancy in at least two of them. You might want to evaluate AWS Marketplace appliances that
terminate VPNs. If you use AWS Marketplace appliances, deploy redundant instances for high availability
in different Availability Zones.
AWS Direct Connect is a cloud service that makes it easy to establish a dedicated network connection
from your on-premises environment to AWS. Using Direct Connect Gateway, your on-premises data
center can be connected to multiple AWS VPCs spread across multiple AWS Regions.
This redundancy addresses possible failures that impact connectivity resiliency:
265

AWS Well-Architected Framework
Foundations
• How are you going to be resilient to failures in your topology?
• What happens if you misconfigure something and remove connectivity?
• Will you be able to handle an unexpected increase in traffic or use of your services?
• Will you be able to absorb an attempted Distributed Denial of Service (DDoS) attack?
When connecting your VPC to your on-premises data center via VPN, you should consider the resiliency
and bandwidth requirements that you need when you select the vendor and instance size on which you
need to run the appliance. If you use a VPN appliance that is not resilient in its implementation, then you
should have a redundant connection through a second appliance. For all these scenarios, you need to
define an acceptable time to recovery and test to ensure that you can meet those requirements.
If you choose to connect your VPC to your data center using a Direct Connect connection and you need
this connection to be highly available, have redundant Direct Connect connections from each data
center. The redundant connection should use a second Direct Connect connection from different location
than the first. If you have multiple data centers, ensure that the connections terminate at different
locations. Use the Direct Connect Resiliency Toolkit to help you set this up.
If you choose to fail over to VPN over the internet using AWS VPN, it’s important to understand that
it supports up to 1.25-Gbps throughput per VPN tunnel, but does not support Equal Cost Multi Path
(ECMP) for outbound traffic in the case of multiple AWS Managed VPN tunnels terminating on the
same VGW. We do not recommend that you use AWS Managed VPN as a backup for Direct Connect
connections unless you can tolerate speeds less than 1 Gbps during failover.
You can also use VPC endpoints to privately connect your VPC to supported AWS services and VPC
endpoint services powered by AWS PrivateLink without traversing the public internet. Endpoints are
virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They
allow communication between instances in your VPC and services without imposing availability risks or
bandwidth constraints on your network traffic.
Common anti-patterns:
• Having only one connectivity provider between your on-site network and AWS.
• Consuming the connectivity capabilities of your AWS Direct Connect connection, but only having one
connection.
• Having only one path for your VPN connectivity.
Benefits of establishing this best practice: By implementing redundant connectivity between your cloud
environment and you corporate or on-premises environment, you can ensure that the dependent services
between the two environments can communicate reliably.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Ensure that you have highly available connectivity between AWS and on-premises environment.
Use multiple AWS Direct Connect connections or VPN tunnels between separately deployed private
networks. Use multiple Direct Connect locations for high availability. If using multiple AWS Regions,
ensure redundancy in at least two of them. You might want to evaluate AWS Marketplace appliances
that terminate VPNs. If you use AWS Marketplace appliances, deploy redundant instances for high
availability in different Availability Zones.
• Ensure that you have a redundant connection to your on-premises environment You may need
redundant connections to multiple AWS Regions to achieve your availability needs.
• AWS Direct Connect Resiliency Recommendations
• Using Redundant Site-to-Site VPN Connections to Provide Failover
266

AWS Well-Architected Framework
Foundations
• Use service API operations to identify correct use of Direct Connect circuits.
• DescribeConnections
• DescribeConnectionsOnInterconnect
• DescribeDirectConnectGatewayAssociations
• DescribeDirectConnectGatewayAttachments
• DescribeDirectConnectGateways
• DescribeHostedConnections
• DescribeInterconnects
• If only one Direct Connect connection exists or you have none, set up redundant VPN tunnels to
your virtual private gateways.
• What is AWS Site-to-Site VPN?
•                                                                                                   Capture your current connectivity (for example, Direct Connect, virtual private gateways, AWS
Marketplace appliances).
• Use service API operations to query configuration of Direct Connect connections.
• DescribeConnections
• DescribeConnectionsOnInterconnect
• DescribeDirectConnectGatewayAssociations
• DescribeDirectConnectGatewayAttachments
• DescribeDirectConnectGateways
• DescribeHostedConnections
• DescribeInterconnects
• Use service API operations to collect virtual private gateways where route tables use them.
• DescribeVpnGateways
• DescribeRouteTables
• Use service API operations to collect AWS Marketplace applications where route tables use them.
• DescribeRouteTables
Resources
Related documents:
• APN Partner: partners that can help plan your networking
• AWS Direct Connect Resiliency Recommendations
• AWS Marketplace for Network Infrastructure
• Amazon Virtual Private Cloud Connectivity Options Whitepaper
• Multiple data center HA network connectivity
• Using Redundant Site-to-Site VPN Connections to Provide Failover
• Using the Direct Connect Resiliency Toolkit to get started
• VPC Endpoints and VPC Endpoint Services (AWS PrivateLink)
• What Is Amazon VPC?
• What Is a Transit Gateway?
• What is AWS Site-to-Site VPN?
• Working with Direct Connect Gateways
Related videos:
• AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)
267

AWS Well-Architected Framework
Foundations
• AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)
REL02-BP03 Ensure IP subnet allocation accounts for expansion and availability
Amazon VPC IP address ranges must be large enough to accommodate workload requirements, including
factoring in future expansion and allocation of IP addresses to subnets across Availability Zones. This
includes load balancers, EC2 instances, and container-based applications.
When you plan your network topology, the first step is to define the IP address space itself. Private IP
address ranges (following RFC 1918 guidelines) should be allocated for each VPC. Accommodate the
following requirements as part of this process:
• Allow IP address space for more than one VPC per Region.
• Within a VPC, allow space for multiple subnets that span multiple Availability Zones.
• Always leave unused CIDR block space within a VPC for future expansion.
• Ensure that there is IP address space to meet the needs of any transient fleets of EC2 instances that
you might use, such as Spot Fleets for machine learning, Amazon EMR clusters, or Amazon Redshift
clusters.
• Note that the first four IP addresses and the last IP address in each subnet CIDR block are reserved and
not available for your use.
• You should plan on deploying large VPC CIDR blocks. Note that the initial VPC CIDR block allocated to
your VPC cannot be changed or deleted, but you can add additional non-overlapping CIDR blocks to
the VPC. Subnet IPv4 CIDRs cannot be changed, however IPv6 CIDRs can. Keep in mind that deploying
the largest VPC possible (/16) results in over 65,000 IP addresses. In the base 10.x.x.x IP address space
alone, you could provision 255 such VPCs. You should therefore err on the side of being too large
rather than too small to make it easier to manage your VPCs.
Common anti-patterns:
• Creating small VPCs.
• Creating small subnets and then having to add subnets to configurations as you grow.
• Incorrectly estimating how many IP addresses a elastic load balancer can use.
• Deploying many high traffic load balancers into the same subnets.
Benefits of establishing this best practice: This ensures that you can accommodate the growth of your
workloads and continue to provide availability as you scale up.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Plan your network to accommodate for growth, regulatory compliance, and integration with others.
Growth can be underestimated, regulatory compliance can change, and acquisitions or private network
connections can be difficult to implement without proper planning.
• Select relevant AWS accounts and Regions based on your service requirements, latency, regulatory,
and disaster recovery (DR) requirements.
• Identify your needs for regional VPC deployments.
• Identify the size of the VPCs.
• Determine if you are going to deploy multi-VPC connectivity.
• What Is a Transit Gateway?
• Single Region Multi-VPC Connectivity
268

AWS Well-Architected Framework
Foundations
• Determine if you need segregated networking for regulatory requirements.
• Make VPCs as large as possible. The initial VPC CIDR block allocated to your VPC cannot be
changed or deleted, but you can add additional non-overlapping CIDR blocks to the VPC. This
however may fragment your address ranges.
Resources
Related documents:
• APN Partner: partners that can help plan your networking
• AWS Marketplace for Network Infrastructure
• Amazon Virtual Private Cloud Connectivity Options Whitepaper
• Multiple data center HA network connectivity
• Single Region Multi-VPC Connectivity
• What Is Amazon VPC?
Related videos:
• AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)
• AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)
REL02-BP04 Prefer hub-and-spoke topologies over many-to-many mesh
If more than two network address spaces (for example, VPCs and on-premises networks) are connected
via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model, like that provided by
AWS Transit Gateway.
If you have only two such networks, you can simply connect them to each other, but as the number of
networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway
provides an easy to maintain hub-and-spoke model, allowing the routing of traffic across your multiple
networks.
Figure 1: Without AWS Transit Gateway: You need to peer each Amazon VPC to each other and to each
onsite location using a VPN connection, which can become complex as it scales.
269

AWS Well-Architected Framework
Foundations
Figure 2: With AWS Transit Gateway: You simply connect each Amazon VPC or VPN to the AWS Transit
Gateway and it routes traffic to and from each VPC or VPN.
Common anti-patterns:
• Using VPC peering to connect more than two VPCs.
• Establishing multiple BGP sessions for each VPC to establish connectivity that spans Virtual Private
Clouds (VPCs) spread across multiple AWS Regions.
Benefits of establishing this best practice: As the number of networks grows, the complexity of such
meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-
spoke model, allowing routing of traffic among your multiple networks.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Prefer hub-and-spoke topologies over many-to-many mesh. If more than two network address spaces
(VPCs, on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a
hub-and-spoke model like that provided by AWS Transit Gateway.
• For only two such networks, you can simply connect them to each other, but as the number of
networks grows, the complexity of such meshed connections becomes untenable. AWS Transit
Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic across your
multiple networks.
• What Is a Transit Gateway?
Resources
Related documents:
• APN Partner: partners that can help plan your networking
• AWS Marketplace for Network Infrastructure
• Multiple data center HA network connectivity
• VPC Endpoints and VPC Endpoint Services (AWS PrivateLink)
270

AWS Well-Architected Framework
Foundations
• What Is Amazon VPC?
• What Is a Transit Gateway?
Related videos:
• AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)
• AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)
REL02-BP05 Enforce non-overlapping private IP address ranges in all private
address spaces where they are connected
The IP address ranges of each of your VPCs must not overlap when peered or connected via VPN. You
must similarly avoid IP address conflicts between a VPC and on-premises environments or with other
cloud providers that you use. You must also have a way to allocate private IP address ranges when
needed.
An IP address management (IPAM) system can help with this. Several IPAMs are available from the AWS
Marketplace.
Common anti-patterns:
• Using the same IP range in your VPC as you have on premises or in your corporate network.
• Not tracking IP ranges of VPCs used to deploy your workloads.
Benefits of establishing this best practice: Active planning of your network will ensure that you do
not have multiple occurrences of the same IP address in interconnected networks. This prevents routing
problems from occurring in parts of the workload that are using the different applications.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Monitor and manage your CIDR use. Evaluate your potential usage on AWS, add CIDR ranges to
existing VPCs, and create VPCs to allow planned growth in usage.
• Capture current CIDR consumption (for example, VPCs, subnets)
• Use service API operations to collect current CIDR consumption.
• Capture your current subnet usage.
• Use service API operations to collect subnets per VPC in each Region.
• DescribeSubnets
• Record the current usage.
• Determine if you created any overlapping IP ranges.
• Calculate the spare capacity.
• Identify overlapping IP ranges. You can either migrate to a new range of addresses or use
Network and Port Translation (NAT) appliances from AWS Marketplace if you need to connect the
overlapping ranges.
Resources
Related documents:
• APN Partner: partners that can help plan your networking
• AWS Marketplace for Network Infrastructure
271

AWS Well-Architected Framework
Workload architecture
• Amazon Virtual Private Cloud Connectivity Options Whitepaper
• Multiple data center HA network connectivity
• What Is Amazon VPC?
• What is IPAM?
Related videos:
• AWS re:Invent 2018: Advanced VPC Design and New Capabilities for Amazon VPC (NET303)
• AWS re:Invent 2019: AWS Transit Gateway reference architectures for many VPCs (NET406-R1)
Workload architecture
Questions
• REL 3. How do you design your workload service architecture?  (p. 272)
• REL 4. How do you design interactions in a distributed system to prevent failures?  (p. 279)
• REL 5. How do you design interactions in a distributed system to mitigate or withstand failures?
(p. 284)
REL 3. How do you design your workload service architecture?
Build highly scalable and reliable workloads using a service-oriented architecture (SOA) or a
microservices architecture. Service-oriented architecture (SOA) is the practice of making software
components reusable via service interfaces. Microservices architecture goes further to make components
smaller and simpler.
Best practices
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL03-BP02 Build services focused on specific business domains and functionality (p. 274)
• REL03-BP03 Provide service contracts per API (p. 277)
REL03-BP01 Choose how to segment your workload
Workload segmentation is important when determining the resilience requirements of your application.
Monolithic architecture should be avoided whenever possible. Instead, carefully consider which
application components can be broken out into microservices. Depending on your application
requirements, this may end up being a combination of a service-oriented architecture (SOA) with
microservices where possible. Workloads that are capable of statelessness are more capable of being
deployed as microservices.
Desired outcome: Workloads should be supportable, scalable, and as loosely coupled as possible.
When making choices about how to segment your workload, balance the benefits against the
complexities. What is right for a new product racing to first launch is different than what a workload
built to scale from the start needs. When refactoring an existing monolith, you will need to consider how
well the application will support a decomposition towards statelessness. Breaking services into smaller
pieces allows small, well-defined teams to develop and manage them. However, smaller services can
introduce complexities which include possible increased latency, more complex debugging, and increased
operational burden.
Common anti-patterns:
272

AWS Well-Architected Framework
Workload architecture
• The microservice Death Star is a situation in which the atomic components become so highly
interdependent that a failure of one results in a much larger failure, making the components as rigid
and fragile as a monolith.
Benefits of establishing this practice:
• More specific segments lead to greater agility, organizational flexibility, and scalability.
• Reduced impact of service interruptions.
• Application components may have different availability requirements, which can be supported by a
more atomic segmentation.
• Well-defined responsibilities for teams supporting the workload.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Choose your architecture type based on how you will segment your workload. Choose an SOA or
microservices architecture (or in some rare cases, a monolithic architecture). Even if you choose to start
with a monolith architecture, you must ensure that it’s modular and can ultimately evolve to SOA or
microservices as your product scales with user adoption. SOA and microservices offer respectively smaller
segmentation, which is preferred as a modern scalable and reliable architecture, but there are trade-offs
to consider, especially when deploying a microservice architecture.
One primary trade-off is that you now have a distributed compute architecture that can make it harder
to achieve user latency requirements and there is additional complexity in the debugging and tracing of
user interactions. You can use AWS X-Ray to assist you in solving this problem. Another effect to consider
is increased operational complexity as you increase the number of applications that you are managing,
which requires the deployment of multiple independency components.
Monolithic, service-oriented, and microservices architectures
Implementation steps
• Determine the appropriate architecture to refactor or build your application. SOA and microservices
offer respectively smaller segmentation, which is preferred as a modern scalable and reliable
architecture. SOA can be a good compromise for achieving smaller segmentation while avoiding some
of the complexities of microservices. For more details, see Microservice Trade-Offs.
273

AWS Well-Architected Framework
Workload architecture
• If your workload is amenable to it, and your organization can support it, you should use a
microservices architecture to achieve the best agility and reliability. For more details, see
Implementing Microservices on AWS.
• Consider following the Strangler Fig pattern to refactor a monolith into smaller components. This
involves gradually replacing specific application components with new applications and services. AWS
Migration Hub Refactor Spaces acts as the starting point for incremental refactoring. For more details,
see Seamlessly migrate on-premises legacy workloads using a strangler pattern.
• Implementing microservices may require a service discovery mechanism to allow these distributed
services to communicate with each other. AWS App Mesh can be used with service-oriented
architectures to provide reliable discovery and access of services. AWS Cloud Map can also be used for
dynamic, DNS-based service discovery.
• If you’re migrating from a monolith to SOA, Amazon MQ can help bridge the gap as a service bus when
redesigning legacy applications in the cloud.
• For existing monoliths with a single, shared database, choose how to reorganize the data into smaller
segments. This could be by business unit, access pattern, or data structure. At this point in the
refactoring process, you should choose to move forward with a relational or non-relational (NoSQL)
type of database. For more details, see From SQL to NoSQL.
Level of effort for the implementation plan: High
Resources
Related best practices:
• REL03-BP02 Build services focused on specific business domains and functionality (p. 274)
Related documents:
• Amazon API Gateway: Configuring a REST API Using OpenAPI
• What is Service-Oriented Architecture?
• Bounded Context (a central pattern in Domain-Driven Design)
• Implementing Microservices on AWS
• Microservice Trade-Offs
• Microservices - a definition of this new architectural term
• Microservices on AWS
• What is AWS App Mesh?
Related examples:
• Iterative App Modernization Workshop
Related videos:
• Delivering Excellence with Microservices on AWS
REL03-BP02 Build services focused on specific business domains and
functionality
This best practice was updated with new guidance on July 13th, 2023.
274

AWS Well-Architected Framework
Workload architecture
Service-oriented architectures (SOA) define services with well-delineated functions defined by business
needs. Microservices use domain models and bounded context to draw service boundaries along business
context boundaries. Focusing on business domains and functionality helps teams define independent
reliability requirements for their services. Bounded contexts isolate and encapsulate business logic,
allowing teams to better reason about how to handle failures.
Desired outcome: Engineers and business stakeholders jointly define bounded contexts and use them to
design systems as services that fulfill specific business functions. These teams use established practices
like event storming to define requirements. New applications are designed as services well-defined
boundaries and loosely coupling. Existing monoliths are decomposed into bounded contexts and system
designs move towards SOA or microservice architectures. When monoliths are refactored, established
approaches like bubble contexts and monolith decomposition patterns are applied.
Domain-oriented services are executed as one or more processes that don’t share state. They
independently respond to fluctuations in demand and handle fault scenarios in light of domain specific
requirements.
Common anti-patterns:
• Teams are formed around specific technical domains like UI and UX, middleware, or database instead
of specific business domains.
• Applications span domain responsibilities. Services that span bounded contexts can be more difficult
to maintain, require larger testing efforts, and require multiple domain teams to participate in
software updates.
• Domain dependencies, like domain entity libraries, are shared across services such that changes for
one service domain require changes to other service domains
• Service contracts and business logic don’t express entities in a common and consistent domain
language, resulting in translation layers that complicate systems and increase debugging efforts.
Benefits of establishing this best practice: Applications are designed as independent services bounded
by business domains and use a common business language. Services are independently testable and
deployable. Services meet domain specific resiliency requirements for the domain implemented.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Domain-driven decision (DDD) is the foundational approach of designing and building software around
business domains. It’s helpful to work with an existing framework when building services focused on
business domains. When working with existing monolithic applications, you can take advantage of
decomposition patterns that provide established techniques to modernize applications into services.
Domain-driven decision
Implementation steps
• Teams can hold event storming workshops to quickly identify events, commands, aggregates and
domains in a lightweight sticky note format.
275

AWS Well-Architected Framework
Workload architecture
• Once domain entities and functions have been formed in a domain context, you can divide your
domain into services using bounded context, where entities that share similar features and attributes
are grouped together. With the model divided into contexts, a template for how to boundary
microservices emerges.
• For example, the Amazon.com website entities might include package, delivery, schedule, price,
discount, and currency.
• Package, delivery, and schedule are grouped into the shipping context, while price, discount, and
currency are grouped into the pricing context.
• Decomposing monoliths into microservices outlines patterns for refactoring microservices. Using
patterns for decomposition by business capability, subdomain, or transaction aligns well with domain-
driven approaches.
• Tactical techniques such as the bubble context allow you to introduce DDD in existing or legacy
applications without up-front rewrites and full commitments to DDD. In a bubble context approach,
a small bounded context is established using a service mapping and coordination, or anti-corruption
layer, which protects the newly defined domain model from external influences.
After teams have performed domain analysis and defined entities and service contracts, they can take
advantage of AWS services to implement their domain-driven design as cloud-based services.
• Start your development by defining tests that exercise business rules of your domain. Test-driven
development (TDD) and behavior-driven development (BDD) help teams keep services focused on
solving business problems.
• Select the AWS services that best meet your business domain requirements and microservice
architecture:
• AWS Serverless allows your team focus on specific domain logic instead of managing servers and
infrastructure.
• Containers at AWS simplify the management of your infrastructure, so you can focus on your
domain requirements.
• Purpose built databases help you match your domain requirements to the best fit database type.
• Building hexagonal architectures on AWS outlines a framework to build business logic into services
working backwards from a business domain to fulfill functional requirements and then attach
integration adapters. Patterns that separate interface details from business logic with AWS services
help teams focus on domain functionality and improve software quality.
Resources
Related best practices:
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL03-BP03 Provide service contracts per API (p. 277)
Related documents:
• AWS Microservices
• Implementing Microservices on AWS
• How to break a Monolith into Microservices
• Getting Started with DDD when Surrounded by Legacy Systems
• Domain-Driven Design: Tackling Complexity in the Heart of Software
• Building hexagonal architectures on AWS
• Decomposing monoliths into microservices
• Event Storming
276

AWS Well-Architected Framework
Workload architecture
• Messages Between Bounded Contexts
• Microservices
• Test-driven development
• Behavior-driven development
Related examples:
• Enterprise Cloud Native Workshop
• Designing Cloud Native Microservices on AWS (from DDD/EventStormingWorkshop)
Related tools:
• AWS Cloud Databases
• Serverless on AWS
• Containers at AWS
REL03-BP03 Provide service contracts per API
This best practice was updated with new guidance on July 13th, 2023.
Service contracts are documented agreements between API producers and consumers defined in a
machine-readable API definition. A contract versioning strategy allows consumers to continue using the
existing API and migrate their applications to a newer API when they are ready. Producer deployment can
happen any time as long as the contract is followed. Service teams can use the technology stack of their
choice to satisfy the API contract.
Desired outcome:
Common anti-patterns: Applications built with service-oriented or microservice architectures are able
to operate independently while having integrated runtime dependency. Changes deployed to an API
consumer or producer do not interrupt the stability of the overall system when both sides follow a
common API contract. Components that communicate over service APIs can perform independent
functional releases, upgrades to runtime dependencies, or fail over to a disaster recovery (DR) site with
little or no impact to each other. In addition, discrete services are able to independently scale absorbing
resource demand without requiring other services to scale in unison.
• Creating service APIs without strongly typed schemas. This results in APIs that cannot be used to
generate API bindings and payloads that can’t be programmatically validated.
• Not adopting a versioning strategy, which forces API consumers to update and release or fail when
service contracts evolve.
• Error messages that leak details of the underlying service implementation rather than describe
integration failures in the domain context and language.
• Not using API contracts to develop test cases and mock API implementations to allow for independent
testing of service components.
Benefits of establishing this best practice: Distributed systems composed of components that
communicate over API service contracts can improve reliability. Developers can catch potential issues
early in the development process with type checking during compilation to verify that requests and
responses follow the API contract and required fields are present. API contracts provide a clear self-
documenting interface for APIs and provider better interoperability between different systems and
programming languages.
277

AWS Well-Architected Framework
Workload architecture
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Once you have identified business domains and determined your workload segmentation, you can
develop your service APIs. First, define machine-readable service contracts for APIs, and then implement
an API versioning strategy. When you are ready to integrate services over common protocols like REST,
GraphQL, or asynchronous events, you can incorporate AWS services into your architecture to integrate
your components with strongly-typed API contracts.
AWS services for service API contrats
Incorporate AWS services including Amazon API Gateway, AWS AppSync, and Amazon EventBridge
into your architecture to use API service contracts in your application. Amazon API Gateway helps you
integrate with directly native AWS services and other web services. API Gateway supports the OpenAPI
specification and versioning. AWS AppSync is a managed GraphQL endpoint you configure by defining
a GraphQL schema to define a service interface for queries, mutations and subscriptions. Amazon
EventBridge uses event schemas to define events and generate code bindings for your events.
Implementation steps
•                                                                                                          First, define a contract for your API. A contract will express the capabilities of an API as well as define
strongly typed data objects and fields for the API input and output.
•                                                                                                          When you configure APIs in API Gateway, you can import and export OpenAPI Specifications for your
endpoints.
• Importing an OpenAPI definition simplifies the creation of your API and can be integrated with AWS
infrastructure as code tools like the AWS Serverless Application Model and AWS Cloud Development
Kit (AWS CDK).
• Exporting an API definition simplifies integrating with API testing tools and provides services
consumer an integration specification.
•                                                                                                          You can define and manage GraphQL APIs with AWS AppSync by defining a GraphQL schema file
to generate your contract interface and simplify interaction with complex REST models, multiple
database tables or legacy services.
•                                                                                                          AWS Amplify projects that are integrated with AWS AppSync generate strongly typed JavaScript
query files for use in your application as well as an AWS AppSync GraphQL client library for Amazon
DynamoDB tables.
•                                                                                                          When you consume service events from Amazon EventBridge, events adhere to schemas that already
exist in the schema registry or that you define with the OpenAPI Spec. With a schema defined in the
registry, you can also generate client bindings from the schema contract to integrate your code with
events.
•                                                                                                          Extending or version your API. Extending an API is a simpler option when adding fields that can be
configured with optional fields or default values for required fields.
• JSON based contracts for protocols like REST and GraphQL can be a good fit for contract extension.
• XML based contracts for protocols like SOAP should be tested with service consumers to determine
the feasibility of contract extension.
•                                                                                                          When versioning an API, consider implementing proxy versioning where a facade is used to support
versions so that logic can be maintained in a single codebase.
• With API Gateway you can use request and response mappings to simplify absorbing contract
changes by establishing a facade to provide default values for new fields or to strip removed
fields from a request or response. With this approach the underlying service can maintain a single
codebase.
Resources
Related best practices:
278

AWS Well-Architected Framework
Workload architecture
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL03-BP02 Build services focused on specific business domains and functionality (p. 274)
• REL04-BP02 Implement loosely coupled dependencies (p. 280)
• REL05-BP03 Control and limit retry calls (p. 290)
• REL05-BP05 Set client timeouts (p. 294)
Related documents:
• What Is An API (Application Programming Interface)?
• Implementing Microservices on AWS
• Microservice Trade-Offs
• Microservices - a definition of this new architectural term
• Microservices on AWS
• Working with API Gateway extensions to OpenAPI
• OpenAPI-Specification
• GraphQL: Schemas and Types
• Amazon EventBridge code bindings
Related examples:
• Amazon API Gateway: Configuring a REST API Using OpenAPI
• Amazon API Gateway to Amazon DynamoDB CRUD application using OpenAPI
• Modern application integration patterns in a serverless age: API Gateway Service Integration
• Implementing header-based API Gateway versioning with Amazon CloudFront
• AWS AppSync: Building a client application
Related videos:
• Using OpenAPI in AWS SAM to manage API Gateway
Related tools:
• Amazon API Gateway
• AWS AppSync
• Amazon EventBridge
REL 4. How do you design interactions in a distributed system to
prevent failures?
Distributed systems rely on communications networks to interconnect components, such as servers
or services. Your workload must operate reliably despite data loss or latency in these networks.
Components of the distributed system must operate in a way that does not negatively impact other
components or the workload. These best practices prevent failures and improve mean time between
failures (MTBF).
Best practices
• REL04-BP01 Identify which kind of distributed system is required (p. 280)
• REL04-BP02 Implement loosely coupled dependencies (p. 280)
279

AWS Well-Architected Framework
Workload architecture
• REL04-BP03 Do constant work (p. 283)
• REL04-BP04 Make all responses idempotent (p. 284)
REL04-BP01 Identify which kind of distributed system is required
Hard real-time distributed systems require responses to be given synchronously and rapidly, while soft
real-time systems have a more generous time window of minutes or more for response. Offline systems
handle responses through batch or asynchronous processing. Hard real-time distributed systems have
the most stringent reliability requirements.
The most difficult challenges with distributed systems are for the hard real-time distributed systems,
also known as request/reply services. What makes them difficult is that requests arrive unpredictably
and responses must be given rapidly (for example, the customer is actively waiting for the response).
Examples include front-end web servers, the order pipeline, credit card transactions, every AWS API, and
telephony.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Identify which kind of distributed system is required. Challenges with distributed systems involved
latency, scaling, understanding networking APIs, marshalling and unmarshalling data, and the
complexity of algorithms such as Paxos. As the systems grow larger and more distributed, what had
been theoretical edge cases turn into regular occurrences.
• The Amazon Builders' Library: Challenges with distributed systems
• Hard real-time distributed systems require responses to be given synchronously and rapidly.
• Soft real-time systems have a more generous time window of minutes or greater for response.
• Offline systems handle responses through batch or asynchronous processing.
• Hard real-time distributed systems have the most stringent reliability requirements.
Resources
Related documents:
• Amazon EC2: Ensuring Idempotency
• The Amazon Builders' Library: Challenges with distributed systems
• The Amazon Builders' Library: Reliability, constant work, and a good cup of coffee
• What Is Amazon EventBridge?
• What Is Amazon Simple Queue Service?
Related videos:
• AWS New York Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
ARC337 (includes loose coupling, constant work, static stability)
• AWS re:Invent 2019: Moving to event-driven architectures (SVS308)
REL04-BP02 Implement loosely coupled dependencies
Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely
coupled. Loose coupling helps isolate behavior of a component from other components that depend on
it, increasing resiliency and agility.
280

AWS Well-Architected Framework
Workload architecture
If changes to one component force other components that rely on it to also change, then they
are tightly coupled. Loose coupling breaks this dependency so that dependent components only need
to know the versioned and published interface. Implementing loose coupling between dependencies
isolates a failure in one from impacting another.
Loose coupling allows you to add additional code or features to a component while minimizing risk
to components that depend on it. Also, scalability is improved as you can scale out or even change
underlying implementation of the dependency.
To further improve resiliency through loose coupling, make component interactions asynchronous where
possible. This model is suitable for any interaction that does not need an immediate response and where
an acknowledgment that a request has been registered will suffice. It involves one component that
generates events and another that consumes them. The two components do not integrate through direct
point-to-point interaction but usually through an intermediate durable storage layer, such as an SQS
queue or a streaming data platform such as Amazon Kinesis, or AWS Step Functions.
Figure 4: Dependencies such as queuing systems and load balancers are loosely coupled
Amazon SQS queues and Elastic Load Balancers are just two ways to add an intermediate layer for loose
coupling. Event-driven architectures can also be built in the AWS Cloud using Amazon EventBridge,
which can abstract clients (event producers) from the services they rely on (event consumers). Amazon
Simple Notification Service (Amazon SNS) is an effective solution when you need high-throughput,
push-based, many-to-many messaging. Using Amazon SNS topics, your publisher systems can fan out
messages to a large number of subscriber endpoints for parallel processing.
281

AWS Well-Architected Framework
Workload architecture
While queues offer several advantages, in most hard real-time systems, requests older than a threshold
time (often seconds) should be considered stale (the client has given up and is no longer waiting for
a response), and not processed. This way more recent (and likely still valid requests) can be processed
instead.
Common anti-patterns:
• Deploying a singleton as part of a workload.
• Directly invoking APIs between workload tiers with no capability of failover or asynchronous
processing of the request.
Benefits of establishing this best practice: Loose coupling helps isolate behavior of a component
from other components that depend on it, increasing resiliency and agility. Failure in one component is
isolated from others.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Implement loosely coupled dependencies. Dependencies such as queuing systems, streaming systems,
workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a
component from other components that depend on it, increasing resiliency and agility.
• AWS re:Invent 2019: Moving to event-driven architectures (SVS308)
• What Is Amazon EventBridge?
• What Is Amazon Simple Queue Service?
• Amazon EventBridge allows you to build event driven architectures, which are loosely coupled and
distributed.
• AWS New York Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge
(MAD205)
• If changes to one component force other components that rely on it to also change, then they
are tightly coupled. Loose coupling breaks this dependency so that dependency components only
need to know the versioned and published interface.
• Make component interactions asynchronous where possible. This model is suitable for any
interaction that does not need an immediate response and where an acknowledgement that a
request has been registered will suffice.
• AWS re:Invent 2019: Scalable serverless event-driven applications using Amazon SQS and
Lambda (API304)
Resources
Related documents:
• AWS re:Invent 2019: Moving to event-driven architectures (SVS308)
• Amazon EC2: Ensuring Idempotency
• The Amazon Builders' Library: Challenges with distributed systems
• The Amazon Builders' Library: Reliability, constant work, and a good cup of coffee
• What Is Amazon EventBridge?
• What Is Amazon Simple Queue Service?
Related videos:
• AWS New York Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)
282

AWS Well-Architected Framework
Workload architecture
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
ARC337 (includes loose coupling, constant work, static stability)
• AWS re:Invent 2019: Moving to event-driven architectures (SVS308)
• AWS re:Invent 2019: Scalable serverless event-driven applications using Amazon SQS and Lambda
(API304)
REL04-BP03 Do constant work
Systems can fail when there are large, rapid changes in load. For example, if your workload is doing a
health check that monitors the health of thousands of servers, it should send the same size payload (a
full snapshot of the current state) each time. Whether no servers are failing, or all of them, the health
check system is doing constant work with no large, rapid changes.
For example, if the health check system is monitoring 100,000 servers, the load on it is nominal under
the normally light server failure rate. However, if a major event makes half of those servers unhealthy,
then the health check system would be overwhelmed trying to update notification systems and
communicate state to its clients. So instead the health check system should send the full snapshot of
the current state each time. 100,000 server health states, each represented by a bit, would only be a
12.5-KB payload. Whether no servers are failing, or all of them are, the health check system is doing
constant work, and large, rapid changes are not a threat to the system stability. This is actually how
Amazon Route 53 handles health checks for endpoints (such as IP addresses) to determine how end users
are routed to them.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
• Do constant work so that systems do not fail when there are large, rapid changes in load.
• Implement loosely coupled dependencies. Dependencies such as queuing systems, streaming systems,
workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a
component from other components that depend on it, increasing resiliency and agility.
• The Amazon Builders' Library: Reliability, constant work, and a good cup of coffee
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and
Small ARC337 (includes constant work)
• For the example of a health check system monitoring 100,000 servers, engineer workloads so that
payload sizes remain constant regardless of number of successes or failures.
Resources
Related documents:
• Amazon EC2: Ensuring Idempotency
• The Amazon Builders' Library: Challenges with distributed systems
• The Amazon Builders' Library: Reliability, constant work, and a good cup of coffee
Related videos:
• AWS New York Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
ARC337 (includes constant work)
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
ARC337 (includes loose coupling, constant work, static stability)
• AWS re:Invent 2019: Moving to event-driven architectures (SVS308)
283

AWS Well-Architected Framework
Workload architecture
REL04-BP04 Make all responses idempotent
An idempotent service promises that each request is completed exactly once, such that making multiple
identical requests has the same effect as making a single request. An idempotent service makes it easier
for a client to implement retries without fear that a request will be erroneously processed multiple times.
To do this, clients can issue API requests with an idempotency token—the same token is used whenever
the request is repeated. An idempotent service API uses the token to return a response identical to the
response that was returned the first time that the request was completed.
In a distributed system, it’s easy to perform an action at most once (client makes only one request), or at
least once (keep requesting until client gets confirmation of success). But it’s hard to guarantee an action
is idempotent, which means it’s performed exactly once, such that making multiple identical requests
has the same effect as making a single request. Using idempotency tokens in APIs, services can receive a
mutating request one or more times without creating duplicate records or side effects.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Make all responses idempotent. An idempotent service promises that each request is completed
exactly once, such that making multiple identical requests has the same effect as making a single
request.
• Clients can issue API requests with an idempotency token—the same token is used whenever the
request is repeated. An idempotent service API uses the token to return a response identical to the
response that was returned the first time that the request was completed.
• Amazon EC2: Ensuring Idempotency
Resources
Related documents:
• Amazon EC2: Ensuring Idempotency
• The Amazon Builders' Library: Challenges with distributed systems
• The Amazon Builders' Library: Reliability, constant work, and a good cup of coffee
Related videos:
• AWS New York Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
ARC337 (includes loose coupling, constant work, static stability)
• AWS re:Invent 2019: Moving to event-driven architectures (SVS308)
REL 5. How do you design interactions in a distributed system to
mitigate or withstand failures?
Distributed systems rely on communications networks to interconnect components (such as servers
or services). Your workload must operate reliably despite data loss or latency over these networks.
Components of the distributed system must operate in a way that does not negatively impact other
components or the workload. These best practices permit workloads to withstand stresses or failures,
more quickly recover from them, and mitigate the impact of such impairments. The result is improved
mean time to recovery (MTTR).
Best practices
284

AWS Well-Architected Framework
Workload architecture
• REL05-BP01 Implement graceful degradation to transform applicable hard dependencies into soft
dependencies (p. 285)
• REL05-BP02 Throttle requests (p. 287)
• REL05-BP03 Control and limit retry calls (p. 290)
• REL05-BP04 Fail fast and limit queues (p. 292)
• REL05-BP05 Set client timeouts (p. 294)
• REL05-BP06 Make services stateless where possible (p. 297)
• REL05-BP07 Implement emergency levers (p. 298)
REL05-BP01 Implement graceful degradation to transform applicable hard
dependencies into soft dependencies
This best practice was updated with new guidance on July 13th, 2023.
Application components should continue to perform their core function even if dependencies become
unavailable. They might be serving slightly stale data, alternate data, or even no data. This ensures
overall system function is only minimally impeded by localized failures while delivering the central
business value.
Desired outcome: When a component's dependencies are unhealthy, the component itself can still
function, although in a degraded manner. Failure modes of components should be seen as normal
operation. Workflows should be designed in such a way that such failures do not lead to complete failure
or at least to predictable and recoverable states.
Common anti-patterns:
• Not identifying the core business functionality needed. Not testing that components are functional
even during dependency failures.
• Serving no data on errors or when only one out of multiple dependencies is unavailable and partial
results can still be returned.
• Creating an inconsistent state when a transaction partially fails.
• Not having an alternative way to access a central parameter store.
• Invalidating or emptying local state as a result of a failed refresh without considering the
consequences of doing so.
Benefits of establishing this best practice: Graceful degradation improves the availability of the system
as a whole and maintains the functionality of the most important functions even during failures.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Implementing graceful degradation helps minimize the impact of dependency failures on component
function. Ideally, a component detects dependency failures and works around them in a way that
minimally impacts other components or customers.
Architecting for graceful degradation means considering potential failure modes during dependency
design. For each failure mode, have a way to deliver most or at least the most critical functionality of the
component to callers or customers. These considerations can become additional requirements that can
be tested and verified. Ideally, a component is able to perform its core function in an acceptable manner
even when one or multiple dependencies fail.
285

AWS Well-Architected Framework
Workload architecture
This is as much a business discussion as a technical one. All business requirements are important and
should be fulfilled if possible. However, it still makes sense to ask what should happen when not all of
them can be fulfilled. A system can be designed to be available and consistent, but under circumstances
where one requirement must be dropped, which one is more important? For payment processing, it
might be consistency. For a real-time application, it might be availability. For a customer facing website,
the answer may depend on customer expectations.
What this means depends on the requirements of the component and what should be considered its core
function. For example:
• An ecommerce website might display data from multiple different systems like personalized
recommendations, highest ranked products, and status of customer orders on the landing page. When
one upstream system fails, it still makes sense to display everything else instead of showing an error
page to a customer.
• A component performing batch writes can still continue processing a batch if one of the individual
operations fails. It should be simple to implement a retry mechanism. This can be done by returning
information on which operations succeeded, which failed, and why they failed to the caller, or putting
failed requests into a dead letter queue to implement asynchronous retries. Information about failed
operations should be logged as well.
• A system that processes transactions must verify that either all or no individual updates are executed.
For distributed transactions, the saga pattern can be used to roll back previous operations in case a
later operation of the same transaction fails. Here, the core function is maintaining consistency.
• Time critical systems should be able to deal with dependencies not responding in a timely manner. In
these cases, the circuit breaker pattern can be used. When responses from a dependency start timing
out, the system can switch to a closed state where no additional call are made.
• An application may read parameters from a parameter store. It can be useful to create container
images with a default set of parameters and use these in case the parameter store is unavailable.
Note that the pathways taken in case of component failure need to be tested and should be significantly
simpler than the primary pathway. Generally, fallback strategies should be avoided.
Implementation steps
Identify external and internal dependencies. Consider what kinds of failures can occur in them. Think
about ways that minimize negative impact on upstream and downstream systems and customers during
those failures.
The following is a list of dependencies and how to degrade gracefully when they fail:
1. Partial failure of dependencies: A component may make multiple requests to downstream systems,
either as multiple requests to one system or one request to multiple systems each. Depending on the
business context, different ways of handling for this may be appropriate (for more detail, see previous
examples in Implementation guidance).
2. A downstream system is unable to process requests due to high load: If requests to a downstream
system are consistently failing, it does not make sense to continue retrying. This may create additional
load on an already overloaded system and make recovery more difficult. The circuit breaker pattern
can be utilized here, which monitors failing calls to a downstream system. If a high number of calls are
failing, it will stop sending more requests to the downstream system and only occasionally let calls
through to test whether the downstream system is available again.
3. A parameter store is unavailable: To transform a parameter store, soft dependency caching or sane
defaults included in container or machine images may be used. Note that these defaults need to be
kept up-to-date and included in test suites.
4. A monitoring service or other non-functional dependncy is unavailable: If a component is
intermittently unable to send logs, metrics, or traces to a central monitoring service, it is often best
to still execute business functions as usual. Silently not logging or pushing metrics for a long time is
286

AWS Well-Architected Framework
Workload architecture
often not acceptable. Also, some use cases may require complete auditing entries to fulfill compliance
requirements.
5. A primary instance of a relational database may be unavailable: Amazon Relational Database
Service, like almost all relational databases, can only have one primary writer instance. This creates
a single point of failure for write workloads and makes scaling more difficult. This can partially be
mitigated by using a Multi-AZ configuration for high availability or Amazon Aurora Serverless for
better scaling. For very high availability requirements, it can make sense to not rely on the primary
writer at all. For queries that only read, read replicas can be used, which provide redundancy and
the ability to scale out, not just up. Writes can be buffered, for example in an Amazon Simple Queue
Service queue, so that write requests from customers can still be accepted even if the primary is
temporarily unavailable.
Resources
Related documents:
• Amazon API Gateway: Throttle API Requests for Better Throughput
• CircuitBreaker (summarizes Circuit Breaker from “Release It!” book)
• Error Retries and Exponential Backoff in AWS
• Michael Nygard “Release It! Design and Deploy Production-Ready Software”
• The Amazon Builders' Library: Avoiding fallback in distributed systems
• The Amazon Builders' Library: Avoiding insurmountable queue backlogs
• The Amazon Builders' Library: Caching challenges and strategies
• The Amazon Builders' Library: Timeouts, retries, and backoff with jitter
Related videos:
• Retry, backoff, and jitter: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328)
Related examples:
• Well-Architected lab: Level 300: Implementing Health Checks and Managing Dependencies to Improve
Reliability
REL05-BP02 Throttle requests
This best practice was updated with new guidance on July 13th, 2023.
Throttle requests to mitigate resource exhaustion due to unexpected increases in demand. Requests
below throttling rates are processed while those over the defined limit are rejected with a return a
message indicating the request was throttled.
Desired outcome: Large volume spikes either from sudden customer traffic increases, flooding attacks,
or retry storms are mitigated by request throttling, allowing workloads to continue normal processing of
supported request volume.
Common anti-patterns:
• API endpoint throttles are not implemented or are left at default values without considering expected
volumes.
• API endpoints are not load tested or throttling limits are not tested.
287

AWS Well-Architected Framework
Workload architecture
• Throttling request rates without considering request size or complexity.
• Testing maximum request rates or maximum request size, but not testing both together.
• Resources are not provisioned to the same limits established in testing.
• Usage plans have not been configured or considered for application to application (A2A) API
consumers.
• Queue consumers that horizontally scale do not have maximum concurrency settings configured.
• Rate limiting on a per IP address basis has not been implemented.
Benefits of establishing this best practice: Workloads that set throttle limits are able to operate
normally and process accepted request load successfully under unexpected volume spikes. Sudden or
sustained spikes of requests to APIs and queues are throttled and do not exhaust request processing
resources. Rate limits throttle individual requestors so that high volumes of traffic from a single IP
address or API consumer will not exhaust resources impact other consumers.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Services should be designed to process a known capacity of requests; this capacity can be established
through load testing. If request arrival rates exceed limits, the appropriate response signals that a
request has been throttled. This allows the consumer to handle the error and retry later.
When your service requires a throttling implementation, consider implementing the token bucket
algorithm, where a token counts for a request. Tokens are refilled at a throttle rate per second and
emptied asynchronously by one token per request.
The token bucket algorithm.
Amazon API Gateway implements the token bucket algorithm according to account and region limits
and can be configured per-client with usage plans. Additionally, Amazon Simple Queue Service (Amazon
SQS) and Amazon Kinesis can buffer requests to smooth out the request rate, and allow higher throttling
rates for requests that can be addressed. Finally, you can implement rate limiting with AWS WAF to
throttle specific API consumers that generate unusually high load.
Implementation steps
You can configure API Gateway with throttling limits for your APIs and return 429 Too Many
Requests errors when limits are exceeded. You can use AWS WAF with your AWS AppSync and API
288

AWS Well-Architected Framework
Workload architecture
Gateway endpoints to enable rate limiting on a per IP address basis. Additionally, where your system can
tolerate asynchronous processing, you can put messages into a queue or stream to speed up responses
to service clients, which allows you to burst to higher throttle rates.
With asynchronous processing, when you’ve configured Amazon SQS as an event source for AWS
Lambda, you can configure maximum concurrency to avoid high event rates from consuming available
account concurrent execution quota needed for other services in your workload or account.
While API Gateway provides a managed implementation of the token bucket, in cases where you cannot
use API Gateway, you can take advantage of language specific open-source implementations (see related
examples in Resources) of the token bucket for your services.
• Understand and configure API Gateway throttling limits at the account level per region, API per stage,
and API key per usage plan levels.
• Apply AWS WAF rate limiting rules to API Gateway and AWS AppSync endpoints to protect against
floods and block malicious IPs. Rate limiting rules can also be configured on AWS AppSync API keys for
A2A consumers.
• Consider whether you require more throttling control than rate limiting for AWS AppSync APIs, and if
so, configure an API Gateway in front of your AWS AppSync endpoint.
• When Amazon SQS queues are set up as triggers for Lambda queue consumers, set maximum
concurrency to a value that processes enough to meet your service level objectives but does not
consume concurrency limits impacting other Lambda functions. Consider setting reserved concurrency
on other Lambda functions in the same account and region when you consume queues with Lambda.
• Use API Gateway with native service integrations to Amazon SQS or Kinesis to buffer requests.
• If you cannot use API Gateway, look at language specific libraries to implement the token bucket
algorithm for your workload. Check the examples section and do your own research to find a suitable
library.
• Test limits that you plan to set, or that you plan to allow to be increased, and document the tested
limits.
• Do not increase limits beyond what you establish in testing. When increasing a limit, verify that
provisioned resources are already equivalent to or greater than those in test scenarios before applying
the increase.
Resources
Related best practices:
• REL04-BP03 Do constant work (p. 283)
• REL05-BP03 Control and limit retry calls (p. 290)
Related documents:
• Amazon API Gateway: Throttle API Requests for Better Throughput
• AWS WAF: Rate-based rule statement
• Introducing maximum concurrency of AWS Lambda when using Amazon SQS as an event source
• AWS Lambda: Maximum Concurrency
Related examples:
• The three most important AWS WAF rate-based rules
• Java Bucket4j
• Python token-bucket
289

AWS Well-Architected Framework
Workload architecture
• Node token-bucket
•                                                                                                               .NET System Threading Rate Limiting
Related videos:
• Implementing GraphQL API security best practices with AWS AppSync
Related tools:
• Amazon API Gateway
• AWS AppSync
• Amazon SQS
• Amazon Kinesis
• AWS WAF
REL05-BP03 Control and limit retry calls
This best practice was updated with new guidance on July 13th, 2023.
Use exponential backoff to retry requests at progressively longer intervals between each retry. Introduce
jitter between retries to randomize retry intervals. Limit the maximum number of retries.
Desired outcome: Typical components in a distributed software system include servers, load balancers,
databases, and DNS servers. During normal operation, these components can respond to requests with
errors that are temporary or limited, and also errors that would be persistent regardless of retries.
When clients make requests to services, the requests consume resources including memory, threads,
connections, ports, or any other limited resources. Controlling and limiting retries is a strategy to release
and minimize consumption of resources so that system components under strain are not overwhelmed.
When client requests time out or receive error responses, they should determine whether or not to
retry. If they do retry, they do so with exponential backoff with jitter and a maximum retry value. As a
result, backend services and processes are given relief from load and time to self-heal, resulting in faster
recovery and successful request servicing.
Common anti-patterns:
• Implementing retries without adding exponential backoff, jitter, and maximum retry values. Backoff
and jitter help avoid artificial traffic spikes due to unintentionally coordinated retries at common
intervals.
• Implementing retries without testing their effects or assuming retries are already built into an SDK
without testing retry scenarios.
• Failing to understand published error codes from dependencies, leading to retrying all errors, including
those with a clear cause that indicates lack of permission, configuration error, or another condition
that predictably will not resolve without manual intervention.
• Not addressing observability practices, including monitoring and alerting on repeated service failures
so that underlying issues are made known and can be addressed.
• Developing custom retry mechanisms when built-in or third-party retry capabilities suffice.
• Retrying at multiple layers of your application stack in a manner which compounds retry attempts
further consuming resources in a retry storm. Be sure to understand how these errors affect your
application the dependencies you rely on, then implement retries at only one level.
290

AWS Well-Architected Framework
Workload architecture
• Retrying service calls that are not idempotent, causing unexpected side effects like duplicated results.
Benefits of establishing this best practice: Retries help clients acquire desired results when requests
fail but also consume more of a server’s time to get the successful responses they want. When failures
are rare or transient, retries work well. When failures are caused by resource overload, retries can make
things worse. Adding exponential backoff with jitter to client retries allows servers to recover when
failures are caused by resource overload. Jitter avoids alignment of requests into spikes, and backoff
diminishes load escalation caused by adding retries to normal request load. Finally, it’s important
to configure a maximum number of retries or elapsed time to avoid creating backlogs that produce
metastable failures.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Control and limit retry calls. Use exponential backoff to retry after progressively longer intervals.
Introduce jitter to randomize retry intervals and limit the maximum number of retries.
Some AWS SDKs implement retries and exponential backoff by default. Use these built-in AWS
implementations where applicable in your workload. Implement similar logic in your workload when
calling services that are idempotent and where retries improve your client availability. Decide what the
timeouts are and when to stop retrying based on your use case. Build and exercise testing scenarios for
those retry use cases.
Implementation steps
• Determine the optimal layer in your application stack to implement retries for the services your
application relies on.
• Be aware of existing SDKs that implement proven retry strategies with exponential backoff and jitter
for your language of choice, and favor these over writing your own retry implementations.
• Verify that services are idempotent before implementing retries. Once retries are implemented, be
sure they are both tested and regularly exercise in production.
• When calling AWS service APIs, use the AWS SDKs and AWS CLI and understand the retry configuration
options. Determine if the defaults work for your use case, test, and adjust as needed.
Resources
Related best practices:
• REL04-BP04 Make all responses idempotent (p. 284)
• REL05-BP02 Throttle requests (p. 287)
• REL05-BP04 Fail fast and limit queues (p. 292)
• REL05-BP05 Set client timeouts (p. 294)
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
Related documents:
• Error Retries and Exponential Backoff in AWS
• The Amazon Builders' Library: Timeouts, retries, and backoff with jitter
• Exponential Backoff and Jitter
• Making retries safe with idempotent APIs
Related examples:
291

AWS Well-Architected Framework
Workload architecture
• Spring Retry
• Resilience4j Retry
Related videos:
• Retry, backoff, and jitter: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328)
Related tools:
• AWS SDKs and Tools: Retry behavior
• AWS Command Line Interface: AWS CLI retries
REL05-BP04 Fail fast and limit queues
This best practice was updated with new guidance on July 13th, 2023.
When a service is unable to respond successfully to a request, fail fast. This allows resources associated
with a request to be released, and permits a service to recover if it’s running out of resources. Failing fast
is a well-established software design pattern that can be leveraged to build highly reliable workloads in
the cloud. Queuing is also a well-established enterprise integration pattern that can smooth load and
allow clients to release resources when asynchronous processing can be tolerated. When a service is
able to respond successfully under normal conditions but fails when the rate of requests is too high, use
a queue to buffer requests. However, do not allow a buildup of long queue backlogs that can result in
processing stale requests that a client has already given up on.
Desired outcome: When systems experience resource contention, timeouts, exceptions, or grey failures
that make service level objectives unachievable, fail fast strategies allow for faster system recovery.
Systems that must absorb traffic spikes and can accommodate asynchronous processing can improve
reliability by allowing clients to quickly release requests by using queues to buffer requests to backend
services. When buffering requests to queues, queue management strategies are implemented to avoid
insurmountable backlogs.
Common anti-patterns:
• Implementing message queues but not configuring dead letter queues (DLQ) or alarms on DLQ
volumes to detect when a system is in failure.
• Not measuring the age of messages in a queue, a measurement of latency to understand when queue
consumers are falling behind or erroring out causing retrying.
• Not clearing backlogged messages from a queue, when there is no value in processing these messages
if the business need no longer exists.
• Configuring first in first out (FIFO) queues when last in first out (LIFO) queues would better serve client
needs, for example when strict ordering is not required and backlog processing is delaying all new and
time sensitive requests resulting in all clients experiencing breached service levels.
• Exposing internal queues to clients instead of exposing APIs that manage work intake and place
requests into internal queues.
• Combining too many work request types into a single queue which can exacerbate backlog conditions
by spreading resource demand across request types.
• Processing complex and simple requests in the same queue, despite needing different monitoring,
timeouts and resource allocations.
• Not validating inputs or using assertions to implement fail fast mechanisms in software that bubble up
exceptions to higher level components that can handle errors gracefully.
292

AWS Well-Architected Framework
Workload architecture
• Not removing faulty resources from request routing, especially when failures are grey emitting both
successes and failures due to crashing and restarting, intermittent dependency failure, reduced
capacity, or network packet loss.
Benefits of establishing this best practice: Systems that fail fast are easier to debug and fix, and often
expose issues in coding and configuration before releases are published into production. Systems that
incorporate effective queueing strategies provide greater resilience and reliability to traffic spikes and
intermittent system fault conditions.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Fail fast strategies can be coded into software solutions as well as configured into infrastructure. In
addition to failing fast, queues are a straightforward yet powerful architectural technique to decouple
system components smooth load. Amazon CloudWatch provides capabilities to monitor for and alarm
on failures. Once a system is known to be failing, mitigation strategies can be invoked, including failing
away from impaired resources. When systems implement queues with Amazon SQS and other queue
technologies to smooth load, they must consider how to manage queue backlogs, as well as message
consumption failures.
Implementation steps
• Implement programmatic assertions or specific metrics in your software and use them to explicitly
alert on system issues. Amazon CloudWatch helps you create metrics and alarms based on application
log pattern and SDK instrumentation.
• Use CloudWatch metrics and alarms to fail away from impaired resources that are adding latency to
processing or repeatedly failing to process requests.
• Use asynchronous processing by designing APIs to accept requests and append requests to internal
queues using Amazon SQS and then respond to the message-producing client with a success message
so the client can release resources and move on with other work while backend queue consumers
process requests.
• Measure and monitor for queue processing latency by producing a CloudWatch metric each time you
take a message off a queue by comparing now to message timestamp.
• When failures prevent successful message processing or traffic spikes in volumes that cannot be
processed within service level agreements, sideline older or excess traffic to a spillover queue. This
allows priority processing of new work, and older work when capacity is available. This technique is an
approximation of LIFO processing and allows normal system processing for all new work.
• Use dead letter or redrive queues to move messages that can’t be processed out of the backlog into a
location that can be researched and resolved later
• Either retry or, when tolerable, drop old messages by comparing now to the message timestamp and
discarding messages that are no longer relevant to the requesting client.
Resources
Related best practices:
• REL04-BP02 Implement loosely coupled dependencies (p. 280)
• REL05-BP02 Throttle requests (p. 287)
• REL05-BP03 Control and limit retry calls (p. 290)
• REL06-BP02 Define and calculate metrics (Aggregation) (p. 301)
• REL06-BP07 Monitor end-to-end tracing of requests through your system (p. 306)
Related documents:
293

AWS Well-Architected Framework
Workload architecture
• Avoiding insurmountable queue backlogs
• Fail Fast
• How can I prevent an increasing backlog of messages in my Amazon SQS queue?
• Elastic Load Balancing: Zonal Shift
• Amazon Route 53 Application Recovery Controller: Routing control for traffic failover
Related examples:
• Enterprise Integration Patterns: Dead Letter Channel
Related videos:
• AWS re:Invent 2022 - Operating highly available Multi-AZ applications
Related tools:
• Amazon SQS
• Amazon MQ
• AWS IoT Core
• Amazon CloudWatch
REL05-BP05 Set client timeouts
This best practice was updated with new guidance on July 13th, 2023.
Set timeouts appropriately on connections and requests, verify them systematically, and do not rely on
default values as they are not aware of workload specifics.
Desired outcome: Client timeouts should consider the cost to the client, server, and workload associated
with waiting for requests that take abnormal amounts of time to complete. Since it is not possible to
know the exact cause of any timeout, clients must use knowledge of services to develop expectations of
probable causes and appropriate timeouts
Client connections time out based on configured values. After encountering a timeout, clients make
decisions to back off and retry or open a circuit breaker. These patterns avoid issuing requests that may
exacerbate an underlying error condition.
Common anti-patterns:
• Not being aware of system timeouts or default timeouts.
• Not being aware of normal request completion timing.
• Not being aware of possible causes for requests to take abnormally long to complete, or the costs to
client, service, or workload performance associated with waiting on these completions.
• Not being aware of the probability of impaired network causing a request to fail only once timeout is
reached, and the costs to client and workload performance for not adopting a shorter timeout.
• Not testing timeout scenarios both for connections and requests.
• Setting timeouts too high, which can result in long wait times and increase resource utilization.
• Setting timeouts too low, resulting in artificial failures.
• Overlooking patterns to deal with timeout errors for remote calls like circuit breakers and retries.
294

AWS Well-Architected Framework
Workload architecture
• Not considering monitoring for service call error rates, service level objectives for latency, and latency
outliers. These metrics can provide insight to aggressive or permissive timeouts
Benefits of establishing this best practice: Remote call timeouts are configured and systems are
designed to handle timeouts gracefully so that resources are conserved when remote calls respond
abnormally slow and timeout errors are handled gracefully by service clients.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Set both a connection timeout and a request timeout on any service dependency call and generally on
any call across processes. Many frameworks offer built-in timeout capabilities, but be careful, as some
have default values that are infinite or higher than acceptable for your service goals. A value that is too
high reduces the usefulness of the timeout because resources continue to be consumed while the client
waits for the timeout to occur. A value that is too low can generate increased traffic on the backend
and increased latency because too many requests are retried. In some cases, this can lead to complete
outages because all requests are being retried.
Consider the following when determining timeout strategies:
• Requests may take longer than normal to process because of their content, impairments in a target
service, or a networking partition failure.
• Requests with abnormally expensive content could consume unnecessary server and client resources.
In this case, timing out these requests and not retrying can preserve resources. Services should also
protect themselves from abnormally expensive content with throttles and server-side timeouts.
• Requests that take abnormally long due to a service impairment can be timed out and retried.
Consideration should be given to service costs for the request and retry, but if the cause is a localized
impairment, a retry is not likely to be expensive and will reduce client resource consumption. The
timeout may also release server resources depending on the nature of the impairment.
• Requests that take a long time to complete because the request or response has failed to be delivered
by the network can be timed out and retried. Because the request or response was not delivered,
failure would have been the outcome regardless of the length of timeout. Timing out in this case will
not release server resources, but it will release client resources and improve workload performance.
Take advantage of well-established design patterns like retries and circuit breakers to handle timeouts
gracefully and support fail-fast approaches. AWS SDKs and AWS CLI allow for configuration of both
connection and request timeouts and for retries with exponential backoff and jitter. AWS Lambda
functions support configuration of timeouts, and with AWS Step Functions, you can build low code
circuit breakers that take advantage of pre-built integrations with AWS services and SDKs. AWS App
Mesh Envoy provides timeout and circuit breaker capabilities.
Implementation steps
• Configure timeouts on remote service calls and take advantage of built-in language timeout features
or open source timeout libraries.
• When your workload makes calls with an AWS SDK, review the documentation for language specific
timeout configuration.
• Python
• PHP
•                                                                                                              .NET
• Ruby
• Java
• Go
295

AWS Well-Architected Framework
Workload architecture
• Node.js
• C++
•                                                                                                    When using AWS SDKs or AWS CLI commands in your workload, configure default timeout
values by setting the AWS configuration defaults for connectTimeoutInMillis and
tlsNegotiationTimeoutInMillis.
•                                                                                                    Apply command line options cli-connect-timeout and cli-read-timeout to control one-off
AWS CLI commands to AWS services.
•                                                                                                    Monitor remote service calls for timeouts, and set alarms on persistent errors so that you can
proactively handle error scenarios.
•                                                                                                    Implement CloudWatch Metrics and CloudWatch anomaly detection on call error rates, service level
objectives for latency, and latency outliers to provide insight into managing overly aggressive or
permissive timeouts.
•                                                                                                    Configure timeouts on Lambda functions.
•                                                                                                    API Gateway clients must implement their own retries when handling timeouts. API Gateway supports
a 50 millisecond to 29 second integration timeout for downstream integrations and does not retry
when integration requests timeout.
•                                                                                                    Implement the circuit breaker pattern to avoid making remote calls when they are timing out. Open
the circuit to avoid failing calls and close the circuit when calls are responding normally.
•                                                                                                    For container based workloads, review App Mesh Envoy features to leverage built in timeouts and
circuit breakers.
•                                                                                                    Use AWS Step Functions to build low code circuit breakers for remote service calls, especially where
calling AWS native SDKs and supported Step Functions integrations to simplify your workload.
Resources
Related best practices:
• REL05-BP03 Control and limit retry calls (p. 290)
• REL05-BP04 Fail fast and limit queues (p. 292)
• REL06-BP07 Monitor end-to-end tracing of requests through your system (p. 306)
Related documents:
• AWS SDK: Retries and Timeouts
• The Amazon Builders' Library: Timeouts, retries, and backoff with jitter
• Amazon API Gateway quotas and important notes
• AWS Command Line Interface: Command line options
• AWS SDK for Java 2.x: Configure API Timeouts
• AWS Botocore using the config object and Config Reference
• AWS SDK for .NET: Retries and Timeouts
• AWS Lambda: Configuring Lambda function options
Related examples:
• Using the circuit breaker pattern with AWS Step Functions and Amazon DynamoDB
• Martin Fowler: CircuitBreaker
Related tools:
• AWS SDKs
296

AWS Well-Architected Framework
Workload architecture
• AWS Lambda
• Amazon SQS
• AWS Step Functions
• AWS Command Line Interface
REL05-BP06 Make services stateless where possible
Services should either not require state, or should offload state such that between different client
requests, there is no dependence on locally stored data on disk and in memory. This allows servers to be
replaced at will without causing an availability impact. Amazon ElastiCache or Amazon DynamoDB are
good destinations for offloaded state.
Figure 7: In this stateless web application, session state is offloaded to Amazon ElastiCache.
When users or services interact with an application, they often perform a series of interactions that
form a session. A session is unique data for users that persists between requests while they use
the application. A stateless application is an application that does not need knowledge of previous
interactions and does not store session information.
Once designed to be stateless, you can then use serverless compute services, such as AWS Lambda or
AWS Fargate.
297

AWS Well-Architected Framework
Workload architecture
In addition to server replacement, another benefit of stateless applications is that they can scale
horizontally because any of the available compute resources (such as EC2 instances and AWS Lambda
functions) can service any request.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Make your applications stateless. Stateless applications allow horizontal scaling and are tolerant to the
failure of an individual node.
• Remove state that could actually be stored in request parameters.
• After examining whether the state is required, move any state tracking to a resilient multi-zone
cache or data store like Amazon ElastiCache, Amazon RDS, Amazon DynamoDB, or a third-party
distributed data solution. Store a state that could not be moved to resilient data stores.
• Some data (like cookies) can be passed in headers or query parameters.
• Refactor to remove state that can be quickly passed in requests.
• Some data may not actually be needed per request and can be retrieved on demand.
• Remove data that can be asynchronously retrieved.
• Decide on a data store that meets the requirements for a required state.
• Consider a NoSQL database for non-relational data.
Resources
Related documents:
• The Amazon Builders' Library: Avoiding fallback in distributed systems
• The Amazon Builders' Library: Avoiding insurmountable queue backlogs
• The Amazon Builders' Library: Caching challenges and strategies
REL05-BP07 Implement emergency levers
Emergency levers are rapid processes that can mitigate availability impact on your workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Implement emergency levers. These are rapid processes that may mitigate availability impact on your
workload. They can be operated in the absence of a root cause. An ideal emergency lever reduces the
cognitive burden on the resolvers to zero by providing fully deterministic activation and deactivation
criteria. Levers are often manual, but they can also be automated
• Example levers include
• Block all robot traffic
• Serve static pages instead of dynamic ones
• Reduce frequency of calls to a dependency
• Throttle calls from dependencies
• Tips for implementing and using emergency levers
• When levers are activated, do LESS, not more
• Keep it simple, avoid bimodal behavior
• Test your levers periodically
• These are examples of actions that are NOT emergency levers
• Add capacity
298

AWS Well-Architected Framework
Change management
• Call up service owners of clients that depend on your service and ask them to reduce calls
• Making a change to code and releasing it
Change management
Questions
• REL 6. How do you monitor workload resources?  (p. 299)
• REL 7. How do you design your workload to adapt to changes in demand? (p. 308)
• REL 8. How do you implement change? (p. 313)
REL 6. How do you monitor workload resources?
Logs and metrics are powerful tools to gain insight into the health of your workload. You can configure
your workload to monitor logs and metrics and send notifications when thresholds are crossed
or significant events occur. Monitoring allows your workload to recognize when low-performance
thresholds are crossed or failures occur, so it can recover automatically in response.
Best practices
• REL06-BP01 Monitor all components for the workload (Generation) (p. 299)
• REL06-BP02 Define and calculate metrics (Aggregation) (p. 301)
• REL06-BP03 Send notifications (Real-time processing and alarming) (p. 302)
• REL06-BP04 Automate responses (Real-time processing and alarming) (p. 303)
• REL06-BP05 Analytics (p. 304)
• REL06-BP06 Conduct reviews regularly (p. 305)
• REL06-BP07 Monitor end-to-end tracing of requests through your system (p. 306)
REL06-BP01 Monitor all components for the workload (Generation)
Monitor the components of the workload with Amazon CloudWatch or third-party tools. Monitor AWS
services with AWS Health Dashboard.
All components of your workload should be monitored, including the front-end, business logic,
and storage tiers. Define key metrics, describe how to extract them from logs (if necessary), and set
thresholds for invoking corresponding alarm events. Ensure metrics are relevant to the key performance
indicators (KPIs) of your workload, and use metrics and logs to identify early warning signs of service
degradation. For example, a metric related to business outcomes such as the number of orders
successfully processed per minute, can indicate workload issues faster than technical metric, such as CPU
Utilization. Use AWS Health Dashboard for a personalized view into the performance and availability of
the AWS services underlying your AWS resources.
Monitoring in the cloud offers new opportunities. Most cloud providers have developed customizable
hooks and can deliver insights to help you monitor multiple layers of your workload. AWS services such
as Amazon CloudWatch apply statistical and machine learning algorithms to continually analyze metrics
of systems and applications, determine normal baselines, and surface anomalies with minimal user
intervention. Anomaly detection algorithms account for the seasonality and trend changes of metrics.
AWS makes an abundance of monitoring and log information available for consumption that can be
used to define workload-specific metrics, change-in-demand processes, and adopt machine learning
techniques regardless of ML expertise.
In addition, monitor all of your external endpoints to ensure that they are independent of your base
implementation. This active monitoring can be done with synthetic transactions (sometimes referred
299

AWS Well-Architected Framework
Change management
to as user canaries, but not to be confused with canary deployments) which periodically run a number
of common tasks matching actions performed by clients of the workload. Keep these tasks short in
duration and be sure not to overload your workload during testing. Amazon CloudWatch Synthetics
allows you to create synthetic canaries to monitor your endpoints and APIs. You can also combine
the synthetic canary client nodes with AWS X-Ray console to pinpoint which synthetic canaries are
experiencing issues with errors, faults, or throttling rates for the selected time frame.
Desired Outcome:
Collect and use critical metrics from all components of the workload to ensure workload reliability and
optimal user experience. Detecting that a workload is not achieving business outcomes allows you to
quickly declare a disaster and recover from an incident.
Common anti-patterns:
• Only monitoring external interfaces to your workload.
• Not generating any workload-specific metrics and only relying on metrics provided to you by the AWS
services your workload uses.
• Only using technical metrics in your workload and not monitoring any metrics related to non-technical
KPIs the workload contributes to.
• Relying on production traffic and simple health checks to monitor and evaluate workload state.
Benefits of establishing this best practice: Monitoring at all tiers in your workload allows you to more
rapidly anticipate and resolve problems in the components that comprise the workload.
Level of risk exposed if this best practice is not established: High
Implementation guidance
1. Turn on logging where available. Monitoring data should be obtained from all components of the
workloads. Turn on additional logging, such as S3 Access Logs, and permit your workload to log
workload specific data. Collect metrics for CPU, network I/O, and disk I/O averages from services such
as Amazon ECS, Amazon EKS, Amazon EC2, Elastic Load Balancing, AWS Auto Scaling, and Amazon
EMR. See AWS Services That Publish CloudWatch Metrics for a list of AWS services that publish metrics
to CloudWatch.
2. Review all default metrics and explore any data collection gaps. Every service generates default
metrics. Collecting default metrics allows you to better understand the dependencies between
workload components, and how component reliability and performance affect the workload. You can
also create and publish your own metrics to CloudWatch using the AWS CLI or an API.
3. Evaluate all the metrics to decide which ones to alert on for each AWS service in your workload.
You may choose to select a subset of metrics that have a major impact on workload reliability.
Focusing on critical metrics and threshold allows you to refine the number of alerts and can help
minimize false-positives.
4. Define alerts and the recovery process for your workload after the alert is invoked. Defining alerts
allows you to quickly notify, escalate, and follow steps necessary to recover from an incident and meet
your prescribed Recovery Time Objective (RTO). You can use Amazon CloudWatch Alarms to invoke
automated workflows and initiate recovery procedures based on defined thresholds.
5. Explore use of synthetic transactions to collect relevant data about workloads state. Synthetic
monitoring follows the same routes and perform the same actions as a customer, which makes
it possible for you to continually verify your customer experience even when you don't have any
customer traffic on your workloads. By using synthetic transactions, you can discover issues before
your customers do.
Resources
Related best practices:
300

AWS Well-Architected Framework
Change management
• REL11-BP03 Automate healing on all layers (p. 342)
Related documents:
•                                                                                                           Getting started with your AWS Health Dashboard - Your account health
•                                                                                                           AWS Services That Publish CloudWatch Metrics
•                                                                                                           Access Logs for Your Network Load Balancer
•                                                                                                           Access logs for your application load balancer
•                                                                                                           Accessing Amazon CloudWatch Logs for AWS Lambda
•                                                                                                           Amazon S3 Server Access Logging
•                                                                                                           Enable Access Logs for Your Classic Load Balancer
•                                                                                                           Exporting log data to Amazon S3
•                                                                                                           Install the CloudWatch agent on an Amazon EC2 instance
•                                                                                                           Publishing Custom Metrics
•                                                                                                           Using Amazon CloudWatch Dashboards
•                                                                                                           Using Amazon CloudWatch Metrics
•                                                                                                           Using Canaries (Amazon CloudWatch Synthetics)
•                                                                                                           What are Amazon CloudWatch Logs?
                                                                                                            User guides:
•                                                                                                           Creating a trail
•                                                                                                           Monitoring memory and disk metrics for Amazon EC2 Linux instances
•                                                                                                           Using CloudWatch Logs with container instances
•                                                                                                           VPC Flow Logs
•                                                                                                           What is Amazon DevOps Guru?
•                                                                                                           What is AWS X-Ray?
Related blogs:
• Debugging with Amazon CloudWatch Synthetics and AWS X-Ray
Related examples and workshops:
• AWS Well-Architected Labs: Operational Excellence - Dependency Monitoring
• The Amazon Builders' Library: Instrumenting distributed systems for operational visibility
• Observability workshop
REL06-BP02 Define and calculate metrics (Aggregation)
Store log data and apply filters where necessary to calculate metrics, such as counts of a specific log
event, or latency calculated from log event timestamps.
Amazon CloudWatch and Amazon S3 serve as the primary aggregation and storage layers. For some
services, such as AWS Auto Scaling and Elastic Load Balancing, default metrics are provided by default
for CPU load or average request latency across a cluster or instance. For streaming services, such as VPC
Flow Logs and AWS CloudTrail, event data is forwarded to CloudWatch Logs and you need to define and
apply metrics filters to extract metrics from the event data. This gives you time series data, which can
serve as inputs to CloudWatch alarms that you define to invoke alerts.
301

AWS Well-Architected Framework
Change management
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Define and calculate metrics (Aggregation). Store log data and apply filters where necessary to
calculate metrics, such as counts of a specific log event, or latency calculated from log event
timestamps
• Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs.
CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that
you can graph or set an alarm on.
• Searching and Filtering Log Data
• Use a trusted third party to aggregate logs.
• Follow the instructions of the third party. Most third-party products integrate with CloudWatch
and Amazon S3.
• Some AWS services can publish logs directly to Amazon S3. If your main requirement for logs is
storage in Amazon S3, you can easily have the service producing the logs send them directly to
Amazon S3 without setting up additional infrastructure.
• Sending Logs Directly to Amazon S3
Resources
Related documents:
• Amazon CloudWatch Logs Insights Sample Queries
• Debugging with Amazon CloudWatch Synthetics and AWS X-Ray
• One Observability Workshop
• Searching and Filtering Log Data
• Sending Logs Directly to Amazon S3
• The Amazon Builders' Library: Instrumenting distributed systems for operational visibility
REL06-BP03 Send notifications (Real-time processing and alarming)
Organizations that need to know, receive notifications when significant events occur.
Alerts can be sent to Amazon Simple Notification Service (Amazon SNS) topics, and then pushed to any
number of subscribers. For example, Amazon SNS can forward alerts to an email alias so that technical
staff can respond.
Common anti-patterns:
• Configuring alarms at too low of threshold, causing too many notifications to be sent.
• Not archiving alarms for future exploration.
Benefits of establishing this best practice: Notifications on events (even those that can be responded
to and automatically resolved) allow you to have a record of events and potentially address them in a
different manner in the future.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Perform real-time processing and alarming. Organizations that need to know, receive notifications
when significant events occur
302

AWS Well-Architected Framework
Change management
• Amazon CloudWatch dashboards are customizable home pages in the CloudWatch console that
you can use to monitor your resources in a single view, even those resources that are spread across
different Regions.
• Using Amazon CloudWatch Dashboards
• Create an alarm when the metric surpasses a limit.
• Using Amazon CloudWatch Alarms
Resources
Related documents:
• One Observability Workshop
• The Amazon Builders' Library: Instrumenting distributed systems for operational visibility
• Using Amazon CloudWatch Alarms
• Using Amazon CloudWatch Dashboards
• Using Amazon CloudWatch Metrics
REL06-BP04 Automate responses (Real-time processing and alarming)
Use automation to take action when an event is detected, for example, to replace failed components.
Alerts can invoke AWS Auto Scaling events, so that clusters react to changes in demand. Alerts can
be sent to Amazon Simple Queue Service (Amazon SQS), which can serve as an integration point for
third-party ticket systems. AWS Lambda can also subscribe to alerts, providing users an asynchronous
serverless model that reacts to change dynamically. AWS Config continually monitors and records your
AWS resource configurations, and can invoke AWS Systems Manager Automation to remediate issues.
Amazon DevOps Guru can automatically monitor application resources for anomalous behavior and
deliver targeted recommendations to speed up problem identification and remediation times.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Use Amazon DevOps Guru to perform automated actions. Amazon DevOps Guru can automatically
monitor application resources for anomalous behavior and deliver targeted recommendations to speed
up problem identification and remediation times.
• What is Amazon DevOps Guru?
• Use AWS Systems Manager to perform automated actions. AWS Config continually monitors and
records your AWS resource configurations, and can invoke AWS Systems Manager Automation to
remediate issues.
• AWS Systems Manager Automation
• Create and use Systems Manager Automation documents. These define the actions that Systems
Manager performs on your managed instances and other AWS resources when an automation
process runs.
• Working with Automation Documents (Playbooks)
• Amazon CloudWatch sends alarm state change events to Amazon EventBridge. Create EventBridge
rules to automate responses.
• Creating an EventBridge Rule That Triggers on an Event from an AWS Resource
• Create and run a plan to automate responses.
• Inventory all your alert response procedures. You must plan your alert responses before you rank the
tasks.
303

AWS Well-Architected Framework
Change management
• Inventory all the tasks with specific actions that must be taken. Most of these actions are
documented in runbooks. You must also have playbooks for alerts of unexpected events.
• Examine the runbooks and playbooks for all automatable actions. In general, if an action can be
defined, it most likely can be automated.
• Rank the error-prone or time-consuming activities first. It is most beneficial to remove sources of
errors and reduce time to resolution.
• Establish a plan to complete automation. Maintain an active plan to automate and update the
automation.
• Examine manual requirements for opportunities for automation. Challenge your manual process for
opportunities to automate.
Resources
Related documents:
• AWS Systems Manager Automation
• Creating an EventBridge Rule That Triggers on an Event from an AWS Resource
• One Observability Workshop
• The Amazon Builders' Library: Instrumenting distributed systems for operational visibility
• What is Amazon DevOps Guru?
• Working with Automation Documents (Playbooks)
REL06-BP05 Analytics
Collect log files and metrics histories and analyze these for broader trends and workload insights.
Amazon CloudWatch Logs Insights supports a simple yet powerful query language that you can use
to analyze log data. Amazon CloudWatch Logs also supports subscriptions that allow data to flow
seamlessly to Amazon S3 where you can use or Amazon Athena to query the data. It also supports
queries on a large array of formats. See Supported SerDes and Data Formats in the Amazon Athena User
Guide for more information. For analysis of huge log file sets, you can run an Amazon EMR cluster to run
petabyte-scale analyses.
There are a number of tools provided by AWS Partners and third parties that allow for aggregation,
processing, storage, and analytics. These tools include New Relic, Splunk, Loggly, Logstash, CloudHealth,
and Nagios. However, outside generation of system and application logs is unique to each cloud provider,
and often unique to each service.
An often-overlooked part of the monitoring process is data management. You need to determine the
retention requirements for monitoring data, and then apply lifecycle policies accordingly. Amazon
S3 supports lifecycle management at the S3 bucket level. This lifecycle management can be applied
differently to different paths in the bucket. Toward the end of the lifecycle, you can transition data to
Amazon S3 Glacier for long-term storage, and then expiration after the end of the retention period is
reached. The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving
data to the most cost-effective access tier, without performance impact or operational overhead.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• CloudWatch Logs Insights allows you to interactively search and analyze your log data in Amazon
CloudWatch Logs.
• Analyzing Log Data with CloudWatch Logs Insights
• Amazon CloudWatch Logs Insights Sample Queries
304

AWS Well-Architected Framework
Change management
• Use Amazon CloudWatch Logs send logs to Amazon S3 where you can use or Amazon Athena to query
the data.
• How do I analyze my Amazon S3 server access logs using Athena?
• Create an S3 lifecycle policy for your server access logs bucket. Configure the lifecycle policy to
periodically remove log files. Doing so reduces the amount of data that Athena analyzes for each
query.
• How Do I Create a Lifecycle Policy for an S3 Bucket?
Resources
Related documents:
• Amazon CloudWatch Logs Insights Sample Queries
• Analyzing Log Data with CloudWatch Logs Insights
• Debugging with Amazon CloudWatch Synthetics and AWS X-Ray
• How Do I Create a Lifecycle Policy for an S3 Bucket?
• How do I analyze my Amazon S3 server access logs using Athena?
• One Observability Workshop
• The Amazon Builders' Library: Instrumenting distributed systems for operational visibility
REL06-BP06 Conduct reviews regularly
Frequently review how workload monitoring is implemented and update it based on significant events
and changes.
Effective monitoring is driven by key business metrics. Ensure these metrics are accommodated in your
workload as business priorities change.
Auditing your monitoring helps ensure that you know when an application is meeting its availability
goals. Root cause analysis requires the ability to discover what happened when failures occur. AWS
provides services that allow you to track the state of your services during an incident:
• Amazon CloudWatch Logs: You can store your logs in this service and inspect their contents.
• Amazon CloudWatch Logs Insights: Is a fully managed service that allows you to analyze massive logs
in seconds. It gives you fast, interactive queries and visualizations.
• AWS Config: You can see what AWS infrastructure was in use at different points in time.
• AWS CloudTrail: You can see which AWS APIs were invoked at what time and by what principal.
At AWS, we conduct a weekly meeting to review operational performance and to share learnings
between teams. Because there are so many teams in AWS, we created The Wheel to randomly pick a
workload to review. Establishing a regular cadence for operational performance reviews and knowledge
sharing enhances your ability to achieve higher performance from your operational teams.
Common anti-patterns:
• Collecting only default metrics.
• Setting a monitoring strategy and never reviewing it.
• Not discussing monitoring when major changes are deployed.
Benefits of establishing this best practice: Regularly reviewing your monitoring allows for the
anticipation of potential problems, instead of reacting to notifications when an anticipated problem
actually occurs.
305

AWS Well-Architected Framework
Change management
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Create multiple dashboards for the workload. You must have a top-level dashboard that contains the
key business metrics, as well as the technical metrics you have identified to be the most relevant to
the projected health of the workload as usage varies. You should also have dashboards for various
application tiers and dependencies that can be inspected.
• Using Amazon CloudWatch Dashboards
• Schedule and conduct regular reviews of the workload dashboards. Conduct regular inspection of the
dashboards. You may have different cadences for the depth at which you inspect.
• Inspect for trends in the metrics. Compare the metric values to historic values to see if there
are trends that may indicate that something that needs investigation. Examples of this include:
increasing latency, decreasing primary business function, and increasing failure responses.
• Inspect for outliers/anomalies in your metrics. Averages or medians can mask outliers and
anomalies. Look at the highest and lowest values during the time frame and investigate the causes
of extreme scores. As you continue to eliminate these causes, lowering your definition of extreme
allows you to continue to improve the consistency of your workload performance.
• Look for sharp changes in behavior. An immediate change in quantity or direction of a metric may
indicate that there has been a change in the application, or external factors that you may need to
add additional metrics to track.
Resources
Related documents:
• Amazon CloudWatch Logs Insights Sample Queries
• Debugging with Amazon CloudWatch Synthetics and AWS X-Ray
• One Observability Workshop
• The Amazon Builders' Library: Instrumenting distributed systems for operational visibility
• Using Amazon CloudWatch Dashboards
REL06-BP07 Monitor end-to-end tracing of requests through your system
This best practice was updated with new guidance on July 13th, 2023.
Trace requests as they process through service components so product teams can more easily analyze
and debug issues and improve performance.
Desired outcome: Workloads with comprehensive tracing across all components are easy to debug,
improving mean time to resolution (MTTR) of errors and latency by simplifying root cause discovery.
End-to-end tracing reduces the time it takes to discover impacted components and drill into the detailed
root causes of errors or latency.
Common anti-patterns:
• Tracing is used for some components but not for all. For example, without tracing for AWS Lambda,
teams might not clearly understand latency caused by cold starts in a spiky workload.
• Synthetic canaries or real-user monitoring (RUM) are not configured with tracing. Without canaries
or RUM, client interaction telemetry is omitted from the trace analysis yielding an incomplete
performance profile.
306

AWS Well-Architected Framework
Change management
• Hybrid workloads include both cloud native and third party tracing tools, but steps have not been
taken elect and fully integrate a single tracing solution. Based on the elected tracing solution, cloud
native tracing SDKs should be used to instrument components that are not cloud native or third party
tools should be configured to ingest cloud native trace telemetry.
Benefits of establishing this best practice: When development teams are alerted to issues, they can
see a full picture of system component interactions, including component by component correlation to
logging, performance, and failures. Because tracing makes it easy to visually identify root causes, less
time is spent investigating root causes. Teams that understand component interactions in detail make
better and faster decisions when resolving issues. Decisions like when to invoke disaster recovery (DR)
failover or where to best implement self-healing strategies can be improved by analyzing systems traces,
ultimately improving customer satisfaction with your services.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Teams that operate distributed applications can use tracing tools to establish a correlation identifier,
collect traces of requests, and build service maps of connected components. All application components
should be included in request traces including service clients, middleware gateways and event buses,
compute components, and storage, including key value stores and databases. Include synthetic canaries
and real-user monitoring in your end-to-end tracing configuration to measure remote client interactions
and latency so that you can accurately evaluate your systems performance against your service level
agreements and objectives.
You can use AWS X-Ray and Amazon CloudWatch Application Monitoring instrumentation services to
provide a complete view of requests as they travel through your application. X-Ray collects application
telemetry and allows you to visualize and filter it across payloads, functions, traces, services, APIs, and
can be turned on for system components with no-code or low-code. CloudWatch application monitoring
includes ServiceLens to integrate your traces with metrics, logs, and alarms. CloudWatch application
monitoring also includes synthetics to monitor your endpoints and APIs, as well as real-user monitoring
to instrument your web application clients.
Implementation steps
• Use AWS X-Ray on all supported native services like Amazon S3, AWS Lambda, and Amazon API
Gateway. These AWS services enable X-Ray with configuration toggles using infrastructure as code,
AWS SDKs, or the AWS Management Console.
• Instrument applications AWS Distro for Open Telemetry and X-Ray or third-party collection agents.
• Review the AWS X-Ray Developer Guide for programming language specific implementation. These
documentation sections detail how to instrument HTTP requests, SQL queries, and other processes
specific to your application programming language.
• Use X-Ray tracing for Amazon CloudWatch Synthetic Canaries and Amazon CloudWatch RUM to
analyze the request path from your end user client through your downstream AWS infrastructure.
• Configure CloudWatch metrics and alarms based on resource health and canary telemetry so
that teams are alerted to issues quickly, and can then deep dive into traces and service maps with
ServiceLens.
• Enable X-Ray integration for third party tracing tools like Datadog, New Relic, or Dynatrace if you are
using third party tools for your primary tracing solution.
Resources
Related best practices:
• REL06-BP01 Monitor all components for the workload (Generation) (p. 299)
307

AWS Well-Architected Framework
Change management
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
Related documents:
• What is AWS X-Ray?
• Amazon CloudWatch: Application Monitoring
• Debugging with Amazon CloudWatch Synthetics and AWS X-Ray
• The Amazon Builders' Library: Instrumenting distributed systems for operational visibility
• Integrating AWS X-Ray with other AWS services
• AWS Distro for OpenTelemetry and AWS X-Ray
• Amazon CloudWatch: Using synthetic monitoring
• Amazon CloudWatch: Use CloudWatch RUM
• Set up Amazon CloudWatch synthetics canary and Amazon CloudWatch alarm
• Availability and Beyond: Understanding and Improving the Resilience of Distributed Systems on AWS
Related examples:
• One Observability Workshop
Related videos:
• AWS re:Invent 2022 - How to monitor applications across multiple accounts
• How to Monitor your AWS Applications
Related tools:
• AWS X-Ray
• Amazon CloudWatch
• Amazon Route 53
REL 7. How do you design your workload to adapt to changes in
demand?
A scalable workload provides elasticity to add or remove resources automatically so that they closely
match the current demand at any given point in time.
Best practices
• REL07-BP01 Use automation when obtaining or scaling resources (p. 308)
• REL07-BP02 Obtain resources upon detection of impairment to a workload (p. 311)
• REL07-BP03 Obtain resources upon detection that more resources are needed for a
workload (p. 312)
• REL07-BP04 Load test your workload (p. 313)
REL07-BP01 Use automation when obtaining or scaling resources
When replacing impaired resources or scaling your workload, automate the process by using managed
AWS services, such as Amazon S3 and AWS Auto Scaling. You can also use third-party tools and AWS
SDKs to automate scaling.
308

AWS Well-Architected Framework
Change management
Managed AWS services include Amazon S3, Amazon CloudFront, AWS Auto Scaling, AWS Lambda,
Amazon DynamoDB, AWS Fargate, and Amazon Route 53.
AWS Auto Scaling lets you detect and replace impaired instances. It also lets you build scaling
plans for resources including Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon
DynamoDB tables and indexes, and Amazon Aurora Replicas.
When scaling EC2 instances, ensure that you use multiple Availability Zones (preferably at least three)
and add or remove capacity to maintain balance across these Availability Zones. ECS tasks or Kubernetes
pods (when using Amazon Elastic Kubernetes Service) should also be distributed across multiple
Availability Zones.
When using AWS Lambda, instances scale automatically. Every time an event notification is received for
your function, AWS Lambda quickly locates free capacity within its compute fleet and runs your code up
to the allocated concurrency. You need to ensure that the necessary concurrency is configured on the
specific Lambda, and in your Service Quotas.
Amazon S3 automatically scales to handle high request rates. For example, your application can
achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in
a bucket. There are no limits to the number of prefixes in a bucket. You can increase your read or write
performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to
parallelize reads, you could scale your read performance to 55,000 read requests per second.
Configure and use Amazon CloudFront or a trusted content delivery network (CDN). A CDN can provide
faster end-user response times and can serve requests for content from cache, therefore reducing the
need to scale your workload.
Common anti-patterns:
• Implementing Auto Scaling groups for automated healing, but not implementing elasticity.
• Using automatic scaling to respond to large increases in traffic.
• Deploying highly stateful applications, eliminating the option of elasticity.
Benefits of establishing this best practice: Automation removes the potential for manual error in
deploying and decommissioning resources. Automation removes the risk of cost overruns and denial of
service due to slow response on needs for deployment or decommissioning.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Configure and use AWS Auto Scaling. This monitors your applications and automatically adjusts
capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto
Scaling, you can setup application scaling for multiple resources across multiple services.
• What is AWS Auto Scaling?
• Configure Auto Scaling on your Amazon EC2 instances and Spot Fleets, Amazon ECS tasks,
Amazon DynamoDB tables and indexes, Amazon Aurora Replicas, and AWS Marketplace appliances
as applicable.
• Managing throughput capacity automatically with DynamoDB Auto Scaling
• Use service API operations to specify the alarms, scaling policies, warm up times, and cool
down times.
• Use Elastic Load Balancing. Load balancers can distribute load by path or by network connectivity.
• What is Elastic Load Balancing?
• Application Load Balancers can distribute load by path.
• What is an Application Load Balancer?
309

AWS Well-Architected Framework
Change management
• Configure an Application Load Balancer to distribute traffic to different workloads based on
the path under the domain name.
• Application Load Balancers can be used to distribute loads in a manner that integrates with
AWS Auto Scaling to manage demand.
• Using a load balancer with an Auto Scaling group
• Network Load Balancers can distribute load by connection.
• What is a Network Load Balancer?
• Configure a Network Load Balancer to distribute traffic to different workloads using TCP, or to
have a constant set of IP addresses for your workload.
• Network Load Balancers can be used to distribute loads in a manner that integrates with AWS
Auto Scaling to manage demand.
•                                                                                                          Use a highly available DNS provider. DNS names allow your users to enter names instead of IP
addresses to access your workloads and distributes this information to a defined scope, usually
globally for users of the workload.
• Use Amazon Route 53 or a trusted DNS provider.
• What is Amazon Route 53?
• Use Route 53 to manage your CloudFront distributions and load balancers.
• Determine the domains and subdomains you are going to manage.
• Create appropriate record sets using ALIAS or CNAME records.
• Working with records
•                                                                                                          Use the AWS global network to optimize the path from your users to your applications. AWS Global
Accelerator continually monitors the health of your application endpoints and redirects traffic to
healthy endpoints in less than 30 seconds.
• AWS Global Accelerator is a service that improves the availability and performance of your
applications with local or global users. It provides static IP addresses that act as a fixed entry point
to your application endpoints in a single or multiple AWS Regions, such as your Application Load
Balancers, Network Load Balancers or Amazon EC2 instances.
• What Is AWS Global Accelerator?
•                                                                                                          Configure and use Amazon CloudFront or a trusted content delivery network (CDN). A content delivery
network can provide faster end-user response times and can serve requests for content that may cause
unnecessary scaling of your workloads.
• What is Amazon CloudFront?
• Configure Amazon CloudFront distributions for your workloads, or use a third-party CDN.
• You can limit access to your workloads so that they are only accessible from CloudFront by using
the IP ranges for CloudFront in your endpoint security groups or access policies.
Resources
Related documents:
• APN Partner: partners that can help you create automated compute solutions
• AWS Auto Scaling: How Scaling Plans Work
• AWS Marketplace: products that can be used with auto scaling
• Managing Throughput Capacity Automatically with DynamoDB Auto Scaling
• Using a load balancer with an Auto Scaling group
• What Is AWS Global Accelerator?
• What Is Amazon EC2 Auto Scaling?
• What is AWS Auto Scaling?
• What is Amazon CloudFront?
310

AWS Well-Architected Framework
Change management
• What is Amazon Route 53?
• What is Elastic Load Balancing?
• What is a Network Load Balancer?
• What is an Application Load Balancer?
• Working with records
REL07-BP02 Obtain resources upon detection of impairment to a workload
Scale resources reactively when necessary if availability is impacted, to restore workload availability.
You first must configure health checks and the criteria on these checks to indicate when availability
is impacted by lack of resources. Then either notify the appropriate personnel to manually scale the
resource, or start automation to automatically scale it.
Scale can be manually adjusted for your workload, for example, changing the number of EC2 instances
in an Auto Scaling group or modifying throughput of a DynamoDB table can be done through the AWS
Management Console or AWS CLI. However automation should be used whenever possible (refer to Use
automation when obtaining or scaling resources).
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Obtain resources upon detection of impairment to a workload. Scale resources reactively when
necessary if availability is impacted, to restore workload availability.
• Use scaling plans, which are the core component of AWS Auto Scaling, to configure a set of
instructions for scaling your resources. If you work with AWS CloudFormation or add tags to
AWS resources, you can set up scaling plans for different sets of resources, per application. AWS
Auto Scaling provides recommendations for scaling strategies customized to each resource. After
you create your scaling plan, AWS Auto Scaling combines dynamic scaling and predictive scaling
methods together to support your scaling strategy.
• AWS Auto Scaling: How Scaling Plans Work
• Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2
instances available to handle the load for your application. You create collections of EC2 instances,
called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling
group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can
specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto
Scaling ensures that your group never goes above this size.
• What Is Amazon EC2 Auto Scaling?
• Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically
adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This
allows a table or a global secondary index to increase its provisioned read and write capacity to
handle sudden increases in traffic, without throttling.
• Managing Throughput Capacity Automatically with DynamoDB Auto Scaling
Resources
Related documents:
• APN Partner: partners that can help you create automated compute solutions
• AWS Auto Scaling: How Scaling Plans Work
• AWS Marketplace: products that can be used with auto scaling
• Managing Throughput Capacity Automatically with DynamoDB Auto Scaling
311

AWS Well-Architected Framework
Change management
• What Is Amazon EC2 Auto Scaling?
REL07-BP03 Obtain resources upon detection that more resources are needed
for a workload
Scale resources proactively to meet demand and avoid availability impact.
Many AWS services automatically scale to meet demand. If using Amazon EC2 instances or Amazon ECS
clusters, you can configure automatic scaling of these to occur based on usage metrics that correspond
to demand for your workload. For Amazon EC2, average CPU utilization, load balancer request count, or
network bandwidth can be used to scale out (or scale in) EC2 instances. For Amazon ECS, average CPU
utilization, load balancer request count, and memory utilization can be used to scale out (or scale in)
ECS tasks. Using Target Auto Scaling on AWS, the autoscaler acts like a household thermostat, adding or
removing resources to maintain the target value (for example, 70% CPU utilization) that you specify.
Amazon EC2 Auto Scaling can also do Predictive Auto Scaling, which uses machine learning to analyze
each resource's historical workload and regularly forecasts the future load.
Little’s Law helps calculate how many instances of compute (EC2 instances, concurrent Lambda
functions, etc.) that you need.
L = λW
L = number of instances (or mean concurrency in the system)
λ = mean rate at which requests arrive (req/sec)
W = mean time that each request spends in the system (sec)
For example, at 100 rps, if each request takes 0.5 seconds to process, you will need 50 instances to keep
up with demand.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Obtain resources upon detection that more resources are needed for a workload. Scale resources
proactively to meet demand and avoid availability impact.
• Calculate how many compute resources you will need (compute concurrency) to handle a given
request rate.
• Telling Stories About Little's Law
• When you have a historical pattern for usage, set up scheduled scaling for Amazon EC2 auto scaling.
• Scheduled Scaling for Amazon EC2 Auto Scaling
• Use AWS predictive scaling.
• Predictive scaling for Amazon EC2 Auto Scaling
Resources
Related documents:
• AWS Marketplace: products that can be used with auto scaling
• Managing Throughput Capacity Automatically with DynamoDB Auto Scaling
• Predictive Scaling for EC2, Powered by Machine Learning
• Scheduled Scaling for Amazon EC2 Auto Scaling
• Telling Stories About Little's Law
• What Is Amazon EC2 Auto Scaling?
312

AWS Well-Architected Framework
Change management
REL07-BP04 Load test your workload
Adopt a load testing methodology to measure if scaling activity meets workload requirements.
It’s important to perform sustained load testing. Load tests should discover the breaking point and
test the performance of your workload. AWS makes it easy to set up temporary testing environments
that model the scale of your production workload. In the cloud, you can create a production-scale test
environment on demand, complete your testing, and then decommission the resources. Because you only
pay for the test environment when it's running, you can simulate your live environment for a fraction of
the cost of testing on premises.
Load testing in production should also be considered as part of game days where the production system
is stressed, during hours of lower customer usage, with all personnel on hand to interpret results and
address any problems that arise.
Common anti-patterns:
• Performing load testing on deployments that are not the same configuration as your production.
• Performing load testing only on individual pieces of your workload, and not on the entire workload.
• Performing load testing with a subset of requests and not a representative set of actual requests.
• Performing load testing to a small safety factor above expected load.
Benefits of establishing this best practice: You know what components in your architecture fail under
load and be able to identify what metrics to watch to indicate that you are approaching that load in time
to address the problem, preventing the impact of that failure.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Perform load testing to identify which aspect of your workload indicates that you must add or remove
capacity. Load testing should have representative traffic similar to what you receive in production.
Increase the load while watching the metrics you have instrumented to determine which metric
indicates when you must add or remove resources.
• Distributed Load Testing on AWS: simulate thousands of connected users
• Identify the mix of requests. You may have varied mixes of requests, so you should look at various
time frames when identifying the mix of traffic.
• Implement a load driver. You can use custom code, open source, or commercial software to
implement a load driver.
• Load test initially using small capacity. You see some immediate effects by driving load onto a
lesser capacity, possibly as small as one instance or container.
• Load test against larger capacity. The effects will be different on a distributed load, so you must
test against as close to a product environment as possible.
Resources
Related documents:
• Distributed Load Testing on AWS: simulate thousands of connected users
REL 8. How do you implement change?
Controlled changes are necessary to deploy new functionality, and to verify that the workloads and the
operating environment are running known software and can be patched or replaced in a predictable
313

AWS Well-Architected Framework
Change management
manner. If these changes are uncontrolled, then it makes it difficult to predict the effect of these
changes, or to address issues that arise because of them.
Best practices
• REL08-BP01 Use runbooks for standard activities such as deployment (p. 314)
• REL08-BP02 Integrate functional testing as part of your deployment (p. 315)
• REL08-BP03 Integrate resiliency testing as part of your deployment (p. 315)
• REL08-BP04 Deploy using immutable infrastructure (p. 316)
• REL08-BP05 Deploy changes with automation (p. 318)
REL08-BP01 Use runbooks for standard activities such as deployment
Runbooks are the predefined procedures to achieve specific outcomes. Use runbooks to perform
standard activities, whether done manually or automatically. Examples include deploying a workload,
patching a workload, or making DNS modifications.
For example, put processes in place to ensure rollback safety during deployments. Ensuring that you can
roll back a deployment without any disruption for your customers is critical in making a service reliable.
For runbook procedures, start with a valid effective manual process, implement it in code, and invoke it
to automatically run where appropriate.
Even for sophisticated workloads that are highly automated, runbooks are still useful for running game
days or meeting rigorous reporting and auditing requirements.
Note that playbooks are used in response to specific incidents, and runbooks are used to achieve specific
outcomes. Often, runbooks are for routine activities, while playbooks are used for responding to non-
routine events.
Common anti-patterns:
• Performing unplanned changes to configuration in production.
• Skipping steps in your plan to deploy faster, resulting in a failed deployment.
• Making changes without testing the reversal of the change.
Benefits of establishing this best practice: Effective change planning increases your ability to
successfully run the change because you are aware of all the systems impacted. Validating your change
in test environments increases your confidence.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Provide consistent and prompt responses to well-understood events by documenting procedures in
runbooks.
• AWS Well-Architected Framework: Concepts: Runbook
• Use the principle of infrastructure as code to define your infrastructure. By using AWS CloudFormation
(or a trusted third party) to define your infrastructure, you can use version control software to version
and track changes.
• Use AWS CloudFormation (or a trusted third-party provider) to define your infrastructure.
• What is AWS CloudFormation?
• Create templates that are singular and decoupled, using good software design principles.
• Determine the permissions, templates, and responsible parties for implementation.
314

AWS Well-Architected Framework
Change management
• Controlling access with AWS Identity and Access Management
• Use source control, like AWS CodeCommit or a trusted third-party tool, for version control.
• What is AWS CodeCommit?
Resources
Related documents:
• APN Partner: partners that can help you create automated deployment solutions
• AWS Marketplace: products that can be used to automate your deployments
• AWS Well-Architected Framework: Concepts: Runbook
• What is AWS CloudFormation?
• What is AWS CodeCommit?
Related examples:
• Automating operations with Playbooks and Runbooks
REL08-BP02 Integrate functional testing as part of your deployment
Functional tests are run as part of automated deployment. If success criteria are not met, the pipeline is
halted or rolled back.
These tests are run in a pre-production environment, which is staged prior to production in the pipeline.
Ideally, this is done as part of a deployment pipeline.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Integrate functional testing as part of your deployment. Functional tests are run as part of automated
deployment. If success criteria are not met, the pipeline is halted or rolled back.
• Invoke AWS CodeBuild during the ‘Test Action’ of your software release pipelines modeled in AWS
CodePipeline. This capability allows you to easily run a variety of tests against your code, such as
unit tests, static code analysis, and integration tests.
• AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild
• Use AWS Marketplace solutions for invoking automated tests as part of your software delivery
pipeline.
• Software test automation
Resources
Related documents:
• AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild
• Software test automation
• What Is AWS CodePipeline?
REL08-BP03 Integrate resiliency testing as part of your deployment
Resiliency tests (using the principles of chaos engineering) are run as part of the automated deployment
pipeline in a pre-production environment.
315

AWS Well-Architected Framework
Change management
These tests are staged and run in the pipeline in a pre-production environment. They should also be run
in production as part of game days.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Integrate resiliency testing as part of your deployment. Use Chaos Engineering, the discipline of
experimenting on a workload to build confidence in the workload’s capability to withstand turbulent
conditions in production.
• Resiliency tests inject faults or resource degradation to assess that your workload responds with its
designed resilience.
• Well-Architected lab: Level 300: Testing for Resiliency of EC2 RDS and S3
• These tests can be run regularly in pre-production environments in automated deployment
pipelines.
• They should also be run in production, as part of scheduled game days.
• Using Chaos Engineering principles, propose hypotheses about how your workload will perform
under various impairments, then test your hypotheses using resiliency testing.
• Principles of Chaos Engineering
Resources
Related documents:
• Principles of Chaos Engineering
• What is AWS Fault Injection Simulator?
Related examples:
• Well-Architected lab: Level 300: Testing for Resiliency of EC2 RDS and S3
REL08-BP04 Deploy using immutable infrastructure
Immutable infrastructure is a model that mandates that no updates, security patches, or configuration
changes happen in-place on production workloads. When a change is needed, the architecture is built
onto new infrastructure and deployed into production.
The most common implementation of the immutable infrastructure paradigm is the immutable server.
This means that if a server needs an update or a fix, new servers are deployed instead of updating the
ones already in use. So, instead of logging into the server via SSH and updating the software version,
every change in the application starts with a software push to the code repository, for example, git
push. Since changes are not allowed in immutable infrastructure, you can be sure about the state of the
deployed system. Immutable infrastructures are inherently more consistent, reliable, and predictable,
and they simplify many aspects of software development and operations.
Use a canary or blue/green deployment when deploying applications in immutable infrastructures.
Canary deployment is the practice of directing a small number of your customers to the new version,
usually running on a single service instance (the canary). You then deeply scrutinize any behavior
changes or errors that are generated. You can remove traffic from the canary if you encounter critical
problems and send the users back to the previous version. If the deployment is successful, you can
continue to deploy at your desired velocity, while monitoring the changes for errors, until you are fully
deployed. AWS CodeDeploy can be configured with a deployment configuration that will allow a canary
deployment.
316

AWS Well-Architected Framework
Change management
Blue/green deployment is similar to the canary deployment except that a full fleet of the application is
deployed in parallel. You alternate your deployments across the two stacks (blue and green). Once again,
you can send traffic to the new version, and fall back to the old version if you see problems with the
deployment. Commonly all traffic is switched at once, however you can also use fractions of your traffic
to each version to dial up the adoption of the new version using the weighted DNS routing capabilities
of Amazon Route 53. AWS CodeDeploy and AWS Elastic Beanstalk can be configured with a deployment
configuration that will allow a blue/green deployment.
Figure 8: Blue/green deployment with AWS Elastic Beanstalk and Amazon Route 53
Benefits of immutable infrastructure:
• Reduction in configuration drifts: By frequently replacing servers from a base, known and version-
controlled configuration, the infrastructure is reset to a known state, avoiding configuration drifts.
• Simplified deployments: Deployments are simplified because they don’t need to support upgrades.
Upgrades are just new deployments.
• Reliable atomic deployments: Deployments either complete successfully, or nothing changes. It gives
more trust in the deployment process.
• Safer deployments with fast rollback and recovery processes: Deployments are safer because the
previous working version is not changed. You can roll back to it if errors are detected.
• Consistent testing and debugging environments: Since all servers use the same image, there are no
differences between environments. One build is deployed to multiple environments. It also prevents
inconsistent environments and simplifies testing and debugging.
• Increased scalability: Since servers use a base image, are consistent, and repeatable, automatic scaling
is trivial.
• Simplified toolchain: The toolchain is simplified since you can get rid of configuration management
tools managing production software upgrades. No extra tools or agents are installed on servers.
Changes are made to the base image, tested, and rolled-out.
• Increased security: By denying all changes to servers, you can disable SSH on instances and remove
keys. This reduces the attack vector, improving your organization’s security posture.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Deploy using immutable infrastructure. Immutable infrastructure is a model in which no updates,
security patches, or configuration changes happen in-place on production systems. If any change is
needed, a new version of the architecture is built and deployed into production.
317

AWS Well-Architected Framework
Change management
• Overview of a Blue/Green Deployment
• Deploying Serverless Applications Gradually
• Immutable Infrastructure: Reliability, consistency and confidence through immutability
• CanaryRelease
Resources
Related documents:
• CanaryRelease
• Deploying Serverless Applications Gradually
• Immutable Infrastructure: Reliability, consistency and confidence through immutability
• Overview of a Blue/Green Deployment
• The Amazon Builders' Library: Ensuring rollback safety during deployments
REL08-BP05 Deploy changes with automation
Deployments and patching are automated to eliminate negative impact.
Making changes to production systems is one of the largest risk areas for many organizations. We
consider deployments a first-class problem to be solved alongside the business problems that the
software addresses. Today, this means the use of automation wherever practical in operations, including
testing and deploying changes, adding or removing capacity, and migrating data. AWS CodePipeline
lets you manage the steps required to release your workload. This includes a deployment state using
AWS CodeDeploy to automate deployment of application code to Amazon EC2 instances, on-premises
instances, serverless Lambda functions, or Amazon ECS services.
Recommendation
Although conventional wisdom suggests that you keep humans in the loop for the most difficult
operational procedures, we suggest that you automate the most difficult procedures for that
very reason.
Common anti-patterns:
• Manually performing changes.
• Skipping steps in your automation through emergency work flows.
• Not following your plans.
Benefits of establishing this best practice: Using automation to deploy all changes removes the
potential for introduction of human error and provides the ability to test before changing production to
ensure that your plans are complete.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Automate your deployment pipeline. Deployment pipelines allow you to invoke automated testing and
detection of anomalies, and either halt the pipeline at a certain step before production deployment, or
automatically roll back a change.
• The Amazon Builders' Library: Ensuring rollback safety during deployments
• The Amazon Builders' Library: Going faster with continuous delivery
• Use AWS CodePipeline (or a trusted third-party product) to define and run your pipelines.
318

AWS Well-Architected Framework
Failure management
• Configure the pipeline to start when a change is committed to your code repository.
• What is AWS CodePipeline?
• Use Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Email Service
(Amazon SES) to send notifications about problems in the pipeline or integrate with a team chat
tool, like Amazon Chime.
• What is Amazon Simple Notification Service?
• What is Amazon SES?
• What is Amazon Chime?
• Automate chat messages with webhooks.
Resources
Related documents:
• APN Partner: partners that can help you create automated deployment solutions
• AWS Marketplace: products that can be used to automate your deployments
• Automate chat messages with webhooks.
• The Amazon Builders' Library: Ensuring rollback safety during deployments
• The Amazon Builders' Library: Going faster with continuous delivery
• What Is AWS CodePipeline?
• What Is CodeDeploy?
• AWS Systems Manager Patch Manager
• What is Amazon SES?
• What is Amazon Simple Notification Service?
Related videos:
• AWS Summit 2019: CI/CD on AWS
Failure management
Questions
• REL 9. How do you back up data? (p. 319)
• REL 10. How do you use fault isolation to protect your workload? (p. 328)
• REL 11. How do you design your workload to withstand component failures? (p. 339)
• REL 12. How do you test reliability? (p. 350)
• REL 13. How do you plan for disaster recovery (DR)?  (p. 361)
REL 9. How do you back up data?
Back up data, applications, and configuration to meet your requirements for recovery time objectives
(RTO) and recovery point objectives (RPO).
Best practices
• REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce the data from
sources (p. 320)
• REL09-BP02 Secure and encrypt backups (p. 322)
319

AWS Well-Architected Framework
Failure management
• REL09-BP03 Perform data backup automatically (p. 324)
• REL09-BP04 Perform periodic recovery of the data to verify backup integrity and processes (p. 325)
REL09-BP01 Identify and back up all data that needs to be backed up, or
reproduce the data from sources
Understand and use the backup capabilities of the data services and resources used by the workload.
Most services provide capabilities to back up workload data.
Desired outcome: Data sources have been identified and classified based on criticality. Then, establish
a strategy for data recovery based on the RPO. This strategy involves either backing up these data
sources, or having the ability to reproduce data from other sources. In the case of data loss, the strategy
implemented allows recovery or the reproduction of data within the defined RPO and RTO.
Cloud maturity phase: Foundational
Common anti-patterns:
• Not aware of all data sources for the workload and their criticality.
• Not taking backups of critical data sources.
• Taking backups of only some data sources without using criticality as a criterion.
• No defined RPO, or backup frequency cannot meet RPO.
• Not evaluating if a backup is necessary or if data can be reproduced from other sources.
Benefits of establishing this best practice: Identifying the places where backups are necessary and
implementing a mechanism to create backups, or being able to reproduce the data from an external
source improves the ability to restore and recover data during an outage.
Level of risk exposed if this best practice is not established: High
Implementation guidance
All AWS data stores offer backup capabilities. Services such as Amazon RDS and Amazon DynamoDB
additionally support automated backup that allows point-in-time recovery (PITR), which allows you to
restore a backup to any time up to five minutes or less before the current time. Many AWS services offer
the ability to copy backups to another AWS Region. AWS Backup is a tool that gives you the ability to
centralize and automate data protection across AWS services. AWS Elastic Disaster Recovery allows you
to copy full server workloads and maintain continuous data protection from on-premise, cross-AZ or
cross-Region, with a Recovery Point Objective (RPO) measured in seconds.
Amazon S3 can be used as a backup destination for self-managed and AWS-managed data sources. AWS
services such as Amazon EBS, Amazon RDS, and Amazon DynamoDB have built in capabilities to create
backups. Third-party backup software can also be used.
On-premises data can be backed up to the AWS Cloud using AWS Storage Gateway or AWS DataSync.
Amazon S3 buckets can be used to store this data on AWS. Amazon S3 offers multiple storage tiers such
as Amazon S3 Glacier or S3 Glacier Deep Archive to reduce cost of data storage.
You might be able to meet data recovery needs by reproducing the data from other sources. For
example, Amazon ElastiCache replica nodes or Amazon RDS read replicas could be used to reproduce
data if the primary is lost. In cases where sources like this can be used to meet your Recovery Point
Objective (RPO) and Recovery Time Objective (RTO), you might not require a backup. Another example, if
working with Amazon EMR, it might not be necessary to backup your HDFS data store, as long as you can
reproduce the data into Amazon EMR from Amazon S3.
320

AWS Well-Architected Framework
Failure management
When selecting a backup strategy, consider the time it takes to recover data. The time needed to recover
data depends on the type of backup (in the case of a backup strategy), or the complexity of the data
reproduction mechanism. This time should fall within the RTO for the workload.
Implementation steps
1. Identify all data sources for the workload. Data can be stored on a number of resources such as
databases, volumes, filesystems, logging systems, and object storage. Refer to the Resources section
to find Related documents on different AWS services where data is stored, and the backup capability
these services provide.
2. Classify data sources based on criticality. Different data sets will have different levels of criticality
for a workload, and therefore different requirements for resiliency. For example, some data might be
critical and require a RPO near zero, while other data might be less critical and can tolerate a higher
RPO and some data loss. Similarly, different data sets might have different RTO requirements as well.
3. Use AWS or third-party services to create backups of the data. AWS Backup is a managed service
that allows creating backups of various data sources on AWS. AWS Elastic Disaster Recovery handles
automated sub-second data replication to an AWS Region. Most AWS services also have native
capabilities to create backups. The AWS Marketplace has many solutions that provide these capabilites
as well. Refer to the Resources listed below for information on how to create backups of data from
various AWS services.
4. For data that is not backed up, establish a data reproduction mechanism. You might choose not
to backup data that can be reproduced from other sources for various reasons. There might be a
situation where it is cheaper to reproduce data from sources when needed rather than creating a
backup as there may be a cost associated with storing backups. Another example is where restoring
from a backup takes longer than reproducing the data from sources, resulting in a breach in RTO.
In such situations, consider tradeoffs and establish a well-defined process for how data can be
reproduced from these sources when data recovery is necessary. For example, if you have loaded data
from Amazon S3 to a data warehouse (like Amazon Redshift), or MapReduce cluster (like Amazon
EMR) to do analysis on that data, this may be an example of data that can be reproduced from
other sources. As long as the results of these analyses are either stored somewhere or reproducible,
you would not suffer a data loss from a failure in the data warehouse or MapReduce cluster. Other
examples that can be reproduced from sources include caches (like Amazon ElastiCache) or RDS read
replicas.
5. Establish a cadence for backing up data. Creating backups of data sources is a periodic process and
the frequency should depend on the RPO.
Level of effort for the Implementation Plan: Moderate
Resources
Related Best Practices:
REL13-BP01 Define recovery objectives for downtime and data loss (p. 361)
REL13-BP02 Use defined recovery strategies to meet the recovery objectives (p. 365)
Related documents:
• What Is AWS Backup?
• What is AWS DataSync?
• What is Volume Gateway?
• APN Partner: partners that can help with backup
• AWS Marketplace: products that can be used for backup
• Amazon EBS Snapshots
321

AWS Well-Architected Framework
Failure management
• Backing Up Amazon EFS
• Backing up Amazon FSx for Windows File Server
• Backup and Restore for ElastiCache for Redis
• Creating a DB Cluster Snapshot in Neptune
• Creating a DB Snapshot
• Creating an EventBridge Rule That Triggers on a Schedule
• Cross-Region Replication with Amazon S3
• EFS-to-EFS AWS Backup
• Exporting Log Data to Amazon S3
• Object lifecycle management
• On-Demand Backup and Restore for DynamoDB
• Point-in-time recovery for DynamoDB
• Working with Amazon OpenSearch Service Index Snapshots
• What is AWS Elastic Disaster Recovery?
Related videos:
• AWS re:Invent 2021 - Backup, disaster recovery, and ransomware protection with AWS
• AWS Backup Demo: Cross-Account and Cross-Region Backup
• AWS re:Invent 2019: Deep dive on AWS Backup, ft. Rackspace (STG341)
Related examples:
• Well-Architected Lab - Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon S3
• Well-Architected Lab - Testing Backup and Restore of Data
• Well-Architected Lab - Backup and Restore with Failback for Analytics Workload
• Well-Architected Lab - Disaster Recovery - Backup and Restore
REL09-BP02 Secure and encrypt backups
Control and detect access to backups using authentication and authorization. Prevent and detect if data
integrity of backups is compromised using encryption.
Common anti-patterns:
• Having the same access to the backups and restoration automation as you do to the data.
• Not encrypting your backups.
Benefits of establishing this best practice: Securing your backups prevents tampering with the data,
and encryption of the data prevents access to that data if it is accidentally exposed.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Control and detect access to backups using authentication and authorization, such as AWS Identity
and Access Management (IAM). Prevent and detect if data integrity of backups is compromised using
encryption.
322

AWS Well-Architected Framework
Failure management
Amazon S3 supports several methods of encryption of your data at rest. Using server-side encryption,
Amazon S3 accepts your objects as unencrypted data, and then encrypts them as they are stored. Using
client-side encryption, your workload application is responsible for encrypting the data before it is sent
to Amazon S3. Both methods allow you to use AWS Key Management Service (AWS KMS) to create and
store the data key, or you can provide your own key, which you are then responsible for. Using AWS KMS,
you can set policies using IAM on who can and cannot access your data keys and decrypted data.
For Amazon RDS, if you have chosen to encrypt your databases, then your backups are encrypted also.
DynamoDB backups are always encrypted. When using AWS Elastic Disaster Recovery, all data in transit
and at rest is encrypted. With Elastic Disaster Recovery, data at rest can be encrypted using either the
default Amazon EBS encryption Volume Encryption Key or a custom customer-managed key.
Implementation steps
1. Use encryption on each of your data stores. If your source data is encrypted, then the backup will also
be encrypted.
• Use encryption in Amazon RDS.. You can configure encryption at rest using AWS Key Management
Service when you create an RDS instance.
• Use encryption on Amazon EBS volumes.. You can configure default encryption or specify a unique
key upon volume creation.
• Use the required Amazon DynamoDB encryption. DynamoDB encrypts all data at rest. You can
either use an AWS owned AWS KMS key or an AWS managed KMS key, specifying a key that is stored
in your account.
• Encrypt your data stored in Amazon EFS. Configure the encryption when you create your file
system.
• Configure the encryption in the source and destination Regions. You can configure encryption at
rest in Amazon S3 using keys stored in KMS, but the keys are Region-specific. You can specify the
destination keys when you configure the replication.
• Choose whether to use the default or custom Amazon EBS encryption for Elastic Disaster Recovery.
This option will encrypt your replicated data at rest on the Staging Area Subnet disks and the
replicated disks.
2. Implement least privilege permissions to access your backups. Follow best practices to limit the access
to the backups, snapshots, and replicas in accordance with security best practices.
Resources
Related documents:
• AWS Marketplace: products that can be used for backup
• Amazon EBS Encryption
• Amazon S3: Protecting Data Using Encryption
• CRR Additional Configuration: Replicating Objects Created with Server-Side Encryption (SSE) Using
Encryption Keys stored in AWS KMS
• DynamoDB Encryption at Rest
• Encrypting Amazon RDS Resources
• Encrypting Data and Metadata in Amazon EFS
• Encryption for Backups in AWS
• Managing Encrypted Tables
• Security Pillar - AWS Well-Architected Framework
• What is AWS Elastic Disaster Recovery?
Related examples:
323

AWS Well-Architected Framework
Failure management
• Well-Architected Lab - Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon S3
REL09-BP03 Perform data backup automatically
Configure backups to be taken automatically based on a periodic schedule informed by the Recovery
Point Objective (RPO), or by changes in the dataset. Critical datasets with low data loss requirements
need to be backed up automatically on a frequent basis, whereas less critical data where some loss is
acceptable can be backed up less frequently.
Desired outcome: An automated process that creates backups of data sources at an established cadence.
Common anti-patterns:
• Performing backups manually.
• Using resources that have backup capability, but not including the backup in your automation.
Benefits of establishing this best practice: Automating backups verifies that they are taken regularly
based on your RPO, and alerts you if they are not taken.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
AWS Backup can be used to create automated data backups of various AWS data sources. Amazon
RDS instances can be backed up almost continuously every five minutes and Amazon S3 objects can
be backed up almost continuously every fifteen minutes, providing for point-in-time recovery (PITR)
to a specific point in time within the backup history. For other AWS data sources, such as Amazon EBS
volumes, Amazon DynamoDB tables, or Amazon FSx file systems, AWS Backup can run automated
backup as frequently as every hour. These services also offer native backup capabilities. AWS services
that offer automated backup with point-in-time recovery include Amazon DynamoDB, Amazon RDS,
and Amazon Keyspaces (for Apache Cassandra) - these can be restored to a specific point in time within
the backup history. Most other AWS data storage services offer the ability to schedule periodic backups,
as frequently as every hour.
Amazon RDS and Amazon DynamoDB offer continuous backup with point-in-time recovery. Amazon
S3 versioning, once turned on, is automatic. Amazon Data Lifecycle Manager can be used to automate
the creation, copy and deletion of Amazon EBS snapshots. It can also automate the creation, copy,
deprecation and deregistration of Amazon EBS-backed Amazon Machine Images (AMIs) and their
underlying Amazon EBS snapshots.
AWS Elastic Disaster Recovery provides continuous block-level replication from the source environment
(on-premises or AWS) to the target recovery region. Point-in-time Amazon EBS snapshots are
automatically created and managed by the service.
For a centralized view of your backup automation and history, AWS Backup provides a fully managed,
policy-based backup solution. It centralizes and automates the back up of data across multiple AWS
services in the cloud as well as on premises using the AWS Storage Gateway.
In additional to versioning, Amazon S3 features replication. The entire S3 bucket can be automatically
replicated to another bucket in the same, or a different AWS Region.
Implementation steps
1. Identify data sources that are currently being backed up manually. For more detail, see REL09-
BP01 Identify and back up all data that needs to be backed up, or reproduce the data from
sources (p. 320).
324

AWS Well-Architected Framework
Failure management
2. Determine the RPO for the workload. For more detail, see REL13-BP01 Define recovery objectives for
downtime and data loss (p. 361).
3. Use an automated backup solution or managed service. AWS Backup is a fully-managed service that
makes it easy to centralize and automate data protection across AWS services, in the cloud, and on-
premises. Using backup plans in AWS Backup, create rules which define the resources to backup, and
the frequency at which these backups should be created. This frequency should be informed by the
RPO established in Step 2. For hands-on guidance on how to create automated backups using AWS
Backup, see Testing Backup and Restore of Data. Native backup capabilities are offered by most AWS
services that store data. For example, RDS can be leveraged for automated backups with point-in-time
recovery (PITR).
4. For data sources not supported by an automated backup solution or managed service such as on-
premises data sources or message queues, consider using a trusted third-party solution to create
automated backups. Alternatively, you can create automation to do this using the AWS CLI or SDKs.
You can use AWS Lambda Functions or AWS Step Functions to define the logic involved in creating a
data backup, and use Amazon EventBridge to invoke it at a frequency based on your RPO.
Level of effort for the Implementation Plan: Low
Resources
Related documents:
• APN Partner: partners that can help with backup
• AWS Marketplace: products that can be used for backup
• Creating an EventBridge Rule That Triggers on a Schedule
• What Is AWS Backup?
• What Is AWS Step Functions?
• What is AWS Elastic Disaster Recovery?
Related videos:
• AWS re:Invent 2019: Deep dive on AWS Backup, ft. Rackspace (STG341)
Related examples:
• Well-Architected Lab - Testing Backup and Restore of Data
REL09-BP04 Perform periodic recovery of the data to verify backup integrity
and processes
Validate that your backup process implementation meets your Recovery Time Objectives (RTO) and
Recovery Point Objectives (RPO) by performing a recovery test.
Desired outcome: Data from backups is periodically recovered using well-defined mechanisms to verify
that recovery is possible within the established recovery time objective (RTO) for the workload. Verify
that restoration from a backup results in a resource that contains the original data without any of it
being corrupted or inaccessible, and with data loss within the recovery point objective (RPO).
Common anti-patterns:
• Restoring a backup, but not querying or retrieving any data to check that the restoration is usable.
• Assuming that a backup exists.
• Assuming that the backup of a system is fully operational and that data can be recovered from it.
325

AWS Well-Architected Framework
Failure management
• Assuming that the time to restore or recover data from a backup falls within the RTO for the workload.
• Assuming that the data contained on the backup falls within the RPO for the workload
• Restoring when necessary, without using a runbook or outside of an established automated procedure.
Benefits of establishing this best practice: Testing the recovery of the backups verifies that data can
be restored when needed without having any worry that data might be missing or corrupted, that the
restoration and recovery is possible within the RTO for the workload, and any data loss falls within the
RPO for the workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Testing backup and restore capability increases confidence in the ability to perform these actions during
an outage. Periodically restore backups to a new location and run tests to verify the integrity of the
data. Some common tests that should be performed are checking if all data is available, is not corrupted,
is accessible, and that any data loss falls within the RPO for the workload. Such tests can also help
ascertain if recovery mechanisms are fast enough to accommodate the workload's RTO.
Using AWS, you can stand up a testing environment and restore your backups to assess RTO and RPO
capabilities, and run tests on data content and integrity.
Additionally, Amazon RDS and Amazon DynamoDB allow point-in-time recovery (PITR). Using continuous
backup, you can restore your dataset to the state it was in at a specified date and time.
If all the data is available, is not corrupted, is accessible, and any data loss falls within the RPO for the
workload. Such tests can also help ascertain if recovery mechanisms are fast enough to accommodate
the workload's RTO.
AWS Elastic Disaster Recovery offers continual point-in-time recovery snapshots of Amazon EBS
volumes. As source servers are replicated, point-in-time states are chronicled over time based on the
configured policy. Elastic Disaster Recovery helps you verify the integrity of these snapshots by launching
instances for test and drill purposes without redirecting the traffic.
Implementation steps
1. Identify data sources that are currently being backed up and where these backups are being stored.
For implementation guidance, see REL09-BP01 Identify and back up all data that needs to be backed
up, or reproduce the data from sources (p. 320).
2. Establish criteria for data validation for each data source. Different types of data will have different
properties which might require different validation mechanisms. Consider how this data might be
validated before you are confident to use it in production. Some common ways to validate data are
using data and backup properties such as data type, format, checksum, size, or a combination of
these with custom validation logic. For example, this might be a comparison of the checksum values
between the restored resource and the data source at the time the backup was created.
3. Establish RTO and RPO for restoring the data based on data criticality. For implementation guidance,
see REL13-BP01 Define recovery objectives for downtime and data loss (p. 361).
4. Assess your recovery capability. Review your backup and restore strategy to understand if it can meet
your RTO and RPO, and adjust the strategy as necessary. Using AWS Resilience Hub, you can run an
assessment of your workload. The assessment evaluates your application configuration against the
resiliency policy and reports if your RTO and RPO targets can be met.
5. Do a test restore using currently established processes used in production for data restoration. These
processes depend on how the original data source was backed up, the format and storage location
of the backup itself, or if the data is reproduced from other sources. For example, if you are using a
managed service such as AWS Backup, this might be as simple as restoring the backup into a new
resource. If you used AWS Elastic Disaster Recovery you can launch a recovery drill.
326

AWS Well-Architected Framework
Failure management
6. Validate data recovery from the restored resource based on criteria you previously established for
data validation. Does the restored and recovered data contain the most recent record or item at the
time of backup? Does this data fall within the RPO for the workload?
7. Measure time required for restore and recovery and compare it to your established RTO. Does this
process fall within the RTO for the workload? For example, compare the timestamps from when the
restoration process started and when the recovery validation completed to calculate how long this
process takes. All AWS API calls are timestamped and this information is available in AWS CloudTrail.
While this information can provide details on when the restore process started, the end timestamp for
when the validation was completed should be recorded by your validation logic. If using an automated
process, then services like Amazon DynamoDB can be used to store this information. Additionally,
many AWS services provide an event history which provides timestamped information when certain
actions occurred. Within AWS Backup, backup and restore actions are referred to as jobs, and these
jobs contain timestamp information as part of its metadata which can be used to measure time
required for restoration and recovery.
8. Notify stakeholders if data validation fails, or if the time required for restoration and recovery
exceeds the established RTO for the workload. When implementing automation to do this, such
as in this lab, services like Amazon Simple Notification Service (Amazon SNS) can be used to send
push notifications such as email or SMS to stakeholders. These messages can also be published to
messaging applications such as Amazon Chime, Slack, or Microsoft Teams or used to create tasks as
OpsItems using AWS Systems Manager OpsCenter.
9. Automate this process to run periodically. For example, services like AWS Lambda or a State Machine
in AWS Step Functions can be used to automate the restore and recovery processes, and Amazon
EventBridge can be used to invoke this automation workflow periodically as shown in the architecture
diagram below. Learn how to Automate data recovery validation with AWS Backup. Additionally, this
Well-Architected lab provides a hands-on experience on one way to do automation for several of the
steps here.
Figure 9. An automated backup and restore process
Level of effort for the Implementation Plan: Moderate to high depending on the complexity of the
validation criteria.
Resources
Related documents:
327

AWS Well-Architected Framework
Failure management
• Automate data recovery validation with AWS Backup
• APN Partner: partners that can help with backup
• AWS Marketplace: products that can be used for backup
• Creating an EventBridge Rule That Triggers on a Schedule
• On-demand backup and restore for DynamoDB
• What Is AWS Backup?
• What Is AWS Step Functions?
• What is AWS Elastic Disaster Recovery
• AWS Elastic Disaster Recovery
Related examples:
• Well-Architected lab: Testing Backup and Restore of Data
REL 10. How do you use fault isolation to protect your
workload?
Fault isolated boundaries limit the effect of a failure within a workload to a limited number of
components. Components outside of the boundary are unaffected by the failure. Using multiple fault
isolated boundaries, you can limit the impact on your workload.
Best practices
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL10-BP02 Select the appropriate locations for your multi-location deployment (p. 332)
• REL10-BP03 Automate recovery for components constrained to a single location (p. 335)
• REL10-BP04 Use bulkhead architectures to limit scope of impact (p. 336)
REL10-BP01 Deploy the workload to multiple locations
Distribute workload data and resources across multiple Availability Zones or, where necessary, across
AWS Regions. These locations can be as diverse as required.
One of the bedrock principles for service design in AWS is the avoidance of single points of failure in
underlying physical infrastructure. This motivates us to build software and systems that use multiple
Availability Zones and are resilient to failure of a single zone. Similarly, systems are built to be resilient to
failure of a single compute node, single storage volume, or single instance of a database. When building
a system that relies on redundant components, it’s important to ensure that the components operate
independently, and in the case of AWS Regions, autonomously. The benefits achieved from theoretical
availability calculations with redundant components are only valid if this holds true.
Availability Zones (AZs)
AWS Regions are composed of multiple Availability Zones that are designed to be independent of
each other. Each Availability Zone is separated by a meaningful physical distance from other zones
to avoid correlated failure scenarios due to environmental hazards like fires, floods, and tornadoes.
Each Availability Zone also has independent physical infrastructure: dedicated connections to utility
power, standalone backup power sources, independent mechanical services, and independent network
connectivity within and beyond the Availability Zone. This design limits faults in any of these systems
to just the one affected AZ. Despite being geographically separated, Availability Zones are located in
the same regional area which allows high-throughput, low-latency networking. The entire AWS Region
328

AWS Well-Architected Framework
Failure management
(across all Availability Zones, consisting of multiple physically independent data centers) can be treated
as a single logical deployment target for your workload, including the ability to synchronously replicate
data (for example, between databases). This allows you to use Availability Zones in an active/active or
active/standby configuration.
Availability Zones are independent, and therefore workload availability is increased when the workload
is architected to use multiple zones. Some AWS services (including the Amazon EC2 instance data plane)
are deployed as strictly zonal services where they have shared fate with the Availability Zone they are in.
Amazon EC2 instances in the other AZs will however be unaffected and continue to function. Similarly, if
a failure in an Availability Zone causes an Amazon Aurora database to fail, a read-replica Aurora instance
in an unaffected AZ can be automatically promoted to primary. Regional AWS services, such as Amazon
DynamoDB on the other hand internally use multiple Availability Zones in an active/active configuration
to achieve the availability design goals for that service, without you needing to configure AZ placement.
Figure 9: Multi-tier architecture deployed across three Availability Zones. Note that Amazon S3 and Amazon
DynamoDB are always Multi-AZ automatically. The ELB also is deployed to all three zones.
While AWS control planes typically provide the ability to manage resources within the entire Region
(multiple Availability Zones), certain control planes (including Amazon EC2 and Amazon EBS) have the
ability to filter results to a single Availability Zone. When this is done, the request is processed only in
the specified Availability Zone, reducing exposure to disruption in other Availability Zones. This AWS CLI
example illustrates getting Amazon EC2 instance information from only the us-east-2c Availability Zone:
AWS ec2 describe-instances --filters Name=availability-zone,Values=us-east-2c
AWS Local Zones
AWS Local Zones act similarly to Availability Zones within their respective AWS Region in that they can
be selected as a placement location for zonal AWS resources such as subnets and EC2 instances. What
makes them special is that they are located not in the associated AWS Region, but near large population,
industry, and IT centers where no AWS Region exists today. Yet they still retain high-bandwidth, secure
connection between local workloads in the local zone and those running in the AWS Region. You should
use AWS Local Zones to deploy workloads closer to your users for low-latency requirements.
Amazon Global Edge Network
Amazon Global Edge Network consists of edge locations in cities around the world. Amazon CloudFront
uses this network to deliver content to end users with lower latency. AWS Global Accelerator allows
329

AWS Well-Architected Framework
Failure management
you to create your workload endpoints in these edge locations to provide onboarding to the AWS
global network close to your users. Amazon API Gateway allows edge-optimized API endpoints using a
CloudFront distribution to facilitate client access through the closest edge location.
AWS Regions
AWS Regions are designed to be autonomous, therefore, to use a multi-Region approach you would
deploy dedicated copies of services to each Region.
A multi-Region approach is common for disaster recovery strategies to meet recovery objectives when
one-off large-scale events occur. See Plan for Disaster Recovery (DR) for more information on these
strategies. Here however, we focus instead on availability, which seeks to deliver a mean uptime objective
over time. For high-availability objectives, a multi-region architecture will generally be designed to be
active/active, where each service copy (in their respective regions) is active (serving requests).
Recommendation
Availability goals for most workloads can be satisfied using a Multi-AZ strategy within a single
AWS Region. Consider multi-Region architectures only when workloads have extreme availability
requirements, or other business goals, that require a multi-Region architecture.
AWS provides you with the capabilities to operate services cross-region. For example, AWS provides
continuous, asynchronous data replication of data using Amazon Simple Storage Service (Amazon S3)
Replication, Amazon RDS Read Replicas (including Aurora Read Replicas), and Amazon DynamoDB Global
Tables. With continuous replication, versions of your data are available for near immediate use in each of
your active Regions.
Using AWS CloudFormation, you can define your infrastructure and deploy it consistently across AWS
accounts and across AWS Regions. And AWS CloudFormation StackSets extends this functionality by
allowing you to create, update, or delete AWS CloudFormation stacks across multiple accounts and
regions with a single operation. For Amazon EC2 instance deployments, an AMI (Amazon Machine Image)
is used to supply information such as hardware configuration and installed software. You can implement
an Amazon EC2 Image Builder pipeline that creates the AMIs you need and copy these to your active
regions. This ensures that these Golden AMIs have everything you need to deploy and scale-out your
workload in each new region.
To route traffic, both Amazon Route 53 and AWS Global Accelerator permit the definition of policies that
determine which users go to which active regional endpoint. With Global Accelerator you set a traffic
dial to control the percentage of traffic that is directed to each application endpoint. Route 53 supports
this percentage approach, and also multiple other available policies including geoproximity and latency
based ones. Global Accelerator automatically leverages the extensive network of AWS edge servers, to
onboard traffic to the AWS network backbone as soon as possible, resulting in lower request latencies.
All of these capabilities operate so as to preserve each Region’s autonomy. There are very few exceptions
to this approach, including our services that provide global edge delivery (such as Amazon CloudFront
and Amazon Route 53), along with the control plane for the AWS Identity and Access Management (IAM)
service. Most services operate entirely within a single Region.
On-premises data center
For workloads that run in an on-premises data center, architect a hybrid experience when possible. AWS
Direct Connect provides a dedicated network connection from your premises to AWS allowing you to run
in both.
Another option is to run AWS infrastructure and services on premises using AWS Outposts. AWS
Outposts is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to
your data center. The same hardware infrastructure used in the AWS Cloud is installed in your data
center. AWS Outposts are then connected to the nearest AWS Region. You can then use AWS Outposts to
support your workloads that have low latency or local data processing requirements.
330

AWS Well-Architected Framework
Failure management
Level of risk exposed if this best practice is not established: High
Implementation guidance
•                                                                                                          Use multiple Availability Zones and AWS Regions. Distribute workload data and resources across
multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse
as required.
• Regional services are inherently deployed across Availability Zones.
• This includes Amazon S3, Amazon DynamoDB, and AWS Lambda (when not connected to a VPC)
• Deploy your container, instance, and function-based workloads into multiple Availability Zones. Use
multi-zone datastores, including caches. Use the features of Amazon EC2 Auto Scaling, Amazon ECS
task placement, AWS Lambda function configuration when running in your VPC, and ElastiCache
clusters.
• Use subnets that are in separate Availability Zones when you deploy Auto Scaling groups.
• Example: Distributing instances across Availability Zones
• Amazon ECS task placement strategies
• Configuring an AWS Lambda function to access resources in an Amazon VPC
• Choosing Regions and Availability Zones
• Use subnets in separate Availability Zones when you deploy Auto Scaling groups.
• Example: Distributing instances across Availability Zones
• Use ECS task placement parameters, specifying DB subnet groups.
• Amazon ECS task placement strategies
• Use subnets in multiple Availability Zones when you configure a function to run in your VPC.
• Configuring an AWS Lambda function to access resources in an Amazon VPC
• Use multiple Availability Zones with ElastiCache clusters.
• Choosing Regions and Availability Zones
•                                                                                                          If your workload must be deployed to multiple Regions, choose a multi-Region strategy. Most
reliability needs can be met within a single AWS Region using a multi-Availability Zone strategy. Use a
multi-Region strategy when necessary to meet your business needs.
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)
• Backup to another AWS Region can add another layer of assurance that data will be available
when needed.
• Some workloads have regulatory requirements that require use of a multi-Region strategy.
•                                                                                                          Evaluate AWS Outposts for your workload. If your workload requires low latency to your on-premises
data center or has local data processing requirements. Then run AWS infrastructure and services on
premises using AWS Outposts
• What is AWS Outposts?
•                                                                                                          Determine if AWS Local Zones helps you provide service to your users. If you have low-latency
requirements, see if AWS Local Zones is located near your users. If yes, then use it to deploy workloads
closer to those users.
• AWS Local Zones FAQ
Resources
Related documents:
• AWS Global Infrastructure
• AWS Local Zones FAQ
• Amazon ECS task placement strategies
• Choosing Regions and Availability Zones
331

AWS Well-Architected Framework
Failure management
• Example: Distributing instances across Availability Zones
• Global Tables: Multi-Region Replication with DynamoDB
• Using Amazon Aurora global databases
• Creating a Multi-Region Application with AWS Services blog series
• What is AWS Outposts?
Related videos:
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)
• AWS re:Invent 2019: Innovation and operation of the AWS global network infrastructure (NET339)
REL10-BP02 Select the appropriate locations for your multi-location
deployment
Desired Outcome
For high availability, always (when possible) deploy your workload components to multiple Availability
Zones (AZs), as shown in Figure 10. For workloads with extreme resilience requirements, carefully
evaluate the options for a multi-Region architecture.
Figure 10: A resilient multi-AZ database deployment with backup to another AWS Region
Common anti-patterns
• Choosing to design a multi-Region architecture when a multi-AZ architecture would satisfy
requirements.
• Not accounting for dependencies between application components if resilience and multi-location
requirements differ between those components.
Benefits of establishing this best practice
For resilience, you should use an approach that builds layers of defense. One layer protects against
smaller, more common, disruptions by building a highly available architecture using multiple AZs.
Another layer of defense is meant to protect against rare events like widespread natural disasters and
Region-level disruptions. This second layer involves architecting your application to span multiple AWS
Regions.
• The difference between a 99.5% availability and 99.99% availability is over 3.5 hours per month. The
expected availability of a workload can only reach “four nines” if it is in multiple AZs.
• By running your workload in multiple AZs, you can isolate faults in power, cooling, and networking,
and most natural disasters like fire and flood.
332

AWS Well-Architected Framework
Failure management
• Implementing a multi-Region strategy for your workload helps protect it against widespread natural
disasters that affect a large geographic region of a country, or technical failures of Region-wide scope.
Be aware that implementing a multi-Region architecture can be significantly complex, and is usually
not required for most workloads.
Level of risk exposed if this best practice is not established: High
Implementation guidance
For a disaster event based on disruption or partial loss of one Availability Zone, implementing a highly
available workload in multiple Availability Zones within a single AWS Region helps mitigate against
natural and technical disasters. Each AWS Region is comprised of multiple Availability Zones, each
isolated from faults in the other zones and separated by a meaningful distance. However, for a disaster
event that includes the risk of losing multiple Availability Zone components, which are a significant
distance away from each other, you should implement disaster recovery options to mitigate against
failures of a Region-wide scope. For workloads that require extreme resilience (critical infrastructure,
health-related applications, financial system infrastructure, etc.), a multi-Region strategy may be
required.
Implementation Steps
1. Evaluate your workload and determine whether the resilience needs can be met by a multi-AZ
approach (single AWS Region), or if they require a multi-Region approach. Implementing a multi-
Region architecture to satisfy these requirements will introduce additional complexity, therefore
carefully consider your use case and its requirements. Resilience requirements can almost always
be met using a single AWS Region. Consider the following possible requirements when determining
whether you need to use multiple Regions:
a. Disaster recovery (DR): For a disaster event based on disruption or partial loss of one Availability
Zone, implementing a highly available workload in multiple Availability Zones within a single AWS
Region helps mitigate against natural and technical disasters. For a disaster event that includes the
risk of losing multiple Availability Zone components, which are a significant distance away from
each other, you should implement disaster recovery across multiple Regions to mitigate against
natural disasters or technical failures of a Region-wide scope.
b. High availability (HA): A multi-Region architecture (using multiple AZs in each Region) can be used
to achieve greater then four 9’s (> 99.99%) availability.
c. Stack localization: When deploying a workload to a global audience, you can deploy localized
stacks in different AWS Regions to serve audiences in those Regions. Localization can include
language, currency, and types of data stored.
d. Proximity to users: When deploying a workload to a global audience, you can reduce latency by
deploying stacks in AWS Regions close to where the end users are.
e. Data residency: Some workloads are subject to data residency requirements, where data from
certain users must remain within a specific country’s borders. Based on the regulation in question,
you can choose to deploy an entire stack, or just the data, to the AWS Region within those borders.
2. Here are some examples of multi-AZ functionality provided by AWS services:
a. To protect workloads using EC2 or ECS, deploy an Elastic Load Balancer in front of the compute
resources. Elastic Load Balancing then provides the solution to detect instances in unhealthy zones
and route traffic to the healthy ones.
i.  Getting started with Application Load Balancers
ii. Getting started with Network Load Balancers
b. In the case of EC2 instances running commercial off-the-shelf software that do not support load
balancing, you can achieve a form of fault tolerance by implementing a multi-AZ disaster recovery
methodology.
i.  the section called “REL13-BP02 Use defined recovery strategies to meet the recovery
objectives” (p. 365)
333

AWS Well-Architected Framework
Failure management
c. For Amazon ECS tasks, deploy your service evenly across three AZs to achieve a balance of
availability and cost.
i.  Amazon ECS availability best practices | Containers
d. For non-Aurora Amazon RDS, you can choose Multi-AZ as a configuration option. Upon failure of
the primary database instance, Amazon RDS automatically promotes a standby database to receive
traffic in another availability zone. Multi-Region read-replicas can also be created to improve
resilience.
i.  Amazon RDS Multi AZ Deployments
ii. Creating a read replica in a different AWS Region
3. Here are some examples of multi-Region functionality provided by AWS services:
a. For Amazon S3 workloads, where multi-AZ availability is provided automatically by the service,
consider Multi-Region Access Points if a multi-Region deployment is needed.
i.  Multi-Region Access Points in Amazon S3
b. For DynamoDB tables, where multi-AZ availability is provided automatically by the service, you can
easily convert existing tables to global tables to take advantage of multiple regions.
i.  Convert Your Single-Region Amazon DynamoDB Tables to Global Tables
c. If your workload is fronted by Application Load Balancers or Network Load Balancers, use AWS
Global Accelerator to improve the availability of your application by directing traffic to multiple
regions that contain healthy endpoints.
i.  Endpoints for standard accelerators in AWS Global Accelerator - AWS Global Accelerator
(amazon.com)
d. For applications that leverage AWS EventBridge, consider cross-Region buses to forward events to
other Regions you select.
i.  Sending and receiving Amazon EventBridge events between AWS Regions
e. For Amazon Aurora databases, consider Aurora global databases, which span multiple AWS regions.
Existing clusters can be modified to add new Regions as well.
i.  Getting started with Amazon Aurora global databases
f. If your workload includes AWS Key Management Service (AWS KMS) encryption keys, consider
whether multi-Region keys are appropriate for your application.
i.  Multi-Region keys in AWS KMS
g. For other AWS service features, see this blog series on Creating a Multi-Region Application with
AWS Services series
Level of effort for the Implementation Plan: Moderate to High
Resources
Related documents:
• Creating a Multi-Region Application with AWS Services series
• Disaster Recovery (DR) Architecture on AWS, Part IV: Multi-site Active/Active
• AWS Global Infrastructure
• AWS Local Zones FAQ
• Disaster Recovery (DR) Architecture on AWS, Part I: Strategies for Recovery in the Cloud
• Disaster recovery is different in the cloud
• Global Tables: Multi-Region Replication with DynamoDB
Related videos:
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)
334

AWS Well-Architected Framework
Failure management
• Auth0: Multi-Region High-Availability Architecture that Scales to 1.5B+ Logins a Month with
automated failover
Related examples:
• Disaster Recovery (DR) Architecture on AWS, Part I: Strategies for Recovery in the Cloud
• DTCC achieves resilience well beyond what they can do on premises
• Expedia Group uses a multi-Region, multi-Availability Zone architecture with a proprietary DNS service
to add resilience to the applications
• Uber: Disaster Recovery for Multi-Region Kafka
• Netflix: Active-Active for Multi-Regional Resilience
• How we build Data Residency for Atlassian Cloud
• Intuit TurboTax runs across two Regions
REL10-BP03 Automate recovery for components constrained to a single location
If components of the workload can only run in a single Availability Zone or in an on-premises data
center, implement the capability to do a complete rebuild of the workload within your defined recovery
objectives.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
If the best practice to deploy the workload to multiple locations is not possible due to technological
constraints, you must implement an alternate path to resiliency. You must automate the ability to
recreate necessary infrastructure, redeploy applications, and recreate necessary data for these cases.
For example, Amazon EMR launches all nodes for a given cluster in the same Availability Zone because
running a cluster in the same zone improves performance of the jobs flows as it provides a higher data
access rate. If this component is required for workload resilience, then you must have a way to redeploy
the cluster and its data. Also for Amazon EMR, you should provision redundancy in ways other than using
Multi-AZ. You can provision multiple nodes. Using EMR File System (EMRFS), data in EMR can be stored in
Amazon S3, which in turn can be replicated across multiple Availability Zones or AWS Regions.
Similarly, for Amazon Redshift, by default it provisions your cluster in a randomly selected Availability
Zone within the AWS Region that you select. All the cluster nodes are provisioned in the same zone.
For stateful server-based workloads deployed to an on-premise data center, you can use AWS Elastic
Disaster Recovery to protect your workloads in AWS. If you are already hosted in AWS, you can use Elastic
Disaster Recovery to protect your workload to an alternative Availability Zone or Region. Elastic Disaster
Recovery uses continual block-level replication to a lightweight staging area to provide fast, reliable
recovery of on-premises and cloud-based applications.
Implementation steps
1. Implement self-healing. Deploy your instances or containers using automatic scaling when possible. If
you cannot use automatic scaling, use automatic recovery for EC2 instances or implement self-healing
automation based on Amazon EC2 or ECS container lifecycle events.
• Use Amazon EC2 Auto Scaling groups for instances and container workloads that have no
requirements for a single instance IP address, private IP address, Elastic IP address, and instance
metadata.
• The launch template user data can be used to implement automation that can self-heal most
workloads.
335

AWS Well-Architected Framework
Failure management
• Use automatic recovery of Amazon EC2 instances for workloads that require a single instance ID
address, private IP address, elastic IP address, and instance metadata.
• Automatic Recovery will send recovery status alerts to a SNS topic as the instance failure is
detected.
• Use Amazon EC2 instance lifecycle events or Amazon ECS events to automate self-healing where
automatic scaling or EC2 recovery cannot be used.
• Use the events to invoke automation that will heal your component according to the process logic
you require.
• Protect stateful workloads that are limited to a single location using AWS Elastic Disaster Recovery.
Resources
Related documents:
• Amazon ECS events
• Amazon EC2 Auto Scaling lifecycle hooks
• Recover your instance.
• Service automatic scaling
• What Is Amazon EC2 Auto Scaling?
• AWS Elastic Disaster Recovery
REL10-BP04 Use bulkhead architectures to limit scope of impact
Implement bulkhead architectures (also known as cell-based architectures) to restrict the effect of failure
within a workload to a limited number of components.
Desired outcome: A cell-based architecture uses multiple isolated instances of a workload, where each
instance is known as a cell. Each cell is independent, does not share state with other cells, and handles
a subset of the overall workload requests. This reduces the potential impact of a failure, such as a bad
software update, to an individual cell and the requests it is processing. If a workload uses 10 cells to
service 100 requests, when a failure occurs, 90% of the overall requests would be unaffected by the
failure.
Common anti-patterns:
• Allowing cells to grow without bounds.
• Applying code updates or deployments to all cells at the same time.
• Sharing state or components between cells (with the exception of the router layer).
• Adding complex business or routing logic to the router layer.
• Not minimizing cross-cell interactions.
Benefits of establishing this best practice: With cell-based architectures, many common types of
failure are contained within the cell itself, providing additional fault isolation. These fault boundaries
can provide resilience against failure types that otherwise are hard to contain, such as unsuccessful code
deployments or requests that are corrupted or invoke a specific failure mode (also known as poison pill
requests).
Implementation guidance
On a ship, bulkheads ensure that a hull breach is contained within one section of the hull. In complex
systems, this pattern is often replicated to allow fault isolation. Fault isolated boundaries restrict the
effect of a failure within a workload to a limited number of components. Components outside of the
336

AWS Well-Architected Framework
Failure management
boundary are unaffected by the failure. Using multiple fault isolated boundaries, you can limit the
impact on your workload. On AWS, customers can use multiple Availability Zones and Regions to provide
fault isolation, but the concept of fault isolation can be extended to your workload’s architecture as well.
The overall workload is partitioned cells by a partition key. This key needs to align with the grain of
the service, or the natural way that a service's workload can be subdivided with minimal cross-cell
interactions. Examples of partition keys are customer ID, resource ID, or any other parameter easily
accessible in most API calls. A cell routing layer distributes requests to individual cells based on the
partition key and presents a single endpoint to clients.
Figure 11: Cell-based architecture
Implementation steps
When designing a cell-based architecture, there are several design considerations to consider:
1. Partition key: Special consideration should be taken while choosing the partition key.
• It should align with the grain of the service, or the natural way that a service's workload can be
subdivided with minimal cross-cell interactions. Examples are customer ID or resource ID.
• The partition key must be available in all requests, either directly or in a way that could be easily
inferred deterministically by other parameters.
2. Persistent cell mapping: Upstream services should only interact with a single cell for the lifecycle of
their resources.
• Depending on the workload, a cell migration strategy may be needed to migrate data from one
cell to another. A possible scenario when a cell migration may be needed is if a particular user or
resource in your workload becomes too big and requires it to have a dedicated cell.
• Cells should not share state or components between cells.
• Consequently, cross-cell interactions should be avoided or kept to a minimum, as those interactions
create dependencies between cells and therefore diminish the fault isolation improvements.
337

AWS Well-Architected Framework
Failure management
3. Router layer: The router layer is a shared component between cells, and therefore cannot follow the
same compartmentalization strategy as with cells.
• It is recommended for the router layer to distribute requests to individual cells using a partition
mapping algorithm in a computationally efficient manner, such as combining cryptographic hash
functions and modular arithmetic to map partition keys to cells.
• To avoid multi-cell impacts, the routing layer must remain as simple and horizontally scalable as
possible, which necessitates avoiding complex business logic within this layer. This has the added
benefit of making it easy to understand its expected behavior at all times, allowing for thorough
testability. As explained by Colm MacCárthaigh in Reliability, constant work, and a good cup of
coffee, simple designs and constant work patterns produce reliable systems and reduce anti-
fragility.
4. Cell size: Cells should have a maximum size and should not be allowed to grow beyond it.
• The maximum size should be identified by performing thorough testing, until breaking points are
reached and safe operating margins are established. For more detail on how to implement testing
practices, see REL07-BP04 Load test your workload (p. 313)
• The overall workload should grow by adding additional cells, allowing the workload to scale with
increases in demand.
5. Multi-AZ or Multi-Region strategies: Multiple layers of resilience should be leveraged to protect
against different failure domains.
• For resilience, you should use an approach that builds layers of defense. One layer protects against
smaller, more common disruptions by building a highly available architecture using multiple AZs.
Another layer of defense is meant to protect against rare events like widespread natural disasters
and Region-level disruptions. This second layer involves architecting your application to span
multiple AWS Regions. Implementing a multi-Region strategy for your workload helps protect it
against widespread natural disasters that affect a large geographic region of a country, or technical
failures of Region-wide scope. Be aware that implementing a multi-Region architecture can be
significantly complex, and is usually not required for most workloads. For more detail, see REL10-
BP02 Select the appropriate locations for your multi-location deployment (p. 332).
6. Code deployment: A staggered code deployment strategy should be preferred over deploying code
changes to all cells at the same time.
• This will help minimize potential failure to multiple cells due to a bad deployment or human error.
For more detail, see Automating safe, hands-off deployment.
Level of risk exposed if this best practice is not established: High
Resources
Related best practices:
• REL07-BP04 Load test your workload (p. 313)
• REL10-BP02 Select the appropriate locations for your multi-location deployment (p. 332)
Related documents:
• Reliability, constant work, and a good cup of coffee
• AWS and Compartmentalization
• Workload isolation using shuffle-sharding
• Automating safe, hands-off deployment
Related videos:
• AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small
338

AWS Well-Architected Framework
Failure management
• AWS re:Invent 2018: How AWS Minimizes the Blast Radius of Failures (ARC338)
• Shuffle-sharding: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328)
• AWS Summit ANZ 2021 - Everything fails, all the time: Designing for resilience
Related examples:
• Well-Architected Lab - Fault isolation with shuffle sharding
REL 11. How do you design your workload to withstand
component failures?
Workloads with a requirement for high availability and low mean time to recovery (MTTR) must be
architected for resiliency.
Best practices
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
• REL11-BP02 Fail over to healthy resources (p. 341)
• REL11-BP03 Automate healing on all layers (p. 342)
• REL11-BP04 Rely on the data plane and not the control plane during recovery (p. 344)
• REL11-BP05 Use static stability to prevent bimodal behavior (p. 345)
• REL11-BP06 Send notifications when events impact availability (p. 347)
• REL11-BP07 Architect your product to meet availability targets and uptime service level agreements
(SLAs) (p. 348)
REL11-BP01 Monitor all components of the workload to detect failures
Continuously monitor the health of your workload so that you and your automated systems are aware
of degradation or failure as soon as they occur. Monitor for key performance indicators (KPIs) based on
business value.
All recovery and healing mechanisms must start with the ability to detect problems quickly. Technical
failures should be detected first so that they can be resolved. However, availability is based on the ability
of your workload to deliver business value, so key performance indicators (KPIs) that measure this need
to be a part of your detection and remediation strategy.
Common anti-patterns:
• No alarms have been configured, so outages occur without notification.
• Alarms exist, but at thresholds that don't provide adequate time to react.
• Metrics are not collected often enough to meet the recovery time objective (RTO).
• Only the customer facing tier of the workload is actively monitored.
• Only collecting technical metrics, no business function metrics.
• No metrics measuring the user experience of the workload.
Benefits of establishing this best practice: Having appropriate monitoring at all layers allows you to
reduce recovery time by reducing time to detection.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Determine the collection interval for your components based on your recovery goals.
339

AWS Well-Architected Framework
Failure management
• Your monitoring interval is dependent on how quickly you must recover. Your recovery time is driven
by the time it takes to recover, so you must determine the frequency of collection by accounting for
this time and your recovery time objective (RTO).
•                                                                                                       Configure detailed monitoring for components.
• Determine if detailed monitoring for EC2 instances and Auto Scaling is necessary. Detailed
monitoring provides 1-min interval metrics, and default monitoring provides 5-minute interval
metrics.
• Enable or Disable Detailed Monitoring for Your Instance
• Monitoring Your Auto Scaling Groups and Instances Using Amazon CloudWatch
• Determine if enhanced monitoring for RDS is necessary. Enhanced monitoring uses an agent on the
RDS instances to get useful information about different process or threads on an RDS instance.
• Enhanced Monitoring
•                                                                                                       Create custom metrics to measure business key performance indicators (KPIs). Workloads implement
key business functions. These functions should be used as KPIs that help identify when an indirect
problem happens.
• Publishing Custom Metrics
•                                                                                                       Monitor the user experience for failures using user canaries. Synthetic transaction testing (also
known as canary testing, but not to be confused with canary deployments) that can run and simulate
customer behavior is among the most important testing processes. Run these tests constantly against
your workload endpoints from diverse remote locations.
• Amazon CloudWatch Synthetics allows you to create user canaries
•                                                                                                       Create custom metrics that track the user's experience. If you can instrument the experience of the
customer, you can determine when the consumer experience degrades.
• Publishing Custom Metrics
•                                                                                                       Set alarms to detect when any part of your workload is not working properly, and to indicate when to
Auto Scale resources. Alarms can be visually displayed on dashboards, send alerts via Amazon SNS or
email, and work with Auto Scaling to scale up or down the resources for a workload.
• Using Amazon CloudWatch Alarms
•                                                                                                       Create dashboards to visualize your metrics. Dashboards can be used to visually see trends, outliers,
and other indicators of potential problems, or to provide an indication of problems you may want to
investigate.
• Using CloudWatch Dashboards
Resources
Related documents:
• Amazon CloudWatch Synthetics enables you to create user canaries
• Enable or Disable Detailed Monitoring for Your Instance
• Enhanced Monitoring
• Monitoring Your Auto Scaling Groups and Instances Using Amazon CloudWatch
• Publishing Custom Metrics
• Using Amazon CloudWatch Alarms
• Using CloudWatch Dashboards
Related examples:
• Well-Architected lab: Level 300: Implementing Health Checks and Managing Dependencies to Improve
Reliability
340

AWS Well-Architected Framework
Failure management
REL11-BP02 Fail over to healthy resources
Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. For
location failures (such as Availability Zone or AWS Region) ensure that you have systems in place to fail
over to healthy resources in unimpaired locations.
AWS services, such as Elastic Load Balancing and Amazon EC2 Auto Scaling, help distribute load across
resources and Availability Zones. Therefore, failure of an individual resource (such as an EC2 instance) or
impairment of an Availability Zone can be mitigated by shifting traffic to remaining healthy resources.
For multi-region workloads, this is more complicated. For example, cross-region read replicas allow you
to deploy your data to multiple AWS Regions, but you still must promote the read replica to primary and
point your traffic at it in the event of a failover. Amazon Route 53 and AWS Global Accelerator can help
route traffic across AWS Regions.
If your workload is using AWS services, such as Amazon S3 or Amazon DynamoDB, then they are
automatically deployed to multiple Availability Zones. In case of failure, the AWS control plane
automatically routes traffic to healthy locations for you. Data is redundantly stored in multiple
Availability Zones, and remains available. For Amazon RDS, you must choose Multi-AZ as a configuration
option, and then on failure AWS automatically directs traffic to the healthy instance. For Amazon EC2
instances, Amazon ECS tasks, or Amazon EKS pods, you choose which Availability Zones to deploy to.
Elastic Load Balancing then provides the solution to detect instances in unhealthy zones and route traffic
to the healthy ones. Elastic Load Balancing can even route traffic to components in your on-premises
data center.
For Multi-Region approaches (which might also include on-premises data centers), Amazon Route 53
provides a way to define internet domains, and assign routing policies that can include health checks
to ensure that traffic is routed to healthy regions. Alternately, AWS Global Accelerator provides static IP
addresses that act as a fixed entry point to your application, then routes to endpoints in AWS Regions
of your choosing, using the AWS global network instead of the internet for better performance and
reliability.
AWS approaches the design of our services with fault recovery in mind. We design services to minimize
the time to recover from failures and impact on data. Our services primarily use data stores that
acknowledge requests only after they are durably stored across multiple replicas within a Region. These
services and resources include Amazon Aurora, Amazon Relational Database Service (Amazon RDS) Multi-
AZ DB instances, Amazon S3, Amazon DynamoDB, Amazon Simple Queue Service (Amazon SQS), and
Amazon Elastic File System (Amazon EFS). They are constructed to use cell-based isolation and use
the fault isolation provided by Availability Zones. We use automation extensively in our operational
procedures. We also optimize our replace-and-restart functionality to recover quickly from interruptions.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Fail over to healthy resources. Ensure that if a resource failure occurs, that healthy resources can
continue to serve requests. For location failures (such as Availability Zone or AWS Region) ensure you
have systems in place to fail over to healthy resources in unimpaired locations.
• If your workload is using AWS services, such as Amazon S3 or Amazon DynamoDB, then they are
automatically deployed to multiple Availability Zones. In case of failure, the AWS control plane
automatically routes traffic to healthy locations for you.
• For Amazon RDS you must choose Multi-AZ as a configuration option, and then on failure AWS
automatically directs traffic to the healthy instance.
• High Availability (Multi-AZ) for Amazon RDS
• For Amazon EC2 instances or Amazon ECS tasks, you choose which Availability Zones to deploy to.
Elastic Load Balancing then provides the solution to detect instances in unhealthy zones and route
traffic to the healthy ones. Elastic Load Balancing can even route traffic to components in your on-
premises data center.
341

AWS Well-Architected Framework
Failure management
• For multi-region approaches (which might also include on-premises data centers), ensure that data
and resources from healthy locations can continue to serve requests
• For example, cross-region read replicas allow you to deploy your data to multiple AWS Regions,
but you still must promote the read replica to master and point your traffic at it in the event of a
primary location failure.
• Overview of Amazon RDS Read Replicas
• Amazon Route 53 provides a way to define internet domains, and assign routing policies, which
might include health checks, to ensure that traffic is routed to healthy Regions. Alternately, AWS
Global Accelerator provides static IP addresses that act as a fixed entry point to your application,
then routes to endpoints in AWS Regions of your choosing, using the AWS global network instead
of the public internet for better performance and reliability.
• Amazon Route 53: Choosing a Routing Policy
• What Is AWS Global Accelerator?
Resources
Related documents:
• APN Partner: partners that can help with automation of your fault tolerance
• AWS Marketplace: products that can be used for fault tolerance
• AWS OpsWorks: Using Auto Healing to Replace Failed Instances
• Amazon Route 53: Choosing a Routing Policy
• High Availability (Multi-AZ) for Amazon RDS
• Overview of Amazon RDS Read Replicas
• Amazon ECS task placement strategies
• Creating Kubernetes Auto Scaling Groups for Multiple Availability Zones
• What is AWS Global Accelerator?
Related examples:
• Well-Architected lab: Level 300: Implementing Health Checks and Managing Dependencies to Improve
Reliability
REL11-BP03 Automate healing on all layers
Upon detection of a failure, use automated capabilities to perform actions to remediate.
Ability to restart is an important tool to remediate failures. As discussed previously for distributed
systems, a best practice is to make services stateless where possible. This prevents loss of data or
availability on restart. In the cloud, you can (and generally should) replace the entire resource (for
example, EC2 instance, or Lambda function) as part of the restart. The restart itself is a simple and
reliable way to recover from failure. Many different types of failures occur in workloads. Failures
can occur in hardware, software, communications, and operations. Rather than constructing novel
mechanisms to trap, identify, and correct each of the different types of failures, map many different
categories of failures to the same recovery strategy. An instance might fail due to hardware failure, an
operating system bug, memory leak, or other causes. Rather than building custom remediation for each
situation, treat any of them as an instance failure. Terminate the instance, and allow AWS Auto Scaling to
replace it. Later, carry out the analysis on the failed resource out of band.
Another example is the ability to restart a network request. Apply the same recovery approach to both
a network timeout and a dependency failure where the dependency returns an error. Both events have
a similar effect on the system, so rather than attempting to make either event a “special case”, apply a
similar strategy of limited retry with exponential backoff and jitter.
342

AWS Well-Architected Framework
Failure management
Ability to restart is a recovery mechanism featured in Recovery Oriented Computing and high availability
cluster architectures.
Amazon EventBridge can be used to monitor and filter for events such as CloudWatch Alarms or changes
in state in other AWS services. Based on event information, it can then invoke AWS Lambda, AWS
Systems Manager Automation, or other targets to run custom remediation logic on your workload.
Amazon EC2 Auto Scaling can be configured to check for EC2 instance health. If the instance is in
any state other than running, or if the system status is impaired, Amazon EC2 Auto Scaling considers
the instance to be unhealthy and launches a replacement instance. If using AWS OpsWorks, you can
configure Auto Healing of EC2 instances at the OpsWorks layer level.
For large-scale replacements (such as the loss of an entire Availability Zone), static stability is preferred
for high availability instead of trying to obtain multiple new resources at once.
Common anti-patterns:
• Deploying applications in instances or containers individually.
• Deploying applications that cannot be deployed into multiple locations without using automatic
recovery.
• Manually healing applications that automatic scaling and automatic recovery fail to heal.
Benefits of establishing this best practice: Automated healing, even if the workload can only deployed
into one location at a time will reduce your mean time to recovery, and ensure availability of the
workload.
Level of risk exposed if this best practice is not established: High
Implementation guidance
•                                                                                                               Use Auto Scaling groups to deploy tiers in an workload. Auto scaling can perform self-healing on
stateless applications, and add and remove capacity.
• How AWS Auto Scaling Works
•                                                                                                               Implement automatic recovery on EC2 instances that have applications deployed that cannot be
deployed in multiple locations, and can tolerate rebooting upon failures. Automatic recovery can be
used to replace failed hardware and restart the instance when the application is not capable of being
deployed in multiple locations. The instance metadata and associated IP addresses are kept, as well
as the Amazon EBS volumes and mount points to Elastic File Systems or File Systems for Lustre and
Windows.
• Amazon EC2 Automatic Recovery
• Amazon Elastic Block Store (Amazon EBS)
• Amazon Elastic File System (Amazon EFS)
• What is Amazon FSx for Lustre?
• What is Amazon FSx for Windows File Server?
• Using AWS OpsWorks, you can configure Auto Healing of EC2 instances at the layer level
• AWS OpsWorks: Using Auto Healing to Replace Failed Instances
•                                                                                                               Implement automated recovery using AWS Step Functions and AWS Lambda when you cannot use
automatic scaling or automatic recovery, or when automatic recovery fails. When you cannot use
automatic scaling, and either cannot use automatic recovery or automatic recovery fails, you can
automate the healing using AWS Step Functions and AWS Lambda.
• What is AWS Step Functions?
• What is AWS Lambda?
• Amazon EventBridge can be used to monitor and filter for events such as CloudWatch Alarms
or changes in state in other AWS services. Based on event information, it can then invoke AWS
Lambda (or other targets) to run custom remediation logic on your workload.
343

AWS Well-Architected Framework
Failure management
• What Is Amazon EventBridge?
• Using Amazon CloudWatch Alarms
Resources
Related documents:
• APN Partner: partners that can help with automation of your fault tolerance
• AWS Marketplace: products that can be used for fault tolerance
• AWS OpsWorks: Using Auto Healing to Replace Failed Instances
• Amazon EC2 Automatic Recovery
• Amazon Elastic Block Store (Amazon EBS)
• Amazon Elastic File System (Amazon EFS)
• How AWS Auto Scaling Works
• Using Amazon CloudWatch Alarms
• What Is Amazon EventBridge?
• What is AWS Lambda?
• AWS Systems Manager Automation
• What is AWS Step Functions?
• What is Amazon FSx for Lustre?
• What is Amazon FSx for Windows File Server?
Related videos:
• Static stability in AWS: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328)
Related examples:
• Well-Architected lab: Level 300: Implementing Health Checks and Managing Dependencies to Improve
Reliability
REL11-BP04 Rely on the data plane and not the control plane during recovery
The control plane is used to configure resources, and the data plane delivers services. Data planes
typically have higher availability design goals than control planes and are usually less complex. When
implementing recovery or mitigation responses to potentially resiliency-impacting events, using control
plane operations can lower the overall resiliency of your architecture. For example, you can rely on
the Amazon Route 53 data plane to reliably route DNS queries based on health checks, but updating
Route 53 routing policies uses the control plane, so do not rely on it for recovery.
The Route 53 data planes answer DNS queries, and perform and evaluate health checks. They are
globally distributed and designed for a 100% availability service level agreement (SLA). The Route 53
management APIs and consoles where you create, update, and delete Route 53 resources run on
control planes that are designed to prioritize the strong consistency and durability that you need when
managing DNS. To achieve this, the control planes are located in a single Region, US East (N. Virginia).
While both systems are built to be very reliable, the control planes are not included in the SLA. There
could be rare events in which the data plane’s resilient design allows it to maintain availability while the
control planes do not. For disaster recovery and failover mechanisms, use data plane functions to provide
the best possible reliability.
344

AWS Well-Architected Framework
Failure management
For more information about data planes, control planes, and how AWS builds services to meet high
availability targets, see the Static stability using Availability Zones paper and the Amazon Builders’
Library.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Rely on the data plane and not the control plane when using Amazon Route 53 for disaster recovery.
Route 53 Application Recovery Controller helps you manage and coordinate failover using readiness
checks and routing controls. These features continually monitor your application’s ability to recover
from failures, and allows you to control your application recovery across multiple AWS Regions,
Availability Zones, and on premises.
• What is Route 53 Application Recovery Controller
• Creating Disaster Recovery Mechanisms Using Amazon Route 53
• Building highly resilient applications using Amazon Route 53 Application Recovery Controller, Part 1:
Single-Region stack
• Building highly resilient applications using Amazon Route 53 Application Recovery Controller, Part 2:
Multi-Region stack
• Understand which operations are on the data plane and which are on the control plane.
• Amazon Builders' Library: Avoiding overload in distributed systems by putting the smaller service in
control
• Amazon DynamoDB API (control plane and data plane)
• AWS Lambda Executions (split into the control plane and the data plane)
• AWS Lambda Executions (split into the control plane and the data plane)
Resources
Related documents:
• APN Partner: partners that can help with automation of your fault tolerance
• AWS Marketplace: products that can be used for fault tolerance
• Amazon Builders' Library: Avoiding overload in distributed systems by putting the smaller service in
control
• Amazon DynamoDB API (control plane and data plane)
• AWS Lambda Executions (split into the control plane and the data plane)
• AWS Elemental MediaStore Data Plane
• Building highly resilient applications using Amazon Route 53 Application Recovery Controller, Part 1:
Single-Region stack
• Building highly resilient applications using Amazon Route 53 Application Recovery Controller, Part 2:
Multi-Region stack
• Creating Disaster Recovery Mechanisms Using Amazon Route 53
• What is Route 53 Application Recovery Controller
Related examples:
• Introducing Amazon Route 53 Application Recovery Controller
REL11-BP05 Use static stability to prevent bimodal behavior
Bimodal behavior is when your workload exhibits different behavior under normal and failure modes,
for example, relying on launching new instances if an Availability Zone fails. You should instead build
345

AWS Well-Architected Framework
Failure management
workloads that are statically stable and operate in only one mode. In this case, provision enough
instances in each Availability Zone to handle the workload load if one AZ were removed and then use
Elastic Load Balancing or Amazon Route 53 health checks to shift load away from the impaired instances.
Static stability for compute deployment (such as EC2 instances or containers) will result in the highest
reliability. This must be weighed against cost concerns. It’s less expensive to provision less compute
capacity and rely on launching new instances in the case of a failure. But for large-scale failures (such as
an Availability Zone failure) this approach is less effective because it relies on reacting to impairments
as they happen, rather than being prepared for those impairments before they happen. Your solution
should weigh reliability versus the cost needs for your workload. By using more Availability Zones, the
amount of additional compute you need for static stability decreases.
Figure 14: Static stability of EC2 instances across Availability Zones
After traffic has shifted, use AWS Auto Scaling to asynchronously replace instances from the failed zone
and launch them in the healthy zones.
Another example of bimodal behavior would be a network timeout that could cause a system to attempt
to refresh the configuration state of the entire system. This would add unexpected load to another
component, and might cause it to fail, resulting in other unexpected consequences. This negative
feedback loop impacts availability of your workload. Instead, you should build systems that are statically
stable and operate in only one mode. A statically stable design would be to do constant work, and
always refresh the configuration state on a fixed cadence. When a call fails, the workload uses the
previously cached value, and initiates an alarm.
Another example of bimodal behavior is allowing clients to bypass your workload cache when failures
occur. This might seem to be a solution that accommodates client needs, but should not be allowed
because it significantly changes the demands on your workload and is likely to result in failures.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Use static stability to prevent bimodal behavior. Bimodal behavior is when your workload exhibits
different behavior under normal and failure modes, for example, relying on launching new instances if
an Availability Zone fails.
• Minimizing Dependencies in a Disaster Recovery Plan
• The Amazon Builders' Library: Static stability using Availability Zones
• Static stability in AWS: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328)
346

AWS Well-Architected Framework
Failure management
• You should instead build systems that are statically stable and operate in only one mode. In this
case, provision enough instances in each zone to handle workload load if one AZ were removed
and then use Elastic Load Balancing or Amazon Route 53 health checks to shift load away from
the impaired instances.
• Another example of bimodal behavior is allowing clients to bypass your workload cache when
failures occur. This might seem to be a solution to accommodate client needs, but should not be
allowed since it significantly changes demands on your workload and is likely to result in failures.
Resources
Related documents:
• Minimizing Dependencies in a Disaster Recovery Plan
• The Amazon Builders' Library: Static stability using Availability Zones
Related videos:
• Static stability in AWS: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328)
REL11-BP06 Send notifications when events impact availability
Notifications are sent upon the detection of significant events, even if the issue caused by the event was
automatically resolved.
Automated healing allows your workload to be reliable. However, it can also obscure underlying
problems that need to be addressed. Implement appropriate monitoring and events so that you can
detect patterns of problems, including those addressed by auto healing, so that you can resolve root
cause issues. Amazon CloudWatch Alarms can be invoked based on failures that occur. They can also be
invoked based on automated healing actions that run. CloudWatch Alarms can be configured to send
emails, or to log incidents in third-party incident tracking systems using Amazon SNS integration.
Common anti-patterns:
• Sending alarms that no one acts upon.
• Performing auto healing automation, but not notifying that healing was needed.
Benefits of establishing this best practice: Notifications of recovery events will ensure that you don’t
ignore problems that occur infrequently.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Alarms on business Key Performance Indicators when they exceed a low threshold Having a low
threshold alarm on your business KPIs help you know when your workload is unavailable or non-
functional.
• Creating a CloudWatch Alarm Based on a Static Threshold
• Alarm on events that invoke healing automation You can directly invoke an SNS API to send
notifications with any automation that you create.
• What is Amazon Simple Notification Service?
Resources
Related documents:
347

AWS Well-Architected Framework
Failure management
• Creating a CloudWatch Alarm Based on a Static Threshold
• What Is Amazon EventBridge?
• What is Amazon Simple Notification Service?
REL11-BP07 Architect your product to meet availability targets and uptime
service level agreements (SLAs)
Architect your product to meet availability targets and uptime service level agreements (SLAs). If you
publish or privately agree to availability targets or uptime SLAs, verify that your architecture and
operational processes are designed to support them.
Desired outcome: Each application has a defined target for availability and SLA for performance metrics,
which can be monitored and maintained in order to meet business outcomes.
Common anti-patterns:
• Designing and deploying workload’s without setting any SLAs.
• SLA metrics are set to high without rationale or business requirements.
• Setting SLAs without taking into account for dependencies and their underlying SLA.
• Application designs are created without considering the Shared Responsibility Model for Resilience.
Benefits of establishing this best practice: Designing applications based on key resiliency targets helps
you meet business objectives and customer expectations. These objectives help drive the application
design process that evaluates different technologies and considers various tradeoffs.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Application designs have to account for a diverse set of requirements that are derived from business,
operational, and financial objectives. Within the operational requirements, workloads need to have
specific resilience metric targets so they can be properly monitored and supported. Resilience metrics
should not be set or derived after deploying the workload. They should be defined during the design
phase and help guide various decisions and tradeoffs.
• Every workload should have its own set of resilience metrics. Those metrics may be different from
other business applications.
• Reducing dependencies can have a positive impact on availability. Each workload should consider its
dependencies and their SLAs. In general, select dependencies with availability goals equal to or greater
than the goals of your workload.
• Consider loosely coupled designs so your workload can operate correctly despite dependency
impairment, where possible.
• Reduce control plane dependencies, especially during recovery or a degradation. Evaluate designs that
are statically stable for mission critical workloads. Use resource sparing to increase the availability of
those dependencies in a workload.
• Observability and instrumentation are critical for achieving SLAs by reducing Mean Time to Detection
(MTTD) and Mean Time to Repair (MTTR).
• Less frequent failure (longer MTBF), shorter failure detection times (shorter MTTD), and shorter repair
times (shorter MTTR) are the three factors that are used to improve availability in distributed systems.
• Establishing and meeting resilience metrics for a workload is foundational to any effective design.
Those designs must factor in tradeoffs of design complexity, service dependencies, performance,
scaling, and costs.
348

AWS Well-Architected Framework
Failure management
Implementation steps
• Review and document the workload design considering the following questions:
• Where are control planes used in the workload?
• How does the workload implement fault tolerance?
• What are the design patterns for scaling, automatic scaling, redundancy, and highly available
components?
• What are the requirements for data consistency and availability?
• Are there considerations for resource sparing or resource static stability?
• What are the service dependencies?
• Define SLA metrics based on the workload architecture while working with stakeholders. Consider the
SLAs of all dependencies used by the workload.
• Once the SLA target has been set, optimize the architecture to meet the SLA.
• Once the design is set that will meet the SLA, implement operational changes, process automation,
and runbooks that also will have focus on reducing MTTD and MTTR.
• Once deployed, monitor and report on the SLA.
Resources
Related best practices:
• REL03-BP01 Choose how to segment your workload (p. 272)
• REL10-BP01 Deploy the workload to multiple locations (p. 328)
• REL11-BP01 Monitor all components of the workload to detect failures (p. 339)
• REL11-BP03 Automate healing on all layers (p. 342)
• REL12-BP05 Test resiliency using chaos engineering (p. 353)
• REL13-BP01 Define recovery objectives for downtime and data loss (p. 361)
• Understanding workload health
Related documents:
• Availability with redundancy
• Reliability pillar - Availability
• Measuring availability
• AWS Fault Isolation Boundaries
• Shared Responsibility Model for Resiliency
• Static stability using Availability Zones
• AWS Service Level Agreements (SLAs)
• Guidance for Cell-based Architecture on AWS
• AWS infrastructure
• Advanced Multi-AZ Resiliance Patterns whitepaper
Related services:
• Amazon CloudWatch
• AWS Config
• AWS Trusted Advisor
349

AWS Well-Architected Framework
Failure management
REL 12. How do you test reliability?
After you have designed your workload to be resilient to the stresses of production, testing is the only
way to verify that it will operate as designed, and deliver the resiliency you expect.
Best practices
• REL12-BP01 Use playbooks to investigate failures (p. 350)
• REL12-BP02 Perform post-incident analysis (p. 351)
• REL12-BP03 Test functional requirements (p. 352)
• REL12-BP04 Test scaling and performance requirements (p. 353)
• REL12-BP05 Test resiliency using chaos engineering (p. 353)
• REL12-BP06 Conduct game days regularly (p. 360)
REL12-BP01 Use playbooks to investigate failures
Permit consistent and prompt responses to failure scenarios that are not well understood, by
documenting the investigation process in playbooks. Playbooks are the predefined steps performed
to identify the factors contributing to a failure scenario. The results from any process step are used to
determine the next steps to take until the issue is identified or escalated.
The playbook is proactive planning that you must do, to be able to take reactive actions effectively.
When failure scenarios not covered by the playbook are encountered in production, first address the
issue (put out the fire). Then go back and look at the steps you took to address the issue and use these to
add a new entry in the playbook.
Note that playbooks are used in response to specific incidents, while runbooks are used to achieve
specific outcomes. Often, runbooks are used for routine activities and playbooks are used to respond to
non-routine events.
Common anti-patterns:
• Planning to deploy a workload without knowing the processes to diagnose issues or respond to
incidents.
• Unplanned decisions about which systems to gather logs and metrics from when investigating an
event.
• Not retaining metrics and events long enough to be able to retrieve the data.
Benefits of establishing this best practice: Capturing playbooks ensures that processes can be
consistently followed. Codifying your playbooks limits the introduction of errors from manual activity.
Automating playbooks shortens the time to respond to an event by eliminating the requirement for
team member intervention or providing them additional information when their intervention begins.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Use playbooks to identify issues. Playbooks are documented processes to investigate issues. Allow
consistent and prompt responses to failure scenarios by documenting processes in playbooks.
Playbooks must contain the information and guidance necessary for an adequately skilled person
to gather applicable information, identify potential sources of failure, isolate faults, and determine
contributing factors (perform post-incident analysis).
• Implement playbooks as code. Perform your operations as code by scripting your playbooks
to ensure consistency and limit reduce errors caused by manual processes. Playbooks can be
composed of multiple scripts representing the different steps that might be necessary to identify the
350

AWS Well-Architected Framework
Failure management
contributing factors to an issue. Runbook activities can be invoked or performed as part of playbook
activities, or might prompt to run a playbook in response to identified events.
• Automate your operational playbooks with AWS Systems Manager
• AWS Systems Manager Run Command
• AWS Systems Manager Automation
• What is AWS Lambda?
• What Is Amazon EventBridge?
• Using Amazon CloudWatch Alarms
Resources
Related documents:
• AWS Systems Manager Automation
• AWS Systems Manager Run Command
• Automate your operational playbooks with AWS Systems Manager
• Using Amazon CloudWatch Alarms
• Using Canaries (Amazon CloudWatch Synthetics)
• What Is Amazon EventBridge?
• What is AWS Lambda?
Related examples:
• Automating operations with Playbooks and Runbooks
REL12-BP02 Perform post-incident analysis
Review customer-impacting events, and identify the contributing factors and preventative action items.
Use this information to develop mitigations to limit or prevent recurrence. Develop procedures for
prompt and effective responses. Communicate contributing factors and corrective actions as appropriate,
tailored to target audiences. Have a method to communicate these causes to others as needed.
Assess why existing testing did not find the issue. Add tests for this case if tests do not already exist.
Common anti-patterns:
• Finding contributing factors, but not continuing to look deeper for other potential problems and
approaches to mitigate.
• Only identifying human error causes, and not providing any training or automation that could prevent
human errors.
Benefits of establishing this best practice: Conducting post-incident analysis and sharing the results
permits other workloads to mitigate the risk if they have implemented the same contributing factors,
and allows them to implement the mitigation or automated recovery before an incident occurs.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Establish a standard for your post-incident analysis. Good post-incident analysis provides
opportunities to propose common solutions for problems with architecture patterns that are used in
other places in your systems.
• Ensure that the contributing factors are honest and blame free.
351

AWS Well-Architected Framework
Failure management
• If you do not document your problems, you cannot correct them.
• Ensure post-incident analysis is blame free so you can be dispassionate about the proposed
corrective actions and promote honest self-assessment and collaboration on your application
teams.
• Use a process to determine contributing factors. Have a process to identify and document the
contributing factors of an event so that you can develop mitigations to limit or prevent recurrence and
you can develop procedures for prompt and effective responses. Communicate contributing factors as
appropriate, tailored to target audiences.
• What is log analytics?
Resources
Related documents:
• What is log analytics?
• Why you should develop a correction of error (COE)
REL12-BP03 Test functional requirements
Use techniques such as unit tests and integration tests that validate required functionality.
You achieve the best outcomes when these tests are run automatically as part of build and deployment
actions. For instance, using AWS CodePipeline, developers commit changes to a source repository where
CodePipeline automatically detects the changes. Those changes are built, and tests are run. After the
tests are complete, the built code is deployed to staging servers for testing. From the staging server,
CodePipeline runs more tests, such as integration or load tests. Upon the successful completion of those
tests, CodePipeline deploys the tested and approved code to production instances.
Additionally, experience shows that synthetic transaction testing (also known as canary testing, but not
to be confused with canary deployments) that can run and simulate customer behavior is among the
most important testing processes. Run these tests constantly against your workload endpoints from
diverse remote locations. Amazon CloudWatch Synthetics allows you to create canaries to monitor your
endpoints and APIs.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Test functional requirements. These include unit tests and integration tests that validate required
functionality.
• Use CodePipeline with AWS CodeBuild to test code and run builds
• AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild
• Continuous Delivery and Continuous Integration
• Using Canaries (Amazon CloudWatch Synthetics)
• Software test automation
Resources
Related documents:
• APN Partner: partners that can help with implementation of a continuous integration pipeline
• AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild
• AWS Marketplace: products that can be used for continuous integration
• Continuous Delivery and Continuous Integration
352

AWS Well-Architected Framework
Failure management
• Software test automation
• Use CodePipeline with AWS CodeBuild to test code and run builds
• Using Canaries (Amazon CloudWatch Synthetics)
REL12-BP04 Test scaling and performance requirements
Use techniques such as load testing to validate that the workload meets scaling and performance
requirements.
In the cloud, you can create a production-scale test environment on demand for your workload. If you
run these tests on scaled down infrastructure, you must scale your observed results to what you think
will happen in production. Load and performance testing can also be done in production if you are
careful not to impact actual users, and tag your test data so it does not comingle with real user data and
corrupt usage statistics or production reports.
With testing, ensure that your base resources, scaling settings, service quotas, and resiliency design
operate as expected under load.
Level of risk exposed if this best practice is not established: High
Implementation guidance
• Test scaling and performance requirements. Perform load testing to validate that the workload meets
scaling and performance requirements.
• Distributed Load Testing on AWS: simulate thousands of connected users
• Apache JMeter
• Deploy your application in an environment identical to your production environment and run a
load test.
• Use infrastructure as code concepts to create an environment as similar to your production
environment as possible.
Resources
Related documents:
• Distributed Load Testing on AWS: simulate thousands of connected users
• Apache JMeter
REL12-BP05 Test resiliency using chaos engineering
Run chaos experiments regularly in environments that are in or as close to production as possible to
understand how your system responds to adverse conditions.
Desired outcome:
The resilience of the workload is regularly verified by applying chaos engineering in the form of fault
injection experiments or injection of unexpected load, in addition to resilience testing that validates
known expected behavior of your workload during an event. Combine both chaos engineering and
resilience testing to gain confidence that your workload can survive component failure and can recover
from unexpected disruptions with minimal to no impact.
Common anti-patterns:
• Designing for resiliency, but not verifying how the workload functions as a whole when faults occur.
• Never experimenting under real-world conditions and expected load.
353
