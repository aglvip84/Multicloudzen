AWS Well-Architected Framework
Expenditure and usage awareness
workloads. Common policies include which services and features can be used (for example, lower
performance storage in test and development environments), which types of resources can be used by
different groups (for example, the largest size of resource in a development account is medium) and how
long these resources will be in use (whether temporary, short term, or for a specific period of time).
Policy example
The following is a sample policy you can review to create your own cloud governance policies, which
focus on cost optimization. Make sure you adjust policy based on your organization’s requirements and
your stakeholders’ requests.
• Policy name: Define a clear policy name, such as Resource Optimization and Cost Reduction Policy.
• Purpose: Explain why this policy should be used and what is the expected outcome. The objective of
this policy is to verify that there is a minimum cost required to deploy and run the desired workload to
meet business requirements.
• Scope: Clearly define who should use this policy and when it should be used, such as DevOps X Team
to use this policy in us-east customers for X environment (production or non-production).
Policy statement
1. Select us-east-1or multiple us-east regions based on your workload’s environment and business
requirement (development, user acceptance testing, pre-production, or production).
2. Schedule Amazon EC2 and Amazon RDS instances to run between six in the morning and eight at
night (Eastern Standard Time (EST)).
3. Stop all unused Amazon EC2 instances after eight hours and unused Amazon RDS instances after 24
hours of inactivity.
4. Terminate all unused Amazon EC2 instances after 24 hours of inactivity in non-production
environments. Remind Amazon EC2 instance owner (based on tags) to review their stopped Amazon
EC2 instances in production and inform them that their Amazon EC2 instances will be terminated
within 72 hours if they are not in use.
5. Use generic instance family and size such as m5.large and then resize the instance based on CPU and
memory utilization using AWS Compute Optimizer.
6. Prioritize using auto scaling to dynamically adjust the number of running instances based on traffic.
7. Use spot instances for non-critical workloads.
8. Review capacity requirements to commit saving plans or reserved instances for predictable workloads
and inform Cloud Financial Management Team.
9. Use Amazon S3 lifecycle policies to move infrequently accessed data to cheaper storage tiers. If
no retention policy defined, use Amazon S3 Intelligent Tiering to move objects to archived tier
automatically.
10Monitor resource utilization and set alarms to trigger scaling events using Amazon CloudWatch.
11For each AWS account, use AWS Budgets to set cost and usage budgets for your account based on cost
center and business units.
12Using AWS Budgets to set cost and usage budgets for your account can help you stay on top of your
spending and avoid unexpected bills, allowing you to better control your costs.
Procedure: Provide detailed procedures for implementing this policy or refer to other documents that
describe how to implement each policy statement. This section should provide step-by-step instructions
for carrying out the policy requirements.
To implement this policy, you can use various third-party tools or AWS Config rules to check for
compliance with the policy statement and trigger automated remediation actions using AWS Lambda
functions. You can also use AWS Organizations to enforce the policy. Additionally, you should regularly
470

AWS Well-Architected Framework
Expenditure and usage awareness
review your resource usage and adjust the policy as necessary to verify that it continues to meet your
business needs.
Implementation steps
•                                                                                                          Meet with stakeholders: To develop policies, ask stakeholders (cloud business office, engineers,
or functional decision makers for policy enforcement) within your organization to specify their
requirements and document them. Take an iterative approach by starting broadly and continually
refine down to the smallest units at each step. Team members include those with direct interest in
the workload, such as organization units or application owners, as well as supporting groups, such as
security and finance teams.
•                                                                                                          Get confirmation: Make sure teams agree on policies who can access and deploy to the AWS Cloud.
Make sure they follow your organization’s policies and confirm that their resource creations align with
the agreed policies and procedures.
•                                                                                                          Create onboarding training sessions: Ask new organization members to complete onboarding
training courses to create cost awareness and organization requirements. They may assume different
policies from their previous experience or not think of them at all.
•                                                                                                          Define locations for your workload: Define where your workload operates, including the country and
the area within the country. This information is used for mapping to AWS Regions and Availability
Zones.
•                                                                                                          Define and group services and resources: Define the services that the workloads require. For each
service, specify the types, the size, and the number of resources required. Define groups for the
resources by function, such as application servers or database storage. Resources can belong to
multiple groups.
•                                                                                                          Define and group the users by function: Define the users that interact with the workload, focusing
on what they do and how they use the workload, not on who they are or their position in the
organization. Group similar users or functions together. You can use the AWS managed policies as a
guide.
•                                                                                                          Define the actions: Using the locations, resources, and users identified previously, define the actions
that are required by each to achieve the workload outcomes over its life time (development, operation,
and decommission). Identify the actions based on the groups, not the individual elements in the
groups, in each location. Start broadly with read or write, then refine down to specific actions to each
service.
•                                                                                                          Define the review period: Workloads and organizational requirements can change over time. Define
the workload review schedule to ensure it remains aligned with organizational priorities.
•                                                                                                          Document the policies: Verify the policies that have been defined are accessible as required by your
organization. These policies are used to implement, maintain, and audit access of your environments.
Resources
Related documents:
• Change Management in the Cloud
• AWS Managed Policies for Job Functions
• AWS multiple account billing strategy
• Actions, Resources, and Condition Keys for AWS Services
• AWS Management and Governance
• Control access to AWS Regions using IAM policies
• Global Infrastructures Regions and AZs
Related videos:
471

AWS Well-Architected Framework
Expenditure and usage awareness
• AWS Management and Governance at Scale
Related examples:
• VMware - What Are Cloud Policies?
COST02-BP02 Implement goals and targets
This best practice was updated with new guidance on July 13th, 2023.
Implement both cost and usage goals and targets for your workload. Goals provide direction to your
organization on expected outcomes, and targets provide specific measurable outcomes to be achieved
for your workloads.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Develop cost and usage goals and targets for your organization. As a growing organization on AWS,
it is important to set and track goals for cost optimization. These goals or key performance indicators
(KPIs) can include things like percent of spend on-demand, or adoption of certain optimized services
such as AWS Graviton instances or gp3 EBS volume types. Setting measurable and achievable goals
can help you to continue to measure efficiency improvements which is important to ongoing business
operations. Goals provide guidance and direction to your organization on expected outcomes. Targets
provide specific measurable outcomes to be achieved. In short, a goal is the direction you want to go
and target is how far in that direction and when that goal should be achieved (using guidance of specific,
measurable, assignable, realistic, and timely, or SMART). An example of a goal is that platform usage
should increase significantly, with only a minor (non-linear) increase in cost. An example target is a 20%
increase in platform usage, with less than a five percent increase in costs. Another common goal is that
workloads need to be more efficient every six months. The accompanying target would be that the cost
per business metrics needs to decrease by five percent every six months.
A goal for cost optimization is to increase workload efficiency, which means decreasing the cost
per business outcome of the workload over time. It is recommended to implement this goal for all
workloads, and also set a target such as a five percent increase in efficiency every six months to a year.
This can be achieved in the cloud through building capability in cost optimization, and releasing new
services and features.
It's important to have near real-time visibility over your KPIs and related savings opportunities and track
your progress over time. To get started with defining and tracking KPI goals, we recommend the KPI
dashboard from the Cloud Intelligence Dashboards (CID) framework. Based on the data from AWS Cost
and Usage Report, the KPI dashboard provides a series of recommended cost optimization KPIs with the
ability to set custom goals and track progress over time.
If you have another solution that allows you to set and track KPI goals, make sure it's adopted by all
cloud financial management stakeholders in your organization.
Implementation steps
• Define expected usage levels: To begin, focus on usage levels. Engage with the application owners,
marketing, and greater business teams to understand what the expected usage levels will be for the
workload. How will customer demand change over time, and will there be any changes due to seasonal
increases or marketing campaigns?
• Define workload resourcing and costs: With usage levels defined, quantify the changes in workload
resources required to meet these usage levels. You may need to increase the size or number of
472

AWS Well-Architected Framework
Expenditure and usage awareness
resources for a workload component, increase data transfer, or change workload components to a
different service at a specific level. Specify what the costs will be at each of these major points, and
what the changes in cost will be when there are changes in usage.
• Define business goals: Taking the output from the expected changes in usage and cost, combine this
with expected changes in technology, or any programs that you are running, and develop goals for
the workload. Goals must address usage, cost and the relationship between the two. Goals must be
simple, high level, and help people understand what the business expects in terms of outcomes (such
as making sure unused resources are kept below certain cost level). You don't need to define goals for
each unused resource type or define costs that cause losses for goals and targets. Verify that there
are organizational programs (for example, capability building like training and education) if there are
expected changes in cost without changes in usage.
• Define targets: For each of the defined goals specify a measurable target. If the goal is to increase
efficiency in the workload, the target will quantify the amount of improvement (typically in business
outputs for each dollar spent) and when it will be delivered. For example, if you set a goal of
minimizing waste due to over-provisioning, then your target can be waste due to compute over-
provisioning in the first tier of production workloads should not exceed 10% of tier compute cost, and
waste due to compute over-provisioning in the second tier of production workloads should not exceed
5% of tier compute cost.
Resources
Related documents:
• AWS managed policies for job functions
• AWS multi-account strategy for your AWS Control Tower landing zone
• Control access to AWS Regions using IAM policies
• SMART Goals
Related videos:
• Well-Architected Labs: Goals and Targets (Level 100)
Related examples:
• Well-Architected Labs: Decommission resources (Goals and Targets)
• Well-Architected Labs: Resource Type, Size, and Number (Goals and Targets)
COST02-BP03 Implement an account structure
Implement a structure of accounts that maps to your organization. This assists in allocating and
managing costs throughout your organization.
Level of risk exposed if this best practice is not established: High
Implementation guidance
AWS Organizations allows you to create multiple AWS accounts which can help you centrally govern
your environment as you scale your workloads on AWS. You can model your organizational hierarchy
by grouping AWS accounts in organizational unit (OU) structure and creating multiple AWS accounts
under each OU. To create an account structure, you need to decide which of your AWS accounts will be
the management account first. After that, you can create new AWS accounts or select existing accounts
as member accounts based on your designed account structure by following management account best
practices and member account best practices.
473

AWS Well-Architected Framework
Expenditure and usage awareness
It is advised to always have at least one management account with one member account linked to it,
regardless of your organization size or usage. All workload resources should reside only within member
accounts and no resource should be created in management account. There is no one size fits all answer
for how many AWS accounts you should have. Assess your current and future operational and cost
models to ensure that the structure of your AWS accounts reflects your organization’s goals. Some
companies create multiple AWS accounts for business reasons, for example:
• Administrative or fiscal and billing isolation is required between organization units, cost centers, or
specific workloads.
• AWS service limits are set to be specific to particular workloads.
• There is a requirement for isolation and separation between workloads and resources.
Within AWS Organizations, consolidated billing creates the construct between one or more member
accounts and the management account. Member accounts allow you to isolate and distinguish your cost
and usage by groups. A common practice is to have separate member accounts for each organization unit
(such as finance, marketing, and sales), or for each environment lifecycle (such as development, testing
and production), or for each workload (workload a, b, and c), and then aggregate these linked accounts
using consolidated billing.
Consolidated billing allows you to consolidate payment for multiple member AWS accounts under a
single management account, while still providing visibility for each linked account’s activity. As costs
and usage are aggregated in the management account, this allows you to maximize your service volume
discounts, and maximize the use of your commitment discounts (Savings Plans and Reserved Instances)
to achieve the highest discounts.
The following diagram shows how you can use AWS Organizations with organizational units (OU) to
group multiple accounts, and place multiple AWS accounts under each OU. It is recommended to use
OUs for various use cases and workloads which provides patterns for organizing accounts.
474

AWS Well-Architected Framework
Expenditure and usage awareness
475

AWS Well-Architected Framework
Expenditure and usage awareness
Example of grouping multiple AWS accounts under organizational units.
AWS Control Tower can quickly set up and configure multiple AWS accounts, ensuring that governance is
aligned with your organization’s requirements.
Implementation steps
• Define separation requirements: Requirements for separation are a combination of multiple factors,
including security, reliability, and financial constructs. Work through each factor in order and specify
whether the workload or workload environment should be separate from other workloads. Security
promotes adhesion to access and data requirements. Reliability manages limits so that environments
and workloads do not impact others. Review the security and reliability pillars of the Well-Architected
Framework periodically and follow the provided best practices. Financial constructs create strict
financial separation (different cost center, workload ownerships and accountability). Common
examples of separation are production and test workloads being run in separate accounts, or using a
separate account so that the invoice and billing data can be provided to the individual business units or
departments in the organization or stakeholder who owns the account.
• Define grouping requirements: Requirements for grouping do not override the separation
requirements, but are used to assist management. Group together similar environments or workloads
that do not require separation. An example of this is grouping multiple test or development
environments from one or more workloads together.
• Define account structure: Using these separations and groupings, specify an account for each group
and maintain separation requirements. These accounts are your member or linked accounts. By
grouping these member accounts under a single management or payer account, you combine usage,
which allows for greater volume discounts across all accounts, which provides a single bill for all
accounts. It's possible to separate billing data and provide each member account with an individual
view of their billing data. If a member account must not have its usage or billing data visible to
any other account, or if a separate bill from AWS is required, define multiple management or payer
accounts. In this case, each member account has its own management or payer account. Resources
should always be placed in member or linked accounts. The management or payer accounts should
only be used for management.
Resources
Related documents:
• Using Cost Allocation Tags
• AWS managed policies for job functions
• AWS multiple account billing strategy
• Control access to AWS Regions using IAM policies
• AWS Control Tower
• AWS Organizations
• Best practices for management accounts and member accounts
• Organizing Your AWS Environment Using Multiple Accounts
• Turning on shared reserved instances and Savings Plans discounts
• Consolidated billing
• Consolidated billing
Related examples:
• Splitting the CUR and Sharing Access
Related videos:
476

AWS Well-Architected Framework
Expenditure and usage awareness
• Introducing AWS Organizations
• Set Up a Multi-Account AWS Environment that Uses Best Practices for AWS Organizations
Related examples:
• Well-Architected Labs: Create an AWS Organization (Level 100)
• Splitting the AWS Cost and Usage Report and Sharing Access
• Defining an AWS Multi-Account Strategy for telecommunications companies
• Best Practices for Optimizing AWS accounts
• Best Practices for Organizational Units with AWS Organizations
COST02-BP04 Implement groups and roles
Implement groups and roles that align to your policies and control who can create, modify, or
decommission instances and resources in each group. For example, implement development, test, and
production groups. This applies to AWS services and third-party solutions.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
After you develop policies, you can create logical groups and roles of users within your organization.
This allows you to assign permissions and control usage. Begin with high-level groupings of people.
Typically this aligns with organizational units and job roles (for example, systems administrator in the IT
Department, or financial controller). The groups join people that do similar tasks and need similar access.
Roles define what a group must do. For example, a systems administrator in IT requires access to create
all resources, but an analytics team member only needs to create analytics resources.
Implementation steps
• Implement groups: Using the groups of users defined in your organizational policies, implement the
corresponding groups, if necessary. Refer to the security pillar for best practices on users, groups, and
authentication.
• Implement roles and policies: Using the actions defined in your organizational policies, create the
required roles and access policies. Refer to the security pillar for best practices on roles and policies.
Resources
Related documents:
• AWS managed policies for job functions
• AWS multiple account billing strategy
• Control access to AWS Regions using IAM policies
• Well-Architected Security Pillar
Related examples:
• Well-Architected Lab Basic Identity and Access
COST02-BP05 Implement cost controls
Implement controls based on organization policies and defined groups and roles. These certify that costs
are only incurred as defined by organization requirements such as control access to regions or resource
types.
477

AWS Well-Architected Framework
Expenditure and usage awareness
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
A common first step in implementing cost controls is to set up notifications when cost or usage events
occur outside of policies. You can act quickly and verify if corrective action is required without restricting
or negatively impacting workloads or new activity. After you know the workload and environment limits,
you can enforce governance. AWS Budgets allows you to set notifications and define monthly budgets
for your AWS costs, usage, and commitment discounts (Savings Plans and Reserved Instances). You can
create budgets at an aggregate cost level (for example, all costs), or at a more granular level where you
include only specific dimensions such as linked accounts, services, tags, or Availability Zones.
Once you set up your budget limits with AWS Budgets, use AWS Cost Anomaly Detection to reduce your
unexpected cost. AWS Cost Anomaly Detection is a cost management services that uses machine learning
to continually monitor your cost and usage to detect unusual spends. It helps you identify anomalous
spend and root causes, so you can quickly take action. First, create a cost monitor in AWS Cost Anomaly
Detection, then choose your alerting preference by setting up a dollar threshold (such as an alert on
anomalies with impact greater than $1,000). Once you receive alerts, you can analyze the root cause
behind the anomaly and impact on your costs. You can also monitor and perform your own anomaly
analysis in AWS Cost Explorer.
Enforce governance policies in AWS through AWS Identity and Access Management and AWS
Organizations Service Control Policies (SCP). IAM allows you to securely manage access to AWS services
and resources. Using IAM, you can control who can create or manage AWS resources, the type of
resources that can be created, and where they can be created. This minimizes the possibility of resources
being created outside of the defined policy. Use the roles and groups created previously and assign IAM
policies to enforce the correct usage. SCP offers central control over the maximum available permissions
for all accounts in your organization, keeping your accounts stay within your access control guidelines.
SCPs are available only in an organization that has all features turned on, and you can configure the SCPs
to either deny or allow actions for member accounts by default. For more details on implementing access
management, see the Well-Architected Security Pillar whitepaper.
Governance can also be implemented through management of AWS service quotas. By ensuring
service quotas are set with minimum overhead and accurately maintained, you can minimize resource
creation outside of your organization’s requirements. To achieve this, you must understand how quickly
your requirements can change, understand projects in progress (both creation and decommission of
resources), and factor in how fast quota changes can be implemented. Service quotas can be used to
increase your quotas when required.
Implementation steps
• Implement notifications on spend: Using your defined organization policies, create AWS Budgets
to notify you when spending is outside of your policies. Configure multiple cost budgets, one for
each account, which notify you about overall account spending. Configure additional cost budgets
within each account for smaller units within the account. These units vary depending on your account
structure. Some common examples are AWS Regions, workloads (using tags), or AWS services.
Configure an email distribution list as the recipient for notifications, and not an individual's email
account. You can configure an actual budget for when an amount is exceeded, or use a forecasted
budget for notifying on forecasted usage. You can also preconfigure AWS Budget Actions that can
enforce specific IAM or SCP policies, or stop target Amazon EC2 or Amazon RDS instances. Budget
Actions can be started automatically or require workflow approval.
• Implement notifications on anomalous spend: Use AWS Cost Anomaly Detection to reduce your
surprise costs in your organization and analyze root cause of potential anomalous spend. Once you
create cost monitor to identify unusual spend at your specified granularity and configure notifications
in AWS Cost Anomaly Detection, it sends you alert when unusual spend is detected. This will allow
you to analyze root case behind the anomaly and understand the impact on your cost. Use AWS Cost
Categories while configuring AWS Cost Anomaly Detection to identify which project team or business
unit team can analyze the root cause of the unexpected cost and take timely necessary actions.
478

AWS Well-Architected Framework
Expenditure and usage awareness
• Implement controls on usage: Using your defined organization policies, implement IAM policies
and roles to specify which actions users can perform and which actions they cannot. Multiple
organizational policies may be included in an AWS policy. In the same way that you defined policies,
start broadly and then apply more granular controls at each step. Service limits are also an effective
control on usage. Implement the correct service limits on all your accounts.
Resources
Related documents:
• AWS managed policies for job functions
• AWS multiple account billing strategy
• Control access to AWS Regions using IAM policies
• AWS Budgets
• AWS Cost Anomaly Detection
• Control Your AWS Costs
Related videos:
• How can I use AWS Budgets to track my spending and usage
Related examples:
• Example IAM access management policies
• Example service control policies
• AWS Budgets Actions
• Create IAM Policy to control access to Amazon EC2 resources using Tags
• Restrict the access of IAM Identity to specific Amazon EC2 resources
• Create an IAM Policy to restrict Amazon EC2 usage by family
• Well-Architected Labs: Cost and Usage Governance (Level 100)
• Well-Architected Labs: Cost and Usage Governance (Level 200)
• Slack integrations for Cost Anomaly Detection using AWS Chatbot
COST02-BP06 Track project lifecycle
Track, measure, and audit the lifecycle of projects, teams, and environments to avoid using and paying
for unnecessary resources.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Ensure that you track the entire lifecycle of the workload. This ensures that when workloads or workload
components are no longer required, they can be decommissioned or modified. This is especially useful
when you release new services or features. The existing workloads and components may appear to be in
use, but should be decommissioned to redirect customers to the new service. Notice previous stages of
workloads — after a workload is in production, previous environments can be decommissioned or greatly
reduced in capacity until they are required again.
AWS provides a number of management and governance services you can use for entity lifecycle
tracking. You can use AWS Config or AWS Systems Manager to provide a detailed inventory of your AWS
resources and configuration. It is recommended that you integrate with your existing project or asset
management systems to keep track of active projects and products within your organization. Combining
479

AWS Well-Architected Framework
Expenditure and usage awareness
your current system with the rich set of events and metrics provided by AWS allows you to build a view
of significant lifecycle events and proactively manage resources to reduce unnecessary costs.
Refer to the Well-Architected Operational Excellence Pillar whitepaper for more details on implementing
entity lifecycle tracking.
Implementation steps
• Perform workload reviews: As defined by your organizational policies, audit your existing projects.
The amount of effort spent in the audit should be proportional to the approximate risk, value, or cost
to the organization. Key areas to include in the audit would be risk to the organization of an incident
or outage, value, or contribution to the organization (measured in revenue or brand reputation),
cost of the workload (measured as total cost of resources and operational costs), and usage of the
workload (measured in number of organization outcomes per unit of time). If these areas change over
the lifecycle, adjustments to the workload are required, such as full or partial decommissioning.
Resources
Related documents:
• AWS Config
• AWS Systems Manager
• AWS managed policies for job functions
• AWS multiple account billing strategy
• Control access to AWS Regions using IAM policies
COST 3. How do you monitor usage and cost?
Establish policies and procedures to monitor and appropriately allocate your costs. This permits you to
measure and improve the cost efficiency of this workload.
Best practices
• COST03-BP01 Configure detailed information sources (p. 480)
• COST03-BP02 Add organization information to cost and usage (p. 481)
• COST03-BP03 Identify cost attribution categories (p. 483)
• COST03-BP04 Establish organization metrics (p. 485)
• COST03-BP05 Configure billing and cost management tools (p. 486)
• COST03-BP06 Allocate costs based on workload metrics (p. 488)
COST03-BP01 Configure detailed information sources
Configure the AWS Cost and Usage Report, and Cost Explorer hourly granularity, to provide detailed
cost and usage information. Configure your workload to have log entries for every delivered business
outcome.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Enable hourly granularity in AWS Cost Explorer and create a AWS Cost and Usage Report (CUR). These
data sources provide the most accurate view of cost and usage across your entire organization. The CUR
provides daily or hourly usage granularity, rates, costs, and usage attributes for all chargeable AWS
services. All possible dimensions are in the CUR including: tagging, location, resource attributes, and
account IDs.
480

AWS Well-Architected Framework
Expenditure and usage awareness
Configure your CUR with the following customizations:
• Include resource IDs
• Automatically refresh the CUR
• Hourly granularity
• Versioning: Overwrite existing report
• Data integration: Amazon Athena (Parquet format and compression)
Use AWS Glue to prepare the data for analysis, and use Amazon Athena to perform data analysis, using
SQL to query the data. You can also use Amazon QuickSight to build custom and complex visualizations
and distribute them throughout your organization.
Implementation steps
• Configure the cost and usage report: Using the billing console, configure at least one cost and usage
report. Configure a report with hourly granularity that includes all identifiers and resource IDs. You can
also create other reports with different granularities to provide higher-level summary information.
• Configure hourly granularity in Cost Explorer: Using the billing console, turn on Hourly and Resource
Level Data.
Note
There will be associated costs with activating this feature. For details, refer to the pricing.
• Configure application logging: Verify that your application logs each business outcome that it
delivers so it can be tracked and measured. Ensure that the granularity of this data is at least hourly so
it matches with the cost and usage data. Refer to the Well-Architected Operational Excellence Pillar for
more detail on logging and monitoring.
Resources
Related documents:
• AWS Account Setup
• AWS Cost and Usage Report (CUR)
• AWS Glue
• Amazon QuickSight
• AWS Cost Management Pricing
• Tagging AWS resources
• Analyzing your costs with AWS Budgets
• Analyzing your costs with Cost Explorer
• Managing AWS Cost and Usage Reports
• Well-Architected Operational Excellence Pillar
Related examples:
• AWS Account Setup
COST03-BP02 Add organization information to cost and usage
Define a tagging schema based on your organization, workload attributes, and cost allocation categories
so that you can filter and search for resources or monitor cost and usage in cost management tools.
Implement consistent tagging across all resources where possible by purpose, team, environment, or
other criteria relevant to your business.
481

AWS Well-Architected Framework
Expenditure and usage awareness
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Implement tagging in AWS to add organization information to your resources, which will then be added
to your cost and usage information. A tag is a key-value pair — the key is defined and must be unique
across your organization, and the value is unique to a group of resources. An example of a key-value pair
is the key is Environment, with a value of Production. All resources in the production environment
will have this key-value pair. Tagging allows you categorize and track your costs with meaningful,
relevant organization information. You can apply tags that represent organization categories (such
as cost centers, application names, projects, or owners), and identify workloads and characteristics of
workloads (such as test or production) to attribute your costs and usage throughout your organization.
When you apply tags to your AWS resources (such as Amazon Elastic Compute Cloud instances or
Amazon Simple Storage Service buckets) and activate the tags, AWS adds this information to your Cost
and Usage Reports. You can run reports and perform analysis on tagged and untagged resources to allow
greater compliance with internal cost management policies and ensure accurate attribution.
Creating and implementing an AWS tagging standard across your organization’s accounts helps you
manage and govern your AWS environments in a consistent and uniform manner. Use Tag Policies in
AWS Organizations to define rules for how tags can be used on AWS resources in your accounts in AWS
Organizations. Tag Policies allow you to easily adopt a standardized approach for tagging AWS resources
AWS Tag Editor allows you to add, delete, and manage tags of multiple resources. With Tag Editor, you
search for the resources that you want to tag, and then manage tags for the resources in your search
results.
AWS Cost Categories allows you to assign organization meaning to your costs, without requiring tags on
resources. You can map your cost and usage information to unique internal organization structures. You
define category rules to map and categorize costs using billing dimensions, such as accounts and tags.
This provides another level of management capability in addition to tagging. You can also map specific
accounts and tags to multiple projects.
Implementation steps
• Define a tagging schema: Gather all stakeholders from across your business to define a schema. This
typically includes people in technical, financial, and management roles. Define a list of tags that all
resources must have, as well as a list of tags that resources should have. Verify that the tag names and
values are consistent across your organization.
• Tag resources: Using your defined cost attribution categories, place tags on all resources in your
workloads according to the categories. Use tools such as the CLI, Tag Editor, or AWS Systems Manager
to increase efficiency.
• Implement AWS Cost Categories: You can create Cost Categories without implementing tagging. Cost
categories use the existing cost and usage dimensions. Create category rules from your schema and
implement them into cost categories.
• Automate tagging: To verify that you maintain high levels of tagging across all resources, automate
tagging so that resources are automatically tagged when they are created. Use services such as AWS
CloudFormation to verify that resources are tagged when created. You can also create a custom
solution to tag automatically using Lambda functions or use a microservice that scans the workload
periodically and removes any resources that are not tagged, which is ideal for test and development
environments.
• Monitor and report on tagging: To verify that you maintain high levels of tagging across your
organization, report and monitor the tags across your workloads. You can use AWS Cost Explorer to
view the cost of tagged and untagged resources, or use services such as Tag Editor. Regularly review
the number of untagged resources and take action to add tags until you reach the desired level of
tagging.
482

AWS Well-Architected Framework
Expenditure and usage awareness
Resources
Related documents:
• Tagging Best Practices
• AWS CloudFormation Resource Tag
• AWS Cost Categories
• Tagging AWS resources
• Analyzing your costs with AWS Budgets
• Analyzing your costs with Cost Explorer
• Managing AWS Cost and Usage Reports
Related videos:
• How can I tag my AWS resources to divide up my bill by cost center or project
• Tagging AWS Resources
Related examples:
• Automatically tag new AWS resources based on identity or role
COST03-BP03 Identify cost attribution categories
This best practice was updated with new guidance on July 13th, 2023.
Identify organization categories such as business units, departments, or projects that could be used to
allocate cost within your organization to the internal consuming entities so that spend accountability can
be enforced and consumption behaviors can be driven effectively.
Level of risk exposed if this best practice is not established: High
Implementation guidance
The process of categorizing costs is crucial in budgeting, accounting, financial reporting, decision making,
benchmarking, and project management. By classifying and categorizing expenses, teams can gain a
better understanding of the types of costs they will incur throughout their cloud journey, helping teams
make informed decisions and manage budgets effectively.
Cloud spend accountability establishes a strong incentive for disciplined demand and cost management.
The result is significantly greater cloud cost savings for organizations that allocate most of their cloud
spend to consuming business units or teams.
Work with your finance team and other relevant stakeholders to understand the requirements of how
costs must be allocated within your organization. Workload costs must be allocated throughout the
entire lifecycle, including development, testing, production, and decommissioning. Understand how the
costs incurred for learning, staff development, and idea creation are attributed in the organization. This
can be helpful to correctly allocate accounts used for this purpose to training and development budgets,
instead of generic IT cost budgets.
After defining your cost attribution categories with your stakeholders in your organization, use AWS Cost
Categories to group your cost and usage information into meaningful categories in the AWS Cloud such
as cost for specific project or AWS accounts for departments or business units. You can create custom
483

AWS Well-Architected Framework
Expenditure and usage awareness
categories and map your cost and usage information into these categories based on rules you define
using various dimensions such as account, tag, service, charge type, and even other cost categories. Once
cost categories are set up, you will be able to view your cost and usage information by these categories
allowing your organization to make better strategic and purchasing decisions. These categories will be
visible in AWS Cost Explorer, AWS Budgets, and AWS Cost and Usage Report as well.
As an example, the following diagram shows you how can you group your costs and usage information
in your organization such as having multiple teams (cost category) having multiple environments (rules)
and each environment having multiple resources or assets (dimensions).
Cost and usage organization chart.
Implementation steps
• Define your organization categories: Meet with stakeholders to define categories that reflect
your organization's structure and requirements. These will directly map to the structure of existing
financial categories, such as business unit, budget, cost center, or department. Look at the outcomes
the cloud delivers for your business, such as training or education, as these are also organization
categories. Multiple categories can be assigned to a resource, and a resource can be in multiple
different categories, so define as many categories as needed.
• Define your functional categories: Meet with stakeholders to define categories that reflect the
functions that you have within your business. This may be the workload or application names, and
the type of environment, such as production, testing, or development. Multiple categories can be
assigned to a resource, and a resource can be in multiple different categories, so define as many
categories as needed so that you can manage your costs within the categorized structure using AWS
Cost Categories.
• Define AWS Cost Categories: You can create cost categories to organize your cost and usage
information. Use AWS Cost Categories to map your AWS costs and usage into meaningful categories.
With cost categories, you can organize your costs using a rule-based engine. The rules that you
configure organize your costs into categories. Within these rules, you can filter with using multiple
dimensions for each category such as specific AWS accounts, specific AWS services, or specific charge
484

AWS Well-Architected Framework
Expenditure and usage awareness
types. You can then use these categories across multiple products in the AWS Billing and Cost
Management console. This includes AWS Cost Explorer, AWS Budgets, AWS Cost and Usage Report,
and AWS Cost Anomaly Detection. You can create groupings of costs using cost categories as well.
After you create the cost categories (allowing up to 24 hours after creating a cost category for your
usage records to be updated with values), they appear in AWS Cost Explorer, AWS Budgets, AWS
Cost and Usage Report, and AWS Cost Anomaly Detection. For example, create cost categories for
your business units (DevOps Team), and under each category create multiple rules (rules for each sub
category) with multiple dimensions (AWS accounts, cost allocation tags, services or charge type) based
on your defined groupings. In AWS Cost Explorer and AWS Budgets, a cost category appears as an
additional billing dimension. You can use this to filter for the specific cost category value, or group by
the cost category.
Resources
Related documents:
• Tagging AWS resources
• Using Cost Allocation Tags
• Analyzing your costs with AWS Budgets
• Analyzing your costs with Cost Explorer
• Managing AWS Cost and Usage Reports
• AWS Cost Categories
• Managing your costs with AWS Cost Categories
• Creating cost categories
• Tagging cost categories
• Splitting charges within cost categories
• AWS Cost Categories Features
Related examples:
• Organize your cost and usage data with AWS Cost Categories
• Managing your costs with AWS Cost Categories
COST03-BP04 Establish organization metrics
Establish the organization metrics that are required for this workload. Example metrics of a workload are
customer reports produced, or web pages served to customers.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Understand how your workload’s output is measured against business success. Each workload typically
has a small set of major outputs that indicate performance. If you have a complex workload with many
components, then you can prioritize the list, or define and track metrics for each component. Work with
your teams to understand which metrics to use. This unit will be used to understand the efficiency of the
workload, or the cost for each business output.
Implementation steps
• Define workload outcomes: Meet with the stakeholders in the business and define the outcomes for
the workload. These are a primary measure of customer usage and must be business metrics and not
technical metrics. There should be a small number of high-level metrics (less than five) per workload.
485

AWS Well-Architected Framework
Expenditure and usage awareness
If the workload produces multiple outcomes for different use cases, then group them into a single
metric.
• Define workload component outcomes: Optionally, if you have a large and complex workload, or
can easily break your workload into components (such as microservices) with well-defined inputs
and outputs, define metrics for each component. The effort should reflect the value and cost of the
component. Start with the largest components and work towards the smaller components.
Resources
Related documents:
• Tagging AWS resources
• Analyzing your costs with AWS Budgets
• Analyzing your costs with Cost Explorer
• Managing AWS Cost and Usage Reports
COST03-BP05 Configure billing and cost management tools
This best practice was updated with new guidance on July 13th, 2023.
Configure cost management tools in line with your organization policies to manage and optimize cloud
spend. This includes services, tools, and resources to organize and track cost and usage data, enhance
control through consolidated billing and access permission, improve planning through budgeting and
forecasts, receive notifications or alerts, and further lower cost with resources and pricing optimizations.
Level of risk exposed if this best practice is not established: High
Implementation guidance
To establish strong accountability, your account strategy should be considered first as part of your cost
allocation strategy. Get this right, and you may not need to go any further. Otherwise, there will be
unawareness and further pain points.
To encourage accountability of cloud spend, users should have access to tools that provide visibility into
their costs and usage. It is recommended that all workloads and teams have the tools configured for the
following details and purposes:
• Organize: Establish your cost allocation and governance baseline with your own tagging strategy
and taxonomy. Tag supported AWS resources and categorize them meaningfully based on your
organization structure (business units, departments, or projects). Tag account names for specific cost
centers and map them with AWS Cost Categories to group accounts for particular business units for
their cost centers so that business unit owner can see multiple accounts’ consumption in one place.
• Access: Track organization-wide billing information in consolidated billing and verify the right
stakeholders and business owners have access.
• Control: Build effective governance mechanisms with the right guardrails to prevent unexpected
scenarios when using SCP, tag policies, and budget alerts For example, with effective control
mechanism, you can prevent teams from creating resources in unsupported Regions.
• Current State: Configure a dashboard showing current levels of cost and usage. The dashboard
should be available in a highly visible place within the work environment similar to an operations
dashboard. You can use Cloud Intelligence Dashboard (CID) or any other supported products to create
this visibility.
• Notifications: Provide notifications when cost or usage is outside of defined limits and when
anomalies occur with AWS Budgets or AWS Cost Anomaly Detection.
486

AWS Well-Architected Framework
Expenditure and usage awareness
• Reports: Summarize all cost and usage information and raise awareness and accountability of your
cloud spend with detailed, allocable cost data. Reports should be relevant to the team consuming
them and ideally should contain recommendations.
• Tracking: Show the current cost and usage against configured goals or targets.
• Analysis: Allow team members to perform custom and deep analysis down to the hourly granularity,
with all possible dimensions.
• Inspect: Stay up to date with your resource deployment and cost optimization opportunities. Get
notifications (using Amazon CloudWatch, Amazon SNS, or Amazon SES) for resource deployments
at organization level and review cost optimization recommendations (for example, AWS Compute
Optimizer or AWS Trusted Advisor).
• Trending: Display the variability in cost and usage over the required period of time, with the required
granularity.
• Forecasts: Show estimated future costs, estimate your resource usage, and spend with forecast
dashboards that you create.
You can use AWS tools like AWS Cost Explorer, AWS Billing, or AWS Budgets for essentials, or you can
integrate CUR data with Amazon Athena and Amazon QuickSight to provide this capability for more
detailed views. If you don't have essential skills or bandwidth in your organization, you can work with
AWS ProServ, AWS Managed Services (AMS), or AWS Partners and use their tools. You can also use third-
party tools, but verify first that the cost provides value to your organization.
Implementation steps
• Allow team-based access to tools: Configure your accounts and create groups that have access to the
required cost and usage reports for their consumptions and use AWS Identity and Access Management
to control access to the tools such as AWS Cost Explorer. These groups must include representatives
from all teams that own or manage an application. This certifies that every team has access to their
cost and usage information to track their consumption.
• Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set budgets for
the overall account spend, and budgets for the workloads by using tags. Configure notifications in
AWS Budgets to receive alerts for when you exceed your budgeted amounts, or when your estimated
costs exceed your budgets.
• Configure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts to
visualize your cost data for further analysis. Create a dashboard for the workload that tracks overall
spend, key usage metrics for the workload, and forecast of future costs based on your historical cost
data.
• Configure AWS Cost Anomaly Detection: Use AWS Cost Anomaly Detection for your accounts, core
services, or Cost Categories you created to monitor your cost and usage and detect unusual spends.
You can receive alerts individually in aggregated reports, and receive alerts in an email or an Amazon
Simple Notification Service topic which allows you to analyze and determine the root cause of the
anomaly, and identify the factor that is driving the cost increase.
• Configure advanced tools: Optionally, you can create custom tools for your organization that provide
additional detail and granularity. You can implement advanced analysis capability using Amazon
Athena, and dashboards using Amazon QuickSight. Consider using Cloud Intelligence Dashboards
(CID) for pre-configured, advanced dashboards. There are also AWS Partners you can work with and
adopt their cloud management solutions to activate cloud bill monitoring and optimization in one
convenient location.
Resources
Related documents:
• AWS Cost Management
• Tagging AWS resources
487

AWS Well-Architected Framework
Expenditure and usage awareness
• Analyzing your costs with AWS Budgets
• Analyzing your costs with Cost Explorer
• Managing AWS Cost and Usage Reports
• AWS Cost Categories
• Cloud Financial Management with AWS
• AWS APN Partners - Cost Management
Related videos:
• Deploying Cloud Intelligence Dashboards
• Get Alerts on any FinOps or Cost Optimization Metric or KPI
Related examples:
• Well-Architected Labs - AWS Account Setup
• Well-Architected Labs: Billing Visualization
• Well-Architected Labs: Cost and Governance Usage
• Well-Architected Labs: Cost and Usage Analysis
• Well-Architected Labs: Cost and Usage Visualization
• Well-Architected Labs: Cloud Intelligence Dashboards
COST03-BP06 Allocate costs based on workload metrics
This best practice was updated with new guidance on July 13th, 2023.
Allocate the workload's costs by usage metrics or business outcomes to measure workload cost efficiency.
Implement a process to analyze the cost and usage data with analytics services, which can provide
insight and charge back capability.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Cost optimization is delivering business outcomes at the lowest price point, which can only be achieved
by allocating workload costs by workload metrics (measured by workload efficiency). Monitor the
defined workload metrics through log files or other application monitoring. Combine this data with
the workload costs, which can be obtained by looking at costs with a specific tag value or account ID.
It is recommended to perform this analysis at the hourly level. Your efficiency will typically change if
you have some static cost components (for example, a backend database running permanently) with a
varying request rate (for example, usage peaks at nine in the morning to five in the evening, with few
requests at night). Understanding the relationship between the static and variable costs will help you to
focus your optimization activities.
Creating workload metrics for shared resources may be challenging compared to resources like
containerized applications on Amazon Elastic Container Service (Amazon ECS) and Amazon API
Gateway. However, there are certain ways you can categorize usage and track cost. If you need to track
Amazon ECS and AWS Batch shared resources, you can enable split cost allocation data in AWS Cost
Explorer. With split cost allocation data, you can understand and optimize the cost and usage of your
containerized applications and allocate application costs back to individual business entities based on
how shared compute and memory resources are consumed. If you have shared API Gateway and AWS
Lambda function usage, then you can use AWS Application Cost Profiler to categorize their consumption
based on their Tenant ID or Customer ID.
488

AWS Well-Architected Framework
Expenditure and usage awareness
Implementation steps
• Allocate costs to workload metrics: Using the defined metrics and configured tags, create a metric
that combines the workload output and workload cost. Use analytics services such as Amazon
Athena and Amazon QuickSight to create an efficiency dashboard for the overall workload and any
components.
Resources
Related documents:
• Tagging AWS resources
• Analyzing your costs with AWS Budgets
• Analyzing your costs with Cost Explorer
• Managing AWS Cost and Usage Reports
Related examples:
• Improve cost visibility of Amazon ECS and AWS Batch with AWS Split Cost Allocation Data
COST 4. How do you decommission resources?
Implement change control and resource management from project inception to end-of-life. This ensures
you shut down or terminates unused resources to reduce waste.
Best practices
• COST04-BP01 Track resources over their lifetime (p. 489)
• COST04-BP02 Implement a decommissioning process (p. 490)
• COST04-BP03 Decommission resources (p. 492)
• COST04-BP04 Decommission resources automatically (p. 493)
• COST04-BP05 Enforce data retention policies (p. 494)
COST04-BP01 Track resources over their lifetime
Define and implement a method to track resources and their associations with systems over their
lifetime. You can use tagging to identify the workload or function of the resource.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Decommission workload resources that are no longer required. A common example is resources used for
testing: after testing has been completed, the resources can be removed. Tracking resources with tags
(and running reports on those tags) can help you identify assets for decommission, as they will not be
in use or the license on them will expire. Using tags is an effective way to track resources, by labeling
the resource with its function, or a known date when it can be decommissioned. Reporting can then be
run on these tags. Example values for feature tagging are feature-X testing to identify the purpose
of the resource in terms of the workload lifecycle. Another example is using LifeSpan or TTL for the
resources, such as to-be-deleted tag key name and value to define the time period or specific time for
decommissioning.
Implementation steps
489

AWS Well-Architected Framework
Expenditure and usage awareness
• Implement a tagging scheme: Implement a tagging scheme that identifies the workload the resource
belongs to, verifying that all resources within the workload are tagged accordingly. Tagging helps you
categorize resources by purpose, team, environment, or other criteria relevant to your business. For
more detail on tagging uses cases, strategies, and techniques, see AWS Tagging Best Practices.
• Implement workload throughput or output monitoring: Implement workload throughput
monitoring or alarming, initiating on either input requests or output completions. Configure it to
provide notifications when workload requests or outputs drop to zero, indicating the workload
resources are no longer used. Incorporate a time factor if the workload periodically drops to zero under
normal conditions. For more detail on unused or underutilized resources, see AWS Trusted Advisor Cost
Optimization checks.
• Group AWS resources: Create groups for AWS resources. You can use AWS Resource Groups to
organize and manage your AWS resources that are in the same AWS Region. You can add tags to most
of your resources to help identify and sort your resources within your organization. Use Tag Editor
add tags to supported resources in bulk. Consider using AWS Service Catalog to create, manage, and
distribute portfolios of approved products to end users and manage the product lifecycle.
Resources
Related documents:
• AWS Auto Scaling
• AWS Trusted Advisor
• AWS Trusted Advisor Cost Optimization Checks
• Tagging AWS resources
• Publishing Custom Metrics
Related videos:
• How to optimize costs using AWS Trusted Advisor
Related examples:
• Organize AWS resources
• Optimize cost using AWS Trusted Advisor
COST04-BP02 Implement a decommissioning process
Implement a process to identify and decommission unused resources.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Implement a standardized process across your organization to identify and remove unused resources.
The process should define the frequency searches are performed and the processes to remove the
resource to verify that all organization requirements are met.
Implementation steps
• Create and implement a decommissioning process: Work with the workload developers and owners
to build a decommissioning process for the workload and its resources. The process should cover
the method to verify if the workload is in use, and also if each of the workload resources are in use.
Detail the steps necessary to decommission the resource, removing them from service while ensuring
compliance with any regulatory requirements. Any associated resources should be included, such as
490

AWS Well-Architected Framework
Expenditure and usage awareness
licenses or attached storage. Notify the workload owners that the decommissioning process has been
started.
Use the following decommission steps to guide you on what should be checked as part of your
process:
• Identify resources to be decommissioned: Identify resources that are eligible for decommissioning
in your AWS Cloud. Record all necessary information and schedule the decommission. In your
timeline, be sure to account for if (and when) unexpected issues arise during the process.
• Coordinate and communicate: Work with workload owners to confirm the resource to be
decommissioned
• Record metadata and create backups: Record metadata (such as public IPs, Region, AZ, VPC,
Subnet, and Security Groups) and create backups (such as Amazon Elastic Block Store snapshots or
taking AMI, keys export, and Certificate export) if it is required for the resources in the production
environment or if they are critical resources.
• Validate infrastructure-as-code: Determine whether resources were deployed with AWS
CloudFormation, Terraform, AWS Cloud Development Kit (AWS CDK), or any other infrastructure-as-
code deployment tool so they can be re-deployed if necessary.
• Prevent access: Apply restrictive controls for a period of time, to prevent the use of resources while
you determine if the resource is required. Verify that the resource environment can be reverted to its
original state if required.
• Follow your internal decommissioning process: Follow the administrative tasks and
decommissioning process of your organization, like removing the resource from your organization
domain, removing the DNS record, and removing the resource from your configuration management
tool, monitoring tool, automation tool and security tools.
If the resource is an Amazon EC2 instance, consult the following list. For more detail, see How do I
delete or terminate my Amazon EC2 resources?
• Stop or terminate all your Amazon EC2 instances and load balancers. Amazon EC2 instances are
visible in the console for a short time after they're terminated. You aren't billed for any instances
that aren't in the running state
• Delete your Auto Scaling infrastructure.
• Release all Dedicated Hosts.
• Delete all Amazon EBS volumes and Amazon EBS snapshots.
• Release all Elastic IP addresses.
• Deregister all Amazon Machine Images (AMIs).
• Terminate all AWS Elastic Beanstalk environments.
If the resource is an object in Amazon S3 Glacier storage and if you delete an archive before meeting
the minimum storage duration, you will be charged a prorated early deletion fee. Amazon S3 Glacier
minimum storage duration depends on the storage class used. For a summary of minimum storage
duration for each storage class, see Performance across the Amazon S3 storage classes. For detail on
how early deletion fees are calculated, see Amazon S3 pricing.
The following simple decommissioning process flowchart outlines the decommissioning steps. Before
decommissioning resources, verify that resources you have identified for decommissioning are not being
used by the organization.
491

AWS Well-Architected Framework
Expenditure and usage awareness
Resource decommissioning flow.
Resources
Related documents:
• AWS Auto Scaling
• AWS Trusted Advisor
• AWS CloudTrail
Related videos:
• Delete CloudFormation stack but retain some resources
• Find out which user launched Amazon EC2 instance
Related examples:
• Delete or terminate Amazon EC2 resources
• Find out which user launched an Amazon EC2 instance
COST04-BP03 Decommission resources
Decommission resources initiated by events such as periodic audits, or changes in usage.
Decommissioning is typically performed periodically and can be manual or automated.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
The frequency and effort to search for unused resources should reflect the potential savings, so an
account with a small cost should be analyzed less frequently than an account with larger costs. Searches
492

AWS Well-Architected Framework
Expenditure and usage awareness
and decommission events can be initiated by state changes in the workload, such as a product going end
of life or being replaced. Searches and decommission events may also be initiated by external events,
such as changes in market conditions or product termination.
Implementation steps
• Decommission resources: This is the depreciation stage of AWS resources that are no longer needed
or ending of a licensing agreement. Complete all final checks completed before moving to the disposal
stage and decommissioning resources to prevent any unwanted disruptions like taking snapshots or
backups. Using the decommissioning process, decommission each of the resources that have been
identified as unused.
Resources
Related documents:
• AWS Auto Scaling
• AWS Trusted Advisor
Related examples:
• Well-Architected Labs: Decommission resources (Level 100)
COST04-BP04 Decommission resources automatically
Design your workload to gracefully handle resource termination as you identify and decommission non-
critical resources, resources that are not required, or resources with low utilization.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Use automation to reduce or remove the associated costs of the decommissioning process. Designing
your workload to perform automated decommissioning will reduce the overall workload costs
during its lifetime. You can use Amazon EC2 Auto Scaling or Application Auto Scaling to perform the
decommissioning process. You can also implement custom code using the API or SDK to decommission
workload resources automatically.
Modern applications are built serverless-first, a strategy that prioritizes the adoption of serverless
services. AWS developed serverless services for all three layers of your stack: compute, integration, and
data stores. Using serverless architecture will allow you to save costs during low-traffic periods with
scaling up and down automatically.
Implementation steps
• Implement Amazon EC2 Auto Scaling or Application Auto Scaling: For resources that are supported,
configure them with Amazon EC2 Auto Scaling or Application Auto Scaling. These services can help
you optimize your utilization and cost efficiencies when consuming AWS services. When demand drops,
these services will automatically remove any excess resource capacity so you avoid overspending.
• Configure CloudWatch to terminate instances: Instances can be configured to terminate using
CloudWatch alarms. Using the metrics from the decommissioning process, implement an alarm with an
Amazon Elastic Compute Cloud action. Verify the operation in a non-production environment before
rolling out.
• Implement code within the workload: You can use the AWS SDK or AWS CLI to decommission
workload resources. Implement code within the application that integrates with AWS and terminates
or removes resources that are no longer used.
493

AWS Well-Architected Framework
Expenditure and usage awareness
• Use serverless services: Prioritize building serverless architectures and event-driven architecture
on AWS to build and run your applications. AWS offers multiple serverless technology services that
inherently provide automatically optimized resource utilization and automated decommissioning
(scale in and scale out). With serverless applications, resource utilization is automatically optimized
and you never pay for over-provisioning.
Resources
Related documents:
• Amazon EC2 Auto Scaling
• Getting Started with Amazon EC2 Auto Scaling
• Application Auto Scaling
• AWS Trusted Advisor
• Serverless on AWS
• Create Alarms to Stop, Terminate, Reboot, or Recover an Instance
• Adding terminate actions to Amazon CloudWatch alarms
Related examples:
• Scheduling automatic deletion of AWS CloudFormation stacks
• Well-Architected Labs - Decommission resources automatically (Level 100)
• Servian AWS Auto Cleanup
COST04-BP05 Enforce data retention policies
Define data retention policies on supported resources to handle object deletion per your organizations’
requirements. Identify and delete unnecessary or orphaned resources and objects that are no longer
required.
Level of risk exposed if this best practice is not established: Medium
Use data retention policies and lifecycle policies to reduce the associated costs of the decommissioning
process and storage costs for the identified resources. Defining your data retention policies and lifecycle
policies to perform automated storage class migration and deletion will reduce the overall storage costs
during its lifetime. You can use Amazon Data Lifecycle Manager to automate the creation and deletion of
Amazon Elastic Block Store snapshots and Amazon EBS-backed Amazon Machine Images (AMIs), and use
Amazon S3 Intelligent-Tiering or an Amazon S3 lifecycle configuration to manage the lifecycle of your
Amazon S3 objects. You can also implement custom code using the API or SDK to create lifecycle policies
and policy rules for objects to be deleted automatically.
Implementation steps
• Use Amazon Data Lifecycle Manager: Use lifecycle policies on Amazon Data Lifecycle Manager to
automate deletion of Amazon EBS snapshots and Amazon EBS-backed AMIs.
• Set up lifecycle configuration on a bucket: Use Amazon S3 lifecycle configuration on a bucket to
define actions for Amazon S3 to take during an object's lifecycle, as well as deletion at the end of the
object's lifecycle, based on your business requirements.
Resources
Related documents:
• AWS Trusted Advisor
494

AWS Well-Architected Framework
Cost-effective resources
• Amazon Data Lifecycle Manager
• How to set lifecycle configuration on Amazon S3 bucket
Related videos:
• Automate Amazon EBS Snapshots with Amazon Data Lifecycle Manager
• Empty an Amazon S3 bucket using a lifecycle configuration rule
Related examples:
• Empty an Amazon S3 bucket using a lifecycle configuration rule
• Well-Architected Lab: Decommission resources automatically (Level 100)
Cost-effective resources
Questions
• COST 5. How do you evaluate cost when you select services? (p. 495)
• COST 6. How do you meet cost targets when you select resource type, size and number?  (p. 501)
• COST 7. How do you use pricing models to reduce cost? (p. 505)
• COST 8. How do you plan for data transfer charges? (p. 512)
COST 5. How do you evaluate cost when you select services?
Amazon EC2, Amazon EBS, and Amazon S3 are building-block AWS services. Managed services, such
as Amazon RDS and Amazon DynamoDB, are higher level, or application level, AWS services. By
selecting the appropriate building blocks and managed services, you can optimize this workload for
cost. For example, using managed services, you can reduce or remove much of your administrative and
operational overhead, freeing you to work on applications and business-related activities.
Best practices
• COST05-BP01 Identify organization requirements for cost (p. 495)
• COST05-BP02 Analyze all components of the workload (p. 496)
• COST05-BP03 Perform a thorough analysis of each component (p. 497)
• COST05-BP04 Select software with cost-effective licensing (p. 498)
• COST05-BP05 Select components of this workload to optimize cost in line with organization
priorities (p. 499)
• COST05-BP06 Perform cost analysis for different usage over time (p. 500)
COST05-BP01 Identify organization requirements for cost
Work with team members to define the balance between cost optimization and other pillars, such as
performance and reliability, for this workload.
Level of risk exposed if this best practice is not established: High
Implementation guidance
When selecting services for your workload, it is key that you understand your organization priorities.
Ensure that you have a balance between cost and other Well-Architected pillars, such as performance
495

AWS Well-Architected Framework
Cost-effective resources
and reliability. A fully cost-optimized workload is the solution that is most aligned to your organization’s
requirements, not necessarily the lowest cost. Meet with all teams within your organization to collect
information, such as product, business, technical, and finance.
Implementation steps
• Identify organization requirements for cost: Meet with team members from your organization,
including those in product management, application owners, development and operational teams,
management, and financial roles. Prioritize the Well-Architected pillars for this workload and its
components, the output is a list of the pillars in order. You can also add a weighting to each, which can
indicate how much additional focus a pillar has, or how similar the focus is between two pillars.
Resources
Related documents:
• AWS Total Cost of Ownership (TCO) Calculator
• Amazon S3 storage classes
• Cloud products
COST05-BP02 Analyze all components of the workload
Verify every workload component is analyzed, regardless of current size or current costs. The review
effort should reflect the potential benefit, such as current and projected costs.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Perform a thorough analysis on all components in your workload. Ensure that balance between the cost
of analysis and the potential savings in the workload over its lifecycle. You must find the current impact,
and potential future impact, of the component. For example, if the cost of the proposed resource is $10
a month, and under forecasted loads would not exceed $15 a month, spending a day of effort to reduce
costs by 50% ($5 a month) could exceed the potential benefit over the life of the system. Using a faster
and more efficient data-based estimation will create the best overall outcome for this component.
Workloads can change over time, and the right set of services may not be optimal if the workload
architecture or usage changes. Analysis for selection of services must incorporate current and future
workload states and usage levels. Implementing a service for future workload state or usage may reduce
overall costs by reducing or removing the effort required to make future changes.
AWS Cost Explorer and the AWS Cost and Usage Report (CUR) can analyze the cost of a Proof of Concept
(PoC) or running environment. You can also use AWS Pricing Calculator to estimate workload costs.
Implementation steps
• List the workload components: Build the list of all the workload components. This is used as
verification to check that each component was analyzed. The effort spent should reflect the criticality
to the workload as defined by your organization’s priorities. Grouping together resources functionally
improves efficiency, for example production database storage, if there are multiple databases.
• Prioritize component list: Take the component list and prioritize it in order of effort. This is typically
in order of the cost of the component from most expensive to least expensive, or the criticality as
defined by your organization’s priorities.
• Perform the analysis: For each component on the list, review the options and services available and
chose the option that aligns best with your organizational priorities.
496

AWS Well-Architected Framework
Cost-effective resources
Resources
Related documents:
• AWS Pricing Calculator
• AWS Cost Explorer
• Amazon S3 storage classes
• Cloud products
COST05-BP03 Perform a thorough analysis of each component
Look at overall cost to the organization of each component. Calculate the total cost of ownership by
factoring in cost of operations and management, especially when using managed services by cloud
provider. The review effort should reflect potential benefit (for example, time spent analyzing is
proportional to component cost).
Level of risk exposed if this best practice is not established: High
Implementation guidance
Consider the time savings that will allow your team to focus on retiring technical debt, innovation,
value-adding features and building what differentiates the business. For example, you might need to
lift and shift (also known as rehost) your databases from your on-premises environment to the cloud
as rapidly as possible and optimize later. It is worth exploring the possible savings attained by using
managed services on AWS that may remove or reduce license costs. Managed services on AWS remove
the operational and administrative burden of maintaining a service, such as patching or upgrading the
OS, and allow you to focus on innovation and business.
Since managed services operate at cloud scale, they can offer a lower cost per transaction or service.
You can make potential optimizations in order to achieve some tangible benefit, without changing the
core architecture of the application. For example, you may be looking to reduce the amount of time
you spend managing database instances by migrating to a database-as-a-service platform like Amazon
Relational Database Service (Amazon RDS) or migrating your application to a fully managed platform
like AWS Elastic Beanstalk.
Usually, managed services have attributes that you can set to ensure sufficient capacity. You must
set and monitor these attributes so that your excess capacity is kept to a minimum and performance
is maximized. You can modify the attributes of AWS Managed Services using the AWS Management
Console or AWS APIs and SDKs to align resource needs with changing demand. For example, you can
increase or decrease the number of nodes on an Amazon EMR cluster (or an Amazon Redshift cluster) to
scale out or in.
You can also pack multiple instances on an AWS resource to activate higher density usage. For example,
you can provision multiple small databases on a single Amazon Relational Database Service (Amazon
RDS) database instance. As usage grows, you can migrate one of the databases to a dedicated Amazon
RDS database instance using a snapshot and restore process.
When provisioning workloads on managed services, you must understand the requirements of adjusting
the service capacity. These requirements are typically time, effort, and any impact to normal workload
operation. The provisioned resource must allow time for any changes to occur, provision the required
overhead to allow this. The ongoing effort required to modify services can be reduced to virtually zero by
using APIs and SDKs that are integrated with system and monitoring tools, such as Amazon CloudWatch.
Amazon RDS, Amazon Redshift, and Amazon ElastiCache provide a managed database service. Amazon
Athena, Amazon EMR, and Amazon OpenSearch Service provide a managed analytics service.
AMS is a service that operates AWS infrastructure on behalf of enterprise customers and partners.
It provides a secure and compliant environment that you can deploy your workloads onto. AMS
497

AWS Well-Architected Framework
Cost-effective resources
uses enterprise cloud operating models with automation to allow you to meet your organization
requirements, move into the cloud faster, and reduce your on-going management costs.
Implementation steps
• Perform a thorough analysis: Using the component list, work through each component from the
highest priority to the lowest priority. For the higher priority and more costly components, perform
additional analysis and assess all available options and their long term impact. For lower priority
components, assess if changes in usage would change the priority of the component, and then
perform an analysis of appropriate effort.
• Compare managed and unmanaged resources: Consider the operational cost for the resources you
manage and compare them with AWS managed resources. For example, review your databases running
on Amazon EC2 instances and compare with Amazon RDS options (an AWS managed service) or
Amazon EMR compared to running Apache Spark on Amazon EC2. When moving from a self-managed
workload to a AWS fully managed workload, research your options carefully. The three most important
factors to consider are the type of managed service you want to use, the process you will use to
migrate your data and understand the AWS shared responsibility model.
Resources
Related documents:
• AWS Total Cost of Ownership (TCO) Calculator
• Amazon S3 storage classes
• AWS Cloud products
• AWS Shared Responsibility Model
Related videos:
• Why move to a managed database?
• What is Amazon EMR and how can I use it for processing data?
Related examples:
• Why to move to a managed database
• Consolidate data from identical SQL Server databases into a single Amazon RDS for SQL Server
database using AWS DMS
• Deliver data at scale to Amazon Managed Streaming for Apache Kafka (Amazon MSK)
• Migrate an ASP.NET web application to AWS Elastic Beanstalk
COST05-BP04 Select software with cost-effective licensing
Open-source software eliminates software licensing costs, which can contribute significant costs to
workloads. Where licensed software is required, avoid licenses bound to arbitrary attributes such as
CPUs, look for licenses that are bound to output or outcomes. The cost of these licenses scales more
closely to the benefit they provide.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
The cost of software licenses can be eliminated through the use of open-source software. This can
have significant impact on workload costs as the size of the workload scales. Measure the benefits of
498

AWS Well-Architected Framework
Cost-effective resources
licensed software against the total cost to ensure that you have the most optimized workload. Model
any changes in licensing and how they would impact your workload costs. If a vendor changes the
cost of your database license, investigate how that impacts the overall efficiency of your workload.
Consider historical pricing announcements from your vendors for trends of licensing changes across
their products. Licensing costs may also scale independently of throughput or usage, such as licenses
that scale by hardware (CPU-bound licenses). These licenses should be avoided because costs can rapidly
increase without corresponding outcomes.
Implementation steps
• Analyze license options: Review the licensing terms of available software. Look for open-source
versions that have the required functionality, and whether the benefits of licensed software outweigh
the cost. Favorable terms will align the cost of the software to the benefit it provides.
• Analyze the software provider: Review any historical pricing or licensing changes from the vendor.
Look for any changes that do not align to outcomes, such as punitive terms for running on specific
vendors hardware or platforms. Additionally look for how they run audits, and penalties that could be
imposed.
Resources
Related documents:
• AWS Total Cost of Ownership (TCO) Calculator
• Amazon S3 storage classes
• Cloud products
COST05-BP05 Select components of this workload to optimize cost in line with
organization priorities
Factor in cost when selecting all components for your workload. This includes using application level and
managed services or serverless, containers, or event-driven architecture to reduce overall cost. Minimize
license costs by using open-source software, software that does not have license fees, or alternatives to
reduce the cost.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Consider the cost of services and options when selecting all components. This includes using application
level and managed services, such as Amazon Relational Database Service (Amazon RDS), Amazon
DynamoDB, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Email Service
(Amazon SES) to reduce overall organization cost. Use serverless and containers for compute, such as
AWS Lambda and Amazon Simple Storage Service (Amazon S3) for static websites. Containerize your
application if possible and use AWS Managed Container Services such as Amazon Elastic Container
Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS). Minimize license costs by
using open-source software, or software that does not have license fees (for example, Amazon Linux for
compute workloads or migrate databases to Amazon Aurora).
You can use serverless or application-level services such as AWS Lambda, Amazon Simple Queue Service
(Amazon SQS), Amazon SNS, and Amazon SES. These services remove the need for you to manage a
resource, and provide the function of running code, queuing services, and message delivery. The other
benefit is that they scale in performance and cost in line with usage, allowing efficient cost allocation
and attribution.
Using event-driven architecture (EDA) is also possible with serverless services. Event-driven architectures
are push-based, so everything happens on demand as the event presents itself in the router. This way,
499

AWS Well-Architected Framework
Cost-effective resources
you’re not paying for continuous polling to check for an event. This means less network bandwidth
consumption, less CPU utilization, less idle fleet capacity, and fewer SSL/TLS handshakes.
For more information on Serverless, refer to the Well-Architected Serverless Application Lens
whitepaper.
Implementation steps
• Select each service to optimize cost: Using your prioritized list and analysis, select each option that
provides the best match with your organizational priorities. Instead of increasing the capacity to
meet the demand, consider other options which may give you better performance with lower cost.
For example, you need to review expected traffic for your databases on AWS and consider either
increasing the instance size or using Amazon ElastiCache services (Redis or Memcached) to provide
cached mechanisms for your databases.
• Evaluate event-driven architecture: Using serverless architecture also allows you to build event-
driven architecture for distributed microservice-based applications, which helps you build scalable,
resilient, agile and cost-effective solutions.
Resources
Related documents:
• AWS Total Cost of Ownership (TCO) Calculator
• AWS Serverless
• What is Event-Driven Architecture
• Amazon S3 storage classes
• Cloud products
• Amazon ElastiCache for Redis
Related examples:
• Getting started with event-driven architecture
• What is an Event-Driven Architecture?
• How Statsig runs 100x more cost-effectively using Amazon ElastiCache for Redis
• Best practices for working with AWS Lambda functions
COST05-BP06 Perform cost analysis for different usage over time
Workloads can change over time. Some services or features are more cost effective at different usage
levels. By performing the analysis on each component over time and at projected usage, the workload
remains cost-effective over its lifetime.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
As AWS releases new services and features, the optimal services for your workload may change. Effort
required should reflect potential benefits. Workload review frequency depends on your organization
requirements. If it is a workload of significant cost, implementing new services sooner will maximize cost
savings, so more frequent review can be advantageous. Another initiation for review is change in usage
patterns. Significant changes in usage can indicate that alternate services would be more optimal.
If you need to move data into AWS Cloud, you can select any wide variety of services AWS offers and
partner tools to help you migrate your data sets, whether they are files, databases, machine images,
500

AWS Well-Architected Framework
Cost-effective resources
block volumes, or even tape backups. For example, to move a large amount of data to and from AWS
or process data at the edge, you can use one of the AWS purpose-built devices to cost effectively move
petabytes of data offline. Another example is for higher data transfer rates, a direct connect service may
be cheaper than a VPN which provides the required consistent connectivity for your business.
Based on the cost analysis for different usage over time, review your scaling activity. Analyze the result to
see if the scaling policy can be tuned to add instances with multiple instance types and purchase options.
Review your settings to see if the minimum can be reduced to serve user requests but with a smaller
fleet size, and add more resources to meet the expected high demand.
Perform cost analysis for different usage over time by discussing with stakeholders in your organization
and use AWS Cost Explorer’s forecast feature to predict the potential impact of service changes. Monitor
usage level launches using AWS Budgets, CloudWatch billing alarms and AWS Cost Anomaly Detection to
identify and implement the most cost-effective services sooner.
Implementation steps
• Define predicted usage patterns: Working with your organization, such as marketing and product
owners, document what the expected and predicted usage patterns will be for the workload. Discuss
with business stakeholders about both historical and forecasted cost and usage increases and make
sure increases align with business requirements. Identify calendar days, weeks, or months where you
expect more users to use your AWS resources, which indicate that you should increase the capacity of
the existing resources or adopt additional services to reduce the cost and increase performance.
• Perform cost analysis at predicted usage: Using the usage patterns defined, perform analysis at each
of these points. The analysis effort should reflect the potential outcome. For example, if the change
in usage is large, a thorough analysis should be performed to verify any costs and changes. In other
words, when cost increases, usage should increase for business as well.
Resources
Related documents:
• AWS Total Cost of Ownership (TCO) Calculator
• Amazon S3 storage classes
• Cloud products
• Amazon EC2 Auto Scaling
• Cloud Data Migration
• AWS Snow Family
Related videos:
• AWS OpsHub for Snow Family
COST 6. How do you meet cost targets when you select resource
type, size and number?
Verify that you choose the appropriate resource size and number of resources for the task at hand. You
minimize waste by selecting the most cost effective type, size, and number.
Best practices
• COST06-BP01 Perform cost modeling (p. 502)
• COST06-BP02 Select resource type, size, and number based on data (p. 503)
501

AWS Well-Architected Framework
Cost-effective resources
• COST06-BP03 Select resource type, size, and number automatically based on metrics (p. 503)
COST06-BP01 Perform cost modeling
Identify organization requirements (such as business needs and existing commitments) and perform cost
modeling (overall costs) of the workload and each of its components. Perform benchmark activities for
the workload under different predicted loads and compare the costs. The modeling effort should reflect
the potential benefit. For example, time spent is proportional to component cost.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Perform cost modelling for your workload and each of its components to understand the balance
between resources, and find the correct size for each resource in the workload, given a specific level
of performance. Understanding cost considerations can inform your organizational business case
and decision-making process when evaluating the value realization outcomes for planned workload
deployment.
Perform benchmark activities for the workload under different predicted loads and compare the
costs. The modelling effort should reflect potential benefit; for example, time spent is proportional to
component cost or predicted saving. For best practices, refer to the Review section of the Performance
Efficiency Pillar of the AWS Well-Architected Framework.
As an example, to create cost modeling for a workload consisting of compute resources, AWS
Compute Optimizer can assist with cost modelling for running workloads. It provides right-sizing
recommendations for compute resources based on historical usage. Make sure CloudWatch Agents are
deployed to the Amazon EC2 instances to collect memory metrics which help you with more accurate
recommendations within AWS Compute Optimizer. This is the ideal data source for compute resources
because it is a free service that uses machine learning to make multiple recommendations depending on
levels of risk.
There are multiple services you can use with custom logs as data sources for rightsizing operations
for other services and workload components, such as AWS Trusted Advisor, Amazon CloudWatch and
Amazon CloudWatch Logs. AWS Trusted Advisor checks resources and flags resources with low utilization
which can help you right size your resources and create cost modelling.
The following are recommendations for cost modelling data and metrics:
• The monitoring must accurately reflect the user experience. Select the correct granularity for the time
period and thoughtfully choose the maximum or 99th percentile instead of the average.
• Select the correct granularity for the time period of analysis that is required to cover any workload
cycles. For example, if a two-week analysis is performed, you might be overlooking a monthly cycle of
high utilization, which could lead to under-provisioning.
• Choose the right AWS services for your planned workload by considering your existing commitments,
selected pricing models for other workloads, and ability to innovate faster and focus on your core
business value.
Implementation steps
• Perform cost modeling for resources: Deploy the workload or a proof of concept into a separate
account with the specific resource types and sizes to test. Run the workload with the test data and
record the output results, along with the cost data for the time the test was run. Afterwards, redeploy
the workload or change the resource types and sizes and run the test again. Include license fees of
any products you may use with these resources and estimated operations (labor or engineer) costs for
deploying and managing these resources while creating cost modeling. Consider cost modeling for a
period (hourly, daily, monthly, yearly or three years).
502

AWS Well-Architected Framework
Cost-effective resources
Resources
Related documents:
• AWS Auto Scaling
• Identifying Opportunities to Right Size
• Amazon CloudWatch features
• Cost Optimization: Amazon EC2 Right Sizing
• AWS Compute Optimizer
• AWS Pricing Calculator
Related examples:
• Perform a Data-Driven Cost Modelling
• Estimate the cost of planned AWS resource configurations
• Choose the right AWS tools
COST06-BP02 Select resource type, size, and number based on data
Select resource size or type based on data about the workload and resource characteristics. For example,
compute, memory, throughput, or write intensive. This selection is typically made using a previous (on-
premises) version of the workload, using documentation, or using other sources of information about the
workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Select resource size or type based on workload and resource characteristics, for example, compute,
memory, throughput, or write intensive. This selection is typically made using cost modelling, a previous
version of the workload (such as an on-premises version), using documentation, or using other sources of
information about the workload (whitepapers, published solutions).
Implementation steps
• Select resources based on data: Using your cost modeling data, select the expected workload usage
level, then select the specified resource type and size.
Resources
Related documents:
• AWS Auto Scaling
• Amazon CloudWatch features
• Cost Optimization: EC2 Right Sizing
COST06-BP03 Select resource type, size, and number automatically based on
metrics
Use metrics from the currently running workload to select the right size and type to optimize for cost.
Appropriately provision throughput, sizing, and storage for compute, storage, data, and networking
503

AWS Well-Architected Framework
Cost-effective resources
services. This can be done with a feedback loop such as automatic scaling or by custom code in the
workload.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Create a feedback loop within the workload that uses active metrics from the running workload to
make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you
configure to perform the right sizing operations for you. AWS also provides APIs, SDKs, and features that
allow resources to be modified with minimal effort. You can program a workload to stop-and-start an
Amazon EC2 instance to allow a change of instance size or instance type. This provides the benefits of
right-sizing while removing almost all the operational cost required to make the change.
Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage
Service Intelligent-Tiering. Amazon S3 Intelligent-Tiering automatically moves your data between two
access tiers, frequent access and infrequent access, based on your usage patterns.
Implementation steps
•                                                                                                           Increase your observability by configuring workload metrics: Capture key metrics for the workload.
These metrics provide an indication of the customer experience, such as workload output, and align
to the differences between resource types and sizes, such as CPU and memory usage. For compute
resource, analyze performance data to right size your Amazon EC2 instances. Identify idle instances
and ones that are underutilized. Key metrics to look for are CPU usage and memory utilization (for
example, 40% CPU utilization at 90% of the time as explained in Rightsizing with AWS Compute
Optimizer and Memory Utilization Enabled). Identify instances with a maximum CPU usage and
memory utilization of less than 40% over a four-week period. These are the instances to right size to
reduce costs. For storage resources such as Amazon S3, you can use Amazon S3 Storage Lens, which
allows you to see 28 metrics across various categories at the bucket level, and 14 days of historical
data in the dashboard by default. You can filter your Amazon S3 Storage Lens dashboard by summary
and cost optimization or events to analyze specific metrics.
•                                                                                                           View rightsizing recommendations: Use the rightsizing recommendations in AWS Compute Optimizer
and the Amazon EC2 rightsizing tool in the Cost Management console, or review AWS Trusted Advisor
right-sizing your resources to make adjustments on your workload. It is important to use the right
tools when right-sizing different resources and follow right-sizing guidelines whether it is an Amazon
EC2 instance, AWS storage classes, or Amazon RDS instance types. For storage resources, you can
use Amazon S3 Storage Lens, which gives you visibility into object storage usage, activity trends, and
makes actionable recommendations to optimize costs and apply data protection best practices. Using
the contextual recommendations that Amazon S3 Storage Lens derives from analysis of metrics across
your organization, you can take immediate steps to optimize your storage.
•                                                                                                           Select resource type and size automatically based on metrics: Using the workload metrics, manually
or automatically select your workload resources. For compute resources, configuring AWS Auto Scaling
or implementing code within your application can reduce the effort required if frequent changes are
needed, and it can potentially implement changes sooner than a manual process. You can launch and
automatically scale a fleet of On-Demand Instances and Spot Instances within a single Auto Scaling
group. In addition to receiving discounts for using Spot Instances, you can use Reserved Instances or
a Savings Plan to receive discounted rates of the regular On-Demand Instance pricing. All of these
factors combined help you optimize your cost savings for Amazon EC2 instances and determine the
desired scale and performance for your application. You can also use an attribute-based instance
type selection (ABS) strategy in Auto Scaling Groups (ASG), which lets you express your instance
requirements as a set of attributes, such as vCPU, memory, and storage. You can automatically use
newer generation instance types when they are released and access a broader range of capacity with
Amazon EC2 Spot Instances. Amazon EC2 Fleet and Amazon EC2 Auto Scaling select and launch
instances that fit the specified attributes, removing the need to manually pick instance types. For
storage resources, you can use the Amazon S3 Intelligent Tiering and Amazon EFS Infrequent Access
features, which allow you to select storage classes automatically that deliver automatic storage cost
savings when data access patterns change, without performance impact or operational overhead.
504

AWS Well-Architected Framework
Cost-effective resources
Resources
Related documents:
• AWS Auto Scaling
• AWS Right-Sizing
• AWS Compute Optimizer
• Amazon CloudWatch features
• CloudWatch Getting Set Up
• CloudWatch Publishing Custom Metrics
• Getting Started with Amazon EC2 Auto Scaling
• Amazon S3 Storage Lens
• Amazon S3 Intelligent-Tiering
• Amazon EFS Infrequent Access
• Launch an Amazon EC2 Instance Using the SDK
Related videos:
• Right Size Your Services
Related examples:
• Attribute based Instance Type Selection for Auto Scaling for Amazon EC2 Fleet
• Optimizing Amazon Elastic Container Service for cost using scheduled scaling
• Predictive scaling with Amazon EC2 Auto Scaling
• Optimize Costs and Gain Visibility into Usage with Amazon S3 Storage Lens
• Well-Architected Labs: Rightsizing Recommendations (Level 100)
• Well-Architected Labs: Rightsizing with AWS Compute Optimizer and Memory Utilization Enabled
(Level 200)
COST 7. How do you use pricing models to reduce cost?
Use the pricing model that is most appropriate for your resources to minimize expense.
Best practices
• COST07-BP01 Perform pricing model analysis (p. 505)
• COST07-BP02 Implement Regions based on cost (p. 507)
• COST07-BP03 Select third-party agreements with cost-efficient terms (p. 509)
• COST07-BP04 Implement pricing models for all components of this workload (p. 510)
• COST07-BP05 Perform pricing model analysis at the management account level (p. 510)
COST07-BP01 Perform pricing model analysis
Analyze each component of the workload. Determine if the component and resources will be running for
extended periods (for commitment discounts) or dynamic and short-running (for spot or on-demand).
Perform an analysis on the workload using the recommendations in cost management tools and apply
business rules to those recommendations to achieve high returns.
Level of risk exposed if this best practice is not established: High
505

AWS Well-Architected Framework
Cost-effective resources
Implementation guidance
AWS has multiple pricing models that allow you to pay for your resources in the most cost-effective way
that suits your organization’s needs and depending on product. Work with your teams to determine the
most appropriate pricing model. Often your pricing model consists of a combination of multiple options,
as determined by your availability
On-Demand Instances allow you pay for compute or database capacity by the hour or by the second (60
seconds minimum) depending on which instances you run, without long-term commitments or upfront
payments.
Savings Plans are a flexible pricing model that offers low prices on Amazon EC2, Lambda, and AWS
Fargate (Fargate) usage, in exchange for a commitment to a consistent amount of usage (measured in
dollars per hour) over one year or three years terms.
Spot Instances are an Amazon EC2 pricing mechanism that allows you request spare compute capacity at
discounted hourly rate (up to 90% off the on-demand price) without upfront commitment.
Reserved Instances allow you up to 75 percent discount by prepaying for capacity. For more details, see
Optimizing costs with reservations.
You might choose to include a Savings Plan for the resources associated with the production, quality,
and development environments. Alternatively, because sandbox resources are only powered on when
needed, you might choose an on-demand model for the resources in that environment. Use Amazon
Spot Instances to reduce Amazon EC2 costs or use Compute Savings Plans to reduce Amazon EC2,
Fargate, and Lambda cost. The AWS Cost Explorer recommendations tool provides opportunities for
commitment discounts with Saving plans.
If you have been purchasing Reserved Instances for Amazon EC2 in the past or have established cost
allocation practices inside your organization, you can continue using Amazon EC2 Reserved Instances
for the time being. However, we recommend working on a strategy to use Savings Plans in the future as
a more flexible cost savings mechanism. You can refresh Savings Plans (SP) Recommendations in AWS
Cost Management to generate new Savings Plans Recommendations at any time. Use Reserved Instances
(RI) to reduce Amazon RDS, Amazon Redshift, Amazon ElastiCache, and Amazon OpenSearch Service
costs. Saving Plans and Reserved Instances are available in three options: all upfront, partial upfront
and no upfront payments. Use the recommendations provided in AWS Cost Explorer RI and SP purchase
recommendations.
To find opportunities for Spot workloads, use an hourly view of your overall usage, and look for regular
periods of changing usage or elasticity. You can use Spot Instances for various fault-tolerant and flexible
applications. Examples include stateless web servers, API endpoints, big data and analytics applications,
containerized workloads, CI/CD, and other flexible workloads.
Analyze your Amazon EC2 and Amazon RDS instances whether they can be turned off when you don’t
use (after hours and weekends). This approach will allow you to reduce costs by 70% or more compared
to using them 24/7. If you have Amazon Redshift clusters that only need to be available at specific times,
you can pause the cluster and later resume it. When the Amazon Redshift cluster or Amazon EC2 and
Amazon RDS Instance is stopped, the compute billing halts and only the storage charge applies.
Note that On-Demand Capacity reservations (ODCR) are not a pricing discount. Capacity Reservations
are charged at the equivalent On-Demand rate, whether you run instances in reserved capacity or not.
They should be considered when you need to provide enough capacity for the resources you plan to
run. ODCRs don't have to be tied to long-term commitments, as they can be cancelled when you no
longer need them, but they can also benefit from the discounts that Savings Plans or Reserved Instances
provide.
Implementation steps
• Analyze workload elasticity: Using the hourly granularity in Cost Explorer or a custom dashboard,
analyze your workload's elasticity. Look for regular changes in the number of instances that are
running. Short duration instances are candidates for Spot Instances or Spot Fleet.
506

AWS Well-Architected Framework
Cost-effective resources
• Well-Architected Lab: Cost Explorer
• Well-Architected Lab: Cost Visualization
• Review existing pricing contracts: Review current contracts or commitments for long term needs.
Analyze what you currently have and how much those commitments are in use. Leverage pre-existing
contractual discounts or enterprise agreements. Enterprise Agreements give customers the option to
tailor agreements that best suit their needs. For long term commitments, consider reserved pricing
discounts, Reserved Instances or Savings Plans for the specific instance type, instance family, AWS
Region, and Availability Zones.
• Perform a commitment discount analysis: Using Cost Explorer in your account, review the
Savings Plans and Reserved Instance recommendations. To verify that you implement the correct
recommendations with the required discounts and risk, follow the Well-Architected labs.
Resources
Related documents:
• Accessing Reserved Instance recommendations
• Instance purchasing options
• AWS Enterprise
Related videos:
• Save up to 90% and run production workloads on Spot
Related examples:
• Well-Architected Lab: Cost Explorer
• Well-Architected Lab: Cost Visualization
• Well-Architected Lab: Pricing Models
COST07-BP02 Implement Regions based on cost
This best practice was updated with new guidance on July 13th, 2023.
Resource pricing may be different in each Region. Identify Regional cost differences and only deploy in
Regions with higher costs to meet latency, data residency and data sovereignty requirements. Factoring
in Region cost helps you pay the lowest overall price for this workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
The AWS Cloud Infrastructure is global, hosted in multiple locations world-wide, and built around AWS
Regions, Availability Zones, Local Zones, AWS Outposts, and Wavelength Zones. A Region is a physical
location in the world and each Region is a separate geographic area where AWS has multiple Availability
Zones. Availability Zones which are multiple isolated locations within each Region consist of one or more
discrete data centers, each with redundant power, networking, and connectivity.
Each AWS Region operates within local market conditions, and resource pricing is different in each
Region due to differences in the cost of land, fiber, electricity, and taxes, for example. Choose a specific
Region to operate a component of or your entire solution so that you can run at the lowest possible price
507

AWS Well-Architected Framework
Cost-effective resources
globally. Use AWS Calculator to estimate the costs of your workload in various Regions by searching
services by location type (Region, wave length zone and local zone) and Region.
When you architect your solutions, a best practice is to seek to place computing resources closer to users
to provide lower latency and strong data sovereignty. Select the geographic location based on your
business, data privacy, performance, and security requirements. For applications with global end users,
use multiple locations.
Use Regions that provide lower prices for AWS services to deploy your workloads if you have no
obligations in data privacy, security and business requirements. For example, if your default Region
is Asia Pacific (Sydney) (ap-southwest-2), and if there are no restrictions (data privacy, security, for
example) to use other Regions, deploying non-critical (development and test) Amazon EC2 instances in
US East (N. Virginia) (us-east-1) will cost you less.
Region feature matrix table
The preceding matrix table shows us that Region 6 is the best option for this given scenario because
latency is low compared to other Regions, service is available, and it is the least expensive Region.
Implementation steps
• Review AWS Region pricing: Analyze the workload costs in the current Region. Starting with the
highest costs by service and usage type, calculate the costs in other Regions that are available. If the
forecasted saving outweighs the cost of moving the component or workload, migrate to the new
Region.
• Review requirements for multi-Region deployments: Analyze your business requirements and
obligations (data privacy, security, or performance) to find out if there are any restrictions for you to
not to use multiple Regions. If there are no obligations to restrict you to use single Region, then use
multiple Regions.
• Analyze required data transfer: Consider data transfer costs when selecting Regions. Keep your data
close to your customer and close to the resources. Select less costly AWS Regions where data flows and
where there is minimal data transfer. Depending on your business requirements for data transfer, you
can use Amazon CloudFront, AWS PrivateLink, AWS Direct Connect, and AWS Virtual Private Network
to reduce your networking costs, improve performance, and enhance security.
508

AWS Well-Architected Framework
Cost-effective resources
Resources
Related documents:
• Accessing Reserved Instance recommendations
• Amazon EC2 pricing
• Instance purchasing options
• Region Table
Related videos:
• Save up to 90% and run production workloads on Spot
Related examples:
• Overview of Data Transfer Costs for Common Architectures
• Cost Considerations for Global Deployments
• What to Consider when Selecting a Region for your Workloads
• Well-Architected Labs: Restrict service usage by Region (Level 200)
COST07-BP03 Select third-party agreements with cost-efficient terms
Cost efficient agreements and terms ensure the cost of these services scales with the benefits they
provide. Select agreements and pricing that scale when they provide additional benefits to your
organization.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
When you utilize third-party solutions or services in the cloud, it is important that the pricing structures
are aligned to Cost Optimization outcomes. Pricing should scale with the outcomes and value it
provides. An example of this is software that takes a percentage of savings it provides, the more you
save (outcome) the more it charges. Agreements that scale with your bill are typically not aligned to
Cost Optimization, unless they provide outcomes for every part of your specific bill. For example, a
solution that provides recommendations for Amazon Elastic Compute Cloud(Amazon EC2) and charges
a percentage of your entire bill will increase if you use other services for which it provides no benefit.
Another example is a managed service that is charged at a percentage of the cost of resources that
are managed. A larger instance size may not necessarily require more management effort, but will be
charged more. Ensure that these service pricing arrangements include a cost optimization program or
features in their service to drive efficiency.
Implementation steps
• Analyze third-party agreements and terms: Review the pricing in third party agreements. Perform
modeling for different levels of your usage, and factor in new costs such as new service usage, or
increases in current services due to workload growth. Decide if the additional costs provide the
required benefits to your business.
Resources
Related documents:
• Accessing Reserved Instance recommendations
509

AWS Well-Architected Framework
Cost-effective resources
• Instance purchasing options
Related videos:
• Save up to 90% and run production workloads on Spot
COST07-BP04 Implement pricing models for all components of this workload
Permanently running resources should utilize reserved capacity such as Savings Plans or Reserved
Instances. Short-term capacity is configured to use Spot Instances, or Spot Fleet. On-Demand Instances
are only used for short-term workloads that cannot be interrupted and do not run long enough for
reserved capacity, between 25% to 75% of the period, depending on the resource type.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Consider the requirements of the workload components and understand the potential pricing models.
Define the availability requirement of the component. Determine if there are multiple independent
resources that perform the function in the workload, and what the workload requirements are over time.
Compare the cost of the resources using the default On-Demand pricing model and other applicable
models. Factor in any potential changes in resources or workload components.
Implementation steps
• Implement pricing models: Using your analysis results, purchase Savings Plans (SPs), Reserved
Instances (RIs) or implement Spot Instances. If it is your first RI purchase then choose the top 5 or
10 recommendations in the list, then monitor and analyze the results over the next month or two.
Purchase small numbers of commitment discounts regular cycles, for example every two weeks or
monthly. Implement Spot Instances for workloads that can be interrupted or are stateless.
• Workload review cycle: Implement a review cycle for the workload that specifically analyzes pricing
model coverage. Once the workload has the required coverage, purchase additional commitment
discounts every two to four weeks, or as your organization usage changes.
Resources
Related documents:
• Accessing Reserved Instance recommendations
• EC2 Fleet
• How to Purchase Reserved Instances
• Instance purchasing options
• Spot Instances
Related videos:
• Save up to 90% and run production workloads on Spot
COST07-BP05 Perform pricing model analysis at the management account level
This best practice was updated with new guidance on July 13th, 2023.
510

AWS Well-Architected Framework
Cost-effective resources
Check billing and cost management tools and see recommended discounts with commitments and
reservations to perform regular analysis at the management account level.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Performing regular cost modeling helps you implement opportunities to optimize across multiple
workloads. For example, if multiple workloads use On-Demand Instances, at an aggregate level, the risk
of change is lower, and implementing a commitment-based discount will achieve a lower overall cost.
It is recommended to perform analysis in regular cycles of two weeks to one month. This allows you to
make small adjustment purchases, so the coverage of your pricing models continues to evolve with your
changing workloads and their components.
Use the AWS Cost Explorer recommendations tool to find opportunities for commitment discounts
in your management account. Recommendations at the management account level are calculated
considering usage across all of the accounts in your AWS organization that have Reserve Instances or
Savings Plans (SP) discount sharing activated to recommend a commitment that maximizes savings
across accounts.
While purchasing at the management account level optimizes for max savings in many cases, there may
be situations where you might consider purchasing SPs at the linked account level, like when you want
the discounts to apply first to usage in that particular linked account. Member account recommendations
are calculated at the individual account level, to maximize savings for each isolated account. If your
account owns both RI and SP commitments, they will be applied in this order:
Zonal RI > Standard RI > Convertible RI > Instance Savings Plan > Compute Savings Plan
If you purchase an SP at the management account level, the savings will be applied based on highest to
lowest discount percentage. SPs at the management account level look across all linked accounts and
apply the savings wherever the discount will be the highest. If you wish to restrict where the savings are
applied, you can purchase a Savings Plan at the linked account level and any time that account is running
eligible compute services, the discount will be applied there first. When the account is not running
eligible compute services, the discount will be shared across the other linked accounts under the same
management account. Discount sharing is turned on by default, but can be turned off if needed.
In a Consolidated Billing Family, Savings Plans are applied first to the owner account's usage, and then
to other accounts' usage. This occurs only if you have sharing enabled. Your Savings Plans are applied
to your highest savings percentage first. If there are multiple usages with equal savings percentages,
Savings Plans are applied to the first usage with the lowest Savings Plans rate. Savings Plans continue
to apply until there are no more remaining uses or your commitment is exhausted. Any remaining usage
is charged at the On-Demand rates. You can refresh Savings Plans Recommendations in AWS Cost
Management to generate new Savings Plans Recommendations at any time.
After analysing flexibility of instances, you can commit by following recommendations. Create cost
modelling with analysing the workload’s short-term costs with potential different resource options,
analysing AWS pricing models and aligning them with your business requirements to find out total cost
of ownership and cost optimization opportunities.
Implementation steps
• Perform a commitment discount analysis: Using Cost Explorer in your account review the
Savings Plans and Reserved Instance recommendations. Make sure you understand Saving Plan
recommendations, estimate your monthly spend and estimate your monthly savings. Review
recommendations at the management account level which are calculated considering usage across
all of the member accounts in your AWS organization that have Reserve Instances or Savings Plans
discount sharing activated for maximum savings across accounts. You can ensure you implemented the
correct recommendations with the required discounts and risk by following the Well-Architected labs.
511

AWS Well-Architected Framework
Cost-effective resources
Resources
Related documents:
• How does AWS pricing work?
• Instance purchasing options
• Saving Plan Overview
• Saving Plan recommendations
• Accessing Reserved Instance recommendations
• How Savings Plans apply to your AWS usage
• Savings Plans with Consolidated Billing
• Turning on shared reserved instances and Savings Plans discounts
Related videos:
• Save up to 90% and run production workloads on Spot
Related examples:
• Well-Architected Lab: Pricing Models (Level 200)
• Well-Architected Labs: Pricing Model Analysis (Level 200)
• What should I consider before purchasing a Savings Plan?
• How can I use rolling Savings Plans to reduce commitment risk>
• When to Use Spot Instances
COST 8. How do you plan for data transfer charges?
Verify that you plan and monitor data transfer charges so that you can make architectural decisions to
minimize costs. A small yet effective architectural change can drastically reduce your operational costs
over time.
Best practices
• COST08-BP01 Perform data transfer modeling (p. 512)
• COST08-BP02 Select components to optimize data transfer cost (p. 513)
• COST08-BP03 Implement services to reduce data transfer costs (p. 514)
COST08-BP01 Perform data transfer modeling
Gather organization requirements and perform data transfer modeling of the workload and each of its
components. This identifies the lowest cost point for its current data transfer requirements.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Understand where the data transfer occurs in your workload, the cost of the transfer, and its associated
benefit. This allows you to make an informed decision to modify or accept the architectural decision.
For example, you may have a Multi-Availability Zone configuration where you replicate data between
the Availability Zones. You model the cost of structure and decide that this is an acceptable cost (similar
to paying for compute and storage in both Availability Zone) to achieve the required reliability and
resilience.
512

AWS Well-Architected Framework
Cost-effective resources
Model the costs over different usage levels. Workload usage can change over time, and different services
may be more cost effective at different levels.
Use AWS Cost Explorer or the AWS Cost and Usage Report (CUR) to understand and model your data
transfer costs. Configure a proof of concept (PoC) or test your workload, and run a test with a realistic
simulated load. You can model your costs at different workload demands.
Implementation steps
• Calculate data transfer costs: Use the AWS pricing pages and calculate the data transfer costs for the
workload. Calculate the data transfer costs at different usage levels, for both increases and reductions
in workload usage. Where there are multiple options for the workload architecture, calculate the cost
for each option for comparison.
• Link costs to outcomes: For each data transfer cost incurred, specify the outcome that it achieves
for the workload. If it is transfer between components, it may be for decoupling, if it is between
Availability Zones it may be for redundancy.
Resources
Related documents:
• AWS caching solutions
• AWS Pricing
• Amazon EC2 Pricing
• Amazon VPC pricing
• Deliver content faster with Amazon CloudFront
COST08-BP02 Select components to optimize data transfer cost
All components are selected, and architecture is designed to reduce data transfer costs. This includes
using components such as wide-area-network (WAN) optimization and Multi-Availability Zone (AZ)
configurations
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Architecting for data transfer ensures that you minimize data transfer costs. This may involve using
content delivery networks to locate data closer to users, or using dedicated network links from your
premises to AWS. You can also use WAN optimization and application optimization to reduce the amount
of data that is transferred between components.
Implementation steps
• Select components for data transfer: Using the data transfer modeling, focus on where the largest
data transfer costs are or where they would be if the workload usage changes. Look for alternative
architectures, or additional components that remove or reduce the need for data transfer, or lower its
cost.
Resources
Related documents:
• AWS caching solutions
• Deliver content faster with Amazon CloudFront
513

AWS Well-Architected Framework
Manage demand and supply resources
COST08-BP03 Implement services to reduce data transfer costs
Implement services to reduce data transfer. For example, using a content delivery network (CDN) such as
Amazon CloudFront to deliver content to end users, caching layers using Amazon ElastiCache, or using
AWS Direct Connect instead of VPN for connectivity to AWS.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Amazon CloudFront is a global content delivery network that delivers data with low latency and high
transfer speeds. It caches data at edge locations across the world, which reduces the load on your
resources. By using CloudFront, you can reduce the administrative effort in delivering content to large
numbers of users globally, with minimum latency.
AWS Direct Connect allows you to establish a dedicated network connection to AWS. This can reduce
network costs, increase bandwidth, and provide a more consistent network experience than internet-
based connections.
AWS VPN allows you to establish a secure and private connection between your private network and the
AWS global network. It is ideal for small offices or business partners because it provides quick and easy
connectivity, and it is a fully managed and elastic service.
VPC Endpoints allow connectivity between AWS services over private networking and can be used to
reduce public data transfer and NAT gateways costs. Gateway VPC endpoints have no hourly charges,
and support Amazon Simple Storage Service(Amazon S3) and Amazon DynamoDB. Interface VPC
endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost.
Implementation steps
• Implement services: Using the data transfer modeling, look at where the largest costs and highest
volume flows are. Review the AWS services and assess whether there is a service that reduces or
removes the transfer, specifically networking and content delivery. Also look for caching services
where there is repeated access to data, or large amounts of data.
Resources
Related documents:
• AWS Direct Connect
• AWS Explore Our Products
• AWS caching solutions
• Amazon CloudFront
• Deliver content faster with Amazon CloudFront
Manage demand and supply resources
Question
• COST 9. How do you manage demand, and supply resources? (p. 514)
COST 9. How do you manage demand, and supply resources?
For a workload that has balanced spend and performance, verify that everything you pay for is used and
avoid significantly underutilizing instances. A skewed utilization metric in either direction has an adverse
514

AWS Well-Architected Framework
Manage demand and supply resources
impact on your organization, in either operational costs (degraded performance due to over-utilization),
or wasted AWS expenditures (due to over-provisioning).
Best practices
• COST09-BP01 Perform an analysis on the workload demand (p. 515)
• COST09-BP02 Implement a buffer or throttle to manage demand (p. 516)
• COST09-BP03 Supply resources dynamically (p. 517)
COST09-BP01 Perform an analysis on the workload demand
Analyze the demand of the workload over time. Verify that the analysis covers seasonal trends and
accurately represents operating conditions over the full workload lifetime. Analysis effort should reflect
the potential benefit, for example, time spent is proportional to the workload cost.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Know the requirements of the workload. The organization requirements should indicate the workload
response times for requests. The response time can be used to determine if the demand is managed, or if
the supply of resources will change to meet the demand.
The analysis should include the predictability and repeatability of the demand, the rate of change
in demand, and the amount of change in demand. Ensure that the analysis is performed over a long
enough period to incorporate any seasonal variance, such as end-of- month processing or holiday peaks.
Ensure that the analysis effort reflects the potential benefits of implementing scaling. Look at the
expected total cost of the component, and any increases or decreases in usage and cost over the
workload lifetime.
You can use AWS Cost Explorer or Amazon QuickSight with the AWS Cost and Usage Report (CUR) or
your application logs to perform a visual analysis of workload demand.
Implementation steps
• Analyze existing workload data: Analyze data from the existing workload, previous versions of the
workload, or predicted usage patterns. Use log files and monitoring data to gain insight on how
customers use the workload. Typical metrics are the actual demand in requests per second, the
times when the rate of demand changes or when it is at different levels, and the rate of change of
demand. Ensure you analyze a full cycle of the workload, ensuring you collect data for any seasonal
changes such as end of month or end of year events. The effort reflected in the analysis should
reflect the workload characteristics. The largest effort should be placed on high-value workloads that
have the largest changes in demand. The least effort should be placed on low-value workloads that
have minimal changes in demand. Common metrics for value are risk, brand awareness, revenue or
workload cost.
• Forecast outside influence: Meet with team members from across the organization that can influence
or change the demand in the workload. Common teams would be sales, marketing, or business
development. Work with them to know the cycles they operate within, and if there are any events that
would change the demand of the workload. Forecast the workload demand with this data.
Resources
Related documents:
• AWS Auto Scaling
• AWS Instance Scheduler
• Getting started with Amazon SQS
515

AWS Well-Architected Framework
Manage demand and supply resources
• AWS Cost Explorer
• Amazon QuickSight
COST09-BP02 Implement a buffer or throttle to manage demand
Buffering and throttling modify the demand on your workload, smoothing out any peaks. Implement
throttling when your clients perform retries. Implement buffering to store the request and defer
processing until a later time. Verify that your throttles and buffers are designed so clients receive a
response in the required time.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Throttling: If the source of the demand has retry capability, then you can implement throttling.
Throttling tells the source that if it cannot service the request at the current time it should try again
later. The source will wait for a period of time and then re-try the request. Implementing throttling has
the advantage of limiting the maximum amount of resources and costs of the workload. In AWS, you
can use Amazon API Gateway to implement throttling. Refer to the Well-Architected Reliability pillar
whitepaper for more details on implementing throttling.
Buffer based: Similar to throttling, a buffer defers request processing, allowing applications that
run at different rates to communicate effectively. A buffer-based approach uses a queue to accept
messages (units of work) from producers. Messages are read by consumers and processed, allowing the
messages to run at the rate that meets the consumers’ business requirements. You don’t have to worry
about producers having to deal with throttling issues, such as data durability and backpressure (where
producers slow down because their consumer is running slowly).
In AWS, you can choose from multiple services to implement a buffering approach. Amazon Simple
Queue Service(Amazon SQS) is a managed service that provides queues that allow a single consumer to
read individual messages. Amazon Kinesis provides a stream that allows many consumers to read the
same messages.
When architecting with a buffer-based approach, ensure that you architect your workload to service the
request in the required time, and that you are able to handle duplicate requests for work.
Implementation steps
• Analyze the client requirements: Analyze the client requests to determine if they are capable of
performing retries. For clients that cannot perform retries, buffers will need to be implemented.
Analyze the overall demand, rate of change, and required response time to determine the size of
throttle or buffer required.
• Implement a buffer or throttle: Implement a buffer or throttle in the workload. A queue such as
Amazon Simple Queue Service (Amazon SQS) can provide a buffer to your workload components.
Amazon API Gateway can provide throttling for your workload components.
Resources
Related documents:
• AWS Auto Scaling
• AWS Instance Scheduler
• Amazon API Gateway
• Amazon Simple Queue Service
• Getting started with Amazon SQS
• Amazon Kinesis
516

AWS Well-Architected Framework
Manage demand and supply resources
COST09-BP03 Supply resources dynamically
This best practice was updated with new guidance on July 13th, 2023.
Resources are provisioned in a planned manner. This can be demand-based, such as through automatic
scaling, or time-based, where demand is predictable and resources are provided based on time. These
methods result in the least amount of over-provisioning or under-provisioning.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
There are several ways for AWS customers to increase the resources available to their applications and
supply resources to meet the demand. One of these options is to use AWS Instance Scheduler, which
automates the starting and stopping of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon
Relational Database Service (Amazon RDS) instances. The other option is to use AWS Auto Scaling, which
allows you to automatically scale your computing resources based on the demand of your application
or service. Supplying resources based on demand will allow you to pay for the resources you use only,
reduce cost by launching resources when they are needed, and terminate them when they aren't.
AWS Instance Scheduler allows you to configure the stop and start of your Amazon EC2 and Amazon RDS
instances at defined times so that you can meet the demand for the same resources within a consistent
time pattern such as every day user access Amazon EC2 instances at eight in the morning that they don’t
need after six at night. This solution helps reduce operational cost by stopping resources that are not in
use and starting them when they are needed.
Cost optimization with AWS Instance Scheduler.
You can also easily configure schedules for your Amazon EC2 instances across your accounts and
Regions with a simple user interface (UI) using AWS Systems Manager Quick Setup. You can schedule
Amazon EC2 or Amazon RDS instances with AWS Instance Scheduler and you can stop and start existing
instances. However, you cannot stop and start instances which are part of your Auto Scaling group (ASG)
or that manage services such as Amazon Redshift or Amazon OpenSearch Service. Auto Scaling groups
have their own scheduling for the instances in the group and these instances are created.
517

AWS Well-Architected Framework
Manage demand and supply resources
AWS Auto Scaling helps you adjust your capacity to maintain steady, predictable performance at the
lowest possible cost to meet changing demand. It is a fully managed and free service to scale the
capacity of your application that integrates with Amazon EC2 instances and Spot Fleets, Amazon ECS,
Amazon DynamoDB, and Amazon Aurora. Auto Scaling provides automatic resource discovery to help
find resources in your workload that can be configured, it has built-in scaling strategies to optimize
performance, costs, or a balance between the two, and provides predictive scaling to assist with regularly
occurring spikes.
There are multiple scaling options available to scale your Auto Scaling group:
• Maintain current instance levels at all times
• Scale manually
• Scale based on a schedule
• Scale based on demand
• Use predictive scaling
Auto Scaling policies differ and can be categorized as dynamic and scheduled scaling policies. Dynamic
policies are manual or dynamic scaling which, scheduled or predictive scaling. You can use scaling
policies for dynamic, scheduled, and predictive scaling. You can also use metrics and alarms from
Amazon CloudWatch to trigger scaling events for your workload. We recommend you use launch
templates, which allow you to access the latest features and improvements. Not all Auto Scaling features
are available when you use launch configurations. For example, you cannot create an Auto Scaling group
that launches both Spot and On-Demand Instances or that specifies multiple instance types. You must
use a launch template to configure these features. When using launch templates, we recommended
you version each one. With versioning of launch templates, you can create a subset of the full set of
parameters. Then, you can reuse it to create other versions of the same launch template.
You can use AWS Auto Scaling or incorporate scaling in your code with AWS APIs or SDKs. This reduces
your overall workload costs by removing the operational cost from manually making changes to your
environment, and changes can be performed much faster. This also matches your workload resourcing
to your demand at any time. In order to follow this best practice and supply resources dynamically for
your organization, you should understand horizontal and vertical scaling in the AWS Cloud, as well as
the nature of the applications running on Amazon EC2 instances. It is better for your Cloud Financial
Management team to work with technical teams to follow this best practice.
Elastic Load Balancing (Elastic Load Balancing) helps you scale by distributing demand across multiple
resources. With using ASG and Elastic Load Balancing, you can manage incoming requests by optimally
routing traffic so that no one instance is overwhelmed in an Auto Scaling group. The requests would be
distributed among all the targets of a target group in a round-robin fashion without consideration for
capacity or utilization.
Typical metrics can be standard Amazon EC2 metrics, such as CPU utilization, network throughput, and
Elastic Load Balancing observed request and response latency. When possible, you should use a metric
that is indicative of customer experience, typically a custom metric that might originate from application
code within your workload. To elaborate how to meet the demand dynamically in this document, we will
group Auto Scaling into two categories as demand-based and time-based supply models and deep dive
into each.
Demand-based supply: Take advantage of elasticity of the cloud to supply resources to meet changing
demand by relying on near real-time demand state. For demand-based supply, use APIs or service
features to programmatically vary the amount of cloud resources in your architecture. This allows you
to scale components in your architecture and increase the number of resources during demand spikes to
maintain performance and decrease capacity when demand subsides to reduce costs.
518

AWS Well-Architected Framework
Manage demand and supply resources
Demand-based dynamic scaling policies
• Simple/Step Scaling: Monitors metrics and adds/removes instances as per steps defined by the
customers manually.
• Target Tracking: Thermostat-like control mechanism that automatically adds or removes instances to
maintain metrics at a customer defined target.
When architecting with a demand-based approach keep in mind two key considerations. First,
understand how quickly you must provision new resources. Second, understand that the size of margin
between supply and demand will shift. You must be ready to cope with the rate of change in demand
and also be ready for resource failures.
Time-based supply: A time-based approach aligns resource capacity to demand that is predictable or
well-defined by time. This approach is typically not dependent upon utilization levels of the resources. A
time-based approach ensures that resources are available at the specific time they are required and can
be provided without any delays due to start-up procedures and system or consistency checks. Using a
time-based approach, you can provide additional resources or increase capacity during busy periods.
519

AWS Well-Architected Framework
Manage demand and supply resources
Time-based scaling policies
You can use scheduled or predictive auto scaling to implement a time-based approach. Workloads can be
scheduled to scale out or in at defined times (for example, the start of business hours), making resources
available when users arrive or demand increases. Predictive scaling uses patterns to scale out while
scheduled scaling uses pre-defined times to scale out. You can also use attribute-based instance type
selection (ABS) strategy in Auto Scaling groups, which lets you express your instance requirements as a
set of attributes, such as vCPU, memory, and storage. This also allows you to automatically use newer
generation instance types when they are released and access a broader range of capacity with Amazon
EC2 Spot Instances. Amazon EC2 Fleet and Amazon EC2 Auto Scaling select and launch instances that fit
the specified attributes, removing the need to manually pick instance types.
You can also leverage the AWS APIs and SDKs and AWS CloudFormation to automatically provision and
decommission entire environments as you need them. This approach is well suited for development
or test environments that run only in defined business hours or periods of time. You can use APIs to
scale the size of resources within an environment (vertical scaling). For example, you could scale up
a production workload by changing the instance size or class. This can be achieved by stopping and
starting the instance and selecting the different instance size or class. This technique can also be applied
to other resources, such as Amazon EBS Elastic Volumes, which can be modified to increase size, adjust
performance (IOPS) or change the volume type while in use.
When architecting with a time-based approach keep in mind two key considerations. First, how
consistent is the usage pattern? Second, what is the impact if the pattern changes? You can increase
the accuracy of predictions by monitoring your workloads and by using business intelligence. If you see
significant changes in the usage pattern, you can adjust the times to ensure that coverage is provided.
Implementation steps
• Configure scheduled scaling: For predictable changes in demand, time-based scaling can provide the
correct number of resources in a timely manner. It is also useful if resource creation and configuration
is not fast enough to respond to changes on demand. Using the workload analysis configure scheduled
scaling using AWS Auto Scaling. To configure time-based scheduling, you can use predictive scaling
of scheduled scaling to increase the number of Amazon EC2 instances in your Auto Scaling groups in
advance according to expected or predictable load changes.
• Configure predictive scaling: Predictive scaling allows you to increase the number of Amazon EC2
instances in your Auto Scaling group in advance of daily and weekly patterns in traffic flows. If you
520

AWS Well-Architected Framework
Optimize over time
have regular traffic spikes and applications that take a long time to start, you should consider using
predictive scaling. Predictive scaling can help you scale faster by initializing capacity before projected
load compared to dynamic scaling alone, which is reactive in nature. For example, if users start using
your workload with the start of the business hours and don’t use after hours, then predictive scaling
can add capacity before the business hours which eliminates delay of dynamic scaling to react to
changing traffic.
• Configure dynamic automatic scaling: To configure scaling based on active workload metrics, use
Auto Scaling. Use the analysis and configure Auto Scaling to launch on the correct resource levels,
and verify that the workload scales in the required time. You can launch and automatically scale a
fleet of On-Demand Instances and Spot Instances within a single Auto Scaling group. In addition to
receiving discounts for using Spot Instances, you can use Reserved Instances or a Savings Plan to
receive discounted rates of the regular On-Demand Instance pricing. All of these factors combined
help you to optimize your cost savings for Amazon EC2 instances and help you get the desired scale
and performance for your application.
Resources
Related documents:
• AWS Auto Scaling
• AWS Instance Scheduler
• Scale the size of your Auto Scaling group
• Getting Started with Amazon EC2 Auto Scaling
• Getting started with Amazon SQS
• Scheduled Scaling for Amazon EC2 Auto Scaling
• Predictive scaling for Amazon EC2 Auto Scaling
Related videos:
• Target Tracking Scaling Policies for Auto Scaling
• AWS Instance Scheduler
Related examples:
• Attribute based Instance Type Selection for Auto Scaling for Amazon EC2 Fleet
• Optimizing Amazon Elastic Container Service for cost using scheduled scaling
• Predictive Scaling with Amazon EC2 Auto Scaling
• How do I use Instance Scheduler with AWS CloudFormation to schedule Amazon EC2 instances?
Optimize over time
Questions
• COST 10. How do you evaluate new services? (p. 521)
• COST 11. How do you evaluate the cost of effort? (p. 524)
COST 10. How do you evaluate new services?
As AWS releases new services and features, it's a best practice to review your existing architectural
decisions to verify they continue to be the most cost effective.
521

AWS Well-Architected Framework
Optimize over time
Best practices
• COST10-BP01 Develop a workload review process (p. 522)
• COST10-BP02 Review and analyze this workload regularly (p. 523)
COST10-BP01 Develop a workload review process
Develop a process that defines the criteria and process for workload review. The review effort should
reflect potential benefit. For example, core workloads or workloads with a value of over ten percent of
the bill are reviewed quarterly or every six months, while workloads below ten percent are reviewed
annually.
Level of risk exposed if this best practice is not established: High
Implementation guidance
To have the most cost-efficient workload, you must regularly review the workload to know if there are
opportunities to implement new services, features, and components. To achieve overall lower costs the
process must be proportional to the potential amount of savings. For example, workloads that are 50%
of your overall spend should be reviewed more regularly, and more thoroughly, than workloads that are
five percent of your overall spend. Factor in any external factors or volatility. If the workload services a
specific geography or market segment, and change in that area is predicted, more frequent reviews could
lead to cost savings. Another factor in review is the effort to implement changes. If there are significant
costs in testing and validating changes, reviews should be less frequent.
Factor in the long-term cost of maintaining outdated and legacy, components and resources and the
inability to implement new features into them. The current cost of testing and validation may exceed
the proposed benefit. However, over time, the cost of making the change may significantly increase
as the gap between the workload and the current technologies increases, resulting in even larger
costs. For example, the cost of moving to a new programming language may not currently be cost
effective. However, in five years time, the cost of people skilled in that language may increase, and due
to workload growth, you would be moving an even larger system to the new language, requiring even
more effort than previously.
Break down your workload into components, assign the cost of the component (an estimate is sufficient),
and then list the factors (for example, effort and external markets) next to each component. Use these
indicators to determine a review frequency for each workload. For example, you may have webservers as
a high cost, low change effort, and high external factors, resulting in high frequency of review. A central
database may be medium cost, high change effort, and low external factors, resulting in a medium
frequency of review.
Define a process to evaluate new services, design patterns, resource types, and configurations to
optimize your workload cost as they become available. Similar to performance pillar review and
reliability pillar review processes, identify, validate, and prioritize optimization and improvement
activities and issue remediation and incorporate this into your backlog.
Implementation steps
• Define review frequency: Define how frequently the workload and its components should be
reviewed. Allocate time and resources to continual improvement and review frequency to improve
the efficiency and optimization of your workload. This is a combination of factors and may differ from
workload to workload within your organization and between components in the workload. Common
factors include the importance to the organization measured in terms of revenue or brand, the total
cost of running the workload (including operation and resource costs), the complexity of the workload,
how easy is it to implement a change, any software licensing agreements, and if a change would
incur significant increases in licensing costs due to punitive licensing. Components can be defined
functionally or technically, such as web servers and databases, or compute and storage resources.
Balance the factors accordingly and develop a period for the workload and its components. You may
522

AWS Well-Architected Framework
Optimize over time
decide to review the full workload every 18 months, review the web servers every six months, the
database every 12 months, compute and short-term storage every six months, and long-term storage
every 12 months.
• Define review thoroughness: Define how much effort is spent on the review of the workload or
workload components. Similar to the review frequency, this is a balance of multiple factors. Evaluate
and prioritize opportunities for improvement to focus efforts where they provide the greatest benefits
while estimating how much effort is required for these activities. If the expected outcomes do not
satisfy the goals, and required effort costs more, then iterate using alternative courses of action.
Your review processes should include dedicated time and resources to make continuous incremental
improvements possible. As an example, you may decide to spend one week of analysis on the database
component, one week of analysis for compute resources, and four hours for storage reviews.
Resources
Related documents:
• AWS News Blog
• Types of Cloud Computing
• What's New with AWS
Related examples:
• AWS Support Proactive Services
• Regular workload reviews for SAP workloads
COST10-BP02 Review and analyze this workload regularly
Existing workloads are regularly reviewed based on each defined process to find out if new services can
be adopted, existing services can be replaced, or workloads can be re-architected.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
AWS is constantly adding new features so you can experiment and innovate faster with the latest
technology. AWS What's New details how AWS is doing this and provides a quick overview of AWS
services, features, and Regional expansion announcements as they are released. You can dive deeper
into the launches that have been announced and use them for your review and analyze of your existing
workloads. To realize the benefits of new AWS services and features, you review on your workloads and
implement new services and features as required. This means you may need to replace existing services
you use for your workload, or modernize your workload to adopt these new AWS services. For example,
you might review your workloads and replace the messaging component with Amazon Simple Email
Service. This removes the cost of operating and maintaining a fleet of instances, while providing all the
functionality at a reduced cost.
To analyze your workload and highlight potential opportunities, you should consider not only new
services but also new ways of building solutions. Review the This is My Architecture videos on AWS to
learn about other customers’ architecture designs, their challenges and their solutions. Check the All-In
series to find out real world applications of AWS services and customer stories. You can also watch the
Back to Basics video series that explains, examines, and breaks down basic cloud architecture pattern
best practices. Another source is How to Build This videos, which are designed to assist people with big
ideas on how to bring their minimum viable product (MVP) to life using AWS services. It is a way for
builders from all over the world who have a strong idea to gain architectural guidance from experienced
AWS Solutions Architects. Finally, you can review the Getting Started resource materials, which has step
by step tutorials.
523

AWS Well-Architected Framework
Optimize over time
Before starting your review process, follow your business’ requirements for the workload, security and
data privacy requirements in order to use specific service or Region and performance requirements while
following your agreed review process.
Implementation steps
• Regularly review the workload: Using your defined process, perform reviews with the frequency
specified. Verify that you spend the correct amount of effort on each component. This process would
be similar to the initial design process where you selected services for cost optimization. Analyze the
services and the benefits they would bring, this time factor in the cost of making the change, not just
the long-term benefits.
• Implement new services: If the outcome of the analysis is to implement changes, first perform a
baseline of the workload to know the current cost for each output. Implement the changes, then
perform an analysis to confirm the new cost for each output.
Resources
Related documents:
• AWS News Blog
• What's New with AWS
• AWS Documentation
• AWS Getting Started
• AWS General Resources
Related videos:
• AWS - This is My Architecture
• AWS - Back to Basics
• AWS - All-In series
• How to Build This
COST 11. How do you evaluate the cost of effort?
Best practices
• COST11-BP01 Perform automations for operations (p. 524)
COST11-BP01 Perform automations for operations
Evaluate cost of effort for operations on cloud. Quantify reduction in time and effort for admin tasks,
deployment and other operations using automation. Evaluate the required time and cost for the effort of
operations and automate admin tasks to reduce the human effort where possible.
Level of risk exposed if this best practice is not established: Low
Automating operations improves consistency and scalability, provides more visibility, reliability, and
flexibility, reduces costs, and accelerates innovation by freeing up human resources and improving
metrics. It reduces the frequency of manual tasks, improves efficiency, and benefits enterprises by
delivering a consistent and reliable experience when deploying, administering, or operating workloads.
You can free up infrastructure resources from manual operational tasks and use them for higher value
tasks and innovations, thereby improving business outcomes. Enterprises require a proven, tested way
to manage their workloads in the cloud. That solution must be secure, fast, and cost effective, with
minimum risk and maximum reliability.
524

AWS Well-Architected Framework
Optimize over time
Start by prioritizing your operations based on required effort by looking at overall operations cost in
the cloud. For example, how long does it take to deploy new resources in the cloud, make optimization
changes to existing ones, or implement necessary configurations? Look at the total cost of human
actions by factoring in cost of operations and management. Prioritize automations for admin tasks
to reduce the human effort. Review effort should reflect the potential benefit. For example, time
spent performing tasks manually as opposed to automatically. Prioritize automating repetitive, high
value activities. Activities that pose a higher risk of human error are typically the better place to start
automating as the risk often poses an unwanted additional operational cost (like operations team
working extra hours).
Using AWS services, tools, or third-party products, you can choose which AWS automations to implement
and customize for your specific requirements. The following table shows some of the core operation
functions and capabilities you can achieve with AWS services to automate administration and operation:
• AWS Audit Manager: Continually audit your AWS usage to simplify risk and compliance assessment
• AWS Backup: Centrally manage and automate data protection.
• AWS Config: Configure compute resources, assess, audit, and evaluate configurations and resource
inventory.
• AWS CloudFormation: Launch highly available resources with infrastructure as code.
• AWS CloudTrail: IT change management, compliance, and control.
• Amazon EventBridge: Schedule events and launch AWS Lambda to take action.
• AWS Lambda: Automate repetitive processes by initiating them with events or by running them on a
fixed schedule with Amazon EventBridge.
• AWS Systems Manager: Start and stop workloads, patch operating systems,a automate configuration,
and ongoing management.
• AWS Step Functions: Schedule jobs and automate workflows.
• AWS Service Catalog: Template consumption and infrastructure as code with compliance and control.
Consider the time savings that will allow your team to focus on retiring technical debt, innovation, and
value-adding features. For example, you might need to lift and shift your on-premises environment into
the cloud as rapidly as possible and optimize later. It is worth exploring the savings you could realize
by using fully managed services by AWS that remove or reduce license costs such as Amazon Relational
Database Service, Amazon EMR, Amazon WorkSpaces, and Amazon SageMaker. Managed services
remove the operational and administrative burden of maintaining a service, which allows you to focus
on innovation. Additionally, because managed services operate at cloud scale, they can offer a lower cost
per transaction or service.
If you would like to adopt automations immediately with using AWS products and service and if don’t
have skills in your organization, reach out to AWS Managed Services (AMS), AWS Professional Services, or
AWS Partners to increase adoption of automation and improve your operational excellence in the cloud.
AWS Managed Services (AMS) is a service that operates AWS infrastructure on behalf of enterprise
customers and partners. It provides a secure and compliant environment that you can deploy your
workloads onto. AMS uses enterprise cloud operating models with automation to allow you to meet your
organization requirements, move into the cloud faster, and reduce your on-going management costs.
AWS Professional Services can also help you achieve your desired business outcomes and automate
operations with AWS. AWS Professional Services provides global specialty practices to support your
efforts in focused areas of enterprise cloud computing. Specialty practices deliver targeted guidance
through best practices, frameworks, tools, and services across solution, technology, and industry subject
areas. They help customers to deploy automated, robust, agile IT operations, and governance capabilities
optimized for the cloud center.
Implementation steps
525

AWS Well-Architected Framework
Sustainability
• Build once and deploy many: Use infrastructure-as-code such as AWS CloudFormation, AWS SDK, or
AWS Command Line Interface (AWS CLI) to deploy once and use many times for same environment or
for disaster recovery scenarios. Tag while deploying to track your consumption as defined in other best
practices. Use AWS Launch Wizard to reduce the time to deploy many popular enterprise workloads.
AWS Launch Wizard guides you through the sizing, configuration, and deployment of enterprise
workloads following AWS best practices. You can also use the AWS Service Catalog, which helps you
create and manage infrastructure-as-code approved templates for use on AWS so anyone can discover
approved, self-service cloud resources.
• Automate operations: Run routine operations automatically without human intervention. Using AWS
services and tools, you can choose which AWS automations to implement and customize for your
specific requirements. For example, use EC2 Image Builder for building, testing, and deployment of
virtual machine and container images for use on AWS or on-premises. If your desired action cannot
be done with AWS services or you need more complex actions with filtering resources, then automate
your operations with using AWS CLI or AWS SDK tools. AWS CLI provides the ability to automate the
entire process of controlling and managing AWS services via scripts without using the AWS Console.
Select your preferred AWS SDKs to interact with AWS services. For other code examples, see AWS SDK
Code examples repository.
Resources
Related documents:
• Modernizing operations in the AWS Cloud
• AWS Services for Automation
• AWS Systems Manager Automation
• AWS automations for SAP administration and operations
• AWS Managed Services
• AWS Professional Services
• Infrastructure and automation
Related examples:
• Reinventing automated operations (Part I)
• Reinventing automated operations (Part II)
• AWS automations for SAP administration and operations
• IT Automations with AWS Lambda
• AWS Code Examples Repository
• AWS Samples
Sustainability
The Sustainability pillar includes understanding the impacts of the services used, quantifying impacts
through the entire workload lifecycle, and applying design principles and best practices to reduce these
impacts when building cloud workloads. You can find prescriptive guidance on implementation in the
Sustainability Pillar whitepaper.
Best practice areas
• Region selection (p. 527)
• Alignment to demand (p. 528)
• Software and architecture (p. 537)
• Data (p. 543)
526

AWS Well-Architected Framework
Region selection
• Hardware and services (p. 556)
• Process and culture (p. 562)
Region selection
Question
• SUS 1 How do you select Regions for your workload? (p. 527)
SUS 1 How do you select Regions for your workload?
The choice of Region for your workload significantly affects its KPIs, including performance, cost, and
carbon footprint. To effectively improve these KPIs, you should choose Regions for your workloads based
on both business requirements and sustainability goals.
Best practices
• SUS01-BP01 Choose Region based on both business requirements and sustainability goals (p. 527)
SUS01-BP01 Choose Region based on both business requirements and
sustainability goals
Choose a Region for your workload based on both your business requirements and sustainability goals to
optimize its KPIs, including performance, cost, and carbon footprint.
Common anti-patterns:
• You select the workload’s Region based on your own location.
• You consolidate all workload resources into one geographic location.
Benefits of establishing this best practice: Placing a workload close to Amazon renewable energy
projects or Regions with low published carbon intensity can help to lower the carbon footprint of a cloud
workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
The AWS Cloud is a constantly expanding network of Regions and points of presence (PoP), with a global
network infrastructure linking them together. The choice of Region for your workload significantly
affects its KPIs, including performance, cost, and carbon footprint. To effectively improve these KPIs, you
should choose Regions for your workload based on both your business requirements and sustainability
goals.
Implementation steps
• Follow these steps to assess and shortlist potential Regions for your workload based on your business
requirements including compliance, available features, cost, and latency:
• Confirm that these Regions are compliant, based on your required local regulations.
• Use the AWS Regional Services Lists to check if the Regions have the services and features you need
to run your workload.
• Calculate the cost of the workload on each Region using the AWS Pricing Calculator.
• Test the network latency between your end user locations and each AWS Region.
• Choose Regions near Amazon renewable energy projects and Regions where the grid has a published
carbon intensity that is lower than other locations (or Regions).
527

AWS Well-Architected Framework
Alignment to demand
• Identify your relevant sustainability guidelines to track and compare year-to-year carbon emissions
based on Greenhouse Gas Protocol (market-based and location based methods).
• Choose region based on method you use to track carbon emissions. For more detail on choosing a
Region based on your sustainability guidelines, see How to select a Region for your workload based
on sustainability goals.
Resources
Related documents:
• Understanding your carbon emission estimations
• Amazon Around the Globe
• Renewable Energy Methodology
• What to Consider when Selecting a Region for your Workloads
Related videos:
• Architecting sustainably and reducing your AWS carbon footprint
Alignment to demand
Question
• SUS 2 How do you align cloud resources to your demand? (p. 528)
SUS 2 How do you align cloud resources to your demand?
The way users and applications consume your workloads and other resources can help you identify
improvements to meet sustainability goals. Scale infrastructure to continually match demand and verify
that you use only the minimum resources required to support your users. Align service levels to customer
needs. Position resources to limit the network required for users and applications to consume them.
Remove unused assets. Provide your team members with devices that support their needs and minimize
their sustainability impact.
Best practices
• SUS02-BP01 Scale workload infrastructure dynamically (p. 528)
• SUS02-BP02 Align SLAs with sustainability goals (p. 530)
• SUS02-BP03 Stop the creation and maintenance of unused assets (p. 531)
• SUS02-BP04 Optimize geographic placement of workloads based on their networking
requirements (p. 532)
• SUS02-BP05 Optimize team member resources for activities performed (p. 534)
• SUS02-BP06 Implement buffering or throttling to flatten the demand curve (p. 535)
SUS02-BP01 Scale workload infrastructure dynamically
Use elasticity of the cloud and scale your infrastructure dynamically to match supply of cloud resources
to demand and avoid overprovisioned capacity in your workload.
Common anti-patterns:
• You do not scale your infrastructure with user load.
528

AWS Well-Architected Framework
Alignment to demand
• You manually scale your infrastructure all the time.
• You leave increased capacity after a scaling event instead of scaling back down.
Benefits of establishing this best practice: Configuring and testing workload elasticity help to efficiently
match supply of cloud resources to demand and avoid overprovisioned capacity. You can take advantage
of elasticity in the cloud to automatically scale capacity during and after demand spikes to make sure
you are only using the right number of resources needed to meet your business requirements.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
The cloud provides the flexibility to expand or reduce your resources dynamically through a variety of
mechanisms to meet changes in demand. Optimally matching supply to demand delivers the lowest
environmental impact for a workload.
Demand can be fixed or variable, requiring metrics and automation to make sure that management does
not become burdensome. Applications can scale vertically (up or down) by modifying the instance size,
horizontally (in or out) by modifying the number of instances, or a combination of both.
You can use a number of different approaches to match supply of resources with demand.
• Target-tracking approach: Monitor your scaling metric and automatically increase or decrease
capacity as you need it.
• Predictive scaling: Scale in anticipation of daily and weekly trends.
• Schedule-based approach: Set your own scaling schedule according to predictable load changes.
• Service scaling: Pick services (like serverless) that are natively scaling by design or provide auto scaling
as a feature.
Identify periods of low or no utilization and scale resources to remove excess capacity and improve
efficiency.
Implementation steps
• Elasticity matches the supply of resources you have against the demand for those resources. Instances,
containers, and functions provide mechanisms for elasticity, either in combination with automatic
scaling or as a feature of the service. AWS provides a range of auto scaling mechanisms to ensure that
workloads can scale down quickly and easily during periods of low user load. Here are some examples
of auto scaling mechanisms:
Auto scaling mechanism                                                                                           Where to use
Amazon EC2 Auto Scaling                                                                                          Use to verify you have the correct number of
Amazon EC2 instances available to handle the
user load for your application.
Application Auto Scaling                                                                                         Use to automatically scale the resources for
individual AWS services beyond Amazon EC2,
such as Lambda functions or Amazon Elastic
Container Service (Amazon ECS) services.
Kubernetes Cluster Autoscaler                                                                                    Use to automatically scale Kubernetes clusters
on AWS.
• Scaling is often discussed related to compute services like Amazon EC2 instances or AWS Lambda
functions. Consider the configuration of non-compute services like Amazon DynamoDB read and write
capacity units or Amazon Kinesis Data Streams shards to match the demand.
529

AWS Well-Architected Framework
Alignment to demand
• Verify that the metrics for scaling up or down are validated against the type of workload being
deployed. If you are deploying a video transcoding application, 100% CPU utilization is expected and
should not be your primary metric. You can use a customized metric (such as memory utilization)
for your scaling policy if required. To choose the right metrics, consider the following guidance for
Amazon EC2:
• The metric should be a valid utilization metric and describe how busy an instance is.
• The metric value must increase or decrease proportionally to the number of instances in the Auto
Scaling group.
• Use dynamic scaling instead of manual scaling for your Auto Scaling group. We also recommend that
you use target tracking scaling policies in your dynamic scaling.
• Verify that workload deployments can handle both scale-out and scale-in events. Create test scenarios
for scale-in events to verify that the workload behaves as expected and does not affect the user
experience (like losing sticky sessions). You can use Activity history to verify a scaling activity for an
Auto Scaling group.
• Evaluate your workload for predictable patterns and proactively scale as you anticipate predicted and
planned changes in demand. With predictive scaling, you can eliminate the need to overprovision
capacity. For more detail, see Predictive Scaling with Amazon EC2 Auto Scaling.
Resources
Related documents:
• Getting Started with Amazon EC2 Auto Scaling
• Predictive Scaling for EC2, Powered by Machine Learning
• Analyze user behavior using Amazon OpenSearch Service, Amazon Kinesis Data Firehose and Kibana
• What is Amazon CloudWatch?
• Monitoring DB load with Performance Insights on Amazon RDS
• Introducing Native Support for Predictive Scaling with Amazon EC2 Auto Scaling
• Introducing Karpenter - An Open-Source, High-Performance Kubernetes Cluster Autoscaler
• Deep Dive on Amazon ECS Cluster Auto Scaling
Related videos:
• Build a cost-, energy-, and resource-efficient compute environment
• Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)
Related examples:
• Lab: Amazon EC2 Auto Scaling Group Examples
• Lab: Implement Autoscaling with Karpenter
SUS02-BP02 Align SLAs with sustainability goals
Review and optimize workload service-level agreements (SLA) based on your sustainability goals to
minimize the resources required to support your workload while continuing to meet business needs.
Common anti-patterns:
• Workload SLAs are unknown or ambiguous.
• You define your SLA just for availability and performance.
• You use the same design pattern (like Multi-AZ architecture) for all your workloads.
530

AWS Well-Architected Framework
Alignment to demand
Benefits of establishing this best practice: Aligning SLAs with sustainability goals leads to optimal
resource usage while meeting business needs.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
SLAs define the level of service expected from a cloud workload, such as response time, availability, and
data retention. They influence the architecture, resource usage, and environmental impact of a cloud
workload. At a regular cadence, review SLAs and make trade-offs that significantly reduce resource usage
in exchange for acceptable decreases in service levels.
Implementation steps
• Define or redesign SLAs that support your sustainability goals while meeting your business
requirements, not exceeding them.
• Make trade-offs that significantly reduce sustainability impacts in exchange for acceptable decreases in
service levels.
• Sustainability and reliability: Highly available workloads tend to consume more resources.
• Sustainability and performance: Using more resources to boost performance could have a higher
environmental impact.
• Sustainability and security: Overly secure workloads could have a higher environmental impact.
• Use design patterns such as microservices on AWS that prioritize business-critical functions and allow
lower service levels (such as response time or recovery time objectives) for non-critical functions.
Resources
Related documents:
• AWS Service Level Agreements (SLAs)
• Importance of Service Level Agreement for SaaS Providers
Related videos:
• Delivering sustainable, high-performing architectures
• Build a cost-, energy-, and resource-efficient compute environment
SUS02-BP03 Stop the creation and maintenance of unused assets
Decommission unused assets in your workload to reduce the number of cloud resources required to
support your demand and minimize waste.
Common anti-patterns:
• You do not analyze your application for assets that are redundant or no longer required.
• You do not remove assets that are redundant or no longer required.
Benefits of establishing this best practice: Removing unused assets frees resources and improves the
overall efficiency of the workload.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Unused assets consume cloud resources like storage space and compute power. By identifying and
eliminating these assets, you can free up these resources, resulting in a more efficient cloud architecture.
531

AWS Well-Architected Framework
Alignment to demand
Perform regular analysis on application assets such as pre-compiled reports, datasets, static images,
and asset access patterns to identify redundancy, underutilization, and potential decommission targets.
Remove those redundant assets to reduce the resource waste in your workload.
Implementation steps
• Use monitoring tools to identify static assets that are no longer required.
• Before removing any asset, evaluate the impact of removing it on the architecture.
• Develop a plan and remove assets that are no longer required.
• Consolidate overlapping generated assets to remove redundant processing.
• Update your applications to no longer produce and store assets that are not required.
• Instruct third parties to stop producing and storing assets managed on your behalf that are no longer
required.
• Instruct third parties to consolidate redundant assets produced on your behalf.
• Regularly review your workload to identify and remove unused assets.
Resources
Related documents:
• Optimizing your AWS Infrastructure for Sustainability, Part II: Storage
• How do I terminate active resources that I no longer need on my AWS account?
Related videos:
• How do I check for and then remove active resources that I no longer need on my AWS account?
SUS02-BP04 Optimize geographic placement of workloads based on their
networking requirements
This best practice was updated with new guidance on July 13th, 2023.
Select cloud location and services for your workload that reduce the distance network traffic must travel
and decrease the total network resources required to support your workload.
Common anti-patterns:
• You select the workload's Region based on your own location.
• You consolidate all workload resources into one geographic location.
• All traffic flows through your existing data centers.
Benefits of establishing this best practice: Placing a workload close to its users provides the lowest
latency while decreasing data movement across the network and reducing environmental impact.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
The AWS Cloud infrastructure is built around location options such as Regions, Availability Zones,
placement groups, and edge locations such as AWS Outposts and AWS Local Zones. These location
options are responsible for maintaining connectivity between application components, cloud services,
edge networks and on-premises data centers.
532

AWS Well-Architected Framework
Alignment to demand
Analyze the network access patterns in your workload to identify how to use these cloud location
options and reduce the distance network traffic must travel.
Implementation steps
•                                                                                                     Analyze network access patterns in your workload to identify how users use your application.
• Use monitoring tools, such as Amazon CloudWatch and AWS CloudTrail, to gather data on network
activities.
• Analyze the data to identify the network access pattern.
•                                                                                                     Select the Regions for your workload deployment based on the following key elements:
• Your Sustainability goal: as explained in Region selection.
• Where your data is located: For data-heavy applications (such as big data and machine learning),
application code should run as close to the data as possible.
• Where your users are located: For user-facing applications, choose a Region (or Regions) close to
your workload’s users.
• Other constraints: Consider constraints such as cost and compliance as explained in What to
Consider when Selecting a Region for your Workloads.
•                                                                                                     Use local caching or AWS Caching Solutions for frequently used assets to improve performance, reduce
data movement, and lower environmental impact.
Service                                                                                               When to use
Amazon CloudFront                                                                                     Use to cache static content such as images,
scripts, and videos, as well as dynamic content
such as API responses or web applications.
Amazon ElastiCache                                                                                    Use to cache content for web applications.
DynamoDB Accelerator                                                                                  Use to add in-memory acceleration to your
DynamoDB tables.
•                                                                                                     Use services that can help you run code closer to users of your workload:
Service                                                                                               When to use
Lambda@Edge                                                                                           Use for compute-heavy operations that are
initiated when objects are not in the cache.
Amazon CloudFront Functions                                                                           Use for simple use cases like HTTP(s) request or
response manipulations that can be initiated by
short-lived functions.
AWS IoT Greengrass                                                                                    Use to run local compute, messaging, and data
caching for connected devices.
•                                                                                                     Use connection pooling to allow for connection reuse and reduce required resources.
•                                                                                                     Use distributed data stores that don’t rely on persistent connections and synchronous updates for
consistency to serve regional populations.
•                                                                                                     Replace pre-provisioned static network capacity with shared dynamic capacity, and share the
sustainability impact of network capacity with other subscribers.
Resources
Related documents:
533

AWS Well-Architected Framework
Alignment to demand
• Optimizing your AWS Infrastructure for Sustainability, Part III: Networking
• Amazon ElastiCache Documentation
• What is Amazon CloudFront?
• Amazon CloudFront Key Features
Related videos:
• Demystifying data transfer on AWS
• Scaling network performance on next-gen Amazon EC2 instances
Related examples:
• AWS Networking Workshops
• Architecting for sustainability - Minimize data movement across networks
SUS02-BP05 Optimize team member resources for activities performed
Optimize resources provided to team members to minimize the environmental sustainability impact
while supporting their needs.
Common anti-patterns:
• You ignore the impact of devices used by your team members on the overall efficiency of your cloud
application.
• You manually manage and update resources used by team members.
Benefits of establishing this best practice: Optimizing team member resources improves the overall
efficiency of cloud-enabled applications.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Understand the resources your team members use to consume your services, their expected lifecycle,
and the financial and sustainability impact. Implement strategies to optimize these resources. For
example, perform complex operations, such as rendering and compilation, on highly utilized scalable
infrastructure instead of on underutilized high-powered single-user systems.
Implementation steps
• Provision workstations and other devices to align with how they’re used.
• Use virtual desktops and application streaming to limit upgrade and device requirements.
• Move processor or memory-intensive tasks to the cloud to use its elasticity.
• Evaluate the impact of processes and systems on your device lifecycle, and select solutions that
minimize the requirement for device replacement while satisfying business requirements.
• Implement remote management for devices to reduce required business travel.
• AWS Systems Manager Fleet Manager is a unified user interface (UI) experience that helps you
remotely manage your nodes running on AWS or on premises.
Resources
Related documents:
534

AWS Well-Architected Framework
Alignment to demand
• What is Amazon WorkSpaces?
• Cost Optimizer for Amazon WorkSpaces
• Amazon AppStream 2.0 Documentation
• NICE DCV
Related videos:
• Managing cost for Amazon WorkSpaces on AWS
SUS02-BP06 Implement buffering or throttling to flatten the demand curve
Buffering and throttling flatten the demand curve and reduce the provisioned capacity required for your
workload.
Common anti-patterns:
• You process the client requests immediately while it is not needed.
• You do not analyze the requirements for client requests.
Benefits of establishing this best practice: Flattening the demand curve reduce the required
provisioned capacity for the workload. Reducing the provisioned capacity means less energy
consumption and less environmental impact.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Flattening the workload demand curve can help you to reduce the provisioned capacity for a workload
and reduce its environmental impact. Assume a workload with the demand curve shown in below figure.
This workload has two peaks, and to handle those peaks, the resource capacity as shown by orange line
is provisioned. The resources and energy used for this workload is not indicated by the area under the
demand curve, but the area under the provisioned capacity line, as provisioned capacity is needed to
handle those two peaks.
Demand curve with two distinct peaks that require high provisioned capacity.
535

AWS Well-Architected Framework
Alignment to demand
You can use buffering or throttling to modify the demand curve and smooth out the peaks, which means
less provisioned capacity and less energy consumed. Implement throttling when your clients can perform
retries. Implement buffering to store the request and defer processing until a later time.
Throttling's effect on the demand curve and provisioned capacity.
Implementation steps
• Analyze the client requests to determine how to respond to them. Questions to consider include:
• Can this request be processed asynchronously?
• Does the client have retry capability?
• If the client has retry capability, then you can implement throttling, which tells the source that if it
cannot service the request at the current time, it should try again later.
• You can use Amazon API Gateway to implement throttling.
• For clients that cannot perform retries, a buffer needs to be implemented to flatten the demand curve.
A buffer defers request processing, allowing applications that run at different rates to communicate
effectively. A buffer-based approach uses a queue or a stream to accept messages from producers.
Messages are read by consumers and processed, allowing the messages to run at the rate that meets
the consumers’ business requirements.
• Amazon Simple Queue Service (Amazon SQS) is a managed service that provides queues that allow a
single consumer to read individual messages.
• Amazon Kinesis provides a stream that allows many consumers to read the same messages.
• Analyze the overall demand, rate of change, and required response time to right size the throttle or
buffer required.
Resources
Related documents:
• Getting started with Amazon SQS
• Application integration Using Queues and Messages
Related videos:
• Choosing the Right Messaging Service for Your Distributed App
536

AWS Well-Architected Framework
Software and architecture
Software and architecture
Question
• SUS 3 How do you take advantage of software and architecture patterns to support your
sustainability goals? (p. 537)
SUS 3 How do you take advantage of software and architecture
patterns to support your sustainability goals?
Implement patterns for performing load smoothing and maintaining consistent high utilization of
deployed resources to minimize the resources consumed. Components might become idle from lack
of use because of changes in user behavior over time. Revise patterns and architecture to consolidate
under-utilized components to increase overall utilization. Retire components that are no longer required.
Understand the performance of your workload components, and optimize the components that consume
the most resources. Be aware of the devices that your customers use to access your services, and
implement patterns to minimize the need for device upgrades.
Best practices
• SUS03-BP01 Optimize software and architecture for asynchronous and scheduled jobs (p. 537)
• SUS03-BP02 Remove or refactor workload components with low or no use (p. 539)
• SUS03-BP03 Optimize areas of code that consume the most time or resources (p. 540)
• SUS03-BP04 Optimize impact on devices and equipment (p. 541)
• SUS03-BP05 Use software patterns and architectures that best support data access and storage
patterns (p. 542)
SUS03-BP01 Optimize software and architecture for asynchronous and
scheduled jobs
Use efficient software and architecture patterns such as queue-driven to maintain consistent high
utilization of deployed resources.
Common anti-patterns:
• You overprovision the resources in your cloud workload to meet unforeseen spikes in demand.
• Your architecture does not decouple senders and receivers of asynchronous messages by a messaging
component.
Benefits of establishing this best practice:
• Efficient software and architecture patterns minimize the unused resources in your workload and
improve the overall efficiency.
• You can scale the processing independently of the receiving of asynchronous messages.
• Through a messaging component, you have relaxed availability requirements that you can meet with
fewer resources.
Level of risk exposed if this best practice is not established: Medium
537

AWS Well-Architected Framework
Software and architecture
Implementation guidance
Use efficient architecture patterns such as event-driven architecture that result in even utilization of
components and minimize overprovisioning in your workload. Using efficient architecture patterns
minimizes idle resources from lack of use due to changes in demand over time.
Understand the requirements of your workload components and adopt architecture patterns that
increase overall utilization of resources. Retire components that are no longer required.
Implementation steps
•                                                                                                          Analyze the demand for your workload to determine how to respond to those.
•                                                                                                          For requests or jobs that don’t require synchronous responses, use queue-driven architectures and auto
scaling workers to maximize utilization. Here are some examples of when you might consider queue-
driven architecture:
Queuing mechanism                                                                                          Description
AWS Batch job queues                                                                                       AWS Batch jobs are submitted to a job queue
where they reside until they can be scheduled to
run in a compute environment.
Amazon Simple Queue Service and Amazon EC2                                                                 Pairing Amazon SQS and Spot Instances to build
Spot Instances                                                                                             fault tolerant and efficient architecture.
•                                                                                                          For requests or jobs that can be processed anytime, use scheduling mechanisms to process jobs in
batch for more efficiency. Here are some examples of scheduling mechanisms on AWS:
Scheduling mechanism                                                                                       Description
Amazon EventBridge Scheduler                                                                               A capability from Amazon EventBridge that
allows you to create, run, and manage scheduled
tasks at scale.
AWS Glue time-based schedule                                                                               Define a time-based schedule for your crawlers
and jobs in AWS Glue.
Amazon Elastic Container Service (Amazon ECS)                                                              Amazon ECS supports creating scheduled tasks.
scheduled tasks                                                                                            Scheduled tasks use Amazon EventBridge rules
to run tasks either on a schedule or in a response
to an EventBridge event.
Instance Scheduler                                                                                         Configure start and stop schedules for your
Amazon EC2 and Amazon Relational Database
Service instances.
•                                                                                                          If you use polling and webhooks mechanisms in your architecture, replace those with events. Use
event-driven architectures to build highly efficient workloads.
•                                                                                                          Leverage serverless on AWS to eliminate over-provisioned infrastructure.
•                                                                                                          Right size individual components of your architecture to prevent idling resources waiting for input.
Resources
Related documents:
• What is Amazon Simple Queue Service?
538

AWS Well-Architected Framework
Software and architecture
• What is Amazon MQ?
• Scaling based on Amazon SQS
• What is AWS Step Functions?
• What is AWS Lambda?
• Using AWS Lambda with Amazon SQS
• What is Amazon EventBridge?
Related videos:
• Moving to event-driven architectures
SUS03-BP02 Remove or refactor workload components with low or no use
Remove components that are unused and no longer required, and refactor components with little
utilization to minimize waste in your workload.
Common anti-patterns:
• You do not regularly check the utilization level of individual components of your workload.
• You do not check and analyze recommendations from AWS rightsizing tools such as AWS Compute
Optimizer.
Benefits of establishing this best practice: Removing unused components minimizes waste and
improves the overall efficiency of your cloud workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Review your workload to identify idle or unused components. This is an iterative improvement process
which can be initiated by changes in demand or the release of a new cloud service. For example, a
significant drop in AWS Lambda function run time can be an indicator of a need to lower the memory
size. Also, as AWS releases new services and features, the optimal services and architecture for your
workload may change.
Continually monitor workload activity and look for opportunities to improve the utilization level of
individual components. By removing idle components and performing rightsizing activities, you meet
your business requirements with the fewest cloud resources.
Implementation steps
• Monitor and capture the utilization metrics for critical components of your workload (like CPU
utilization, memory utilization, or network throughput in Amazon CloudWatch metrics).
• For stable workloads, check AWS rightsizing tools such as AWS Compute Optimizer at regular intervals
to identify idle, unused, or underutilized components.
• For ephemeral workloads, evaluate utilization metrics to identify idle, unused, or underutilized
components.
• Retire components and associated assets (like Amazon ECR images) that are no longer needed.
• Refactor or consolidate underutilized components with other resources to improve utilization
efficiency. For example, you can provision multiple small databases on a single Amazon RDS database
instance instead of running databases on individual under-utilized instances.
• Understand the resources provisioned by your workload to complete a unit of work.
539

AWS Well-Architected Framework
Software and architecture
Resources
Related documents:
• AWS Trusted Advisor
• What is Amazon CloudWatch?
• Automated Cleanup of Unused Images in Amazon ECR
Related examples:
• Well-Architected Lab - Rightsizing with AWS Compute Optimizer
• Well-Architected Lab - Optimize Hardware Patterns and Observe Sustainability KPIs
SUS03-BP03 Optimize areas of code that consume the most time or resources
This best practice was updated with new guidance on July 13th, 2023.
Optimize your code that runs within different components of your architecture to minimize resource
usage while maximizing performance.
Common anti-patterns:
• You ignore optimizing your code for resource usage.
• You usually respond to performance issues by increasing the resources.
• Your code review and development process does not track performance changes.
Benefits of establishing this best practice: Using efficient code minimizes resource usage and improves
performance.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
It is crucial to examine every functional area, including the code for a cloud architected application, to
optimize its resource usage and performance. Continually monitor your workload’s performance in build
environments and production and identify opportunities to improve code snippets that have particularly
high resource usage. Adopt a regular review process to identify bugs or anti-patterns within your code
that use resources inefficiently. Leverage simple and efficient algorithms that produce the same results
for your use case.
Implementation steps
• While developing your workloads, adopt an automated code review process to improve quality and
identify bugs and anti-patterns.
• Automate code reviews with Amazon CodeGuru Reviewer
• Detecting concurrency bugs with Amazon CodeGuru
• Raising code quality for Python applications using Amazon CodeGuru
• As you run your workloads, monitor resources to identify components with high resource requirements
per unit of work as targets for code reviews.
• For code reviews, use a code profiler to identify the areas of code that use the most time or resources
as targets for optimization.
• Reducing your organization's carbon footprint with Amazon CodeGuru Profiler
540

AWS Well-Architected Framework
Software and architecture
• Understanding memory usage in your Java application with Amazon CodeGuru Profiler
• Improving customer experience and reducing cost with Amazon CodeGuru Profiler
• Use the most efficient operating system and programming language for the workload. For details on
energy efficient programming languages (including Rust), see Sustainability with Rust.
• Replace computationally intensive algorithms with simpler and more efficient version that produce the
same result.
• Remove unnecessary code such as sorting and formatting.
Resources
Related documents:
• What is Amazon CodeGuru Profiler?
• FPGA instances
• The AWS SDKs on Tools to Build on AWS
Related videos:
• Improve Code Efficiency Using Amazon CodeGuru Profiler
• Automate Code Reviews and Application Performance Recommendations with Amazon CodeGuru
SUS03-BP04 Optimize impact on devices and equipment
Understand the devices and equipment used in your architecture and use strategies to reduce their
usage. This can minimize the overall environmental impact of your cloud workload.
Common anti-patterns:
• You ignore the environmental impact of devices used by your customers.
• You manually manage and update resources used by customers.
Benefits of establishing this best practice: Implementing software patterns and features that are
optimized for customer device can reduce the overall environmental impact of cloud workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Implementing software patterns and features that are optimized for customer devices can reduce the
environmental impact in several ways:
• Implementing new features that are backward compatible can reduce the number of hardware
replacements.
• Optimizing an application to run efficiently on devices can help to reduce their energy consumption
and extend their battery life (if they are powered by battery).
• Optimizing an application for devices can also reduce the data transfer over the network.
Understand the devices and equipment used in your architecture, their expected lifecycle, and the impact
of replacing those components. Implement software patterns and features that can help to minimize the
device energy consumption, the need for customers to replace the device and also upgrade it manually.
Implementation steps
541

AWS Well-Architected Framework
Software and architecture
• Inventory the devices used in your architecture. Devices can be mobile, tablet, IOT devices, smart light,
or even smart devices in a factory.
• Optimize the application running on the devices:
• Use strategies such as running tasks in the background to reduce their energy consumption.
• Account for network bandwidth and latency when building payloads, and implement capabilities
that help your applications work well on low bandwidth, high latency links.
• Convert payloads and files into optimized formats required by devices. For example, you can use
Amazon Elastic Transcoder or AWS Elemental MediaConvert to convert large, high quality digital
media files into formats that users can play back on mobile devices, tablets, web browsers, and
connected televisions.
• Perform computationally intense activities server-side (such as image rendering), or use application
streaming to improve the user experience on older devices.
• Segment and paginate output, especially for interactive sessions, to manage payloads and limit local
storage requirements.
• Use automated over-the-air (OTA) mechanism to deploy updates to one or more devices.
• You can use a CI/CD pipeline to update mobile applications.
• You can use AWS IoT Device Management to remotely manage connected devices at scale.
• To test new features and updates, use managed device farms with representative sets of hardware
and iterate development to maximize the devices supported. For more details, see SUS06-BP04 Use
managed device farms for testing (p. 566).
Resources
Related documents:
• What is AWS Device Farm?
• Amazon AppStream 2.0 Documentation
• NICE DCV
• OTA tutorial for updating firmware on devices running FreeRTOS
Related videos:
• Introduction to AWS Device Farm
SUS03-BP05 Use software patterns and architectures that best support data
access and storage patterns
Understand how data is used within your workload, consumed by your users, transferred, and stored. Use
software patterns and architectures that best support data access and storage to minimize the compute,
networking, and storage resources required to support the workload.
Common anti-patterns:
• You assume that all workloads have similar data storage and access patterns.
• You only use one tier of storage, assuming all workloads fit within that tier.
• You assume that data access patterns will stay consistent over time.
• Your architecture supports a potential high data access burst, which results in the resources remaining
idle most of the time.
Benefits of establishing this best practice: Selecting and optimizing your architecture based on data
access and storage patterns will help decrease development complexity and increase overall utilization.
542

AWS Well-Architected Framework
Data
Understanding when to use global tables, data partitioning, and caching will help you decrease
operational overhead and scale based on your workload needs.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Use software and architecture patterns that aligns best to your data characteristics and access patterns.
For example, use modern data architecture on AWS that allows you to use purpose-built services
optimized for your unique analytics use cases. These architecture patterns allow for efficient data
processing and reduce the resource usage.
Implementation steps
• Analyze your data characteristics and access patterns to identify the correct configuration for your
cloud resources. Key characteristics to consider include:
• Data type: structured, semi-structured, unstructured
• Data growth: bounded, unbounded
• Data durability: persistent, ephemeral, transient
• Access patterns reads or writes, update frequency, spiky, or consistent
• Use architecture patterns that best support data access and storage patterns.
• Let’s Architect! Modern data architectures
• Databases on AWS: The Right Tool for the Right Job
• Use technologies that work natively with compressed data.
• Use purpose-built analytics services for data processing in your architecture.
• Use the database engine that best supports your dominant query pattern. Manage your database
indexes to ensure efficient querying. For further details, see AWS Databases.
• Select network protocols that reduce the amount of network capacity consumed in your architecture.
Resources
Related documents:
• Athena Compression Support file formats
• COPY from columnar data formats with Amazon Redshift
• Converting Your Input Record Format in Kinesis Data Firehose
• Format Options for ETL Inputs and Outputs in AWS Glue
• Improve query performance on Amazon Athena by Converting to Columnar Formats
• Loading compressed data files from Amazon S3 with Amazon Redshift
• Monitoring DB load with Performance Insights on Amazon Aurora
• Monitoring DB load with Performance Insights on Amazon RDS
• Amazon S3 Intelligent-Tiering storage class
Related videos:
• Building modern data architectures on AWS
Data
Question
543

AWS Well-Architected Framework
Data
• SUS 4 How do you take advantage of data management policies and patterns to support your
sustainability goals? (p. 544)
SUS 4 How do you take advantage of data management policies
and patterns to support your sustainability goals?
Implement data management practices to reduce the provisioned storage required to support your
workload, and the resources required to use it. Understand your data, and use storage technologies and
configurations that more effectively support the business value of the data and how it’s used. Lifecycle
data to more efficient, less performant storage when requirements decrease, and delete data that’s no
longer required.
Best practices
• SUS04-BP01 Implement a data classification policy (p. 544)
• SUS04-BP02 Use technologies that support data access and storage patterns (p. 545)
• SUS04-BP03 Use policies to manage the lifecycle of your datasets (p. 548)
• SUS04-BP04 Use elasticity and automation to expand block storage or file system (p. 549)
• SUS04-BP05 Remove unneeded or redundant data (p. 550)
• SUS04-BP06 Use shared file systems or storage to access common data (p. 552)
• SUS04-BP07 Minimize data movement across networks (p. 553)
• SUS04-BP08 Back up data only when difficult to recreate (p. 554)
SUS04-BP01 Implement a data classification policy
Classify data to understand its criticality to business outcomes and choose the right energy-efficient
storage tier to store the data.
Common anti-patterns:
• You do not identify data assets with similar characteristics (such as sensitivity, business criticality, or
regulatory requirements) that are being processed or stored.
• You have not implemented a data catalog to inventory your data assets.
Benefits of establishing this best practice: Implementing a data classification policy allows you to
determine the most energy-efficient storage tier for data.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Data classification involves identifying the types of data that are being processed and stored in an
information system owned or operated by an organization. It also involves making a determination on
the criticality of the data and the likely impact of a data compromise, loss, or misuse.
Implement data classification policy by working backwards from the contextual use of the data and
creating a categorization scheme that takes into account the level of criticality of a given dataset to an
organization’s operations.
Implementation steps
• Conduct an inventory of the various data types that exist for your workload.
• For more detail on data classification categories, see Data Classification whitepaper.
544

AWS Well-Architected Framework
Data
• Determine criticality, confidentiality, integrity, and availability of data based on risk to the
organization. Use these requirements to group data into one of the data classification tiers that you
adopt.
• As an example, see Four simple steps to classify your data and secure your startup.
• Periodically audit your environment for untagged and unclassified data, and classify and tag the data
appropriately.
• As an example, see Data Catalog and crawlers in AWS Glue.
• Establish a data catalog that provides audit and governance capabilities.
• Determine and document the handling procedures for each data class.
• Use automation to continually audit your environment to identify untagged and unclassified data, and
classify and tag the data appropriately.
Resources
Related documents:
• Leveraging AWS Cloud to Support Data Classification
• Tag policies from AWS Organizations
Related videos:
• Enabling agility with data governance on AWS
SUS04-BP02 Use technologies that support data access and storage patterns
This best practice was updated with new guidance on July 13th, 2023.
Use storage technologies that best support how your data is accessed and stored to minimize the
resources provisioned while supporting your workload.
Common anti-patterns:
• You assume that all workloads have similar data storage and access patterns.
• You only use one tier of storage, assuming all workloads fit within that tier.
• You assume that data access patterns will stay consistent over time.
Benefits of establishing this best practice: Selecting and optimizing your storage technologies based on
data access and storage patterns will help you reduce the required cloud resources to meet your business
needs and improve the overall efficiency of cloud workload.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Select the storage solution that aligns best to your access patterns, or consider changing your access
patterns to align with the storage solution to maximize performance efficiency.
• Evaluate your data characteristics and access pattern to collect the key characteristics of your storage
needs. Key characteristics to consider include:
• Data type: structured, semi-structured, unstructured
• Data growth: bounded, unbounded
545

AWS Well-Architected Framework
Data
• Data durability: persistent, ephemeral, transient
• Access patterns: reads or writes, frequency, spiky, or consistent
•                                                                                            Migrate data to the appropriate storage technology that supports your data characteristics and access
pattern. Here are some examples of AWS storage technologies and their key characteristics:
Type                                                                                         Technology                                                                                              Key characteristics
Object storage                                                                               Amazon S3                                                                                               An object storage service
with unlimited scalability,
high availability, and multiple
options for accessibility.
Transferring and accessing
objects in and out of Amazon
S3 can use a service, such as
Transfer Acceleration or Access
Points, to support your location,
security needs, and access
patterns.
Archiving storage                                                                            Amazon S3 Glacier                                                                                       Storage class of Amazon S3
                                                                                                                                                                                                     built for data-archiving.
Shared file system                                                                           Amazon Elastic File System                                                                              Mountable file system that
                                                                                             (Amazon EFS)                                                                                            can be accessed by multiple
types of compute solutions.
Amazon EFS automatically
grows and shrinks storage and
is performance-optimized to
deliver consistent low latencies.
Shared file system                                                                           Amazon FSx                                                                                              Built on the latest AWS
compute solutions to support
four commonly used file
systems: NetApp ONTAP,
OpenZFS, Windows File Server,
and Lustre. Amazon FSx latency,
throughput, and IOPS vary
per file system and should be
considered when selecting
the right file system for your
workload needs.
Block storage                                                                                Amazon Elastic Block Store                                                                              Scalable, high-performance
                                                                                             (Amazon EBS)                                                                                            block-storage service designed
for Amazon Elastic Compute
Cloud (Amazon EC2). Amazon
EBS includes SSD-backed
storage for transactional, IOPS-
intensive workloads and HDD-
backed storage for throughput-
intensive workloads.
Relational database                                                                          Amazon Aurora, Amazon RDS,                                                                              Designed to support ACID
                                                                                             Amazon Redshift                                                                                         (atomicity, consistency,
isolation, durability)
transactions and maintain
referential integrity and
546

AWS Well-Architected Framework
Data
Type                                                                                              Technology                                                                                           Key characteristics
strong data consistency.
Many traditional applications,
enterprise resource planning
(ERP), customer relationship
management (CRM), and
ecommerce systems use
relational databases to store
their data.
Key-value database                                                                                Amazon DynamoDB                                                                                      Optimized for common access
patterns, typically to store
and retrieve large volumes
of data. High-traffic web
apps, ecommerce systems,
and gaming applications are
typical use-cases for key-value
databases.
•                                                                                                 For storage systems that are a fixed size, such as Amazon EBS or Amazon FSx, monitor the available
storage space and automate storage allocation on reaching a threshold. You can leverage Amazon
CloudWatch to collect and analyze different metrics for Amazon EBS and Amazon FSx.
•                                                                                                 Amazon S3 Storage Classes can be configured at the object level and a single bucket can contain
objects stored across all of the storage classes.
•                                                                                                 You can also use Amazon S3 Lifecycle policies to automatically transition objects between storage
classes or remove data without any application changes. In general, you have to make a trade-
off between resource efficiency, access latency, and reliability when considering these storage
mechanisms.
Resources
Related documents:
• Amazon EBS volume types
• Amazon EC2 instance store
• Amazon S3 Intelligent-Tiering
• Amazon EBS I/O Characteristics
• Using Amazon S3 storage classes
• What is Amazon S3 Glacier?
Related videos:
• Architectural Patterns for Data Lakes on AWS
• Deep dive on Amazon EBS (STG303-R1)
• Optimize your storage performance with Amazon S3 (STG343)
• Building modern data architectures on AWS
Related examples:
• Amazon EFS CSI Driver
• Amazon EBS CSI Driver
• Amazon EFS Utilities
547

AWS Well-Architected Framework
Data
• Amazon EBS Autoscale
• Amazon S3 Examples
SUS04-BP03 Use policies to manage the lifecycle of your datasets
Manage the lifecycle of all of your data and automatically enforce deletion to minimize the total storage
required for your workload.
Common anti-patterns:
• You manually delete data.
• You do not delete any of your workload data.
• You do not move data to more energy-efficient storage tiers based on its retention and access
requirements.
Benefits of establishing this best practice: Using data lifecycle policies ensures efficient data access and
retention in a workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Datasets usually have different retention and access requirements during their lifecycle. For example,
your application may need frequent access to some datasets for a limited period of time. After that,
those datasets are infrequently accessed.
To efficiently manage your datasets throughout their lifecycle, configure lifecycle policies, which are rules
that define how to handle datasets.
With Lifecycle configuration rules, you can tell the specific storage service to transition a dataset to more
energy-efficient storage tiers, archive it, or delete it.
Implementation steps
• Classify datasets in your workload.
• Define handling procedures for each data class.
• Set automated lifecycle policies to enforce lifecycle rules. Here are some examples of how to set up
automated lifecycle policies for different AWS storage services:
Storage service                                                                                                 How to set automated lifecycle policies
Amazon S3                                                                                                       You can use Amazon S3 Lifecycle to manage
your objects throughout their lifecycle. If
your access patterns are unknown, changing,
or unpredictable, you can use Amazon S3
Intelligent-Tiering, which monitors access
patterns and automatically moves objects that
have not been accessed to lower-cost access
tiers. You can leverage Amazon S3 Storage Lens
metrics to identify optimization opportunities
and gaps in lifecycle management.
Amazon Elastic Block Store                                                                                      You can use Amazon Data Lifecycle Manager to
automate the creation, retention, and deletion of
Amazon EBS snapshots and Amazon EBS-backed
AMIs.
548

AWS Well-Architected Framework
Data
Storage service                                                                                             How to set automated lifecycle policies
Amazon Elastic File System                                                                                  Amazon EFS lifecycle management automatically
manages file storage for your file systems.
Amazon Elastic Container Registry                                                                           Amazon ECR lifecycle policies automate the
cleanup of your container images by expiring
images based on age or count.
AWS Elemental MediaStore                                                                                    You can use an object lifecycle policy that
governs how long objects should be stored in the
MediaStore container.
• Delete unused volumes, snapshots, and data that is out of its retention period. Leverage native service
features like Amazon DynamoDB Time To Live or Amazon CloudWatch log retention for deletion.
• Aggregate and compress data where applicable based on lifecycle rules.
Resources
Related documents:
• Optimize your Amazon S3 Lifecycle rules with Amazon S3 Storage Class Analysis
• Evaluating Resources with AWS Config Rules
Related videos:
• Simplify Your Data Lifecycle and Optimize Storage Costs With Amazon S3 Lifecycle
• Reduce Your Storage Costs Using Amazon S3 Storage Lens
SUS04-BP04 Use elasticity and automation to expand block storage or file
system
Use elasticity and automation to expand block storage or file system as data grows to minimize the total
provisioned storage.
Common anti-patterns:
• You procure large block storage or file system for future need.
• You overprovision the input and output operations per second (IOPS) of your file system.
• You do not monitor the utilization of your data volumes.
Benefits of establishing this best practice: Minimizing over-provisioning for storage system reduces the
idle resources and improves the overall efficiency of your workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Create block storage and file systems with size allocation, throughput, and latency that are appropriate
for your workload. Use elasticity and automation to expand block storage or file system as data grows
without having to over-provision these storage services.
Implementation steps
549

AWS Well-Architected Framework
Data
• For fixed size storage like Amazon EBS, verify that you are monitoring the amount of storage used
versus the overall storage size and create automation, if possible, to increase the storage size when
reaching a threshold.
• Use elastic volumes and managed block data services to automate allocation of additional storage as
your persistent data grows. As an example, you can use Amazon EBS Elastic Volumes to change volume
size, volume type, or adjust the performance of your Amazon EBS volumes.
• Choose the right storage class, performance mode, and throughput mode for your file system to
address your business need, not exceeding that.
• Amazon EFS performance
• Amazon EBS volume performance on Linux instances
• Set target levels of utilization for your data volumes, and resize volumes outside of expected ranges.
• Right size read-only volumes to fit the data.
• Migrate data to object stores to avoid provisioning the excess capacity from fixed volume sizes on
block storage.
• Regularly review elastic volumes and file systems to terminate idle volumes and shrink over-
provisioned resources to fit the current data size.
Resources
Related documents:
• Amazon FSx Documentation
• What is Amazon Elastic File System?
Related videos:
• Deep Dive on Amazon EBS Elastic Volumes
• Amazon EBS and Snapshot Optimization Strategies for Better Performance and Cost Savings
• Optimizing Amazon EFS for cost and performance, using best practices
SUS04-BP05 Remove unneeded or redundant data
Remove unneeded or redundant data to minimize the storage resources required to store your datasets.
Common anti-patterns:
• You duplicate data that can be easily obtained or recreated.
• You back up all data without considering its criticality.
• You only delete data irregularly, on operational events, or not at all.
• You store data redundantly irrespective of the storage service's durability.
• You turn on Amazon S3 versioning without any business justification.
Benefits of establishing this best practice: Removing unneeded data reduces the storage size required
for your workload and the workload environmental impact.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Do not store data that you do not need. Automate the deletion of unneeded data. Use technologies that
deduplicate data at the file and block level. Leverage native data replication and redundancy features of
services.
550

AWS Well-Architected Framework
Data
Implementation steps
•                                                                                               Evaluate if you can avoid storing data by using existing publicly available datasets in AWS Data
Exchange and Open Data on AWS.
•                                                                                               Use mechanisms that can deduplicate data at the block and object level. Here are some examples of
how to deduplicate data on AWS:
Storage service                                                                                 Deduplication mechanism
Amazon S3                                                                                       Use AWS Lake Formation FindMatches to find
matching records across a dataset (including
ones without identifiers) by using the new
FindMatches ML Transform.
Amazon FSx                                                                                      Use data deduplication on Amazon FSx for
Windows.
Amazon Elastic Block Store snapshots                                                            Snapshots are incremental backups, which
means that only the blocks on the device that
have changed after your most recent snapshot
are saved.
•                                                                                               Analyze the data access to identify unneeded data. Automate lifecycle policies. Leverage native service
features like Amazon DynamoDB Time To Live, Amazon S3 Lifecycle, or Amazon CloudWatch log
retention for deletion.
•                                                                                               Use data virtualization capabilities on AWS to maintain data at its source and avoid data duplication.
• Cloud Native Data Virtualization on AWS
• Lab: Optimize Data Pattern Using Amazon Redshift Data Sharing
•                                                                                               Use backup technology that can make incremental backups.
•                                                                                               Leverage the durability of Amazon S3 and replication of Amazon EBS to meet your durability goals
instead of self-managed technologies (such as a redundant array of independent disks (RAID)).
•                                                                                               Centralize log and trace data, deduplicate identical log entries, and establish mechanisms to tune
verbosity when needed.
•                                                                                               Pre-populate caches only where justified.
•                                                                                               Establish cache monitoring and automation to resize the cache accordingly.
•                                                                                               Remove out-of-date deployments and assets from object stores and edge caches when pushing new
versions of your workload.
Resources
Related documents:
• Change log data retention in CloudWatch Logs
• Data deduplication on Amazon FSx for Windows File Server
• Features of Amazon FSx for ONTAP including data deduplication
• Invalidating Files on Amazon CloudFront
• Using AWS Backup to back up and restore Amazon EFS file systems
• What is Amazon CloudWatch Logs?
• Working with backups on Amazon RDS
Related videos:
551

AWS Well-Architected Framework
Data
• Fuzzy Matching and Deduplicating Data with ML Transforms for AWS Lake Formation
Related examples:
• How do I analyze my Amazon S3 server access logs using Amazon Athena?
SUS04-BP06 Use shared file systems or storage to access common data
Adopt shared file systems or storage to avoid data duplication and allow for more efficient infrastructure
for your workload.
Common anti-patterns:
• You provision storage for each individual client.
• You do not detach data volume from inactive clients.
• You do not provide access to storage across platforms and systems.
Benefits of establishing this best practice: Using shared file systems or storage allows for sharing data
to one or more consumers without having to copy the data. This helps to reduce the storage resources
required for the workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
If you have multiple users or applications accessing the same datasets, using shared storage technology
is crucial to use efficient infrastructure for your workload. Shared storage technology provides a central
location to store and manage datasets and avoid data duplication. It also enforces consistency of the
data across different systems. Moreover, shared storage technology allows for more efficient use of
compute power, as multiple compute resources can access and process data at the same time in parallel.
Fetch data from these shared storage services only as needed and detach unused volumes to free up
resources.
Implementation steps
• Migrate data to shared storage when the data has multiple consumers. Here are some examples of
shared storage technology on AWS:
Storage option                                                                                               When to use
Amazon EBS Multi-Attach                                                                                      Amazon EBS Multi-Attach allows you to attach a
single Provisioned IOPS SSD (io1 or io2) volume
to multiple instances that are in the same
Availability Zone.
Amazon EFS                                                                                                   See When to Choose Amazon EFS.
Amazon FSx                                                                                                   See Choosing an Amazon FSx File System.
Amazon S3                                                                                                    Applications that do not require a file system
structure and are designed to work with object
storage can use Amazon S3 as a massively
scalable, durable, low-cost object storage
solution.
552

AWS Well-Architected Framework
Data
• Copy data to or fetch data from shared file systems only as needed. As an example, you can create an
Amazon FSx for Lustre file system backed by Amazon S3 and only load the subset of data required for
processing jobs to Amazon FSx.
• Delete data as appropriate for your usage patterns as outlined in SUS04-BP03 Use policies to manage
the lifecycle of your datasets (p. 548).
• Detach volumes from clients that are not actively using them.
Resources
Related documents:
• Linking your file system to an Amazon S3 bucket
• Using Amazon EFS for AWS Lambda in your serverless applications
• Amazon EFS Intelligent-Tiering Optimizes Costs for Workloads with Changing Access Patterns
• Using Amazon FSx with your on-premises data repository
related videos:
• Storage cost optimization with Amazon EFS
SUS04-BP07 Minimize data movement across networks
This best practice was updated with new guidance on July 13th, 2023.
Use shared file systems or object storage to access common data and minimize the total networking
resources required to support data movement for your workload.
Common anti-patterns:
• You store all data in the same AWS Region independent of where the data users are.
• You do not optimize data size and format before moving it over the network.
Benefits of establishing this best practice: Optimizing data movement across the network reduces the
total networking resources required for the workload and lowers its environmental impact.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Moving data around your organization requires compute, networking, and storage resources. Use
techniques to minimize data movement and improve the overall efficiency of your workload.
Implementation steps
• Consider proximity to the data or users as a decision factor when selecting a Region for your workload.
• Partition Regionally consumed services so that their Region-specific data is stored within the Region
where it is consumed.
• Use efficient file formats (such as Parquet or ORC) and compress data before moving it over the
network.
• Don't move unused data. Some examples that can help you avoid moving unused data:
553

AWS Well-Architected Framework
Data
• Reduce API responses to only relevant data.
• Aggregate data where detailed (record-level information is not required).
• See Well-Architected Lab - Optimize Data Pattern Using Amazon Redshift Data Sharing.
• Consider Cross-account data sharing in AWS Lake Formation.
• Use services that can help you run code closer to users of your workload.
Service                                                                                                   When to use
Lambda@Edge                                                                                               Use for compute-heavy operations that are run
when objects are not in the cache.
CloudFront Functions                                                                                      Use for simple use cases such as HTTP(s)
request/response manipulations that can be
initiated by short-lived functions.
AWS IoT Greengrass                                                                                        Run local compute, messaging, and data caching
for connected devices.
Resources
Related documents:
• Optimizing your AWS Infrastructure for Sustainability, Part III: Networking
• AWS Global Infrastructure
• Amazon CloudFront Key Features including the CloudFront Global Edge Network
• Compressing HTTP requests in Amazon OpenSearch Service
• Intermediate data compression with Amazon EMR
• Loading compressed data files from Amazon S3 into Amazon Redshift
• Serving compressed files with Amazon CloudFront
Related videos:
• Demystifying data transfer on AWS
Related examples:
• Architecting for sustainability - Minimize data movement across networks
SUS04-BP08 Back up data only when difficult to recreate
Avoid backing up data that has no business value to minimize storage resources requirements for your
workload.
Common anti-patterns:
• You do not have a backup strategy for your data.
• You back up data that can be easily recreated.
Benefits of establishing this best practice: Avoiding back-up of non-critical data reduces the required
storage resources for the workload and lowers its environmental impact.
554

AWS Well-Architected Framework
Data
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Avoiding the back up of unnecessary data can help lower cost and reduce the storage resources
used by the workload. Only back up data that has business value or is needed to satisfy compliance
requirements. Examine backup policies and exclude ephemeral storage that doesn’t provide value in a
recovery scenario.
Implementation steps
• Implement data classification policy as outlined in SUS04-BP01 Implement a data classification
policy (p. 544).
• Use the criticality of your data classification and design backup strategy based on your recovery time
objective (RTO) and recovery point objective (RPO). Avoid backing up non-critical data.
• Exclude data that can be easily recreated.
• Exclude ephemeral data from your backups.
• Exclude local copies of data, unless the time required to restore that data from a common location
exceeds your service-level agreements (SLAs).
• Use an automated solution or managed service to back up business-critical data.
• AWS Backup is a fully-managed service that makes it easy to centralize and automate data
protection across AWS services, in the cloud, and on premises. For hands-on guidance on how to
create automated backups using AWS Backup, see Well-Architected Labs - Testing Backup and
Restore of Data.
• Automate backups and optimize backup costs for Amazon EFS using AWS Backup.
Resources
Related best practices:
• REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce the data from
sources
• REL09-BP03 Perform data backup automatically
• REL13-BP02 Use defined recovery strategies to meet the recovery objectives
Related documents:
• Using AWS Backup to back up and restore Amazon EFS file systems
• Amazon EBS snapshots
• Working with backups on Amazon Relational Database Service
• APN Partner: partners that can help with backup
• AWS Marketplace: products that can be used for backup
• Backing Up Amazon EFS
• Backing Up Amazon FSx for Windows File Server
• Backup and Restore for Amazon ElastiCache for Redis
Related videos:
• AWS re:Invent 2021 - Backup, disaster recovery, and ransomware protection with AWS
• AWS Backup Demo: Cross-Account and Cross-Region Backup
• AWS re:Invent 2019: Deep dive on AWS Backup, ft. Rackspace (STG341)
555

AWS Well-Architected Framework
Hardware and services
Related examples:
• Well-Architected Lab - Testing Backup and Restore of Data
• Well-Architected Lab - Backup and Restore with Failback for Analytics Workload
• Well-Architected Lab - Disaster Recovery - Backup and Restore
Hardware and services
Question
• SUS 5 How do you select and use cloud hardware and services in your architecture to support your
sustainability goals? (p. 556)
SUS 5 How do you select and use cloud hardware and services in
your architecture to support your sustainability goals?
Look for opportunities to reduce workload sustainability impacts by making changes to your hardware
management practices. Minimize the amount of hardware needed to provision and deploy, and select the
most efficient hardware and services for your individual workload.
Best practices
• SUS05-BP01 Use the minimum amount of hardware to meet your needs (p. 556)
• SUS05-BP02 Use instance types with the least impact (p. 557)
• SUS05-BP03 Use managed services (p. 559)
• SUS05-BP04 Optimize your use of hardware-based compute accelerators (p. 561)
SUS05-BP01 Use the minimum amount of hardware to meet your needs
Use the minimum amount of hardware for your workload to efficiently meet your business needs.
Common anti-patterns:
• You do not monitor resource utilization.
• You have resources with a low utilization level in your architecture.
• You do not review the utilization of static hardware to determine if it should be resized.
• You do not set hardware utilization goals for your compute infrastructure based on business KPIs.
Benefits of establishing this best practice: Rightsizing your cloud resources helps to reduce a workload’s
environmental impact, save money, and maintain performance benchmarks.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Optimally select the total number of hardware required for your workload to improve its overall
efficiency. The AWS Cloud provides the flexibility to expand or reduce the number of resources
dynamically through a variety of mechanisms, such as AWS Auto Scaling, and meet changes in demand.
It also provides APIs and SDKs that allow resources to be modified with minimal effort. Use these
capabilities to make frequent changes to your workload implementations. Additionally, use rightsizing
guidelines from AWS tools to efficiently operate your cloud resource and meet your business needs.
556

AWS Well-Architected Framework
Hardware and services
Implementation steps
• Choose the instances type to best fit your needs.
• How do I choose the appropriate Amazon EC2 instance type for my workload?
• Attribute-based instance type selection for Amazon EC2 Fleet.
• Create an Auto Scaling group using attribute-based instance type selection.
• Scale using small increments for variable workloads.
• Use multiple compute purchase options in order to balance instance flexibility, scalability, and cost
savings.
• On-Demand Instances are best suited for new, stateful, and spiky workloads which can’t be instance
type, location, or time flexible.
• Spot Instances are a great way to supplement the other options for applications that are fault
tolerant and flexible.
• Leverage Compute Savings Plans for steady state workloads that allow flexibility if your needs (like
AZ, Region, instance families, or instance types) change.
• Use instance and availability zone diversity to maximize application availability and take advantage of
excess capacity when possible.
• Use the rightsizing recommendations from AWS tools to make adjustments on your workload.
• AWS Compute Optimizer
• AWS Trusted Advisor
• Negotiate service-level agreements (SLAs) that allow for a temporary reduction in capacity while
automation deploys replacement resources.
Resources
Related documents:
• Optimizing your AWS Infrastructure for Sustainability, Part I: Compute
• Attirbute based Instance Type Selection for Auto Scaling for Amazon EC2 Fleet
• AWS Compute Optimizer Documentation
• Operating Lambda: Performance optimization
• Auto Scaling Documentation
Related videos:
• Build a cost-, energy-, and resource-efficient compute environment
Related examples:
• Well-Architected Lab - Rightsizing with AWS Compute Optimizer and Memory Utilization Enabled
(Level 200)
SUS05-BP02 Use instance types with the least impact
This best practice was updated with new guidance on July 13th, 2023.
Continually monitor and use new instance types to take advantage of energy efficiency improvements.
Common anti-patterns:
557

AWS Well-Architected Framework
Hardware and services
• You are only using one family of instances.
• You are only using x86 instances.
• You specify one instance type in your Amazon EC2 Auto Scaling configuration.
• You use AWS instances in a manner that they were not designed for (for example, you use compute-
optimized instances for a memory-intensive workload).
• You do not evaluate new instance types regularly.
• You do not check recommendations from AWS rightsizing tools such as AWS Compute Optimizer.
Benefits of establishing this best practice: By using energy-efficient and right-sized instances, you are
able to greatly reduce the environmental impact and cost of your workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Using efficient instances in cloud workload is crucial for lower resource usage and cost-effectiveness.
Continually monitor the release of new instance types and take advantage of energy efficiency
improvements, including those instance types designed to support specific workloads such as machine
learning training and inference, and video transcoding.
Implementation steps
•                                                                                                           Learn and explore instance types which can lower your workload environmental impact.
• Subscribe to What's New with AWS to be up-to-date with the latest AWS technologies and instances.
• Learn about different AWS instance types.
• Learn about AWS Graviton-based instances which offer the best performance per watt of energy
use in Amazon EC2 by watching re:Invent 2020 - Deep dive on AWS Graviton2 processor-powered
Amazon EC2 instances and Deep dive into AWS Graviton3 and Amazon EC2 C7g instances.
•                                                                                                           Plan and transition your workload to instance types with the least impact.
• Define a process to evaluate new features or instances for your workload. Take advantage of agility
in the cloud to quickly test how new instance types can improve your workload environmental
sustainability. Use proxy metrics to measure how many resources it takes you to complete a unit of
work.
• If possible, modify your workload to work with different numbers of vCPUs and different amounts of
memory to maximize your choice of instance type.
• Consider transitioning your workload to Graviton-based instances to improve the performance
efficiency of your workload.
• AWS Graviton Fast Start
• Considerations when transitioning workloads to AWS Graviton-based Amazon Elastic Compute
Cloud instances
• AWS Graviton2 for ISVs
• Consider selecting the AWS Graviton option in your usage of AWS managed services.
• Migrate your workload to Regions that offer instances with the least sustainability impact and still
meet your business requirements.
• For machine learning workloads, take advantage of purpose-built hardware that is specific to your
workload such as AWS Trainium, AWS Inferentia, and Amazon EC2 DL1. AWS Inferentia instances
such as Inf2 instances offer up to 50% better performance per watt over comparable Amazon EC2
instances.
• Use Amazon SageMaker Inference Recommender to right size ML inference endpoint.
• For spiky workloads (workloads with infrequent requirements for additional capacity), use burstable
performance instances.
558

AWS Well-Architected Framework
Hardware and services
• For stateless and fault-tolerant workloads, use Amazon EC2 Spot Instances to increase overall
utilization of the cloud, and reduce the sustainability impact of unused resources.
• Operate and optimize your workload instance.
• For ephemeral workloads, evaluate instance Amazon CloudWatch metrics such as CPUUtilization
to identify if the instance is idle or under-utilized.
• For stable workloads, check AWS rightsizing tools such as AWS Compute Optimizer at regular
intervals to identify opportunities to optimize and right-size the instances.
• Well-Architected Lab - Rightsizing Recommendations
• Well-Architected Lab - Rightsizing with Compute Optimizer
• Well-Architected Lab - Optimize Hardware Patterns and Observice Sustainability KPIs
Resources
Related documents:
• Optimizing your AWS Infrastructure for Sustainability, Part I: Compute
• AWS Graviton
• Amazon EC2 DL1
• Amazon EC2 Capacity Reservation Fleets
• Amazon EC2 Spot Fleet
• Functions: Lambda Function Configuration
• Attribute-based instance type selection for Amazon EC2 Fleet
• Building Sustainable, Efficient, and Cost-Optimized Applications on AWS
• How the Contino Sustainability Dashboard Helps Customers Optimize Their Carbon Footprint
Related videos:
• Deep dive on AWS Graviton2 processer-powered Amazon EC2 instances
• Deep dive into AWS Graviton3 and Amazon EC2 C7g instances
• Build a cost-, energy-, and resource-efficient compute environment
Related examples:
• Solution: Guidance for Optimizing Deep Learning Workloads for Sustainability on AWS
• Well-Architected Lab - Rightsizing Recommendations
• Well-Architected Lab - Rightsizing with Compute Optimizer
• Well-Architected Lab - Optimize Hardware Patterns and Observe Sustainability KPIs
• Well-Architected Lab - Migrating Services to Graviton
SUS05-BP03 Use managed services
Use managed services to operate more efficiently in the cloud.
Common anti-patterns:
• You use Amazon EC2 instances with low utilization to run your applications.
• Your in-house team only manages the workload, without time to focus on innovation or
simplifications.
559

AWS Well-Architected Framework
Hardware and services
• You deploy and maintain technologies for tasks that can run more efficiently on managed services.
Benefits of establishing this best practice:
• Using managed services shifts the responsibility to AWS, which has insights across millions of
customers that can help drive new innovations and efficiencies.
• Managed service distributes the environmental impact of the service across many users because of the
multi-tenet control planes.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Managed services shift responsibility to AWS for maintaining high utilization and sustainability
optimization of the deployed hardware. Managed services also remove the operational and
administrative burden of maintaining a service, which allows your team to have more time and focus on
innovation.
Review your workload to identify the components that can be replaced by AWS managed services. For
example, Amazon RDS, Amazon Redshift, and Amazon ElastiCache provide a managed database service.
Amazon Athena, Amazon EMR, and Amazon OpenSearch Service provide a managed analytics service.
Implementation steps
1. Inventory your workload for services and components.
2. Assess and identify components that can be replaced by managed services. Here are some examples
of when you might consider using a managed service:
Task                                                                                                     What to use on AWS
Hosting a database                                                                                       Use managed Amazon Relational Database
Service (Amazon RDS) instances instead of
maintaining your own Amazon RDS instances on
Amazon Elastic Compute Cloud (Amazon EC2).
Hosting a container workload                                                                             Use AWS Fargate, instead of implementing your
own container infrastructure.
Hosting web apps                                                                                         Use AWS Amplify Hosting as fully managed CI/
CD and hosting service for static websites and
server-side rendered web apps.
3. Identify dependencies and create a migrations plan. Update runbooks and playbooks accordingly.
• The AWS Application Discovery Service automatically collects and presents detailed information
about application dependencies and utilization to help you make more informed decisions as you
plan your migration
4. Test the service before migrating to the managed service.
5. Use the migration plan to replace self-hosted services with managed service.
6. Continually monitor the service after the migration is complete to make adjustments as required and
optimize the service.
Resources
Related documents:
560

AWS Well-Architected Framework
Hardware and services
• AWS Cloud Products
• AWS Total Cost of Ownership (TCO) Calculator
• Amazon DocumentDB
• Amazon Elastic Kubernetes Service (EKS)
• Amazon Managed Streaming for Apache Kafka (Amazon MSK)
Related videos:
• Cloud operations at scale with AWS Managed Services
SUS05-BP04 Optimize your use of hardware-based compute accelerators
Optimize your use of accelerated computing instances to reduce the physical infrastructure demands of
your workload.
Common anti-patterns:
• You are not monitoring GPU usage.
• You are using a general-purpose instance for workload while a purpose-built instance can deliver
higher performance, lower cost, and better performance per watt.
• You are using hardware-based compute accelerators for tasks where they’re more efficient using CPU-
based alternatives.
Benefits of establishing this best practice: By optimizing the use of hardware-based accelerators, you
can reduce the physical-infrastructure demands of your workload.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
If you require high processing capability, you can benefit from using accelerated computing instances,
which provide access to hardware-based compute accelerators such as graphics processing units (GPUs)
and field programmable gate arrays (FPGAs). These hardware accelerators perform certain functions
like graphic processing or data pattern matching more efficiently than CPU-based alternatives. Many
accelerated workloads, such as rendering, transcoding, and machine learning, are highly variable in terms
of resource usage. Only run this hardware for the time needed, and decommission them with automation
when not required to minimize resources consumed.
Implementation steps
• Identify which accelerated computing instances can address your requirements.
• For machine learning workloads, take advantage of purpose-built hardware that is specific to your
workload, such as AWS Trainium, AWS Inferentia, and Amazon EC2 DL1. AWS Inferentia instances
such as Inf2 instances offer up to 50% better performance per watt over comparable Amazon EC2
instances.
• Collect usage metric for your accelerated computing instances. For example, you can use CloudWatch
agent to collect metrics such as utilization_gpu and utilization_memory for your GPUs as
shown in Collect NVIDIA GPU metrics with Amazon CloudWatch.
• Optimize the code, network operation, and settings of hardware accelerators to make sure that
underlying hardware is fully utilized.
• Optimize GPU settings
• GPU Monitoring and Optimization in the Deep Learning AMI
561

AWS Well-Architected Framework
Process and culture
• Optimizing I/O for GPU performance tuning of deep learning training in Amazon SageMaker
• Use the latest high performant libraries and GPU drivers.
• Use automation to release GPU instances when not in use.
Resources
Related documents:
• Accelerated Computing
• Let's Architect! Architecting with custom chips and accelerators
• How do I choose the appropriate Amazon EC2 instance type for my workload?
• Amazon EC2 VT1 Instances
• Amazon Elastic Graphics
• Choose the best AI accelerator and model compilation for computer vision inference with Amazon
SageMaker
Related videos:
• How to select Amazon EC2 GPU instances for deep learning
• Deep Dive on Amazon EC2 Elastic GPUs
• Deploying Cost-Effective Deep Learning Inference
Process and culture
Question
• SUS 6 How do your organizational processes support your sustainability goals? (p. 562)
SUS 6 How do your organizational processes support your
sustainability goals?
Look for opportunities to reduce your sustainability impact by making changes to your development,
test, and deployment practices.
Best practices
• SUS06-BP01 Adopt methods that can rapidly introduce sustainability improvements (p. 562)
• SUS06-BP02 Keep your workload up-to-date (p. 563)
• SUS06-BP03 Increase utilization of build environments (p. 565)
• SUS06-BP04 Use managed device farms for testing (p. 566)
SUS06-BP01 Adopt methods that can rapidly introduce sustainability
improvements
Adopt methods and processes to validate potential improvements, minimize testing costs, and deliver
small improvements.
Common anti-patterns:
• Reviewing your application for sustainability is a task done only once at the beginning of a project.
562

AWS Well-Architected Framework
Process and culture
• Your workload has become stale, as the release process is too cumbersome to introduce minor changes
for resource efficiency.
• You do not have mechanisms to improve your workload for sustainability.
Benefits of establishing this best practice: By establishing a process to introduce and track
sustainability improvements, you will be able to continually adopt new features and capabilities, remove
issues, and improve workload efficiency.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Test and validate potential sustainability improvements before deploying them to production. Account
for the cost of testing when calculating potential future benefit of an improvement. Develop low cost
testing methods to deliver small improvements.
Implementation steps
• Add requirements for sustainability improvement to your development backlog.
• Use an iterative improvement process to identify, evaluate, prioritize, test, and deploy these
improvements.
• Continually improve and streamline your development processes. As an example, Automate your
software delivery process using continuous integration and delivery (CI/CD) pipelines to test and
deploy potential improvements to reduce the level of effort and limit errors caused by manual
processes.
• Develop and test potential improvements using the minimum viable representative components to
reduce the cost of testing.
• Continually assess the impact of improvements and make adjustment as needed.
Resources
Related documents:
• AWS enables sustainability solutions
• Scalable agile development practices based on AWS CodeCommit
Related videos:
• Delivering sustainable, high-performing architectures
Related examples:
• Well-Architected Lab - Turning cost & usage reports into efficiency reports
SUS06-BP02 Keep your workload up-to-date
Keep your workload up-to-date to adopt efficient features, remove issues, and improve the overall
efficiency of your workload.
Common anti-patterns:
• You assume your current architecture is static and will not be updated over time.
• You do not have any systems or a regular cadence to evaluate if updated software and packages are
compatible with your workload.
563

AWS Well-Architected Framework
Process and culture
Benefits of establishing this best practice: By establishing a process to keep your workload up to date,
you can adopt new features and capabilities, resolve issues, and improve workload efficiency.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Up to date operating systems, runtimes, middlewares, libraries, and applications can improve workload
efficiency and make it easier to adopt more efficient technologies. Up to date software might also
include features to measure the sustainability impact of your workload more accurately, as vendors
deliver features to meet their own sustainability goals. Adopt a regular cadence to keep your workload
up to date with the latest features and releases.
Implementation steps
•                                                                                                          Define a process and a schedule to evaluate new features or instances for your workload. Take
advantage of agility in the cloud to quickly test how new features can improve your workload to:
• Reduce sustainability impacts.
• Gain performance efficiencies.
• Remove barriers for a planned improvement.
• Improve your ability to measure and manage sustainability impacts.
•                                                                                                          Inventory your workload software and architecture and identify components that need to be updated.
• You can use AWS Systems Manager Inventory to collect operating system (OS), application, and
instance metadata from your Amazon EC2 instances and quickly understand which instances are
running the software and configurations required by your software policy and which instances need
to be updated.
•                                                                                                          Understand how to update the components of your workload.
Workload component                                                                                         How to update
Machine images                                                                                             Use EC2 Image Builder to manage updates to
Amazon Machine Images (AMIs) for Linux or
Windows server images.
Container images                                                                                           Use Amazon Elastic Container Registry (Amazon
ECR) with your existing pipeline to manage
Amazon Elastic Container Service (Amazon ECS)
images.
AWS Lambda                                                                                                 AWS Lambda includes version management
features.
•                                                                                                          Use automation for the update process to reduce the level of effort to deploy new features and limit
errors caused by manual processes.
• You can use CI/CD to automatically update AMIs, container images, and other artifacts related to
your cloud application.
• You can use tools such as AWS Systems Manager Patch Manager to automate the process of system
updates, and schedule the activity using AWS Systems Manager Maintenance Windows.
Resources
Related documents:
• AWS Architecture Center
• What's New with AWS
564

AWS Well-Architected Framework
Process and culture
• AWS Developer Tools
Related examples:
• Well-Architected Labs - Inventory and Patch Management
• Lab: AWS Systems Manager
SUS06-BP03 Increase utilization of build environments
Increase the utilization of resources to develop, test, and build your workloads.
Common anti-patterns:
• You manually provision or terminate your build environments.
• You keep your build environments running independent of test, build, or release activities (for
example, running an environment outside of the working hours of your development team members).
• You over-provision resources for your build environments.
Benefits of establishing this best practice: By increasing the utilization of build environments, you
can improve the overall efficiency of your cloud workload while allocating the resources to builders to
develop, test, and build efficiently.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Use automation and infrastructure-as-code to bring build environments up when needed and take them
down when not used. A common pattern is to schedule periods of availability that coincide with the
working hours of your development team members. Your test environments should closely resemble
the production configuration. However, look for opportunities to use instance types with burst capacity,
Amazon EC2 Spot Instances, automatic scaling database services, containers, and serverless technologies
to align development and test capacity with use. Limit data volume to just meet the test requirements. If
using production data in test, explore possibilities of sharing data from production and not moving data
across.
Implementation steps
• Use infrastructure-as-code to provision your build environments.
• Use automation to manage the lifecycle of your development and test environments and maximize the
efficiency of your build resources.
• Use strategies to maximize the utilization of development and test environments.
• Use minimum viable representative environments to develop and test potential improvements.
• Use serverless technologies if possible.
• Use On-Demand Instances to supplement your developer devices.
• Use instance types with burst capacity, Spot Instances, and other technologies to align build capacity
with use.
• Adopt native cloud services for secure instance shell access rather than deploying fleets of bastion
hosts.
• Automatically scale your build resources depending on your build jobs.
Resources
Related documents:
565

AWS Well-Architected Framework
Process and culture
• AWS Systems Manager Session Manager
• Amazon EC2 Burstable performance instances
• What is AWS CloudFormation?
• What is AWS CodeBuild?
• Instance Scheduler on AWS
Related videos:
• Continuous Integration Best Practices
SUS06-BP04 Use managed device farms for testing
Use managed device farms to efficiently test a new feature on a representative set of hardware.
Common anti-patterns:
• You manually test and deploy your application on individual physical devices.
• You do not use app testing service to test and interact with your apps (for example, Android, iOS, and
web apps) on real, physical devices.
Benefits of establishing this best practice: Using managed device farms for testing cloud-enabled
applications provides a number of benefits:
• They include more efficient features to test application on wide range of devices.
• They eliminate the need for in-house infrastructure for testing.
• They offer diverse device types, including older and less popular hardware, which eliminates the need
for unnecessary device upgrades.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Using Managed device farms can help you to streamline the testing process for new features on a
representative set of hardware. Managed device farms offer diverse device types including older, less
popular hardware, and avoid customer sustainability impact from unnecessary device upgrades.
Implementation steps
• Define your testing requirements and plan (like test type, operating systems, and test schedule).
• You can use Amazon CloudWatch RUM to collect and analyze client-side data and shape your testing
plan.
• Select the managed device farm that can support your testing requirements. For example, you can
use AWS Device Farm to test and understand the impact of your changes on a representative set of
hardware.
• Use continuous integration/continuous deployment (CI/CD) to schedule and run your tests.
• Integrating AWS Device Farm with your CI/CD pipeline to run cross-browser Selenium tests
• Building and testing iOS and iPadOS apps with AWS DevOps and mobile services
• Continually review your testing results and make necessary improvements.
Resources
Related documents:
566

AWS Well-Architected Framework
Process and culture
• AWS Device Farm device list
• Viewing the CloudWatch RUM dashboard
Related examples:
• AWS Device Farm Sample App for Android
• AWS Device Farm Sample App for iOS
• Appium Web tests for AWS Device Farm
Related videos:
• Optimize applications through end user insights with Amazon CloudWatch RUM
567

AWS Well-Architected Framework
Notices
Customers are responsible for making their own independent assessment of the information in this
document. This document: (a) is for informational purposes only, (b) represents current AWS product
offerings and practices, which are subject to change without notice, and (c) does not create any
commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services
are provided “as is” without warranties, representations, or conditions of any kind, whether express or
implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements,
and this document is not part of, nor does it modify, any agreement between AWS and its customers.
Copyright © 2023 Amazon Web Services, Inc. or its affiliates.
568

AWS Well-Architected Framework
AWS glossary
For the latest AWS terminology, see the AWS glossary in the AWS Glossary Reference.
569
