AWS Well-Architected Framework
Failure management
• Not treating your experiments as code or maintaining them through the development cycle.
• Not running chaos experiments both as part of your CI/CD pipeline, as well as outside of deployments.
• Neglecting to use past post-incident analyses when determining which faults to experiment with.
Benefits of establishing this best practice: Injecting faults to verify the resilience of your workload
allows you to gain confidence that the recovery procedures of your resilient design will work in the case
of a real fault.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Chaos engineering provides your teams with capabilities to continually inject real world disruptions
(simulations) in a controlled way at the service provider, infrastructure, workload, and component level,
with minimal to no impact to your customers. It allows your teams to learn from faults and observe,
measure, and improve the resilience of your workloads, as well as validate that alerts fire and teams get
notified in the case of an event.
When performed continually, chaos engineering can highlight deficiencies in your workloads that, if left
unaddressed, could negatively affect availability and operation.
Note
Chaos engineering is the discipline of experimenting on a system in order to build confidence
in the system’s capability to withstand turbulent conditions in production. - Principles of Chaos
Engineering
If a system is able to withstand these disruptions, the chaos experiment should be maintained as an
automated regression test. In this way, chaos experiments should be performed as part of your systems
development lifecycle (SDLC) and as part of your CI/CD pipeline.
To ensure that your workload can survive component failure, inject real world events as part of your
experiments. For example, experiment with the loss of Amazon EC2 instances or failover of the primary
Amazon RDS database instance, and verify that your workload is not impacted (or only minimally
impacted). Use a combination of component faults to simulate events that may be caused by a
disruption in an Availability Zone.
For application-level faults (such as crashes), you can start with stressors such as memory and CPU
exhaustion.
To validate fallback or failover mechanisms for external dependencies due to intermittent network
disruptions, your components should simulate such an event by blocking access to the third-party
providers for a specified duration that can last from seconds to hours.
Other modes of degradation might cause reduced functionality and slow responses, often resulting in a
disruption of your services. Common sources of this degradation are increased latency on critical services
and unreliable network communication (dropped packets). Experiments with these faults, including
networking effects such as latency, dropped messages, and DNS failures, could include the inability to
resolve a name, reach the DNS service, or establish connections to dependent services.
Chaos engineering tools:
AWS Fault Injection Simulator (AWS FIS) is a fully managed service for running fault injection
experiments that can be used as part of your CD pipeline, or outside of the pipeline. AWS FIS is a good
choice to use during chaos engineering game days. It supports simultaneously introducing faults across
different types of resources including Amazon EC2, Amazon Elastic Container Service (Amazon ECS),
Amazon Elastic Kubernetes Service (Amazon EKS), and Amazon RDS. These faults include termination
of resources, forcing failovers, stressing CPU or memory, throttling, latency, and packet loss. Since it is
integrated with Amazon CloudWatch Alarms, you can set up stop conditions as guardrails to rollback an
experiment if it causes unexpected impact.
354

AWS Well-Architected Framework
Failure management
AWS Fault Injection Simulator integrates with AWS resources to allow you to run fault injection
experiments for your workloads.
There are also several third-party options for fault injection experiments. These include open-source
tools such as Chaos Toolkit, Chaos Mesh, and Litmus Chaos, as well as commercial options like Gremlin.
To expand the scope of faults that can be injected on AWS, AWS FIS integrates with Chaos Mesh and
Litmus Chaos, allowing you to coordinate fault injection workflows among multiple tools. For example,
you can run a stress test on a pod’s CPU using Chaos Mesh or Litmus faults while terminating a randomly
selected percentage of cluster nodes using AWS FIS fault actions.
Implementation steps
1. Determine which faults to use for experiments.
Assess the design of your workload for resiliency. Such designs (created using the best practices
of the Well-Architected Framework) account for risks based on critical dependencies, past events,
known issues, and compliance requirements. List each element of the design intended to maintain
resilience and the faults it is designed to mitigate. For more information about creating such lists, see
the Operational Readiness Review whitepaper which guides you on how to create a process to prevent
reoccurrence of previous incidents. The Failure Modes and Effects Analysis (FMEA) process provides
you with a framework for performing a component-level analysis of failures and how they impact
your workload. FMEA is outlined in more detail by Adrian Cockcroft in Failure Modes and Continuous
Resilience.
2. Assign a priority to each fault.
Start with a coarse categorization such as high, medium, or low. To assess priority, consider frequency
of the fault and impact of failure to the overall workload.
When considering frequency of a given fault, analyze past data for this workload when available. If
not available, use data from other workloads running in a similar environment.
When considering impact of a given fault, the larger the scope of the fault, generally the larger the
impact. Also consider the workload design and purpose. For example, the ability to access the source
data stores is critical for a workload doing data transformation and analysis. In this case, you would
prioritize experiments for access faults, as well as throttled access and latency insertion.
Post-incident analyses are a good source of data to understand both frequency and impact of failure
modes.
355

AWS Well-Architected Framework
Failure management
Use the assigned priority to determine which faults to experiment with first and the order with which
to develop new fault injection experiments.
3. For each experiment that you perform, follow the chaos engineering and continuous resilience
flywheel in the following figure.
Chaos engineering and continuous resilience flywheel, using the scientific method by Adrian Hornsby.
a. Define steady state as some measurable output of a workload that indicates normal behavior.
Your workload exhibits steady state if it is operating reliably and as expected. Therefore, validate
that your workload is healthy before defining steady state. Steady state does not necessarily mean
no impact to the workload when a fault occurs, as a certain percentage in faults could be within
acceptable limits. The steady state is your baseline that you will observe during the experiment,
which will highlight anomalies if your hypothesis defined in the next step does not turn out as
expected.
For example, a steady state of a payments system can be defined as the processing of 300 TPS with
a success rate of 99% and round-trip time of 500 ms.
b. Form a hypothesis about how the workload will react to the fault.
A good hypothesis is based on how the workload is expected to mitigate the fault to maintain the
steady state. The hypothesis states that given the fault of a specific type, the system or workload
356

AWS Well-Architected Framework
Failure management
will continue steady state, because the workload was designed with specific mitigations. The
specific type of fault and mitigations should be specified in the hypothesis.
The following template can be used for the hypothesis (but other wording is also acceptable):
Note
If specific fault occurs, the workload name workload will describe mitigating
controls to maintain business or technical metric impact.
For example:
• If 20% of the nodes in the Amazon EKS node-group are taken down, the Transaction Create API
continues to serve the 99th percentile of requests in under 100 ms (steady state). The Amazon
EKS nodes will recover within five minutes, and pods will get scheduled and process traffic within
eight minutes after the initiation of the experiment. Alerts will fire within three minutes.
• If a single Amazon EC2 instance failure occurs, the order system’s Elastic Load Balancing health
check will cause the Elastic Load Balancing to only send requests to the remaining healthy
instances while the Amazon EC2 Auto Scaling replaces the failed instance, maintaining a less than
0.01% increase in server-side (5xx) errors (steady state).
• If the primary Amazon RDS database instance fails, the Supply Chain data collection workload
will failover and connect to the standby Amazon RDS database instance to maintain less than 1
minute of database read or write errors (steady state).
c.                                                                                                    Run the experiment by injecting the fault.
An experiment should by default be fail-safe and tolerated by the workload. If you know that the
workload will fail, do not run the experiment. Chaos engineering should be used to find known-
unknowns or unknown-unknowns. Known-unknowns are things you are aware of but don’t fully
understand, and unknown-unknowns are things you are neither aware of nor fully understand.
Experimenting against a workload that you know is broken won’t provide you with new insights.
Your experiment should be carefully planned, have a clear scope of impact, and provide a rollback
mechanism that can be applied in case of unexpected turbulence. If your due-diligence shows
that your workload should survive the experiment, move forward with the experiment. There are
several options for injecting the faults. For workloads on AWS, AWS FIS provides many predefined
fault simulations called actions. You can also define custom actions that run in AWS FIS using AWS
Systems Manager documents.
We discourage the use of custom scripts for chaos experiments, unless the scripts have the
capabilities to understand the current state of the workload, are able to emit logs, and provide
mechanisms for rollbacks and stop conditions where possible.
An effective framework or toolset which supports chaos engineering should track the current
state of an experiment, emit logs, and provide rollback mechanisms to support the controlled
running of an experiment. Start with an established service like AWS FIS that allows you to perform
experiments with a clearly defined scope and safety mechanisms that rollback the experiment if
the experiment introduces unexpected turbulence. To learn about a wider variety of experiments
using AWS FIS, also see the Resilient and Well-Architected Apps with Chaos Engineering lab. Also,
AWS Resilience Hub will analyze your workload and create experiments that you can choose to
implement and run in AWS FIS.
Note
For every experiment, clearly understand the scope and its impact. We recommend that
faults should be simulated first on a non-production environment before being run in
production.
Experiments should run in production under real-world load using canary deployments that spin
up both a control and experimental system deployment, where feasible. Running experiments
during off-peak times is a good practice to mitigate potential impact when first experimenting
in production. Also, if using actual customer traffic poses too much risk, you can run experiments
using synthetic traffic on production infrastructure against the control and experimental
357

AWS Well-Architected Framework
Failure management
deployments. When using production is not possible, run experiments in pre-production
environments that are as close to production as possible.
You must establish and monitor guardrails to ensure the experiment does not impact production
traffic or other systems beyond acceptable limits. Establish stop conditions to stop an experiment
if it reaches a threshold on a guardrail metric that you define. This should include the metrics
for steady state for the workload, as well as the metric against the components into which
you’re injecting the fault. A synthetic monitor (also known as a user canary) is one metric you
should usually include as a user proxy. Stop conditions for AWS FIS are supported as part of the
experiment template, allowing up to five stop-conditions per template.
One of the principles of chaos is minimize the scope of the experiment and its impact:
While there must be an allowance for some short-term negative impact, it is the responsibility
and obligation of the Chaos Engineer to ensure the fallout from experiments are minimized and
contained.
A method to verify the scope and potential impact is to perform the experiment in a non-
production environment first, verifying that thresholds for stop conditions activate as expected
during an experiment and observability is in place to catch an exception, instead of directly
experimenting in production.
When running fault injection experiments, verify that all responsible parties are well-informed.
Communicate with appropriate teams such as the operations teams, service reliability teams, and
customer support to let them know when experiments will be run and what to expect. Give these
teams communication tools to inform those running the experiment if they see any adverse effects.
You must restore the workload and its underlying systems back to the original known-good
state. Often, the resilient design of the workload will self-heal. But some fault designs or failed
experiments can leave your workload in an unexpected failed state. By the end of the experiment,
you must be aware of this and restore the workload and systems. With AWS FIS you can set a
rollback configuration (also called a post action) within the action parameters. A post action returns
the target to the state that it was in before the action was run. Whether automated (such as using
AWS FIS) or manual, these post actions should be part of a playbook that describes how to detect
and handle failures.
d. Verify the hypothesis.
Principles of Chaos Engineering gives this guidance on how to verify steady state of your workload:
Focus on the measurable output of a system, rather than internal attributes of the system.
Measurements of that output over a short period of time constitute a proxy for the system’s steady
state. The overall system’s throughput, error rates, and latency percentiles could all be metrics
of interest representing steady state behavior. By focusing on systemic behavior patterns during
experiments, chaos engineering verifies that the system does work, rather than trying to validate
how it works.
In our two previous examples, we include the steady state metrics of less than 0.01% increase in
server-side (5xx) errors and less than one minute of database read and write errors.
The 5xx errors are a good metric because they are a consequence of the failure mode that a client
of the workload will experience directly. The database errors measurement is good as a direct
consequence of the fault, but should also be supplemented with a client impact measurement such
as failed customer requests or errors surfaced to the client. Additionally, include a synthetic monitor
(also known as a user canary) on any APIs or URIs directly accessed by the client of your workload.
e.                                                                                                        Improve the workload design for resilience.
If steady state was not maintained, then investigate how the workload design can be improved
to mitigate the fault, applying the best practices of the AWS Well-Architected Reliability pillar.
358

AWS Well-Architected Framework
Failure management
Additional guidance and resources can be found in the AWS Builder’s Library, which hosts articles
about how to improve your health checks or employ retries with backoff in your application code,
among others.
After these changes have been implemented, run the experiment again (shown by the dotted line
in the chaos engineering flywheel) to determine their effectiveness. If the verify step indicates the
hypothesis holds true, then the workload will be in steady state, and the cycle continues.
4. Run experiments regularly.
A chaos experiment is a cycle, and experiments should be run regularly as part of chaos engineering.
After a workload meets the experiment’s hypothesis, the experiment should be automated to run
continually as a regression part of your CI/CD pipeline. To learn how to do this, see this blog on how
to run AWS FIS experiments using AWS CodePipeline. This lab on recurrent AWS FIS experiments in a
CI/CD pipeline allows you to work hands-on.
Fault injection experiments are also a part of game days (see REL12-BP06 Conduct game days
regularly (p. 360)). Game days simulate a failure or event to verify systems, processes, and team
responses. The purpose is to actually perform the actions the team would perform as if an exceptional
event happened.
5. Capture and store experiment results.
Results for fault injection experiments must be captured and persisted. Include all necessary data
(such as time, workload, and conditions) to be able to later analyze experiment results and trends.
Examples of results might include screenshots of dashboards, CSV dumps from your metric’s database,
or a hand-typed record of events and observations from the experiment. Experiment logging with
AWS FIS can be part of this data capture.
Resources
Related best practices:
• REL08-BP03 Integrate resiliency testing as part of your deployment (p. 315)
• REL13-BP03 Test disaster recovery implementation to validate the implementation (p. 375)
Related documents:
• What is AWS Fault Injection Simulator?
• What is AWS Resilience Hub?
• Principles of Chaos Engineering
• Chaos Engineering: Planning your first experiment
• Resilience Engineering: Learning to Embrace Failure
• Chaos Engineering stories
• Avoiding fallback in distributed systems
• Canary Deployment for Chaos Experiments
Related videos:
• AWS re:Invent 2020: Testing resiliency using chaos engineering (ARC316)
• AWS re:Invent 2019: Improving resiliency with chaos engineering (DOP309-R1)
• AWS re:Invent 2019: Performing chaos engineering in a serverless world (CMY301)
Related examples:
359

AWS Well-Architected Framework
Failure management
• Well-Architected lab: Level 300: Testing for Resiliency of Amazon EC2, Amazon RDS, and Amazon S3
• Chaos Engineering on AWS lab
• Resilient and Well-Architected Apps with Chaos Engineering lab
• Serverless Chaos lab
• Measure and Improve Your Application Resilience with AWS Resilience Hub lab
Related tools:
• AWS Fault Injection Simulator
• AWS Marketplace: Gremlin Chaos Engineering Platform
• Chaos Toolkit
• Chaos Mesh
• Litmus
REL12-BP06 Conduct game days regularly
Use game days to regularly exercise your procedures for responding to events and failures as close to
production as possible (including in production environments) with the people who will be involved in
actual failure scenarios. Game days enforce measures to ensure that production events do not impact
users.
Game days simulate a failure or event to test systems, processes, and team responses. The purpose is to
actually perform the actions the team would perform as if an exceptional event happened. This will help
you understand where improvements can be made and can help develop organizational experience in
dealing with events. These should be conducted regularly so that your team builds muscle memory on
how to respond.
After your design for resiliency is in place and has been tested in non-production environments, a game
day is the way to ensure that everything works as planned in production. A game day, especially the
first one, is an “all hands on deck” activity where engineers and operations are all informed when it
will happen, and what will occur. Runbooks are in place. Simulated events are run, including possible
failure events, in the production systems in the prescribed manner, and impact is assessed. If all systems
operate as designed, detection and self-healing will occur with little to no impact. However, if negative
impact is observed, the test is rolled back and the workload issues are remedied, manually if necessary
(using the runbook). Since game days often take place in production, all precautions should be taken to
ensure that there is no impact on availability to your customers.
Common anti-patterns:
• Documenting your procedures, but never exercising them.
• Not including business decision makers in the test exercises.
Benefits of establishing this best practice: Conducting game days regularly ensures that all staff
follows the policies and procedures when an actual incident occurs, and validates that those policies and
procedures are appropriate.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Schedule game days to regularly exercise your runbooks and playbooks. Game days should involve
everyone who would be involved in a production event: business owner, development staff,
operational staff, and incident response teams.
• Run your load or performance tests and then run your failure injection.
360

AWS Well-Architected Framework
Failure management
• Look for anomalies in your runbooks and opportunities to exercise your playbooks.
• If you deviate from your runbooks, refine the runbook or correct the behavior. If you exercise your
playbook, identify the runbook that should have been used, or create a new one.
Resources
Related documents:
• What is AWS GameDay?
Related videos:
• AWS re:Invent 2019: Improving resiliency with chaos engineering (DOP309-R1)
Related examples:
• AWS Well-Architected Labs - Testing Resiliency
REL 13. How do you plan for disaster recovery (DR)?
Having backups and redundant workload components in place is the start of your DR strategy. RTO and
RPO are your objectives for restoration of your workload. Set these based on business needs. Implement
a strategy to meet these objectives, considering locations and function of workload resources and data.
The probability of disruption and cost of recovery are also key factors that help to inform the business
value of providing disaster recovery for a workload.
Best practices
• REL13-BP01 Define recovery objectives for downtime and data loss (p. 361)
• REL13-BP02 Use defined recovery strategies to meet the recovery objectives (p. 365)
• REL13-BP03 Test disaster recovery implementation to validate the implementation (p. 375)
• REL13-BP04 Manage configuration drift at the DR site or Region (p. 376)
• REL13-BP05 Automate recovery (p. 377)
REL13-BP01 Define recovery objectives for downtime and data loss
The workload has a recovery time objective (RTO) and recovery point objective (RPO).
Recovery Time Objective (RTO) is the maximum acceptable delay between the interruption of service and
restoration of service. This determines what is considered an acceptable time window when service is
unavailable.
Recovery Point Objective (RPO)  is the maximum acceptable amount of time since the last data recovery
point. This determines what is considered an acceptable loss of data between the last recovery point and
the interruption of service.
RTO and RPO values are important considerations when selecting an appropriate Disaster Recovery (DR)
strategy for your workload. These objectives are determined by the business, and then used by technical
teams to select and implement a DR strategy.
Desired Outcome:
Every workload has an assigned RTO and RPO, defined based on business impact. The workload is
assigned to a predefined tier, defining service availability and acceptable loss of data, with an associated
RTO and RPO. If such tiering is not possible then this can be assigned bespoke per workload, with the
361

AWS Well-Architected Framework
Failure management
intent to create tiers later. RTO and RPO are used as one of the primary considerations for selection of
a disaster recovery strategy implementation for the workload. Additional considerations in picking a DR
strategy are cost constraints, workload dependencies, and operational requirements.
For RTO, understand impact based on duration of an outage. Is it linear, or are there nonlinear
implications? (for example. after four hours, you shut down a manufacturing line until the start of the
next shift).
A disaster recovery matrix, like the following, can help you understand how workload criticality relates
to recovery objectives. (Note that the actual values for the X and Y axes should be customized to your
organization needs).
Figure 16: Disaster recovery matrix
Common anti-patterns:
• No defined recovery objectives.
• Selecting arbitrary recovery objectives.
• Selecting recovery objectives that are too lenient and do not meet business objectives.
• Not understanding of the impact of downtime and data loss.
• Selecting unrealistic recovery objectives, such as zero time to recover and zero data loss, which may
not be achievable for your workload configuration.
• Selecting recovery objectives more stringent than actual business objectives. This forces DR
implementations that are costlier and more complicated than what the workload needs.
• Selecting recovery objectives incompatible with those of a dependent workload.
• Your recovery objectives do not consider regulatory compliance requirements.
• RTO and RPO defined for a workload, but never tested.
Benefits of establishing this best practice: Your recovery objectives for time and data loss are necessary
to guide your DR implementation.
Level of risk exposed if this best practice is not established: High
Implementation guidance
For the given workload, you must understand the impact of downtime and lost data on your business.
The impact generally grows larger with greater downtime or data loss, but the shape of this growth
can differ based on the workload type. For example, you may be able to tolerate downtime for up to an
362

AWS Well-Architected Framework
Failure management
hour with little impact, but after that impact quickly rises. Impact to business manifests in many forms
including monetary cost (such as lost revenue), customer trust (and impact to reputation), operational
issues (such as missing payroll or decreased productivity), and regulatory risk. Use the following steps to
understand these impacts, and set RTO and RPO for your workload.
Implementation Steps
1. Determine your business stakeholders for this workload, and engage with them to implement these
steps. Recovery objectives for a workload are a business decision. Technical teams then work with
business stakeholders to use these objectives to select a DR strategy.
Note
For steps 2 and 3, you can use the the section called “Implementation worksheet” (p. 364).
2. Gather the necessary information to make a decision by answering the questions below.
3. Do you have categories or tiers of criticality for workload impact in your organization?
a. If yes, assign this workload to a category
b. If no, then establish these categories. Create five or fewer categories and refine the range of your
recovery time objective for each one. Example categories include: critical, high, medium, low. To
understand how workloads map to categories, consider whether the workload is mission critical,
business important, or non-business driving.
c. Set workload RTO and RPO based on category. Always choose a category more strict (lower RTO
and RPO) than the raw values calculated entering this step. If this results in an unsuitably large
change in value, then consider creating a new category.
4. Based on these answers, assign RTO and RPO values to the workload. This can be done directly, or by
assigning the workload to a predefined tier of service.
5. Document the disaster recovery plan (DRP) for this workload, which is a part of your organization’s
business continuity plan (BCP), in a location accessible to the workload team and stakeholders
a. Record the RTO and RPO, and the information used to determine these values. Include the strategy
used for evaluating workload impact to the business
b. Record other metrics besides RTO and RPO are you tracking or plan to track for disaster recovery
objectives
c. You will add details of your DR strategy and runbook to this plan when you create these.
6. By looking up the workload criticality in a matrix such as that in Figure 15, you can begin to establish
predefined tiers of service defined for your organization.
7. After you have implemented a DR strategy (or a proof of concept for a DR strategy) as per the section
called “REL13-BP02 Use defined recovery strategies to meet the recovery objectives” (p. 365), test
this strategy to determine workload actual RTC (Recovery Time Capability) and RPC (Recovery Point
Capability). If these do not meet the target recovery objectives, then either work with your business
stakeholders to adjust those objectives, or make changes to the DR strategy is possible to meet target
objectives.
Primary questions
1. What is the maximum time the workload can be down before severe impact to the business is incurred
a. Determine the monetary cost (direct financial impact) to the business per minute if workload is
disrupted.
b. Consider that impact is not always linear. Impact can be limited at first, and then increase rapidly
past a critical point in time.
2. What is the maximum amount of data that can be lost before severe impact to the business is incurred
a. Consider this value for your most critical data store. Identify the respective criticality for other data
stores.
b. Can workload data be recreated if lost? If this is operationally easier than backup and restore, then
choose RPO based on the criticality of the source data used to recreate the workload data.
363

AWS Well-Architected Framework
Failure management
3. What are the recovery objectives and availability expectations of workloads that this one depends on
(downstream), or workloads that depend on this one (upstream)?
a. Choose recovery objectives that allow this workload to meet the requirements of upstream
dependencies
b. Choose recovery objectives that are achievable given the recovery capabilities of downstream
dependencies. Non-critical downstream dependencies (ones you can “work around”) can be
excluded. Or, work with critical downstream dependencies to improve their recovery capabilities
where necessary.
Additional questions
Consider these questions, and how they may apply to this workload:
4. Do you have different RTO and RPO depending on the type of outage (Region vs. AZ, etc.)?
5. Is there a specific time (seasonality, sales events, product launches) when your RTO/RPO may change?
If so, what is the different measurement and time boundary?
6. How many customers will be impacted if workload is disrupted?
7. What is the impact to reputation if workload is disrupted?
8. What other operational impacts may occur if workload is disrupted? For example, impact to employee
productivity if email systems are unavailable, or if Payroll systems are unable to submit transactions.
9. How does workload RTO and RPO align with Line of Business and Organizational DR Strategy?
10Are there internal contractual obligations for providing a service? Are there penalties for not meeting
them?
11What are the regulatory or compliance constraints with the data?
Implementation worksheet
You can use this worksheet for implementation steps 2 and 3. You may adjust this worksheet to suit your
specific needs, such as adding additional questions.
Worksheet
364

AWS Well-Architected Framework
Failure management
Level of effort for the Implementation Plan: Low
Resources
Related Best Practices:
• the section called “REL09-BP04 Perform periodic recovery of the data to verify backup integrity and
processes” (p. 325)
• the section called “REL13-BP02 Use defined recovery strategies to meet the recovery
objectives” (p. 365)
• the section called “REL13-BP03 Test disaster recovery implementation to validate the
implementation” (p. 375)
Related documents:
• AWS Architecture Blog: Disaster Recovery Series
• Disaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)
• Managing resiliency policies with AWS Resilience Hub
• APN Partner: partners that can help with disaster recovery
• AWS Marketplace: products that can be used for disaster recovery
Related videos:
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)
• Disaster Recovery of Workloads on AWS
REL13-BP02 Use defined recovery strategies to meet the recovery objectives
Define a disaster recovery (DR) strategy that meets your workload's recovery objectives. Choose a
strategy such as backup and restore, standby (active/passive), or active/active.
Desired outcome: For each workload, there is a defined and implemented DR strategy that allows the
workload to achieve DR objectives. DR strategies between workloads make use of reusable patterns (such
as the strategies previously described),
Common anti-patterns:
• Implementing inconsistent recovery procedures for workloads with similar DR objectives.
• Leaving the DR strategy to be implemented ad-hoc when a disaster occurs.
• Having no plan for disaster recovery.
• Dependency on control plane operations during recovery.
Benefits of establishing this best practice:
• Using defined recovery strategies allows you to use common tooling and test procedures.
• Using defined recovery strategies improves knowledge sharing between teams and implementation of
DR on the workloads they own.
Level of risk exposed if this best practice is not established: High. Without a planned, implemented,
and tested DR strategy, you are unlikely to achieve recovery objectives in the event of a disaster.
365

AWS Well-Architected Framework
Failure management
Implementation guidance
A DR strategy relies on the ability to stand up your workload in a recovery site if your primary location
becomes unable to run the workload. The most common recovery objectives are RTO and RPO, as
discussed in REL13-BP01 Define recovery objectives for downtime and data loss (p. 361).
A DR strategy across multiple Availability Zones (AZs) within a single AWS Region, can provide mitigation
against disaster events like fires, floods, and major power outages. If it is a requirement to implement
protection against an unlikely event that prevents your workload from being able to run in a given AWS
Region, you can use a DR strategy that uses multiple Regions.
When architecting a DR strategy across multiple Regions, you should choose one of the following
strategies. They are listed in increasing order of cost and complexity, and decreasing order of RTO and
RPO. Recovery Region refers to an AWS Region other than the primary one used for your workload.
Figure 17: Disaster recovery (DR) strategies
• Backup and restore (RPO in hours, RTO in 24 hours or less): Back up your data and applications into
the recovery Region. Using automated or continuous backups will permit point in time recovery (PITR),
which can lower RPO to as low as 5 minutes in some cases. In the event of a disaster, you will deploy
your infrastructure (using infrastructure as code to reduce RTO), deploy your code, and restore the
backed-up data to recover from a disaster in the recovery Region.
• Pilot light (RPO in minutes, RTO in tens of minutes): Provision a copy of your core workload
infrastructure in the recovery Region. Replicate your data into the recovery Region and create backups
of it there. Resources required to support data replication and backup, such as databases and object
storage, are always on. Other elements such as application servers or serverless compute are not
deployed, but can be created when needed with the necessary configuration and application code.
• Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down but fully functional version
of your workload always running in the recovery Region. Business-critical systems are fully duplicated
and are always on, but with a scaled down fleet. Data is replicated and live in the recovery Region.
When the time comes for recovery, the system is scaled up quickly to handle the production load. The
more scaled-up the warm standby is, the lower RTO and control plane reliance will be. When fully
scales this is known as hot standby.
• Multi-Region (multi-site) active-active (RPO near zero, RTO potentially zero): Your workload is
deployed to, and actively serving traffic from, multiple AWS Regions. This strategy requires you to
synchronize data across Regions. Possible conflicts caused by writes to the same record in two different
regional replicas must be avoided or handled, which can be complex. Data replication is useful for data
synchronization and will protect you against some types of disaster, but it will not protect you against
data corruption or destruction unless your solution also includes options for point-in-time recovery.
366

AWS Well-Architected Framework
Failure management
Note
The difference between pilot light and warm standby can sometimes be difficult to understand.
Both include an environment in your recovery Region with copies of your primary region assets.
The distinction is that pilot light cannot process requests without additional action taken first,
while warm standby can handle traffic (at reduced capacity levels) immediately. Pilot light will
require you to turn on servers, possibly deploy additional (non-core) infrastructure, and scale up,
while warm standby only requires you to scale up (everything is already deployed and running).
Choose between these based on your RTO and RPO needs.
When cost is a concern, and you wish to achieve a similar RPO and RTO objectives as defined in
the warm standby strategy, you could consider cloud native solutions, like AWS Elastic Disaster
Recovery, that take the pilot light approach and offer improved RPO and RTO targets.
Implementation steps
1. Determine a DR strategy that will satisfy recovery requirements for this workload.
Choosing a DR strategy is a trade-off between reducing downtime and data loss (RTO and RPO) and
the cost and complexity of implementing the strategy. You should avoid implementing a strategy that
is more stringent than it needs to be, as this incurs unnecessary costs.
For example, in the following diagram, the business has determined their maximum permissible RTO
as well as the limit of what they can spend on their service restoration strategy. Given the business’
objectives, the DR strategies pilot light or warm standby will satisfy both the RTO and the cost criteria.
Figure 18: Choosing a DR strategy based on RTO and cost
To learn more, see Business Continuity Plan (BCP).
2. Review the patterns for how the selected DR strategy can be implemented.
This step is to understand how you will implement the selected strategy. The strategies are explained
using AWS Regions as the primary and recovery sites. However, you can also choose to use Availability
Zones within a single Region as your DR strategy, which makes use of elements of multiple of these
strategies.
In the following steps, you can apply the strategy to your specific workload.
Backup and restore
367

AWS Well-Architected Framework
Failure management
Backup and restore is the least complex strategy to implement, but will require more time and effort
to restore the workload, leading to higher RTO and RPO. It is a good practice to always make backups
of your data, and copy these to another site (such as another AWS Region).
Figure 19: Backup and restore architecture
For more details on this strategy see Disaster Recovery (DR) Architecture on AWS, Part II: Backup and
Restore with Rapid Recovery.
Pilot light
With the pilot light approach, you replicate your data from your primary Region to your recovery
Region. Core resources used for the workload infrastructure are deployed in the recovery Region,
however additional resources and any dependencies are still needed to make this a functional stack.
For example, in Figure 20, no compute instances are deployed.
Figure 20: Pilot light architecture
368

AWS Well-Architected Framework
Failure management
For more details on this strategy, see Disaster Recovery (DR) Architecture on AWS, Part III: Pilot Light
and Warm Standby.
Warm standby
The warm standby approach involves ensuring that there is a scaled down, but fully functional, copy
of your production environment in another Region. This approach extends the pilot light concept and
decreases the time to recovery because your workload is always-on in another Region. If the recovery
Region is deployed at full capacity, then this is known as hot standby.
Figure 21: Warm standby architecture
Using warm standby or pilot light requires scaling up resources in the recovery Region. To verify
capacity is available when needed, consider the use for capacity reservations for EC2 instances. If
using AWS Lambda, then provisioned concurrency can provide runtime environments so that they are
prepared to respond immediately to your function's invocations.
For more details on this strategy, see Disaster Recovery (DR) Architecture on AWS, Part III: Pilot Light
and Warm Standby.
Multi-site active/active
You can run your workload simultaneously in multiple Regions as part of a multi-site active/
active strategy. Multi-site active/active serves traffic from all regions to which it is deployed.
Customers may select this strategy for reasons other than DR. It can be used to increase availability, or
when deploying a workload to a global audience (to put the endpoint closer to users and/or to deploy
stacks localized to the audience in that region). As a DR strategy, if the workload cannot be supported
in one of the AWS Regions to which it is deployed, then that Region is evacuated, and the remaining
Regions are used to maintain availability. Multi-site active/active is the most operationally complex of
the DR strategies, and should only be selected when business requirements necessitate it.
369

AWS Well-Architected Framework
Failure management
Figure 22: Multi-site active/active architecture
For more details on this strategy, see Disaster Recovery (DR) Architecture on AWS, Part IV: Multi-site
Active/Active.
AWS Elastic Disaster Recovery
If you are considering the pilot light or warm standby strategy for disaster recovery, AWS Elastic
Disaster Recovery could provide an alternative approach with improved benefits. Elastic Disaster
Recovery can offer an RPO and RTO target similar to warm standby, but maintain the low-cost
approach of pilot light. Elastic Disaster Recovery replicates your data from your primary region to your
recovery Region, using continual data protection to achieve an RPO measured in seconds and an RTO
that can be measured in minutes. Only the resources required to replicate the data are deployed in the
recovery region, which keeps costs down, similar to the pilot light strategy. When using Elastic Disaster
Recovery, the service coordinates and orchestrates the recovery of compute resources when initiated
as part of failover or drill.
370

AWS Well-Architected Framework
Failure management
Figure 23: AWS Elastic Disaster Recovery architecture
Additional practices for protecting data
With all strategies, you must also mitigate against a data disaster. Continuous data replication
protects you against some types of disaster, but it may not protect you against data corruption or
destruction unless your strategy also includes versioning of stored data or options for point-in-
time recovery. You must also back up the replicated data in the recovery site to create point-in-time
backups in addition to the replicas.
Using multiple Availability Zones (AZs) within a single AWS Region
When using multiple AZs within a single Region, your DR implementation uses multiple elements of
the above strategies. First you must create a high-availability (HA) architecture, using multiple AZs as
shown in Figure 23. This architecture makes use of a multi-site active/active approach, as the Amazon
EC2 instances and the Elastic Load Balancer have resources deployed in multiple AZs, actively handing
requests. The architecture also demonstrates hot standby, where if the primary Amazon RDS instance
fails (or the AZ itself fails), then the standby instance is promoted to primary.
371

AWS Well-Architected Framework
Failure management
Figure 24: Multi-AZ architecture
In addition to this HA architecture, you need to add backups of all data required to run your workload.
This is especially important for data that is constrained to a single zone such as Amazon EBS volumes
or Amazon Redshift clusters. If an AZ fails, you will need to restore this data to another AZ. Where
possible, you should also copy data backups to another AWS Region as an additional layer of
protection.
An less common alternative approach to single Region, multi-AZ DR is illustrated in the blog post,
Building highly resilient applications using Amazon Route 53 Application Recovery Controller, Part 1:
Single-Region stack. Here, the strategy is to maintain as much isolation between the AZs as possible,
like how Regions operate. Using this alternative strategy, you can choose an active/active or active/
passive approach.
Note
Some workloads have regulatory data residency requirements. If this applies to your
workload in a locality that currently has only one AWS Region, then multi-Region will not suit
your business needs. Multi-AZ strategies provide good protection against most disasters.
3. Assess the resources of your workload, and what their configuration will be in the recovery Region
prior to failover (during normal operation).
For infrastructure and AWS resources use infrastructure as code such as AWS CloudFormation or third-
party tools like Hashicorp Terraform. To deploy across multiple accounts and Regions with a single
operation you can use AWS CloudFormation StackSets. For Multi-site active/active and Hot Standby
strategies, the deployed infrastructure in your recovery Region has the same resources as your
primary Region. For Pilot Light and Warm Standby strategies, the deployed infrastructure will require
additional actions to become production ready. Using CloudFormation parameters and conditional
logic, you can control whether a deployed stack is active or standby with a single template. When
using Elastic Disaster Recovery, the service will replicate and orchestrate the restoration of application
configurations and compute resources.
All DR strategies require that data sources are backed up within the AWS Region, and then those
backups are copied to the recovery Region. AWS Backup provides a centralized view where you can
configure, schedule, and monitor backups for these resources. For Pilot Light, Warm Standby, and
Multi-site active/active, you should also replicate data from the primary Region to data resources
in the recovery Region, such as Amazon Relational Database Service (Amazon RDS) DB instances
372

AWS Well-Architected Framework
Failure management
or Amazon DynamoDB tables. These data resources are therefore live and ready to serve requests in
the recovery Region.
To learn more about how AWS services operate across Regions, see this blog series on Creating a
Multi-Region Application with AWS Services.
4. Determine and implement how you will make your recovery Region ready for failover when
needed (during a disaster event).
For multi-site active/active, failover means evacuating a Region, and relying on the remaining active
Regions. In general, those Regions are ready to accept traffic. For Pilot Light and Warm Standby
strategies, your recovery actions will need to deploy the missing resources, such as the EC2 instances
in Figure 20, plus any other missing resources.
For all of the above strategies you may need to promote read-only instances of databases to become
the primary read/write instance.
For backup and restore, restoring data from backup creates resources for that data such as EBS
volumes, RDS DB instances, and DynamoDB tables. You also need to restore the infrastructure
and deploy code. You can use AWS Backup to restore data in the recovery Region. See REL09-
BP01 Identify and back up all data that needs to be backed up, or reproduce the data from
sources (p. 320) for more details. Rebuilding the infrastructure includes creating resources like EC2
instances in addition to the Amazon Virtual Private Cloud (Amazon VPC), subnets, and security groups
needed. You can automate much of the restoration process. To learn how, see this blog post.
5. Determine and implement how you will reroute traffic to failover when needed (during a disaster
event).
This failover operation can be initiated either automatically or manually. Automatically initiated
failover based on health checks or alarms should be used with caution since an unnecessary failover
(false alarm) incurs costs such as non-availability and data loss. Manually initiated failover is therefore
often used. In this case, you should still automate the steps for failover, so that the manual initiation is
like the push of a button.
There are several traffic management options to consider when using AWS services. One option is
to use Amazon Route 53. Using Amazon Route 53, you can associate multiple IP endpoints in one or
more AWS Regions with a Route 53 domain name. To implement manually initiated failover you can
use Amazon Route 53 Application Recovery Controller, which provides a highly available data plane
API to reroute traffic to the recovery Region. When implementing failover, use data plane operations
and avoid control plane ones as described in REL11-BP04 Rely on the data plane and not the control
plane during recovery (p. 344).
To learn more about this and other options see this section of the Disaster Recovery Whitepaper.
6. Design a plan for how your workload will fail back.
Failback is when you return workload operation to the primary Region, after a disaster event has
abated. Provisioning infrastructure and code to the primary Region generally follows the same steps
as were initially used, relying on infrastructure as code and code deployment pipelines. The challenge
with failback is restoring data stores, and ensuring their consistency with the recovery Region in
operation.
In the failed over state, the databases in the recovery Region are live and have the up-to-date data.
The goal then is to re-synchronize from the recovery Region to the primary Region, ensuring it is up-
to-date.
Some AWS services will do this automatically. If using Amazon DynamoDB global tables, even if the
table in the primary Region had become not available, when it comes back online, DynamoDB resumes
propagating any pending writes. If using Amazon Aurora Global Database and using managed planned
failover, then Aurora global database's existing replication topology is maintained. Therefore, the
373

AWS Well-Architected Framework
Failure management
former read/write instance in the primary Region will become a replica and receive updates from the
recovery Region.
In cases where this is not automatic, you will need to re-establish the database in the primary Region
as a replica of the database in the recovery Region. In many cases this will involve deleting the old
primary database, and creating new replicas.
After a failover, if you can continue running in your recovery Region, consider making this the new
primary Region. You would still do all the above steps to make the former primary Region into a
recovery Region. Some organizations do a scheduled rotation, swapping their primary and recovery
Regions periodically (for example every three months).
All of the steps required to fail over and fail back should be maintained in a playbook that is available
to all members of the team, and is periodically reviewed.
When using Elastic Disaster Recovery, the service will assist in orchestrating and automating the
failback process. For more details, see Performing a failback.
Level of effort for the Implementation Plan: High
Resources
Related best practices:
• the section called “REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce
the data from sources” (p. 320)
• the section called “REL11-BP04 Rely on the data plane and not the control plane during
recovery” (p. 344)
• the section called “REL13-BP01 Define recovery objectives for downtime and data loss” (p. 361)
Related documents:
• AWS Architecture Blog: Disaster Recovery Series
• Disaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)
• Disaster recovery options in the cloud
• Build a serverless multi-region, active-active backend solution in an hour
• Multi-region serverless backend — reloaded
• RDS: Replicating a Read Replica Across Regions
• Route 53: Configuring DNS Failover
• S3: Cross-Region Replication
• What Is AWS Backup?
• What is Route 53 Application Recovery Controller?
• AWS Elastic Disaster Recovery
• HashiCorp Terraform: Get Started - AWS
• APN Partner: partners that can help with disaster recovery
• AWS Marketplace: products that can be used for disaster recovery
Related videos:
• Disaster Recovery of Workloads on AWS
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)
374

AWS Well-Architected Framework
Failure management
• Get Started with AWS Elastic Disaster Recovery | Amazon Web Services
Related examples:
• Well-Architected Lab - Disaster Recovery - Series of workshops illustrating DR strategies
REL13-BP03 Test disaster recovery implementation to validate the
implementation
Regularly test failover to your recovery site to verify that it operates properly and that RTO and RPO are
met.
Common anti-patterns:
• Never exercise failovers in production.
Benefits of establishing this best practice: Regularly testing you disaster recovery plan verifies that it
will work when it needs to, and that your team knows how to perform the strategy.
Level of risk exposed if this best practice is not established: High
Implementation guidance
A pattern to avoid is developing recovery paths that are rarely exercised. For example, you might have a
secondary data store that is used for read-only queries. When you write to a data store and the primary
fails, you might want to fail over to the secondary data store. If you don’t frequently test this failover,
you might find that your assumptions about the capabilities of the secondary data store are incorrect.
The capacity of the secondary, which might have been sufficient when you last tested, might be no
longer be able to tolerate the load under this scenario. Our experience has shown that the only error
recovery that works is the path you test frequently. This is why having a small number of recovery paths
is best. You can establish recovery patterns and regularly test them. If you have a complex or critical
recovery path, you still need to regularly exercise that failure in production to convince yourself that
the recovery path works. In the example we just discussed, you should fail over to the standby regularly,
regardless of need.
Implementation steps
1. Engineer your workloads for recovery. Regularly test your recovery paths. Recovery-oriented
computing identifies the characteristics in systems that enhance recovery: isolation and redundancy,
system-wide ability to roll back changes, ability to monitor and determine health, ability to provide
diagnostics, automated recovery, modular design, and ability to restart. Exercise the recovery path
to verify that you can accomplish the recovery in the specified time to the specified state. Use your
runbooks during this recovery to document problems and find solutions for them before the next test.
2. For Amazon EC2-based workloads, use AWS Elastic Disaster Recovery to implement and launch drill
instances for your DR strategy. AWS Elastic Disaster Recovery provides the ability to efficiently run
drills, which helps you prepare for a failover event. You can also frequently launch of your instances
using Elastic Disaster Recovery for test and drill purposes without redirecting the traffic.
Resources
Related documents:
• APN Partner: partners that can help with disaster recovery
• AWS Architecture Blog: Disaster Recovery Series
• AWS Marketplace: products that can be used for disaster recovery
375

AWS Well-Architected Framework
Failure management
• AWS Elastic Disaster Recovery
• Disaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)
• AWS Elastic Disaster Recovery Preparing for Failover
• The Berkeley/Stanford recovery-oriented computing project
• What is AWS Fault Injection Simulator?
Related videos:
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications
• AWS re:Invent 2019: Backup-and-restore and disaster-recovery solutions with AWS
Related examples:
• Well-Architected Lab - Testing for Resiliency
REL13-BP04 Manage configuration drift at the DR site or Region
Ensure that the infrastructure, data, and configuration are as needed at the DR site or Region. For
example, check that AMIs and service quotas are up to date.
AWS Config continuously monitors and records your AWS resource configurations. It can detect drift
and invoke AWS Systems Manager Automation to fix it and raise alarms. AWS CloudFormation can
additionally detect drift in stacks you have deployed.
Common anti-patterns:
• Failing to make updates in your recovery locations, when you make configuration or infrastructure
changes in your primary locations.
• Not considering potential limitations (like service differences) in your primary and recovery locations.
Benefits of establishing this best practice: Ensuring that your DR environment is consistent with your
existing environment guarantees complete recovery.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Ensure that your delivery pipelines deliver to both your primary and backup sites. Delivery pipelines
for deploying applications into production must distribute to all the specified disaster recovery
strategy locations, including dev and test environments.
• Permit AWS Config to track potential drift locations. Use AWS Config rules to create systems that
enforce your disaster recovery strategies and generate alerts when they detect drift.
• Remediating Noncompliant AWS Resources by AWS Config Rules
• AWS Systems Manager Automation
• Use AWS CloudFormation to deploy your infrastructure. AWS CloudFormation can detect drift between
what your CloudFormation templates specify and what is actually deployed.
• AWS CloudFormation: Detect Drift on an Entire CloudFormation Stack
Resources
Related documents:
376

AWS Well-Architected Framework
Failure management
• APN Partner: partners that can help with disaster recovery
• AWS Architecture Blog: Disaster Recovery Series
• AWS CloudFormation: Detect Drift on an Entire CloudFormation Stack
• AWS Marketplace: products that can be used for disaster recovery
• AWS Systems Manager Automation
• Disaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)
• How do I implement an Infrastructure Configuration Management solution on AWS?
• Remediating Noncompliant AWS Resources by AWS Config Rules
Related videos:
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)
REL13-BP05 Automate recovery
Use AWS or third-party tools to automate system recovery and route traffic to the DR site or Region.
Based on configured health checks, AWS services, such as Elastic Load Balancing and AWS Auto Scaling,
can distribute load to healthy Availability Zones while services, such as Amazon Route 53 and AWS
Global Accelerator, can route load to healthy AWS Regions. Amazon Route 53 Application Recovery
Controller helps you manage and coordinate failover using readiness check and routing control features.
These features continually monitor your application’s ability to recover from failures, so you can control
application recovery across multiple AWS Regions, Availability Zones, and on premises.
For workloads on existing physical or virtual data centers or private clouds, AWS Elastic Disaster Recovery
allows organizations to set up an automated disaster recovery strategy in AWS. Elastic Disaster Recovery
also supports cross-Region and cross-Availability Zone disaster recovery in AWS.
Common anti-patterns:
• Implementing identical automated failover and failback can cause flapping when a failure occurs.
Benefits of establishing this best practice: Automated recovery reduces your recovery time by
eliminating the opportunity for manual errors.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
• Automate recovery paths. For short recovery times, follow your disaster recovery plan to get your IT
systems back online quickly in the case of a disruption.
• Use Elastic Disaster Recovery for automated Failover and Failback. Elastic Disaster Recovery
continuously replicates your machines (including operating system, system state configuration,
databases, applications, and files) into a low-cost staging area in your target AWS account and
preferred Region. In the case of a disaster, after choosing to recover using Elastic Disaster Recovery,
Elastic Disaster Recovery automates the conversion of your replicated servers into fully provisioned
workloads in your recovery Region on AWS.
• Using Elastic Disaster Recovery for Failover and Failback
• AWS Elastic Disaster Recovery resources
Resources
Related documents:
377

AWS Well-Architected Framework
Performance efficiency
• APN Partner: partners that can help with disaster recovery
• AWS Architecture Blog: Disaster Recovery Series
• AWS Marketplace: products that can be used for disaster recovery
• AWS Systems Manager Automation
• AWS Elastic Disaster Recovery
• Disaster Recovery of Workloads on AWS: Recovery in the Cloud (AWS Whitepaper)
Related videos:
• AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)
Performance efficiency
The Performance Efficiency pillar includes the ability to use computing resources efficiently to meet
system requirements, and to maintain that efficiency as demand changes and technologies evolve. You
can find prescriptive guidance on implementation in the Performance Efficiency Pillar whitepaper.
Best practice areas
• Selection (p. 378)
• Review (p. 437)
• Monitoring (p. 441)
• Tradeoffs (p. 448)
Selection
Questions
• PERF 1. How do you select the best performing architecture?  (p. 378)
• PERF 2. How do you select your compute solution? (p. 386)
• PERF 3. How do you select your storage solution? (p. 398)
• PERF 4. How do you select your database solution?  (p. 404)
• PERF 5. How do you configure your networking solution? (p. 420)
PERF 1. How do you select the best performing architecture?
Often, multiple approaches are required more efficient performance across a workload. Well-architected
systems use multiple solutions and features to improve performance.
Best practices
• PERF01-BP01 Understand the available services and resources (p. 379)
• PERF01-BP02 Define a process for architectural choices (p. 379)
• PERF01-BP03 Factor cost requirements into decisions (p. 380)
• PERF01-BP04 Use policies or reference architectures (p. 381)
• PERF01-BP05 Use guidance from your cloud provider or an appropriate partner (p. 382)
• PERF01-BP06 Benchmark existing workloads (p. 383)
• PERF01-BP07 Load test your workload (p. 384)
378

AWS Well-Architected Framework
Selection
PERF01-BP01 Understand the available services and resources
Learn about and understand the wide range of services and resources available in the cloud. Identify the
relevant services and configuration options for your workload, and understand how to achieve optimal
performance.
If you are evaluating an existing workload, you must generate an inventory of the various services
resources it consumes. Your inventory helps you evaluate which components can be replaced with
managed services and newer technologies.
Common anti-patterns:
• You use the cloud as a collocated data center.
• You use shared storage for all things that need persistent storage.
• You do not use automatic scaling.
• You use instance types that are closest matched, but larger where needed, to your current standards.
• You deploy and manage technologies that are available as managed services.
Benefits of establishing this best practice: By considering services you may be unfamiliar with, you may
be able to greatly reduce the cost of infrastructure and the effort required to maintain your services. You
may be able to accelerate your time to market by deploying new services and features.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Inventory your workload software and architecture for related services: Gather an inventory of your
workload and decide which category of products to learn more about. Identify workload components
that can be replaced with managed services to increase performance and reduce operational complexity.
Resources
Related documents:
• AWS Architecture Center
• AWS Partner Network
• AWS Solutions Library
• AWS Knowledge Center
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• This is my Architecture
Related examples:
• AWS Samples
• AWS SDK Examples
PERF01-BP02 Define a process for architectural choices
Use internal experience and knowledge of the cloud, or external resources such as published use cases,
relevant documentation, or whitepapers, to define a process to choose resources and services. You
379

AWS Well-Architected Framework
Selection
should define a process that encourages experimentation and benchmarking with the services that could
be used in your workload.
When you write critical user stories for your architecture, you should include performance requirements,
such as specifying how quickly each critical story should run. For these critical stories, you should
implement additional scripted user journeys to ensure that you have visibility into how these stories
perform against your requirements.
Common anti-patterns:
• You assume your current architecture will become static and not be updated over time.
• You introduce architecture changes over time without justification.
Benefits of establishing this best practice: By having a defined process for making architectural
changes, you permit using the gathered data to influence your workload design over time.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Select an architectural approach: Identify the kind of architecture that meets your performance
requirements. Identify constraints, such as the media for delivery (desktop, web, mobile, IoT), legacy
requirements, and integrations. Identify opportunities for reuse, including refactoring. Consult
other teams, architecture diagrams, and resources such as AWS Solution Architects, AWS Reference
Architectures, and AWS Partners to help you choose an architecture.
Define performance requirements: Use the customer experience to identify the most important
metrics. For each metric, identify the target, measurement approach, and priority. Define the customer
experience. Document the performance experience required by customers, including how customers will
judge the performance of the workload. Prioritize experience concerns for critical user stories. Include
performance requirements and implement scripted user journeys to ensure that you know how the
stories perform against your requirements.
Resources
Related documents:
• AWS Architecture Center
• AWS Partner Network
• AWS Solutions Library
• AWS Knowledge Center
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• This is my Architecture
Related examples:
• AWS Samples
• AWS SDK Examples
PERF01-BP03 Factor cost requirements into decisions
Workloads often have cost requirements for operation. Use internal cost controls to select resource types
and sizes based on predicted resource need.
380

AWS Well-Architected Framework
Selection
Determine which workload components could be replaced with fully managed services, such as managed
databases, in-memory caches, and ETL services. Reducing your operational workload allows you to focus
resources on business outcomes.
For cost requirement best practices, refer to the Cost-Effective Resources section of the Cost Optimization
Pillar whitepaper.
Common anti-patterns:
• You only use one family of instances.
• You do not evaluate licensed solutions versus open-source solutions
• You only use block storage.
• You deploy common software on EC2 instances and Amazon EBS or ephemeral volumes that are
available as a managed service.
Benefits of establishing this best practice: Considering cost when making your selections will allow you
to allow other investments.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Optimize workload components to reduce cost: Right size workload components and allow elasticity to
reduce cost and maximize component efficiency. Determine which workload components can be replaced
with managed services when appropriate, such as managed databases, in-memory caches, and reverse
proxies.
Resources
Related documents:
• AWS Architecture Center
• AWS Partner Network
• AWS Solutions Library
• AWS Knowledge Center
• AWS Compute Optimizer
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• This is my Architecture
• Optimize performance and cost for your AWS compute (CMP323-R1)
Related examples:
• AWS Samples
• AWS SDK Examples
• Rightsizing with Compute Optimizer and Memory utilization enabled
• AWS Compute Optimizer Demo code
PERF01-BP04 Use policies or reference architectures
Maximize performance and efficiency by evaluating internal policies and existing reference architectures
and using your analysis to select services and configurations for your workload.
381

AWS Well-Architected Framework
Selection
Common anti-patterns:
• You allow wide use of technology selection that may impact the management overhead of your
company.
Benefits of establishing this best practice: Establishing a policy for architecture, technology, and vendor
choices will allow decisions to be made quickly.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Deploy your workload using existing policies or reference architectures: Integrate the services into
your cloud deployment, then use your performance tests to ensure that you can continue to meet your
performance requirements.
Resources
Related documents:
• AWS Architecture Center
• AWS Partner Network
• AWS Solutions Library
• AWS Knowledge Center
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• This is my Architecture
Related examples:
• AWS Samples
• AWS SDK Examples
PERF01-BP05 Use guidance from your cloud provider or an appropriate partner
Use cloud company resources, such as solutions architects, professional services, or an appropriate
partner to guide your decisions. These resources can help review and improve your architecture for
optimal performance.
Reach out to AWS for assistance when you need additional guidance or product information. AWS
Solutions Architects and AWS Professional Services provide guidance for solution implementation. AWS
Partners provide AWS expertise to help you unlock agility and innovation for your business.
Common anti-patterns:
• You use AWS as a common data center provider.
• You use AWS services in a manner that they were not designed for.
Benefits of establishing this best practice: Consulting with your provider or a partner will give you
confidence in your decisions.
Level of risk exposed if this best practice is not established: Medium
382

AWS Well-Architected Framework
Selection
Implementation guidance
Reach out to AWS resources for assistance: AWS Solutions Architects and Professional Services provide
guidance for solution implementation. APN Partners provide AWS expertise to help you unlock agility
and innovation for your business.
Resources
Related documents:
• AWS Architecture Center
• AWS Partner Network
• AWS Solutions Library
• AWS Knowledge Center
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• This is my Architecture
Related examples:
• AWS Samples
• AWS SDK Examples
PERF01-BP06 Benchmark existing workloads
Benchmark the performance of an existing workload to understand how it performs on the cloud. Use
the data collected from benchmarks to drive architectural decisions.
Use benchmarking with synthetic tests and real-user monitoring to generate data about how your
workload’s components perform. Benchmarking is generally quicker to set up than load testing and is
used to evaluate the technology for a particular component. Benchmarking is often used at the start of a
new project, when you lack a full solution to load test.
You can either build your own custom benchmark tests, or you can use an industry standard test, such
as TPC-DS to benchmark your data warehousing workloads. Industry benchmarks are helpful when
comparing environments. Custom benchmarks are useful for targeting specific types of operations that
you expect to make in your architecture.
When benchmarking, it is important to pre-warm your test environment to ensure valid results. Run the
same benchmark multiple times to ensure that you’ve captured any variance over time.
Because benchmarks are generally faster to run than load tests, they can be used earlier in the
deployment pipeline and provide faster feedback on performance deviations. When you evaluate a
significant change in a component or service, a benchmark can be a quick way to see if you can justify
the effort to make the change. Using benchmarking in conjunction with load testing is important
because load testing informs you about how your workload will perform in production.
Common anti-patterns:
• You rely on common benchmarks that are not indicative of your workload characteristics.
• You rely on customer feedback and perceptions as your only benchmark.
Benefits of establishing this best practice: Benchmarking your current implementation allows you to
measure the improvement in performance.
383

AWS Well-Architected Framework
Selection
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Monitor performance during development: Implement processes that provide visibility into performance
as your workload evolves.
Integrate into your delivery pipeline: Automatically run load tests in your delivery pipeline. Compare
the test results against pre-defined key performance indicators (KPIs) and thresholds to ensure that you
continue to meet performance requirements.
Test user journeys: Use synthetic or sanitized versions of production data (remove sensitive or identifying
information) for load testing. Exercise your entire architecture by using replayed or pre-programmed user
journeys through your application at scale.
Real-user monitoring: Use CloudWatch RUM to help you collect and view client-side data about your
application performance. Use this data to help establish your real-user performance benchmarks.
Resources
Related documents:
• AWS Architecture Center
• AWS Partner Network
• AWS Solutions Library
• AWS Knowledge Center
• Amazon CloudWatch RUM
• Amazon CloudWatch Synthetics
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• This is my Architecture
• Optimize applications through Amazon CloudWatch RUM
• Demo of Amazon CloudWatch Synthetics
Related examples:
• AWS Samples
• AWS SDK Examples
• Distributed Load Tests
• Measure page load time with Amazon CloudWatch Synthetics
• Amazon CloudWatch RUM Web Client
PERF01-BP07 Load test your workload
Deploy your latest workload architecture on the cloud using different resource types and sizes. Monitor
the deployment to capture performance metrics that identify bottlenecks or excess capacity. Use this
performance information to design or improve your architecture and resource selection.
Load testing uses your actual workload so that you can see how your solution performs in a production
environment. Load tests must be run using synthetic or sanitized versions of production data (remove
384

AWS Well-Architected Framework
Selection
sensitive or identifying information). Use replayed or pre-programmed user journeys through your
workload at scale that exercise your entire architecture. Automatically carry out load tests as part of your
delivery pipeline, and compare the results against pre-defined KPIs and thresholds. This ensures that you
continue to achieve required performance.
Common anti-patterns:
• You load test individual parts of your workload but not your entire workload.
• You load test on infrastructure that is not the same as your production environment.
• You only conduct load testing to your expected load and not beyond, to help foresee where you may
have future problems.
• Performing load testing without informing AWS Support, and having your test defeated as it looks like
a denial of service event.
Benefits of establishing this best practice: Measuring your performance under a load test will show you
where you will be impacted as load increases. This can provide you with the capability of anticipating
needed changes before they impact your workload.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Validate your approach with load testing: Load test a proof-of-concept to find out if you meet your
performance requirements. You can use AWS services to run production-scale environments to test your
architecture. Because you only pay for the test environment when it is needed, you can carry out full-
scale testing at a fraction of the cost of using an on-premises environment.
Monitor metrics: Amazon CloudWatch can collect metrics across the resources in your architecture. You
can also collect and publish custom metrics to surface business or derived metrics. Use CloudWatch or
third-party solutions to set alarms that indicate when thresholds are breached.
Test at scale: Load testing uses your actual workload so you can see how your solution performs in a
production environment. You can use AWS services to run production-scale environments to test your
architecture. Because you only pay for the test environment when it is needed, you can run full-scale
testing at a lower cost than using an on-premises environment. Take advantage of the AWS Cloud to
test your workload to discover where it fails to scale, or if it scales in a non-linear way. For example, use
Spot Instances to generate loads at low cost and discover bottlenecks before they are experienced in
production.
Resources
Related documents:
• AWS CloudFormation
• Building AWS CloudFormation Templates using CloudFormer
• Amazon CloudWatch RUM
• Amazon CloudWatch Synthetics
• Distributed Load Testing on AWS
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• Optimize applications through Amazon CloudWatch RUM
• Demo of Amazon CloudWatch Synthetics
385

AWS Well-Architected Framework
Selection
Related examples:
• Distributed Load Testing on AWS
PERF 2. How do you select your compute solution?
The most effective compute solution for a workload varies based on application design, usage patterns,
and configuration settings. Architectures can use different compute solutions for various components
and activates different features to improve performance. Selecting the wrong compute solution for an
architecture can lead to lower performance efficiency.
Best practices
• PERF02-BP01 Evaluate the available compute options (p. 386)
• PERF02-BP02 Understand the available compute configuration options (p. 388)
• PERF02-BP03 Collect compute-related metrics (p. 391)
• PERF02-BP04 Determine the required configuration by right-sizing (p. 393)
• PERF02-BP05 Use the available elasticity of resources (p. 394)
• PERF02-BP06 Continually evaluate compute needs based on metrics (p. 396)
PERF02-BP01 Evaluate the available compute options
Understand how your workload can benefit from the use of different compute options, such as instances,
containers and functions.
Desired outcome: By understanding all of the compute options available, you will be aware of
the opportunities to increase performance, reduce unnecessary infrastructure costs, and lower the
operational effort required to maintain your workload. You can also accelerate your time to market when
you deploy new services and features.
Common anti-patterns:
• In a post-migration workload, using the same compute solution that was being used on premises.
• Lacking awareness of the cloud compute solutions and how those solutions might improve your
compute performance.
• Oversizing an existing compute solution to meet scaling or performance requirements, when an
alternative compute solution would align to your workload characteristics more precisely.
Benefits of establishing this best practice: By identifying the compute requirements and evaluating
the available compute solutions, business stakeholders and engineering teams will understand the
benefits and limitations of using the selected compute solution. The selected compute solution should
fit the workload performance criteria. Key criteria include processing needs, traffic patterns, data access
patterns, scaling needs, and latency requirements.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Understand the virtualization, containerization, and management solutions that can benefit your
workload and meet your performance requirements. A workload can contain multiple types of compute
solutions. Each compute solution has differing characteristics. Based on your workload scale and
compute requirements, a compute solution can be selected and configured to meet your needs. The
cloud architect should learn the advantages and disadvantages of instances, containers, and functions.
386

AWS Well-Architected Framework
Selection
The following steps will help you through how to select your compute solution to match your workload
characteristics and performance requirements.
Type                                                                                                     Server                   Containers              Function
AWS service                                                                                              Amazon Elastic           Amazon Elastic          AWS Lambda
                                                                                                         Compute Cloud            Container Service
                                                                                                         (Amazon EC2)             (Amazon ECS), Amazon
                                                                                                                                  Elastic Kubernetes
                                                                                                                                  Service (Amazon EKS)
Key Characteristics                                                                                      Has dedicated            Easy deployment,        Short runtime (15
                                                                                                         option for hardware      consistent              minutes or less),
                                                                                                         license requirements,    environments, runs on   maximum memory
                                                                                                         Placement Options,       top of EC2 instances,   and CPU are not as
                                                                                                         and a large selection    Scalable                high as other services,
                                                                                                         of different instance                            Managed hardware
                                                                                                         families based on                                layer, Scales to millions
                                                                                                         compute metrics                                  of concurrent requests
Common use-cases                                                                                         Lift and shift           Microservices, hybrid   Microservices, event-
                                                                                                         migrations, monolithic   environments,           driven applications
application, hybrid
environments,
enterprise applications
Implementation steps:
1. Select the location of where the compute solution must reside by evaluating the section called
“PERF05-BP06 Choose your workload’s location based on network requirements” (p. 433). This
location will limit the types of compute solution available to you.
2. Identify the type of compute solution that works with the location requirement and application
requirements
a. Amazon Elastic Compute Cloud (Amazon EC2) virtual server instances come in a wide variety of
different families and sizes. They offer a wide variety of capabilities, including solid state drives
(SSDs) and graphics processing units (GPUs). EC2 instances offer the greatest flexibility on instance
choice. When you launch an EC2 instance, the instance type that you specify determines the
hardware of your instance. Each instance type offers different compute, memory, and storage
capabilities. Instance types are grouped in instance families based on these capabilities. Typical use
cases include: running enterprise applications, high performance computing (HPC), training and
deploying machine learning applications and running cloud native applications.
b. Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service
that allows you to automatically run and manage containers on a cluster of EC2 instances or
serverless instances using AWS Fargate. You can use Amazon ECS with other services such as
Amazon Route 53, Secrets Manager, AWS Identity and Access Management (IAM), and Amazon
CloudWatch. Amazon ECS is recommended if your application is containerized and your engineering
team prefers Docker containers.
c. Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can
choose to run your EKS clusters using AWS Fargate, removing the need to provision and manage
servers. Managing Amazon EKS is simplified due to integrations with AWS Services such as Amazon
CloudWatch, Auto Scaling Groups, AWS Identity and Access Management (IAM), and Amazon
Virtual Private Cloud (VPC). When using containers, you must use compute metrics to select the
optimal type for your workload, similar to how you use compute metrics to select your EC2 or AWS
Fargate instance types. Amazon EKS is recommended if your application is containerized and your
engineering team prefers Kubernetes over Docker containers.
387

AWS Well-Architected Framework
Selection
d. You can use AWS Lambda to run code that supports the allowed runtime, memory, and CPU
options. Simply upload your code, and AWS Lambda will manage everything required to run and
scale that code. You can set up your code to automatically run from other AWS services or call it
directly. Lambda is recommended for short running, microservice architectures developed for the
cloud.
3. After you have experimented with your new compute solution, plan your migration and validate your
performance metrics. This is a continual process, see the section called “PERF02-BP04 Determine the
required configuration by right-sizing” (p. 393).
Level of effort for the implementation plan: If a workload is moving from one compute solution to
another, there could be a moderate level of effort involved in refactoring the application.
Resources
Related documents:
• Cloud Compute with AWS
• EC2 Instance Types
• Processor State Control for Your EC2 Instance
• EKS Containers: EKS Worker Nodes
• Amazon ECS Containers: Amazon ECS Container Instances
• Functions: Lambda Function Configuration
• Prescriptive Guidance for Containers
• Prescriptive Guidance for Serverless
Related videos:
• How to choose compute option for startups
• Optimize performance and cost for your AWS compute (CMP323-R1)
• Amazon EC2 foundations (CMP211-R2)
• Powering next-gen Amazon EC2: Deep dive into the Nitro system
• Deliver high-performance ML inference with AWS Inferentia (CMP324-R1)
• Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)
Related examples:
• Migrating the web application to containers
• Run a Serverless Hello World
PERF02-BP02 Understand the available compute configuration options
Each compute solution has options and configurations available to you to support your workload
characteristics. Learn how various options complement your workload, and which configuration options
are best for your application. Examples of these options include instance family, sizes, features (GPU, I/
O), bursting, time-outs, function sizes, container instances, and concurrency.
Desired outcome: The workload characteristics including CPU, memory, network throughput, GPU,
IOPS, traffic patterns, and data access patterns are documented and used to configure the compute
solution to match the workload characteristics. Each of these metrics plus custom metrics specific to your
workload are recorded, monitored, and then used to optimize the compute configuration to best meet
the requirements.
388

AWS Well-Architected Framework
Selection
Common anti-patterns:
• Using the same compute solution that was being used on premises.
• Not reviewing the compute options or instance family to match workload characteristics.
• Oversizing the compute to ensure bursting capability.
• You use multiple compute management platforms for the same workload.
Benefits of establishing this best practice: Be familiar with the AWS compute offerings so that you
can determine the correct solution for each of your workloads. After you have selected the compute
offerings for your workload, you can quickly experiment with those compute offerings to determine
how well they meet your workload needs. A compute solution that is optimized to meet your workload
characteristics will increase your performance, lower your cost and increase your reliability.
Level of risk exposed if this best practice is not established: High
Implementation guidance
If your workload has been using the same compute option for more than four weeks and you anticipate
that the characteristics will remain the same in the future, you can use AWS Compute Optimizer to
provide a recommendation to you based on your compute characteristics. If AWS Compute Optimizer
is not an option due to lack of metrics, a non-supported instance type or a foreseeable change in your
characteristics then you must predict your metrics based on load testing and experimentation.
Implementation steps:
1. Are you running on EC2 instances or containers with the EC2 Launch Type?
a.                                                                                                         Can your workload use GPUs to increase performance?
i.  Accelerated Computing instances are GPU-based instances that provide the highest performance
for machine learning training, inference and high performance computing.
b. Does your workload run machine learning inference applications?
i.  AWS Inferentia (Inf1) — Inf1 instances are built to support machine learning inference
applications. Using Inf1 instances, customers can run large-scale machine learning inference
applications, such as image recognition, speech recognition, natural language processing,
personalization, and fraud detection. You can build a model in one of the popular machine
learning frameworks, such as TensorFlow, PyTorch, or MXNet and use GPU instances, to train
your model. After your machine learning model is trained to meet your requirements, you can
deploy your model on Inf1 instances by using AWS Neuron, a specialized software development
kit (SDK) consisting of a compiler, runtime, and profiling tools that optimize the machine
learning inference performance of Inferentia chips.
c.                                                                                                         Does your workload integrate with the low-level hardware to improve performance?
i.  Field Programmable Gate Arrays (FPGA) — Using FPGAs, you can optimize your workloads by
having custom hardware-accelerated operation for your most demanding workloads. You can
define your algorithms by leveraging supported general programming languages such as C or
Go, or hardware-oriented languages such as Verilog or VHDL.
d. Do you have at least four weeks of metrics and can predict that your traffic pattern and metrics will
remain about the same in the future?
i.                                                                                                         Use Compute Optimizer to get a machine learning recommendation on which compute
configuration best matches your compute characteristics.
e.                                                                                                         Is your workload performance constrained by the CPU metrics?
i.  Compute-optimized instances are ideal for the workloads that require high performing
processors.
f.                                                                                                         Is your workload performance constrained by the memory metrics?
i.  Memory-optimized instances deliver large amounts of memory to support memory intensive
workloads.
389

AWS Well-Architected Framework
Selection
g. Is your workload performance constrained by IOPS?
i.  Storage-optimized instances are designed for workloads that require high, sequential read and
write access (IOPS) to local storage.
h. Do your workload characteristics represent a balanced need across all metrics?
i.                                                                                                         Does your workload CPU need to burst to handle spikes in traffic?
A. Burstable Performance instances are similar to Compute Optimized instances except they
offer the ability to burst past the fixed CPU baseline identified in a compute-optimized
instance.
ii. General Purpose instances provide a balance of all characteristics to support a variety of
workloads.
i.                                                                                                         Is your compute instance running on Linux and constrained by network throughput on the network
interface card?
i.                                                                                                         Review Performance Question 5, Best Practice 2: Evaluate available networking features to find
the right instance type and family to meet your performance needs.
j.                                                                                                         Does your workload need consistent and predictable instances in a specific Availability Zone that
you can commit to for a year?
i.  Reserved Instances confirms capacity reservations in a specific Availability Zone. Reserved
Instances are ideal for required compute power in a specific Availability Zone.
k.                                                                                                         Does your workload have licenses that require dedicated hardware?
i.  Dedicated Hosts support existing software licenses and help you meet compliance requirements.
l.                                                                                                         Does your compute solution burst and require synchronous processing?
i.  On-Demand Instances let you use the compute capacity by the hour or second with no long-term
commitment. These instances are good for bursting above performance baseline needs.
m.Is your compute solution stateless, fault-tolerant, and asynchronous?
i.  Spot Instances let you take advantage of unused instance capacity for your stateless, fault-
tolerant workloads.
2. Are you running containers on Fargate?
a. Is your task performance constrained by the memory or CPU?
i.                                                                                                         Use the Task Size to adjust your memory or CPU.
b. Is your performance being affected by your traffic pattern bursts?
i.                                                                                                         Use the Auto Scaling configuration to match your traffic patterns.
3. Is your compute solution on Lambda?
a. Do you have at least four weeks of metrics and can predict that your traffic pattern and metrics will
remain about the same in the future?
i.                                                                                                         Use Compute Optimizer to get a machine learning recommendation on which compute
configuration best matches your compute characteristics.
b. Do you not have enough metrics to use AWS Compute Optimizer?
i.                                                                                                         If you do not have metrics available to use Compute Optimizer, use AWS Lambda Power Tuning
to help select the best configuration.
c. Is your function performance constrained by the memory or CPU?
i.                                                                                                         Configure your Lambda memory to meet your performance needs metrics.
d. Is your function timing out when running?
i.                                                                                                         Change the timeout settings
e. Is your function performance constrained by bursts of activity and concurrency?
i.                                                                                                         Configure the concurrency settings to meet your performance requirements.
f. Does your function run asynchronously and is failing on retries?
i.                                                                                                         Configure the maximum age of the e3                                                                 90
configuration settings.

AWS Well-Architected Framework
Selection
Level of effort for the implementation plan:
To establish this best practice, you must be aware of your current compute characteristics and
metrics. Gathering those metrics, establishing a baseline and then using those metrics to identify
the ideal compute option is a low to moderate level of effort. This is best validated by load tests and
experimentation.
Resources
Related documents:
• Cloud Compute with AWS
• AWS Compute Optimizer
• EC2 Instance Types
• Processor State Control for Your EC2 Instance
• EKS Containers: EKS Worker Nodes
• Amazon ECS Containers: Amazon ECS Container Instances
• Functions: Lambda Function Configuration
Related videos:
• Amazon EC2 foundations (CMP211-R2)
• Powering next-gen Amazon EC2: Deep dive into the Nitro system
• Optimize performance and cost for your AWS compute (CMP323-R1)
Related examples:
• Rightsizing with Compute Optimizer and Memory utilization enabled
• AWS Compute Optimizer Demo code
PERF02-BP03 Collect compute-related metrics
To understand how your compute resources are performing, you must record and track the utilization
of various systems. This data can be used to make more accurate determinations about resource
requirements.
Workloads can generate large volumes of data such as metrics, logs, and events. Determine if your
existing storage, monitoring, and observability service can manage the data generated. Identify which
metrics reflect resource utilization and can be collected, aggregated, and correlated on a single platform
across. Those metrics should represent all your workload resources, applications, and services, so you
can easily gain system-wide visibility and quickly identify performance improvement opportunities and
issues.
Desired outcome: All metrics related to the compute-related resources are identified, collected,
aggregated, and correlated on a single platform with retention implemented to support cost and
operational goals.
Common anti-patterns:
• You only use manual log file searching for metrics.
• You only publish metrics to internal tools.
• You only use the default metrics recorded by your selected monitoring software.
• You only review metrics when there is an issue.
391

AWS Well-Architected Framework
Selection
Benefits of establishing this best practice: To monitor the performance of your workloads, you must
record multiple performance metrics over a period of time. These metrics allow you to detect anomalies
in performance. They will also help gauge performance against business metrics to ensure that you are
meeting your workload needs.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Identify, collect, aggregate, and correlate compute-related metrics. Using a service such as Amazon
CloudWatch, can make the implementation quicker and easier to maintain. In addition to the default
metrics recorded, identify and track additional system-level metrics within your workload. Record data
such as CPU utilization, memory, disk I/O, and network inbound and outbound metrics to gain insight
into utilization levels or bottlenecks. This data is crucial to understand how the workload is performing
and how the compute solution is utilized. Use these metrics as part of a data-driven approach to actively
tune and optimize your workload's resources.
Implementation steps:
1. Which compute solution metrics are important to track?
a. EC2 default metrics
b. Amazon ECS default metrics
c. EKS default metrics
d. Lambda default metrics
e. EC2 memory and disk metrics
2. Do I currently have an approved logging and monitoring solution?
a. Amazon CloudWatch
b. AWS Distro for OpenTelemetry
c. Amazon Managed Service for Prometheus
3. Have I identified and configured my data retention policies to match my security and operational
goals?
a. Default data retention for CloudWatch metrics
b. Default data retention for CloudWatch Logs
4. How do you deploy your metric and log aggregation agents?
a. AWS Systems Manager automation
b. OpenTelemetry Collector
Level of effort for the Implementation Plan: There is a medium level of effort to identify, track, collect,
aggregate, and correlate metrics from all compute resources.
Resources
Related documents:
• Amazon CloudWatch documentation
• Collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch
Agent
• Accessing Amazon CloudWatch Logs for AWS Lambda
• Using CloudWatch Logs with container instances
• Publish custom metrics
• AWS Answers: Centralized Logging
• AWS Services That Publish CloudWatch Metrics
392

AWS Well-Architected Framework
Selection
• Monitoring Amazon EKS on AWS Fargate
Related videos:
• Application Performance Management on AWS
• Build a Monitoring Plan
Related examples:
• Level 100: Monitoring with CloudWatch Dashboards
• Level 100: Monitoring Windows EC2 instance with CloudWatch Dashboards
• Level 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards
PERF02-BP04 Determine the required configuration by right-sizing
Analyze the various performance characteristics of your workload and how these characteristics relate
to memory, network, I/O, and CPU usage. Use this data to choose resources that best match your
workload’s profile. For example, a memory-intensive workload like a database may benefit from a higher
memory per core ratio. However, a compute intensive workload may need a higher core count and
frequency, but can be satisfied with a lower amount of memory per core.
Common anti-patterns:
• You choose an instance with the largest values across all performance characteristics available for all
workloads.
• You standardize all instances types to one type for ease of management.
• You optimize against standard synthetic benchmarks without validating the actual requirements of a
particular workload.
• You keep the same infrastructure for a long period of time without reevaluating and integrating new
offerings.
Benefits of establishing this best practice: When you are familiar with the requirements of your
workload, you can compare these needs with available compute offerings and quickly experiment
to determine which ones meet the needs of your workload most efficiently. This allows for optimal
performance without overpaying for resources not required.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Modify your workload configuration by right sizing. To optimize performance, overall efficiency, and
cost effectiveness, determine first which resources your workload needs. Choose memory-optimized
instances, such as the R-family of instances, for memory-intensive workloads like a database. For
workloads that require higher compute capacity, choose the C-family of instances, or choose instances
with higher core counts or higher core frequency. Choose I/O performance based on the needs of your
workload instead of comparing against standard, synthetic benchmarks. For higher I/O performance,
choose instances from the I-family of instances, select I/O optimized Amazon EBS volumes, or choose
instances with instance store. For more detail on particular instance types, see Amazon EC2 instance
types.
Right sizing verifies that your workloads perform as well as possible while not overpaying on resources
not needed.
393

AWS Well-Architected Framework
Selection
Implementation steps
• Know your workload or analyze its resource requirements.
• Evaluate workloads separately. The AWS Cloud gives you flexibility and agility to right-size each
workload on its own without needing to compromise.
• Create test environments to find the best match of compute offerings against your workload.
• Continually reevaluate new compute offerings, and compare against your workload’s needs.
• Routinely review new service offers for better price performance.
• Regularly conduct Well-Architected Framework Reviews.
Resources
Related best practices:
• PERF02-BP03 Collect compute-related metrics (p. 391)
• PERF02-BP06 Continually evaluate compute needs based on metrics (p. 396)
Related documents:
• AWS Compute Optimizer
• Cloud Compute with AWS
• Amazon EC2 Instance Types
• Amazon ECS Containers: Amazon ECS Container Instances
• Amazon EKS Containers: Amazon EKS Worker Nodes
• Functions: Lambda Function Configuration
Related videos:
• Amazon EC2 foundations (CMP211-R2)
• Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)
• Deliver high performance ML inference with AWS Inferentia (CMP324-R1)
• Optimize performance and cost for your AWS compute (CMP323-R1)
• Powering next-gen Amazon EC2: Deep dive into the Nitro system
• How to choose compute option for startups
• Optimize performance and cost for your AWS compute (CMP323-R1)
Related examples:
• Rightsizing with Compute Optimizer and Memory utilization enabled
• AWS Compute Optimizer Demo code
PERF02-BP05 Use the available elasticity of resources
The cloud provides the flexibility to expand and reduce your resources dynamically through a variety
of mechanisms to meet changes in demand. Combining this elasticity with compute-related metrics, a
workload can automatically respond to changes to use the resources it needs and only the resources it
needs.
Common anti-patterns:
394

AWS Well-Architected Framework
Selection
• You overprovision to cover possible spikes.
• You react to alarms by manually increasing capacity.
• You increase capacity without considering provisioning time.
• You leave increased capacity after a scaling event instead of scaling back down.
• You monitor metrics that don’t directly reflect your workloads true requirements.
Benefits of establishing this best practice: Demand can be fixed, variable, follow a pattern or be spiky.
Matching supply to demand delivers the lowest cost for a workload. Monitoring, testing, and configuring
workload elasticity will optimize performance, save money, and improve reliability as usage demands
change. Although a manual approach to this is possible, it is impractical at larger scales. An automated
and metrics-based approach assures resources meet demands and any given time.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Metric based automation should be used to take advantage of elasticity with the goal that the supply of
resources you have matches the demand of the resources your workload requires. For example, you can
use Amazon CloudWatch metrics to monitor your resources, or use Amazon CloudWatch metrics for your
Auto Scaling groups.
Combined with compute-related metrics, a workload can automatically respond to changes and use
the optimal set of resources to achieve its goal. You also must plan for provisioning time and potential
resource failures.
Instances, containers, and functions provide mechanisms for elasticity either as a feature of the service,
in the form of Application Auto Scaling, or in combination with Amazon EC2 Auto Scaling. Use elasticity
in your architecture to verify that you have sufficient capacity to meet performance requirements at a
wide variety of scales of use.
Validate your metrics for scaling up or down elastic resources against the type of workload being
deployed. As an example, if you are deploying a video transcoding application, 100% CPU utilization is
expected and should not be your primary metric. Alternatively, you can measure against the queue depth
of transcoding jobs waiting to scale your instance types.
Workload deployments need to handle both scale up and scale down events. Scaling down workload
components safely is as critical as scaling up resources when demand dictates.
Create test scenarios for scaling events to verify that the workload behaves as expected.
Implementation steps
• Leverage historical data to analyze your workload’s resource demands over time. Ask specific questions
like:
• Is your workload steady and increasing over time at a known rate?
• Does your workload increase and decrease in seasonal, repeatable patterns?
• Is your workload spiky? Can the spikes be anticipated or predicted?
• Leverage monitoring services and historical data as much as possible.
• Tagging resources can help with monitoring. When using tags, refer to tagging best practices.
Additionally, tags can help you manage, identify, and organize resources.
• With AWS, you can use a number of different approaches to match supply with demand. The cost
optimization pillar best practices (COST09-BP01 through COST09-03) describe how to use the
following approaches to cost:
• COST09-BP01 Perform an analysis on the workload demand
• COST09-BP02 Implement a buffer or throttle to manage demand
395

AWS Well-Architected Framework
Selection
• COST09-BP03 Supply resources dynamically
• Create test scenarios for scale down events to verify that the workload behaves as expected.
• Most non-production instances should be stopped when they are not being used.
• For storage needs when using Amazon Elastic Block Store (Amazon EBS), take advantage of volume-
based elasticity.
• For Amazon Elastic Compute Cloud (Amazon EC2), consider using Auto Scaling groups, which allow
you to optimize performance and cost by automatically increasing the number of compute instances
during demand spikes and decreasing capacity when demand decreases.
Resources
Related best practices:
• PERF02-BP03 Collect compute-related metrics (p. 391)
• PERF02-BP04 Determine the required configuration by right-sizing (p. 393)
• PERF02-BP06 Continually evaluate compute needs based on metrics (p. 396)
Related documents:
• Cloud Compute with AWS
• Amazon EC2 Instance Types
• Amazon ECS Containers: Amazon ECS Container Instances
• Amazon EKS Containers: Amazon EKS Worker Nodes
• Functions: Lambda Function Configuration
Related videos:
• Amazon EC2 foundations (CMP211-R2)
• Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)
• Deliver high performance ML inference with AWS Inferentia (CMP324-R1)
• Optimize performance and cost for your AWS compute (CMP323-R1)
• Powering next-gen Amazon EC2: Deep dive into the Nitro system
Related examples:
• Amazon EC2 Auto Scaling Group Examples
• Amazon EFS Tutorials
PERF02-BP06 Continually evaluate compute needs based on metrics
Use a data-driven approach to continually evaluate and optimize the compute resources for your
workload over time.
Desired outcome: Use system-level metrics to actively monitor the behavior and requirements of your
workload over time. Evaluate the demands of your workload against available resources based on the
collected data, and make changes to your compute environment to best match your workload's profile.
For example, a workload might be observed over time to be more memory-intensive than initially
specified, so moving to a different instance family or size could improve both performance and efficiency.
Common anti-patterns:
396

AWS Well-Architected Framework
Selection
• Monitoring system-level metrics to gain insight into your workload and not re-evaluating compute
needs.
• Architecting your compute needs for peak workload requirements.
• Oversizing the existing compute solution to meet scaling or performance requirements when moving
to an alternative compute solution would more efficiently match your workload characteristics.
Benefits of establishing this best practice: Optimized compute resources based on real-world data and
your desired balance of cost and performance.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Use a data-driven approach to optimize compute resources based on observed workload behavior. To
achieve maximum performance and efficiency, use the data gathered over time from your workload to
continually tune and optimize your resources. Look at the trends in your workload's usage of current
resources and determine where you can make changes to better match your workload's needs. When
resources are over-committed, system performance degrades, and when resources are not adequately
used, the system is operating less efficiently and at a higher cost.
To optimize performance and resource utilization, you need a unified operational view, real-time
granular data, and a historical reference. You can create automated dashboards to visualize this data and
derive operational and utilization insights.
Implementation steps
1. Collect compute-related metrics over time.
2. Compare workload metrics against available resources in your selected compute solution.
3. Determine any required configuration changes by right-sizing the existing solution or evaluating
alternative compute solutions.
Resources
Related best practices:
• PERF02-BP01 Evaluate the available compute options (p. 386)
• PERF02-BP02 Understand the available compute configuration options (p. 388)
• PERF02-BP03 Collect compute-related metrics (p. 391)
• PERF02-BP04 Determine the required configuration by right-sizing (p. 393)
Related documents:
• Cloud Compute with AWS
• AWS Compute Optimizer
• EC2 Instance Types
• Amazon ECS Containers: Amazon ECS Container Instances
• Amazon EKS Containers: Amazon EKS Worker Nodes
• Best practices for working with AWS Lambda functions
Related videos:
• Amazon EC2 foundations (CMP211-R2)
• Better, faster, cheaper compute: Cost-optimizing Amazon EC2 (CMP202-R1)
397

AWS Well-Architected Framework
Selection
• Deliver high performance ML inference with AWS Inferentia (CMP324-R1)
• Optimize performance and cost for your AWS compute (CMP323-R1)
• Powering next-gen Amazon EC2: Deep dive into the Nitro system
• Selecting and optimizing Amazon EC2 instances
Related examples:
• Rightsizing with Compute Optimizer and Memory utilization enabled
• AWS Compute Optimizer Demo code
PERF 3. How do you select your storage solution?
The most effective storage solution for a system varies based on the kind of access operation (block, file,
or object), patterns of access (random or sequential), required throughput, frequency of access (online,
offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints.
Well-architected systems use multiple storage solutions and activates different features to improve
performance and use resources efficiently.
Best practices
• PERF03-BP01 Understand storage characteristics and requirements (p. 398)
• PERF03-BP02 Evaluate available configuration options (p. 401)
• PERF03-BP03 Make decisions based on access patterns and metrics (p. 403)
PERF03-BP01 Understand storage characteristics and requirements
Identify and document the workload storage needs and define the storage characteristics of each
location. Examples of storage characteristics include: shareable access, file size, growth rate, throughput,
IOPS, latency, access patterns, and persistence of data. Use these characteristics to evaluate if block, file,
object, or instance storage services are the most efficient solution for your storage needs.
Desired outcome: Identify and document the storage requirements per storage requirement and
evaluate the available storage solutions. Based on the key storage characteristics, your team will
understand how the selected storage services will benefit your workload performance. Key criteria
include data access patterns, growth rate, scaling needs, and latency requirements.
Common anti-patterns:
• You only use one storage type, such as Amazon Elastic Block Store (Amazon EBS), for all workloads.
• You assume that all workloads have similar storage access performance requirements.
Benefits of establishing this best practice: Selecting the storage solution based on the identified and
required characteristics will help improve your workloads performance, decrease costs and lower your
operational efforts in maintaining your workload. Your workload performance will benefit from the
solution, configuration, and location of the storage service.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Identify your workload’s most important storage performance metrics and implement improvements as
part of a data-driven approach, using benchmarking or load testing. Use this data to identify where your
storage solution is constrained, and examine configuration options to improve the solution. Determine
the expected growth rate for your workload and choose a storage solution that will meet those rates.
Research the AWS storage offerings to determine the correct storage solution for your various workload
398

AWS Well-Architected Framework
Selection
needs. Provisioning storage solutions in AWS increases the opportunity for you to test storage offerings
and determine if they are appropriate for your workload needs.
AWS service                                                                                                Key characteristics                Common use cases
Amazon S3                                                                                                  99.999999999% durability,          Cloud-native application data,
                                                                                                           unlimited growth, accessible       data archiving, and backups,
                                                                                                           from anywhere, several cost        analytics, data lakes, static
                                                                                                           models based on access and         website hosting, IoT data
                                                                                                           resiliency
Amazon S3 Glacier                                                                                          Seconds to hours latency,          Data archiving, media archives,
                                                                                                           unlimited growth, lowest cost,     long-term backup retention.
                                                                                                           long-term storage
Amazon EBS                                                                                                 Storage size requires              COTS applications, I/O intensive
                                                                                                           management and monitoring,         applications, relational and
                                                                                                           low latency, persistent storage,   NoSQL databases, backup and
                                                                                                           99.8% to 99.9% durability, most    recovery
                                                                                                           volume types are accessible only
                                                                                                           from one EC2 instance.
EC2 Instance Store                                                                                         Pre-determined storage size,       COTS applications, I/O intensive
                                                                                                           lowest latency, not persisted,     applications, in-memory data
                                                                                                           accessible only from one EC2       store
                                                                                                           instance
Amazon EFS                                                                                                 99.999999999% durability,          Modernized applications sharing
                                                                                                           unlimited growth, accessible by    files across multiple compute
                                                                                                           multiple compute services          services, file storage for scaling
                                                                                                                                              content management systems
Amazon FSx                                                                                                 Supports four file systems         Cloud native workloads, private
                                                                                                           (NetApp, OpenZFS, Windows          cloud bursting, migrated
                                                                                                           File Server, and Amazon FSx        workloads that require a specific
                                                                                                           for Lustre), storage available     file system, VMC, ERP systems,
                                                                                                           different per file system,         on-premises file storage and
                                                                                                           accessible by multiple compute     backups
                                                                                                           services
Snow family                                                                                                Portable devices, 256-bit          Migrating data to the cloud,
                                                                                                           encryption, NFS endpoint, on-      storage, and computing in
                                                                                                           board computing, TBs of storage    extreme on-premises conditions,
                                                                                                                                              disaster recovery, remote data
                                                                                                                                              collection
AWS Storage Gateway                                                                                        Provides low-latency on-           On-premises data to cloud
                                                                                                           premises access to cloud-backed    migrations, populate cloud data
                                                                                                           storage, fully managed on-         lakes from on-premises sources,
                                                                                                           premises cache                     modernized file sharing.
Implementation steps:
1. Use benchmarking or load tests to collect the key characteristics of your storage needs. Key
characteristics include:
a. Shareable (what components access this storage)
b. Growth rate
399

AWS Well-Architected Framework
Selection
c. Throughput
d. Latency
e. I/O size
f. Durability
g. Access patterns (reads vs writes, frequency, spikey, or consistent)
2. Identify the type of storage solution that supports your storage characteristics.
a.                                                                                                        Amazon S3 is an object storage service with unlimited scalability, high availability, and multiple
options for accessibility. Transferring and accessing objects in and out of Amazon S3 can use a
service, such as Transfer Acceleration or Access Points to support your location, security needs, and
access patterns. Use the Amazon S3 performance guidelines to help you optimize your Amazon S3
configuration to meet your workload performance needs.
b. Amazon S3 Glacier is a storage class of Amazon S3 built for data archiving. You can choose from
three archiving solutions ranging from millisecond access to 5-12 hour access with different
cost and security options. Amazon S3 Glacier can help you meet performance requirements by
implementing a data lifecycle that supports your business requirements and data characteristics.
c.                                                                                                        Amazon Elastic Block Store (Amazon EBS) is a high-performance block storage service designed for
Amazon Elastic Compute Cloud (Amazon EC2). You can choose from SSD- or HDD-based solutions
with different characteristics that prioritize IOPS or throughput. EBS volumes are well suited for
high-performance workloads, primary storage for file systems, databases, or applications that can
only access attached stage systems.
d. Amazon EC2 Instance Store is similar to Amazon EBS as it attaches to an Amazon EC2 instance
however, the Instance Store is only temporary storage that should ideally be used as a buffer, cache,
or other temporary content. You cannot detach an Instance Store and all data is lost if the instance
shuts down. Instance Stores can be used for high I/O performance and low latency use cases where
data doesn’t need to persist.
e.                                                                                                        Amazon Elastic File System (Amazon EFS) is a mountable file system that can be accessed by
multiple types of compute solutions. Amazon EFS automatically grows and shrinks storage and is
performance-optimized to deliver consistent low latencies. EFS has two performance configuration
modes: General Purpose and Max I/O. General Purpose has a sub-millisecond read latency and
a single-digit millisecond write latency. The Max I/O feature can support thousands of compute
instance requiring a shared file system. Amazon EFS supports two throughput modes: Bursting
and Provisioned. A workload that experiences a spikey access pattern will benefit from the
bursting throughput mode while a workload that is consistently high would be performant with a
provisioned throughput mode.
f.                                                                                                        Amazon FSx is built on the latest AWS compute solutions to support four commonly used
file systems: NetApp ONTAP, OpenZFS, Windows File Server, and Lustre. Amazon FSx latency,
throughput, and IOPS vary per file system and should be considered when selecting the right file
system for your workload needs.
g. AWS Snow Family are storage and compute devices that support online and offline data migration
to the cloud and data storage and computing on premises. AWS Snow devices support collecting
large amounts of on-premises data, processing of that data and moving that data to the cloud.
There are several documented performance best practices when it comes to the number of files, file
sizes, and compression.
h. AWS Storage Gateway provides on-premises applications access to cloud-based storage. AWS
Storage Gateway supports multiple cloud storage services including Amazon S3, Amazon S3
Glacier, Amazon FSx, and Amazon EBS. It supports a number of protocols such as iSCSI, SMB, and
NFS. It provides low-latency performance by caching frequently accessed data on premises and only
sends changed data and compressed data to AWS.
3. After you have experimented with your new storage solution and identified the optimal configuration,
plan your migration and validate your performance metrics. This is a continual process, and should be
reevaluated when key characteristics change or available services or options change.
400

AWS Well-Architected Framework
Selection
Level of effort for the implementation plan: If a workload is moving from one storage solution to
another, there could be a moderate level of effort involved in refactoring the application.
Resources
Related documents:
• Amazon EBS Volume Types
• Amazon EC2 Storage
• Amazon EFS: Amazon EFS Performance
• Amazon FSx for Lustre Performance
• Amazon FSx for Windows File Server Performance
• Amazon FSx for NetApp ONTAP performance
• Amazon FSx for OpenZFS performance
• Amazon S3 Glacier: Amazon S3 Glacier Documentation
• Amazon S3: Request Rate and Performance Considerations
• Cloud Storage with AWS
• AWS Snow Family
• EBS I/O Characteristics
Related videos:
• Deep dive on Amazon EBS (STG303-R1)
• Optimize your storage performance with Amazon S3 (STG343)
Related examples:
• Amazon EFS CSI Driver
• Amazon EBS CSI Driver
• Amazon EFS Utilities
• Amazon EBS Autoscale
• Amazon S3 Examples
• Amazon FSx for Lustre Container Storage Interface (CSI) Driver
PERF03-BP02 Evaluate available configuration options
Evaluate the various characteristics and configuration options and how they relate to storage.
Understand where and how to use provisioned IOPS, SSDs, magnetic storage, object storage, archival
storage, or ephemeral storage to optimize storage space and performance for your workload.
Amazon EBS provides a range of options that allow you to optimize storage performance and cost
for your workload. These options are divided into two major categories: SSD-backed storage for
transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS),
and HDD-backed storage for throughput-intensive workloads, such as MapReduce and log processing
(performance depends primarily on MB/s).
SSD-backed volumes include the highest performance provisioned IOPS SSD for latency-sensitive
transactional workloads and general-purpose SSD that balance price and performance for a wide variety
of transactional data.
401

AWS Well-Architected Framework
Selection
Amazon S3 transfer acceleration allows fast transfer of files over long distances between your client and
your S3 bucket. Transfer acceleration leverages Amazon CloudFront globally distributed edge locations
to route data over an optimized network path. For a workload in an S3 bucket that has intensive GET
requests, use Amazon S3 with CloudFront. When uploading large files, use multi-part uploads with
multiple parts uploading at the same time to help maximize network throughput.
Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file
system for use with AWS Cloud services and on-premises resources. To support a wide variety of cloud
storage workloads, Amazon EFS offers two performance modes: general purpose performance mode,
and max I/O performance mode. There are also two throughput modes to choose from for your file
system: Bursting Throughput, and Provisioned Throughput. To determine which settings to use for your
workload, see the Amazon EFS User Guide.
Amazon FSx provides four file systems to choose from: Amazon FSx for Windows File Server for
enterprise workloads, Amazon FSx for Lustre for high-performance workloads, Amazon FSx for
NetApp ONTAP for NetApps popular ONTAP file system, and Amazon FSx for OpenZFS for Linux-based
file servers. FSx is SSD-backed and is designed to deliver fast, predictable, scalable, and consistent
performance. Amazon FSx file systems deliver sustained high read and write speeds and consistent low
latency data access. You can choose the throughput level you need to match your workload’s needs.
Common anti-patterns:
• You only use one storage type, such as Amazon EBS, for all workloads.
• You use Provisioned IOPS for all workloads without real-world testing against all storage tiers.
• You assume that all workloads have similar storage access performance requirements.
Benefits of establishing this best practice: Evaluating all storage service options can reduce the cost of
infrastructure and the effort required to maintain your workloads. It can potentially accelerate your time
to market for deploying new services and features.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Determine storage characteristics: When you evaluate a storage solution, determine which storage
characteristics you require, such as ability to share, file size, cache size, latency, throughput, and
persistence of data. Then match your requirements to the AWS service that best fits your needs.
Resources
Related documents:
• Cloud Storage with AWS
• Amazon EBS Volume Types
• Amazon EC2 Storage
• Amazon EFS: Amazon EFS Performance
• Amazon FSx for Lustre Performance
• Amazon FSx for Windows File Server Performance
• Amazon Glacier: Amazon Glacier Documentation
• Amazon S3: Request Rate and Performance Considerations
• Cloud Storage with AWS
• Cloud Storage with AWS
• EBS I/O Characteristics
402

AWS Well-Architected Framework
Selection
Related videos:
• Deep dive on Amazon EBS (STG303-R1)
• Optimize your storage performance with Amazon S3 (STG343)
Related examples:
• Amazon EFS CSI Driver
• Amazon EBS CSI Driver
• Amazon EFS Utilities
• Amazon EBS Autoscale
• Amazon S3 Examples
PERF03-BP03 Make decisions based on access patterns and metrics
Choose storage systems based on your workload's access patterns and configure them by determining
how the workload accesses data. Increase storage efficiency by choosing object storage over block
storage. Configure the storage options you choose to match your data access patterns.
How you access data impacts how the storage solution performs. Select the storage solution that aligns
best to your access patterns, or consider changing your access patterns to align with the storage solution
to maximize performance.
Creating a RAID 0 array allows you to achieve a higher level of performance for a file system than what
you can provision on a single volume. Consider using RAID 0 when I/O performance is more important
than fault tolerance. For example, you could use it with a heavily used database where data replication is
already set up separately.
Select appropriate storage metrics for your workload across all of the storage options consumed for
the workload. When using filesystems that use burst credits, create alarms to let you know when you
are approaching those credit limits. You must create storage dashboards to show the overall workload
storage health.
For storage systems that are a fixed size, such as Amazon EBS or Amazon FSx, ensure that you are
monitoring the amount of storage used versus the overall storage size and create automation if possible
to increase the storage size when reaching a threshold
Common anti-patterns:
• You assume that storage performance is adequate if customers are not complaining.
• You only use one tier of storage, assuming all workloads fit within that tier.
Benefits of establishing this best practice: You need a unified operational view, real-time granular data,
and historical reference to optimize performance and resource utilization. You can create automatic
dashboards and data with one-second granularity to perform metric math on your data and derive
operational and utilization insights for your storage needs.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Optimize your storage usage and access patterns: Choose storage systems based on your workload's
access patterns and the characteristics of the available storage options. Determine the best place to
store data that will allow you to meet your requirements while reducing overhead. Use performance
403

AWS Well-Architected Framework
Selection
optimizations and access patterns when configuring and interacting with data based on the
characteristics of your storage (for example, striping volumes or partitioning data).
Select appropriate metrics for storage options: Ensure that you select the appropriate storage metrics for
the workload. Each storage option offers various metrics to track how your workload performs over time.
Ensure that you are measuring against any storage burst metrics (for example, monitoring burst credits
for Amazon EFS). For storage systems that are fixed sized, such as Amazon Elastic Block Store or Amazon
FSx, ensure that you are monitoring the amount of storage used versus the overall storage size. Create
automation when possible to increase the storage size when reaching a threshold.
Monitor metrics: Amazon CloudWatch can collect metrics across the resources in your architecture. You
can also collect and publish custom metrics to surface business or derived metrics. Use CloudWatch or
third-party solutions to set alarms that indicate when thresholds are breached.
Resources
Related documents:
• Amazon EBS Volume Types
• Amazon EC2 Storage
• Amazon EFS: Amazon EFS Performance
• Amazon FSx for Lustre Performance
• Amazon FSx for Windows File Server Performance
• Amazon Glacier: Amazon Glacier Documentation
• Amazon S3: Request Rate and Performance Considerations
• Cloud Storage with AWS
• EBS I/O Characteristics
• Monitoring and understanding Amazon EBS performance using Amazon CloudWatch
Related videos:
• Deep dive on Amazon EBS (STG303-R1)
• Optimize your storage performance with Amazon S3 (STG343)
Related examples:
• Amazon EFS CSI Driver
• Amazon EBS CSI Driver
• Amazon EFS Utilities
• Amazon EBS Autoscale
• Amazon S3 Examples
PERF 4. How do you select your database solution?
The most effective database solution for a system varies based on requirements for availability,
consistency, partition tolerance, latency, durability, scalability, and query capability. Many systems
use different database solutions for various subsystems and activate different features to improve
performance. Selecting the wrong database solution and features for a system can lead to lower
performance efficiency.
Best practices
404

AWS Well-Architected Framework
Selection
• PERF04-BP01 Understand data characteristics (p. 405)
• PERF04-BP02 Evaluate the available options (p. 409)
• PERF04-BP03 Collect and record database performance metrics (p. 414)
• PERF04-BP04 Choose data storage based on access patterns (p. 416)
• PERF04-BP05 Optimize data storage based on access patterns and metrics (p. 419)
PERF04-BP01 Understand data characteristics
Choose your data management solutions to optimally match the characteristics, access patterns,
and requirements of your workload datasets. When selecting and implementing a data management
solution, you must ensure that the querying, scaling, and storage characteristics support the
workload data requirements. Learn how various database options match your data models, and which
configuration options are best for your use-case.
AWS provides numerous database engines including relational, key-value, document, in-memory, graph,
time series, and ledger databases. Each data management solution has options and configurations
available to you to support your use-cases and data models. Your workload might be able to use several
different database solutions, based on the data characteristics. By selecting the best database solutions
to a specific problem, you can break away from monolithic databases, with the one-size-fits-all approach
that is restrictive and focus on managing data to meet your customer's need.
Desired outcome: The workload data characteristics are documented with enough detail to facilitate
selection and configuration of supporting database solutions, and provide insight into potential
alternatives.
Common anti-patterns:
• Not considering ways to segment large datasets into smaller collections of data that have similar
characteristics, resulting in missing opportunities to use more purpose-built databases that better
match data and growth characteristics.
• Not identifying the data access patterns up front, which leads to costly and complex rework later.
• Limiting growth by using data storage strategies that don’t scale as quickly as is needed
• Choosing one database type and vendor for all workloads.
• Sticking to one database solution because there is internal experience and knowledge of one particular
type of database solution.
• Keeping a database solution because it worked well in an on-premises environment.
Benefits of establishing this best practice: Be familiar with all of the AWS database solutions so
that you can determine the correct database solution for your various workloads. After you select the
appropriate database solution for your workload, you can quickly experiment on each of those database
offerings to determine if they continue to meet your workload needs.
Level of risk exposed if this best practice is not established: High
• Potential cost savings may not be identified.
• Data may not be secured to the level required.
• Data access and storage performance may not be optimal.
Implementation guidance
Define the data characteristics and access patterns of your workload. Review all available database
solutions to identify which solution supports your data requirements. Within a given workload, multiple
405

AWS Well-Architected Framework
Selection
databases may be selected. Evaluate each service or group of services and assess them individually. If
potential alternative data management solutions are identified for part or all of the data, experiment
with alternative implementations that might unlock cost, security, performance, and reliability benefits.
Update existing documentation, should a new data management approach be adopted.
Type                                                                                                        AWS Services                                                                                  Key Characteristics       Common use-cases
Relational                                                                                                  Amazon RDS, Amazon                                                                            Referential integrity,    ERP, CRM, Commercial
                                                                                                            Aurora                                                                                        ACID transactions,        off-the-shelf software
                                                                                                                                                                                                          schema on write
Key Value                                                                                                   Amazon DynamoDB                                                                               High throughput, low      Shopping carts
                                                                                                                                                                                                          latency, near-infinite    (ecommerce), product
                                                                                                                                                                                                          scalability               catalogs, chat
                                                                                                                                                                                                                                    applications
Document                                                                                                    Amazon DocumentDB                                                                             Store JSON documents      Content Management
                                                                                                                                                                                                          and query on any          (CMS), customer
                                                                                                                                                                                                          attribute                 profiles, mobile
                                                                                                                                                                                                                                    applications
In Memory                                                                                                   Amazon ElastiCache,                                                                           Microsecond latency       Caching, game
                                                                                                            Amazon MemoryDB                                                                                                         leaderboards
Graph                                                                                                       Amazon Neptune                                                                                Highly relational data    Social networks,
                                                                                                                                                                                                          where the relationships   personalization
                                                                                                                                                                                                          between data have         engines, fraud
                                                                                                                                                                                                          meaning                   detection
Time Series                                                                                                 Amazon Timestream                                                                             Data where the primary    DevOps, IoT, Monitoring
                                                                                                                                                                                                          dimension is time
Wide column                                                                                                 Amazon Keyspaces                                                                              Cassandra workloads.      Industrial equipment
                                                                                                                                                                                                                                    maintenance, route
                                                                                                                                                                                                                                    optimization
Ledger                                                                                                      Amazon QLDB                                                                                   Immutable and             Systems of record,
                                                                                                                                                                                                          cryptographically         healthcare, supply
                                                                                                                                                                                                          verifiable ledger of      chains, financial
                                                                                                                                                                                                          changes                   institutions
Implementation steps
1. How is the data structured? (for example, unstructured, key-value, semi-structured, relational)
a. If the data is unstructured, consider an object-store such as Amazon S3 or a NoSQL database such
as Amazon DocumentDB.
b. For key-value data, consider DynamoDB, ElastiCache for Redis or MemoryDB.
c. If the data has a relational structure, what level of referential integrity is required?
i.                                                                                                          For foreign key constraints, relational databases such as Amazon RDS and Aurora can provide
this level of integrity.
ii. Typically, within a NoSQL data-model, you would de-normalize your data into a single document
or collection of documents to be retrieved in a single request rather than joining across
documents or tables.
2. Is ACID (atomicity, consistency, isolation, durability) compliance required?
a. If the ACID properties associated with relational databases are required, consider a relational
database such as Amazon RDS and Aurora.
406

AWS Well-Architected Framework
Selection
3. What consistency model is required?
a. If your application can tolerate eventual consistency, consider a NoSQL implementation. Review the
other characteristics to help choose which NoSQL database is most appropriate.
b. If strong consistency is required, you can use strongly consistent reads with DynamoDB or a
relational database such as Amazon RDS.
4. What query and result formats must be supported? (for example, SQL, CSV, Parque, Avro, JSON, etc.)
5. What data types, field sizes and overall quantities are present? (for example, text, numeric, spatial,
time-series calculated, binary or blob, document)
6. How will the storage requirements change over time? How does this impact scalability?
a. Serverless databases such as DynamoDB and Amazon Quantum Ledger Database will scale
dynamically up to near-unlimited storage.
b. Relational databases have upper bounds on provisioned storage, and often must be horizontally
partitioned via mechanisms such as sharding once they reach these limits.
7. What is the proportion of read queries in relation to write queries? Would caching be likely to improve
performance?
a. Read-heavy workloads can benefit from a caching layer, this could be ElastiCache or DAX if the
database is DynamoDB.
b. Reads can also be offloaded to read replicas with relational databases such as Amazon RDS.
8. Does storage and modification (OLTP - Online Transaction Processing) or retrieval and reporting
(OLAP - Online Analytical Processing) have a higher priority?
a. For high-throughput transactional processing, consider a NoSQL database such as DynamoDB or
Amazon DocumentDB.
b. For analytical queries, consider a columnar database such as Amazon Redshift or exporting the data
to Amazon S3 and performing analytics using Athena or QuickSight.
9. How sensitive is this data and what level of protection and encryption does it require?
a. All Amazon RDS and Aurora engines support data encryption at rest using AWS KMS. Microsoft SQL
Server and Oracle also support native Transparent Data Encryption (TDE) when using Amazon RDS.
b. For DynamoDB, you can use fine-grained access control with IAM to control who has access to what
data at the key level.
10What level of durability does the data require?
a. Aurora automatically replicates your data across three Availability Zones within a Region, meaning
your data is highly durable with less chance of data loss.
b. DynamoDB is automatically replicated across multiple Availability Zones, providing high availability
and data durability.
c. Amazon S3 provides 11 9s of durability. Many database services such as Amazon RDS and
DynamoDB support exporting data to Amazon S3 for long-term retention and archival.
11Do Recovery Time Objective (RTO) or Recovery Point Objectives (RPO) requirements influence the
solution?
a. Amazon RDS, Aurora, DynamoDB, Amazon DocumentDB, and Neptune all support point in time
recovery and on-demand backup and restore.
b. For high availability requirements, DynamoDB tables can be replicated globally using the Global
Tables feature and Aurora clusters can be replicated across multiple Regions using the Global
database feature. Additionally, S3 buckets can be replicated across AWS Regions using cross-region
replication.
12Is there a desire to move away from commercial database engines / licensing costs?
a. Consider open-source engines such as PostgreSQL and MySQL on Amazon RDS or Aurora
b. Leverage AWS DMS and AWS SCT to perform migrations from commercial database engines to
open-source
13What is the operational expectation for the database? Is moving to managed services a primary
concern?
407

AWS Well-Architected Framework
Selection
a. Leveraging Amazon RDS instead of Amazon EC2, and DynamoDB or Amazon DocumentDB instead
of self-hosting a NoSQL database can reduce operational overhead.
14How is the database currently accessed? Is it only application access, or are there Business Intelligence
(BI) users and other connected off-the-shelf applications?
a. If you have dependencies on external tooling then you may have to maintain compatibility with the
databases they support. Amazon RDS is fully compatible with the difference engine versions that it
supports including Microsoft SQL Server, Oracle, MySQL, and PostgreSQL.
15The following is a list of potential data management services, and where these can best be used:
a.                                                                                                            Relational databases store data with predefined schemas and relationships between them. These
databases are designed to support ACID (atomicity, consistency, isolation, durability) transactions,
and maintain referential integrity and strong data consistency. Many traditional applications,
enterprise resource planning (ERP), customer relationship management (CRM), and ecommerce use
relational databases to store their data. You can run many of these database engines on Amazon
EC2, or choose from one of the AWS-managed database services: Amazon Aurora, Amazon RDS,
and Amazon Redshift.
b. Key-value databases are optimized for common access patterns, typically to store and retrieve
large volumes of data. These databases deliver quick response times, even in extreme volumes
of concurrent requests. High-traffic web apps, ecommerce systems, and gaming applications are
typical use-cases for key-value databases. In AWS, you can utilize Amazon DynamoDB, a fully
managed, multi-Region, multi-master, durable database with built-in security, backup and restore,
and in-memory caching for internet-scale applications.
c.                                                                                                            In-memory databases are used for applications that require real-time access to data, lowest
latency and highest throughput. By storing data directly in memory, these databases deliver
microsecond latency to applications where millisecond latency is not enough. You may use in-
memory databases for application caching, session management, gaming leaderboards, and
geospatial applications. Amazon ElastiCache is a fully managed in-memory data store, compatible
with Redis or Memcached. In case the applications also higher durability requirements, Amazon
MemoryDB for Redis offers this in combination being a durable, in-memory database service for
ultra-fast performance.
d. A document database is designed to store semistructured data as JSON-like documents. These
databases help developers build and update applications such as content management, catalogs,
and user profiles quickly. Amazon DocumentDB is a fast, scalable, highly available, and fully
managed document database service that supports MongoDB workloads.
e.                                                                                                            A wide column store is a type of NoSQL database. It uses tables, rows, and columns, but unlike
a relational database, the names and format of the columns can vary from row to row in the
same table. You typically see a wide column store in high scale industrial apps for equipment
maintenance, fleet management, and route optimization. Amazon Keyspaces (for Apache
Cassandra) is a wide column scalable, highly available, and managed Apache Cassandra-compatible
database service.
f.                                                                                                            Graph databases are for applications that must navigate and query millions of relationships
between highly connected graph datasets with millisecond latency at large scale. Many companies
use graph databases for fraud detection, social networking, and recommendation engines. Amazon
Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run
applications that work with highly connected datasets.
g. Time-series databases efficiently collect, synthesize, and derive insights from data that
changes over time. IoT applications, DevOps, and industrial telemetry can utilize time-series
databases. Amazon Timestream is a fast, scalable, fully managed time series database service for
IoT and operational applications that makes it easy to store and analyze trillions of events per day.
h. Ledger databases provide a centralized and trusted authority to maintain a scalable,
immutable, and cryptographically verifiable record of transactions for every application. We
see ledger databases used for systems of record, supply chain, registrations, and even banking
transactions. Amazon Quantum Ledger Database (Amazon QLDB) is a fully managed ledger
database that provides a transparent, immutable, and cryptographically verifiable transaction log
408

AWS Well-Architected Framework
Selection
owned by a central trusted authority. Amazon QLDB tracks every application data change and
maintains a complete and verifiable history of changes over time.
Level of effort for the implementation plan: If a workload is moving from one database solution to
another, there could be a high level of effort involved in refactoring the data and application.
Resources
Related documents:
• Cloud Databases with AWS
• AWS Database Caching
• Amazon DynamoDB Accelerator
• Amazon Aurora best practices
• Amazon Redshift performance
• Amazon Athena top 10 performance tips
• Amazon Redshift Spectrum best practices
• Amazon DynamoDB best practices
• Choose between EC2 and Amazon RDS
• Best Practices for Implementing Amazon ElastiCache
Related videos:
• AWS purpose-built databases (DAT209-L)
• Amazon Aurora storage demystified: How it all works (DAT309-R)
• Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)
Related examples:
• Optimize Data Pattern using Amazon Redshift Data Sharing
• Database Migrations
• MS SQL Server - AWS Database Migration Service (DMS) Replication Demo
• Database Modernization Hands On Workshop
• Amazon Neptune Samples
PERF04-BP02 Evaluate the available options
Understand the available database options and how it can optimize your performance before you select
your data management solution. Use load testing to identify database metrics that matter for your
workload. While you explore the database options, take into consideration various aspects such as the
parameter groups, storage options, memory, compute, read replica, eventual consistency, connection
pooling, and caching options. Experiment with these various configuration options to improve the
metrics.
Desired outcome: A workload could have one or more database solutions used based on data types.
The database functionality and benefits optimally match the data characteristics, access patterns, and
workload requirements. To optimize your database performance and cost, you must evaluate the data
access patterns to determine the appropriate database options. Evaluate the acceptable query times to
ensure that the selected database options can meet the requirements.
Common anti-patterns:
409

AWS Well-Architected Framework
Selection
• Not identifying the data access patterns.
• Not being aware of the configuration options of your chosen data management solution.
• Relying solely on increasing the instance size without looking at other available configuration options.
• Not testing the scaling characteristics of the chosen solution.
Benefits of establishing this best practice: By exploring and experimenting with the database options
you may be able to reduce the cost of infrastructure, improve performance and scalability and lower the
effort required to maintain your workloads.
Level of risk exposed if this best practice is not established: High
• Having to optimize for a one size fits all database means making unnecessary compromises.
• Higher costs as a result of not configuring the database solution to match the traffic patterns.
• Operational issues may emerge from scaling issues.
• Data may not be secured to the level required.
Implementation guidance
Understand your workload data characteristics so that you can configure your database options. Run
load tests to identify your key performance metrics and bottlenecks. Use these characteristics and
metrics to evaluate database options and experiment with different configurations.
AWS                                                                                                          Amazon       Amazon                Amazon     Amazon                        Amazon      Amazon                Amazon              Amazon
Services                                                                                                     RDS,         DynamoD                          Document ElastiCach Neptune               Timestrea             Keyspaces           QLDB
                                                                                                             Amazon
                                                                                                             Aurora
Scaling                                                                                                      Increase     Automatic             Increase   Increase                      Increase    AutomaticaAutomatic                       Automatically
Compute                                                                                                      instance     read/                 instance   instance                      instance    scales to             read/               scales to
                                                                                                             size,        write                 size       size, add                     size        adjust                write               adjust
                                                                                                             Aurora       scaling                          nodes to                                  capacity              scaling             capacity
                                                                                                             Serverless   with on-                         cluster                                                         with on-
                                                                                                             instances    demand                                                                                           demand
                                                                                                             autoscale    capacity                                                                                         capacity
                                                                                                             in           mode or                                                                                          mode or
                                                                                                             response     automatic                                                                                        automatic
                                                                                                             to           scaling of                                                                                       scaling of
                                                                                                             changes      provisioned                                                                                      provisioned
                                                                                                             in load      read/                                                                                            read/
                                                                                                                          write                                                                                            write
                                                                                                                          capacity                                                                                         capacity
                                                                                                                          in                                                                                               in
                                                                                                                          provisioned                                                                                      provisioned
                                                                                                                          capacity                                                                                         capacity
                                                                                                                          mode                                                                                             mode
Scaling-                                                                                                     All          Increase              Read       Read                          Read        Automatica            Increase            Automatically
out reads                                                                                                    engines      provisionedreplicas              replicas                      replicas.   scales                provisionedscales
                                                                                                             support      read                                                           Supports                          read                up to
                                                                                                             read         capacity                                                       automatic                         capacity            documented
                                                                                                             replicas.    units                                                          scaling                           units               concurrency
                                                                                                             Aurora                                                                      of read                                               limits
                                                                                                             supports                                                                    replica
                                                                                                             automatic                                                                   instances
410

AWS Well-Architected Framework
Selection
AWS                              Amazon         Amazon               Amazon       Amazon                        Amazon       Amazon          Amazon              Amazon
Services                         RDS,           DynamoD                           Document ElastiCach Neptune                Timestrea       Keyspaces           QLDB
Amazon
Aurora
scaling
of read
replica
instances
Scaling-                         Increasing     Increase             Increasing   Using                         Increasing   Write           Increase            Automatically
out                              instance       provisionedprimary                Redis in                      instance     requests        provisionedscales
writes                           size,          write                instance     cluster                       size         may be          write               up to
                                 batching       capacity             size         mode to                                    throttled       capacity            documented
                                 writes         units.                            distribute                                 while           units.              concurrency
                                 in the         Ensuring                          writes                                     scaling.        Ensuring            limits
                                 application    optimal                           across                                     If you          optimal
                                 or adding      partition                         shards                                     encounter       partition
                                 a queue        key to                                                                       throttling      key to
                                 in front       prevent                                                                      exceptions,     prevent
                                 of the         partition                                                                    continue        partition
                                 database.      level                                                                        to send         level
                                 Horizontal     write                                                                        data            write
                                 scaling        throttling                                                                   at the          throttling
                                 via                                                                                         same (or
                                 application-                                                                                higher)
                                 level                                                                                       throughput
                                 sharding                                                                                    to
                                 across                                                                                      automatically
                                 multiple                                                                                    scale.
                                 instances                                                                                   Batch
writes to
reduce
concurrent
write
requests
Engine                           Parameter      Not                  Parameter    Parameter                     Parameter    Not             Not                 Not
configurati                      groups         applicable           groups       groups                        groups       applicable      applicable          applicable
411

AWS Well-Architected Framework
Selection
AWS                              Amazon                  Amazon           Amazon                   Amazon                        Amazon         Amazon               Amazon        Amazon
Services                         RDS,                    DynamoD                                   Document ElastiCach Neptune                  Timestrea            Keyspaces     QLDB
                                 Amazon
                                 Aurora
Caching                          In-                     DAX              In-                      Primary                       Use the        TimestreamDeploy a                 Not
                                 memory                  (DAX)            memory                   function                      query          has two              separate      applicable
                                 caching,                fully            caching.                 is                            results        storage              dedicated
                                 configurablm            anaged           Optionally,              caching                       cache to       tiers; one           cache
                                 via                     cache            pair                                                   cache the      of these             such as
                                 parameter               available        with a                                                 result of      is a high-           ElastiCache
                                 groups.                                  dedicated                                              a read-        performancfor
                                 Pair                                     cache                                                  only           in-                  Redis to
                                 with a                                   such as                                                query          memory               offload
                                 dedicated                                ElastiCache                                                           tier                 requests
                                 cache                                    for                                                                                        for
                                 such as                                  Redis to                                                                                   commonly
                                 ElastiCache                              offload                                                                                    accessed
                                 for                                      requests                                                                                   items
                                 Redis to                                 for
                                 offload                                  commonly
                                 requests                                 accessed
                                 for                                      items
                                 commonly
                                 accessed
                                 items
High                             RecommenHighly                           Create                   RecommenRead                                 Highly               Highly        Highly
                                 configuratioavailable                    multiple                 configuratioreplicas                         available            available     available
disaster                         for                     within a         instances                for                           in other       within a             within a      within a
recovery                         production              Region.          across                   production                    Availability   Region.              Region.       Region.
                                 workloads               Tables           Availability             clusters                      Zones          cross-               Cross-        To
                                 is to                   can be           Zones for                is to                         serve as       Region               Region        replicate
                                 run a                   replicated       availability.create at                                 failover       replication          Replication   across
                                 standby                 across                                    least one                     targets.       requires             requires      Regions,
                                 instance                Regions          Snapshots                node in a                     Snapshots      custom               custom        export
                                 in a                    using            can be                   secondary                     can be         application          application   the
                                 second                  DynamoDBshared                            Availability                  shared         developme lt         ogic or       contents
                                 Availability            global           across                   Zone.                         across         using the            third-        of the
                                 Zone to                 tables           Regions                  ElastiCacheRegion                            Timestreamparty                    Amazon
                                 provide                                  and                      Global                        and            SDK                  tools         QLDB
                                 resiliency                               clusters                 Datastore                     clusters                                          journal
                                 within a                                 can be                   can be                        can be                                            to a S3
                                 Region.                                  replicated               used to                       replicated                                        bucket
                                 For                                      using                    replicate                     using                                             and
                                 resiliency                               DMS to                   clusters                      Neptune                                           configure
                                 across                                   provide                  across                        streams                                           the
                                 Regions,                                 Cross-                   Regions.                      to                                                bucket
                                 Aurora                                   Region                                                 replicate                                         for
                                 Global                                   Replication /                                          data                                              Cross-
                                 Database                                 disaster                                               between                                           Region
                                 can be                                   recovery                                               two                                               Replication.
                                 used                                                                                            clusters
in two
different
Regions.
412

AWS Well-Architected Framework
Selection
Implementation steps
1. What configuration options are available for the selected databases?
a. Parameter Groups for Amazon RDS and Aurora allow you to adjust common database engine level
settings such as the memory allocated for the cache or adjusting the time zone of the database
b. For provisioned database services such as Amazon RDS, Aurora, Neptune, Amazon DocumentDB
and those deployed on Amazon EC2 you can change the instance type, provisioned storage and add
read replicas.
c. DynamoDB allows you to specify two capacity modes: on-demand and provisioned. To account for
differing workloads, you can change between these modes and increase the allocated capacity in
provisioned mode at any time.
2. Is the workload read or write heavy?
a. What solutions are available for offloading reads (read replicas, caching, etc.)?
i.                                                                                                            For DynamoDB tables, you can offload reads using DAX for caching.
ii. For relational databases, you can create an ElastiCache for Redis cluster and configure your
application to read from the cache first, falling back to the database if the requested item is not
present.
iii. Relational databases such as Amazon RDS and Aurora, and provisioned NoSQL databases such as
Neptune and Amazon DocumentDB all support adding read replicas to offload the read portions
of the workload.
iv. Serverless databases such as DynamoDB will scale automatically. Ensure that you have enough
read capacity units (RCU) provisioned to handle the workload.
b. What solutions are available for scaling writes (partition key sharding, introducing a queue, etc.)?
i.                                                                                                            For relational databases, you can increase the size of the instance to accommodate an increased
workload or increase the provisioned IOPs to allow for an increased throughput to the
underlying storage.
• You can also introduce a queue in front of your database rather than writing directly to the
database. This pattern allows you to decouple the ingestion from the database and control the
flow-rate so the database does not get overwhelmed.
• Batching your write requests rather than creating many short-lived transactions can help
improve throughput in high-write volume relational databases.
ii. Serverless databases like DynamoDB can scale the write throughput automatically or by
adjusting the provisioned write capacity units (WCU) depending on the capacity mode.
• You can still run into issues with hot partitions though, when you reach the throughput limits
for a given partition key. This can be mitigated by choosing a more evenly distributed partition
key or by write-sharding the partition key.
3. What are the current or expected peak transactions per second (TPS)? Test using this volume of traffic
and this volume +X% to understand the scaling characteristics.
a. Native tools such as pg_bench for PostgreSQL can be used to stress-test the database and
understand the bottlenecks and scaling characteristics.
b. Production-like traffic should be captured so that it can be replayed to simulate real-world
conditions in addition to synthetic workloads.
4. If using serverless or elastically scalable compute, test the impact of scaling this on the database. If
appropriate, introduce connection management or pooling to lower impact on the database.
a. RDS Proxy can be used with Amazon RDS and Aurora to manage connections to the database.
b. Serverless databases such as DynamoDB do not have connections associated with them, but
consider the provisioned capacity and automatic scaling policies to deal with spikes in load.
5. Is the load predictable, are there spikes in load and periods of inactivity?
a. If there are periods of inactivity consider scaling down the provisioned capacity or instance size
during these times. Aurora Serverless V                                                                       413

AWS Well-Architected Framework
Selection
b. For non-production instances, consider pausing or stopping these during non-work hours.
6. Do you need to segment and break apart your data models based on access patterns and data
characteristics?
a. Consider using AWS DMS or AWS SCT to move your data to other services.
Level of effort for the implementation plan:
To establish this best practice, you must be aware of your current data characteristics and metrics.
Gathering those metrics, establishing a baseline and then using those metrics to identify the ideal
database configuration options is a low to moderate level of effort. This is best validated by load tests
and experimentation.
Resources
Related documents:
• Cloud Databases with AWS
• AWS Database Caching
• Amazon DynamoDB Accelerator
• Amazon Aurora best practices
• Amazon Redshift performance
• Amazon Athena top 10 performance tips
• Amazon Redshift Spectrum best practices
• Amazon DynamoDB best practices
Related videos:
• AWS purpose-built databases (DAT209-L)
• Amazon Aurora storage demystified: How it all works (DAT309-R)
• Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)
Related examples:
• Amazon DynamoDB Examples
• AWS Database migration samples
• Database Modernization Workshop
• Working with parameters on your Amazon RDS for Postgress DB
PERF04-BP03 Collect and record database performance metrics
To understand how your data management systems are performing, it is important to track relevant
metrics. These metrics will help you to optimize your data management resources, to ensure that your
workload requirements are met, and that you have a clear overview on how the workload performs. Use
tools, libraries, and systems that record performance measurements related to database performance.
There are metrics that are related to the system on which the database is being hosted (for example,
CPU, storage, memory, IOPS), and there are metrics for accessing the data itself (for example,
transactions per second, queries rates, response times, errors). These metrics should be readily accessible
for any support or operational staff, and have sufficient historical record to be able to identify trends,
anomalies, and bottlenecks.
414

AWS Well-Architected Framework
Selection
Desired outcome: To monitor the performance of your database workloads, you must record multiple
performance metrics over a period of time. This allows you to detect anomalies as well as measure
performance against business metrics to ensure you are meeting your workload needs.
Common anti-patterns:
• You only use manual log file searching for metrics.
• You only publish metrics to internal tools used by your team and don’t have a comprehensive picture
of your workload.
• You only use the default metrics recorded by your selected monitoring software.
• You only review metrics when there is an issue.
• You only monitor system level metrics, not capturing data access or usage metrics.
Benefits of establishing this best practice: Establishing a performance baseline helps in understanding
normal behavior and requirements of workloads. Abnormal patterns can be identified and debugged
faster improving performance and reliability of the database. Database capacity can be configured to
ensure optimal cost without compromising performance.
Level of risk exposed if this best practice is not established: High
• Inability to differentiate out of normal vs. normal performance level will create difficulties in issue
identification, and decision making.
• Potential cost savings may not be identified.
• Growth patterns will not be identified which might result in reliability or performance degradation.
Implementation guidance
Identify, collect, aggregate, and correlate database-related metrics. Metrics should include both
the underlying system that is supporting the database and the database metrics. The underlying
system metrics might include CPU utilization, memory, available disk storage, disk I/O, and network
inbound and outbound metrics while the database metrics might include transactions per second, top
queries, average queries rates, response times, index usage, table locks, query timeouts, and number
of connections open. This data is crucial to understand how the workload is performing and how the
database solution is used. Use these metrics as part of a data-driven approach to tune and optimize your
workload's resources.
Implementation steps:
1. Which database metrics are important to track?
a. Monitoring metrics for Amazon RDS
b. Monitoring with Performance Insights
c. Enhanced monitoring
d. DynamoDB metrics
e. Monitoring DynamoDB DAX
f. Monitoring MemoryDB
g. Monitoring Amazon Redshift
h. Timeseries metrics and dimensions
i.  Cluster level metrics for Aurora
j.  Monitoring Amazon Keyspaces
k. Monitoring Amazon Neptune
2. Would the database monitoring benefit from a machine learning solution that detects operational
anomalies performance issues?
415

AWS Well-Architected Framework
Selection
a. Amazon DevOps Guru for Amazon RDS provides visibility into performance issues and makes
recommendations for corrective actions.
3. Do you need application level details about SQL usage?
a. AWS X-Ray can be instrumented into the application to gain insights and encapsulate all the data
points for single query.
4. Do you currently have an approved logging and monitoring solution?
a. Amazon CloudWatch can collect metrics across the resources in your architecture. You can also
collect and publish custom metrics to surface business or derived metrics. Use CloudWatch or third-
party solutions to set alarms that indicate when thresholds are breached.
5. You identified and configured your data retention policies to match my security and operational
goals?
a. Default data retention for CloudWatch metrics
b. Default data retention for CloudWatch Logs
Level of effort for the implementation plan: There is a medium level of effort to identify, track, collect,
aggregate, and correlate metrics from all database resources.
Resources
Related documents:
• AWS Database Caching
• Amazon Athena top 10 performance tips
• Amazon Aurora best practices
• Amazon DynamoDB Accelerator
• Amazon DynamoDB best practices
• Amazon Redshift Spectrum best practices
• Amazon Redshift performance
• Cloud Databases with AWS
• Amazon RDS Performance Insights
Related videos:
• AWS purpose-built databases (DAT209-L)
• Amazon Aurora storage demystified: How it all works (DAT309-R)
• Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)
Related examples:
• Level 100: Monitoring with CloudWatch Dashboards
• AWS Dataset Ingestion Metrics Collection Framework
• Amazon RDS Monitoring Workshop
PERF04-BP04 Choose data storage based on access patterns
Use the access patterns of the workload and requirements of the applications to decide on optimal data
services and technologies to use.
Desired outcome: Data storage has been selected based on identified and documented data access
patterns. This might include the most common read, write, and delete queries, the need for as necessary
416

AWS Well-Architected Framework
Selection
calculations and aggregations, complexity of the data, data interdependency, and the required
consistency needs.
Common anti-patterns:
• You only select one database engine to simplify operations management.
• You assume that data access patterns will stay consistent over time.
• You implement complex transactions, rollback, and consistency logic in the application.
• The database is configured to support a potential high traffic burst, which results in the database
resources remaining idle most of the time.
• Using a shared database for transactional and analytical uses.
Benefits of establishing this best practice: Selecting and optimizing your data storage based on access
patterns will help decrease development complexity and optimize your performance opportunities.
Understanding when to use read replicas, global tables, data partitioning, and caching will help you
decrease operational overhead and scale based on your workload needs.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Identify and evaluate your data access pattern to select the correct storage configuration. Each database
solution has options to configure and optimize your storage solution. Use the collected metrics and logs
and experiment with options to find the optimal configuration. Use the following table to review storage
options per database service.
AWS                                                                                                         Amazon              Amazon               Amazon            Amazon                      Amazon     Amazon          Amazon                  Amazon                   Amazon
Services                                                                                                    RDS                 Aurora                                 DynamoD Documen ElastiCac              Neptune         Timestre                                         Keyspace QLDB
Scaling                                                                                                     Storage             Storage              Storage           Storage                     Storage    Storage         Organizes               Scales                   Storage
Storage                                                                                                     can be              scales               automaticscales                               is in-     scales          your                    table                    automatically
                                                                                                            scaled              automaticscales.                       automaticmemory,                       automatictime                           storage                  scales.
                                                                                                            up                  up to                Tables            up to                       tied to    can             series                  up and                   Tables
                                                                                                            manually            maximum              are               maximum                     instance   grow up         data to                 down                     are
                                                                                                            or                  of 128               unconstraiof 64                               type or    to 128          optimize                automaticunconstrained
                                                                                                            configuredTiB and                        in terms          TiB.                        count.     TiB (or         query                   as your                  in terms
                                                                                                            to scale            decreases            of size.          Starting                               64 TiB          processingapplication                            of size.
                                                                                                            automaticwhen                                              Amazon                                 in few          and                     writes,
                                                                                                            to a                data is                                DocumentDB                             Regions).       reduce                  updates,
                                                                                                            maximum             removed.                               4.0                                    Upon            storage                 and
                                                                                                            of 64               Maximum                                storage                                data            costs.                  deletes
                                                                                                            TiB                 storage                                can                                    removal         Retention               data.
                                                                                                            based               size also                              decrease                               from,           period
                                                                                                            on                  depends                                by                                     total           can be
                                                                                                            engine              upon                                   comparable                             allocated       configured
                                                                                                            types.              specific                               amounts                                space           through
                                                                                                            ProvisioneAurora                                           for data                               remains         in-
                                                                                                            storage             MySQL                                  removal                                same            memory
                                                                                                            cannot              or                                     through                                and is          and
                                                                                                            be                  Aurora                                 dropping                               reused          magnetic
                                                                                                                                decreased.Postgres                     a                                      in the          tiers.
                                                                                                                                engine                                 collection                             future.
                                                                                                                                versions.                              or
index.
With
Amazon
417

AWS Well-Architected Framework
Selection
AWS                                                                                                            Amazon   Amazon   Amazon   Amazon                      Amazon   Amazon    Amazon     Amazon          Amazon
Services                                                                                                       RDS      Aurora            DynamoD Documen ElastiCac            Neptune   Timestre   Keyspace QLDB
DocumentDB
3.6
allocated
space
remains
same
and free
space is
reused
when
data
volume
increases.
Implementation steps:
1. Understand the requirement of transactions, atomicity, consistency, isolation, and durability (ACID)
compliance, and consistent reads. Not every database supports these and most of the NoSQL
databases provide an eventual consistency model.
2. Consider the traffic patterns, latency, and access requirements for a globally distributed application in
order to identify the optimal storage solution.
3. Analyze query patterns, random access patterns and one-time queries. Considerations around highly
specialized query functionality for text and natural language processing, time series, and graphs must
also be taken into account.
4. Identify and document the anticipated growth of the data and traffic.
a. Amazon RDS and Aurora support storage automatic scaling up to documented limits. Beyond
this, consider transitioning older data to Amazon S3 for archival, aggregating historical data for
analytics or scaling horizontally using sharding.
b. DynamoDB and Amazon S3 will scale to near limitless storage volume automatically.
c. Amazon RDS instances and databases running on EC2 can be manually resized and EC2 instances
can have new EBS volumes added at a later date for additional storage.
d. Instance types can be changed based on changes in activity. For example, you can start with a
smaller instance while you are testing, then scale the instance as you begin to receive production
traffic to the service. Aurora Serverless V2 automatically scales in response to changes in load.
5. Baseline requirements around normal and peak performance (transactions per second TPS and queries
per second QPS) and consistency (ACID and eventual consistency).
6. Document solution deployment aspects and the database access requirements (like global replication,
Multi-AZ, read replication, and multiple write nodes).
Level of effort for the implementation plan:  Low. If you do not have logs or metrics for your data
management solution, you will need to complete that before identifying and documenting your data
access patterns. Once your data access pattern is understood, selecting and configuring your data
storage is a low level of effort.
Resources
Related documents:
• Cloud Databases with AWS
• Working with storage for Amazon RDS DB instances
418

AWS Well-Architected Framework
Selection
• Amazon DocumentDB Storage
• AWS Database Caching
• Amazon Timestream Storage
• Storage in Amazon Keyspaces
• Amazon ElastiCache FAQs
• Amazon Neptune storage, reliability, and availability
• Amazon Aurora best practices
• Amazon DynamoDB Accelerator
• Amazon DynamoDB best practices
• Amazon RDS Storage Types
• Hardware specifications for Amazon RDS instance classes
• Aurora Storage limits
Related videos:
• AWS purpose-built databases (DAT209-L)
• Amazon Aurora storage demystified: How it all works (DAT309-R)
• Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)
Related examples:
• Experiment and test with Distributed Load Testing on AWS
PERF04-BP05 Optimize data storage based on access patterns and metrics
Use performance characteristics and access patterns that optimize how data is stored or queried to
achieve the best possible performance. Measure how optimizations such as indexing, key distribution,
data warehouse design, or caching strategies impact system performance or overall efficiency.
Common anti-patterns:
• You only use manual log file searching for metrics.
• You only publish metrics to internal tools.
Benefits of establishing this best practice: In order to ensure you are meeting the metrics required for
the workload, you must monitor database performance metrics related to both reads and writes. You can
use this data to add new optimizations for both reads and writes to the data storage layer.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Optimize data storage based on metrics and patterns: Use reported metrics to identify any
underperforming areas in your workload and optimize your database components. Each database system
has different performance related characteristics to evaluate, such as how data is indexed, cached, or
distributed among multiple systems. Measure the impact of your optimizations.
Resources
Related documents:
419

AWS Well-Architected Framework
Selection
• AWS Database Caching
• Amazon Athena top 10 performance tips
• Amazon Aurora best practices
• Amazon DynamoDB Accelerator
• Amazon DynamoDB best practices
• Amazon Redshift Spectrum best practices
• Amazon Redshift performance
• Cloud Databases with AWS
• Analyzing performance anomalies with DevOps Guru for RDS
• Read/Write Capacity Mode for DynamoDB
Related videos:
• AWS purpose-built databases (DAT209-L)
• Amazon Aurora storage demystified: How it all works (DAT309-R)
• Amazon DynamoDB deep dive: Advanced design patterns (DAT403-R1)
Related examples:
• Hands-on Labs for Amazon DynamoDB
PERF 5. How do you configure your networking solution?
The most effective network solution for a workload varies based on latency, throughput requirements,
jitter, and bandwidth. Physical constraints, such as user or on-premises resources, determine location
options. These constraints can be offset with edge locations or resource placement.
Best practices
• PERF05-BP01 Understand how networking impacts performance (p. 420)
• PERF05-BP02 Evaluate available networking features (p. 422)
• PERF05-BP03 Choose appropriately sized dedicated connectivity or VPN for hybrid
workloads (p. 426)
• PERF05-BP04 Leverage load-balancing and encryption offloading (p. 428)
• PERF05-BP05 Choose network protocols to improve performance (p. 431)
• PERF05-BP06 Choose your workload’s location based on network requirements (p. 433)
• PERF05-BP07 Optimize network configuration based on metrics (p. 436)
PERF05-BP01 Understand how networking impacts performance
Analyze and understand how network-related decisions impact workload performance. The network is
responsible for the connectivity between application components, cloud services, edge networks and
on-premises data and therefor it can highly impact workload performance. In addition to workload
performance, user experience is also impacted by network latency, bandwidth, protocols, location,
network congestion, jitter, throughput, and routing rules.
Desired outcome: Have a documented list of networking requirements from the workload including
latency, packet size, routing rules, protocols, and supporting traffic patterns. Review the available
networking solutions and identify which service meets your workload networking characteristics. Cloud-
420

AWS Well-Architected Framework
Selection
based networks can be quickly rebuilt, so evolving your network architecture over time is necessary to
improve performance efficiency.
Common anti-patterns:
• All traffic flows through your existing data centers.
• You overbuild Direct Connect sessions without understanding the actual usage requirements.
• You don’t consider workload characteristics and encryption overhead when defining your networking
solutions.
• You use on-premises concepts and strategies for networking solutions in the cloud.
Benefits of establishing this best practice: Understanding how networking impacts workload
performance will help you identify potential bottlenecks, improve user experience, increase reliability,
and lower operational maintenance as the workload changes.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Identify important network performance metrics of your workload and capture its networking
characteristics. Define and document requirements as part of a data-driven approach, using
benchmarking or load testing. Use this data to identify where your network solution is constrained,
and examine configuration options that could improve the workload. Understand the cloud-native
networking features and options available and how they can impact your workload performance based
on the requirements. Each networking feature has advantages and disadvantages and can be configured
to meet your workload characteristics and scale based on your needs.
Implementation steps:
1. Define and document networking performance requirements:
a. Include metrics such as network latency, bandwidth, protocols, locations, traffic patterns (spikes
and frequency), throughput, encryption, inspection, and routing rules
2. Capture your foundational networking characteristics:
a. VPC Flow Logs
b. AWS Transit Gateway metrics
c. AWS PrivateLink metrics
3. Capture your application networking characteristics:
a. Elastic Network Adaptor
b. AWS App Mesh metrics
c. Amazon API Gateway metrics
4. Capture your edge networking characteristics:
a. Amazon CloudFront metrics
b. Amazon Route 53 metrics
c. AWS Global Accelerator metrics
5. Capture your hybrid networking characteristics:
a. Direct Connect metrics
b. AWS Site-to-Site VPN metrics
c. AWS Client VPN metrics
d. AWS Cloud WAN metrics
6. Capture your security networking characteristics:
a. AWS Shield, WAF, and Network Firewall metrics
421

AWS Well-Architected Framework
Selection
7. Capture end-to-end performance metrics with tracing tools:
a. AWS X-Ray
b. Amazon CloudWatch RUM
8. Benchmark and test network performance:
a. Benchmark network throughput: Some factors that can affect EC2 network performance when the
instances are in the same VPC. Measure the network bandwidth between EC2 Linux instances in the
same VPC.
b. Perform load tests to experiment with networking solutions and options
Level of effort for the implementation plan: There is a medium level of effort to document workload
networking requirements, options, and available solutions.
Resources
Related documents:
• Application Load Balancer
• EC2 Enhanced Networking on Linux
• EC2 Enhanced Networking on Windows
• EC2 Placement Groups
• Enabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux Instances
• Network Load Balancer
• Networking Products with AWS
• Transit Gateway
• Transitioning to latency-based routing in Amazon Route 53
• VPC Endpoints
• VPC Flow Logs
Related videos:
• Connectivity to AWS and hybrid AWS network architectures (NET317-R1)
• Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)
• Improve Global Network Performance for Applications
• EC2 Instances and Performance Optimization Best Practices
• Optimizing Network Performance for Amazon EC2 Instances
• Networking best practices and tips with the Well-Architected Framework
• AWS networking best practices in large-scale migrations
Related examples:
• AWS Transit Gateway and Scalable Security Solutions
• AWS Networking Workshops
PERF05-BP02 Evaluate available networking features
Evaluate networking features in the cloud that may increase performance. Measure the impact of these
features through testing, metrics, and analysis. For example, take advantage of network-level features
that are available to reduce latency, packet loss, or jitter.
422

AWS Well-Architected Framework
Selection
Desired outcome: You have documented the inventory of components within your workload and
have identified which networking configurations per component will help you meet your performance
requirements. After evaluating the networking features, you have experimented and measured the
performance metrics to identify how to use the features available to you.
Common anti-patterns:
• You put all your workloads into an AWS Region closest to your headquarters instead of an AWS Region
close to your users.
• You fail to benchmark your workload performance and do not continually evaluate your workload
performance against that benchmark.
• You do not review service configurations for performance improving options.
Benefits of establishing this best practice: Evaluating all service features and options can increase your
workload performance, reduce the cost of infrastructure, decrease the effort required to maintain your
workload, and increase your overall security posture. You can use the global AWS backbone to ensure
that you provide the optimal networking experience for your customers.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Review which network-related configuration options are available to you, and review how they could
impact your workload. Performance optimization depends on understanding how these options interact
with your architecture and the impact that they will have on both measured performance and user
experience.
Many services are created to improve performance and others commonly offer features to optimize
network performance. Services such as AWS Global Accelerator and Amazon CloudFront exist to improve
performance while most other services have product features to optimize network traffic. Review service
features, such as Amazon EC2 instance network capability, enhanced networking instance types, Amazon
EBS-optimized instances, Amazon S3 transfer acceleration, and CloudFront, to improve your workload
performance.
Implementation steps:
1. Create a list of workload components.
a. Build, manage and monitor your organizations network using AWS Cloud WAN when building a
unified global network.
b. Monitor your global and core networks with Amazon CloudWatch metrics. Leverage CloudWatch
Real-User Monitoring (RUM), which provides insights to help to identify, understand, and enhance
users’ digital experience.
c. View aggregate network latency between AWS Regions and Availability Zones, as well as within
each Availability Zone, using AWS Network Manager to gain insight into how your application
performance relates to the performance of the underlying AWS network.
d. Use an existing configuration management database (CMDB) tool or a service such as AWS Config
to create an inventory of your workload and how it’s configured.
2. If this is an existing workload, identify and document the benchmark for your performance metrics,
focusing on the bottlenecks and areas to improve. Performance-related networking metrics will
differ per workload based on business requirements and workload characteristics. As a start, these
metrics might be important to review for your workload: bandwidth, latency, packet loss, jitter, and
retransmits.
3. If this is a new workload, perform load tests to identify performance bottlenecks.
4. For the performance bottlenecks you identify, review the configuration options for your solutions to
identify performance improvement opportunities.
423

AWS Well-Architected Framework
Selection
5. If you don’t know your network path or routes, use Network Access Analyzer to identify them.
6. Review your network protocols to further reduce your latency (see PERF05-BP05 Choose network
protocols to improve performance (p. 431)).
7. When connection from on-premises environments to AWS is required, review available configuration
options for connectivity and estimate the bandwidth and latency requirements for your hybrid
workload (see PERF05-BP03 Choose appropriately sized dedicated connectivity or VPN for hybrid
workloads (p. 426)).
• If you are using an AWS Site-to-Site VPN across multiple locations to connect to an AWS Region,
then use an accelerated Site-to-Site VPN connection for the opportunity to improve network
performance.
• If your hybrid network design consists of IPSec VPN connection over AWS Direct Connect, consider
using Private IP VPN to improve security and achieve segmentation. AWS Site-to-Site Private IP VPN
is deployed on top of Transit virtual Interface (VIF).
• AWS Direct Connect SiteLink allows creating low-latency and redundant connections between
your data centers worldwide by sending data over the fastest path between AWS Direct Connect
locations, bypassing AWS Regions.
8. When your workload traffic is spread across multiple accounts, evaluate your network topology and
services to reduce latency.
• Evaluate your operational and performance tradeoffs between VPC Peering and AWS Transit
Gateway when connecting multiple accounts. AWS Transit Gateway supports Equal Cost Multipath
(ECMP) across multiple AWS Site-to-Site VPN connections in order to deliver additional bandwidth.
Traffic between an Amazon VPC and AWS Transit Gateway remains on the private AWS network
and is not exposed to the internet. AWS Transit Gateway simplifies how you interconnect all of your
VPCs, which can span across thousands of AWS accounts and into on-premises networks. Share your
AWS Transit Gateway between multiple accounts using Resource Access Manager.
9. Review your user locations and minimize the distance between your users and the workload.
a. AWS Global Accelerator is a networking service that improves the performance of your users’ traffic
by up to 60% using the AWS global network infrastructure. When the internet is congested, AWS
Global Accelerator optimizes the path to your application to keep packet loss, jitter, and latency
consistently low. It also provides static IP addresses that simplify moving endpoints between
Availability Zones or AWS Regions without needing to update your DNS configuration or change
client-facing applications. Add an accelerator when creating a load balancer to improve the
performance and availability of your workload taking advantages of AWS backbone.
b. Amazon CloudFront can improve the performance of your workload content delivery and latency
globally. CloudFront has over 410 globally dispersed points of presence that can cache your content
and lower the latency to the end user. Using Lambda@edge to run functions that customize the
content that CloudFront delivers closer to the users, reduces latency and Improves performance.
c. Amazon Route 53 offers latency-based routing, geolocation routing, geoproximity routing, and
IP-based routing options to help you improve your workload’s performance for a global audience.
Identify which routing option would optimize your workload performance by reviewing your
workload traffic and user location when your workload is distributed globally.
10Evaluate additional Amazon S3 features to improve storage IOPs.
a. Amazon S3 Transfer acceleration is a feature that lets external users benefit from the networking
optimizations of CloudFront to upload data to Amazon S3. This improves the ability to transfer
large amounts of data from remote locations that don’t have dedicated connectivity to the AWS
Cloud.
b. Amazon S3 Multi-Region Access Points replicates content to multiple Regions and simplifies the
workload by providing one access point. When a Multi-Region Access Point is used, you can request
or write data to Amazon S3 with the service identifying the lowest latency bucket.
11Review your compute resource network bandwidth.
a. Elastic Network Interfaces (ENA) used by EC2 instances, containers, and Lambda functions are
limited on a per-flow basis. Review your placement groups to optimize your EC2 networking
throughput. To avoid the bottleneck at the per flow-basis, design your application to use multiple
424

AWS Well-Architected Framework
Selection
flows. To monitor and get visibility into your compute related networking metrics, use CloudWatch
Metrics and ethtool. ethtool is included in the ENA driver and exposes additional network-
related metrics that can be published as a custom metric to CloudWatch.
b. Newer Amazon EC2 instances can leverage enhanced networking. C7gn instances featuring new
AWS Nitro Cards (Nitro V5) with enhanced networking offer the highest network bandwidth and
packet rate performance across Amazon EC2 network-optimized instances.
c. Amazon Elastic Network Adapters (ENA) provide further optimization by delivering better
throughput for your instances within a cluster placement group. Elastic Network Adapter Express
(ENA-X) is a new ENA feature using scalable reliable datagram (SRD) protocol that improves single
flow bandwidth and lower tail latency by increasing maximum single flow bandwidth of Amazon
EC2 instances from 5 Gbps up to 25 Gbps, and it can provide up to 85% improvement in P99.9
latency for high throughput workloads. ENA-X is currently available for C6gn.16xl.
d. Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that allows you to
run workloads requiring high levels of internode communications at scale on AWS. With EFA, High
Performance Computing (HPC) applications using the Message Passing Interface (MPI) and Machine
Learning (ML) applications using NVIDIA Collective Communications Library (NCCL) can scale to
thousands of CPUs or GPUs.
e. Amazon EBS-optimized instances use an optimized configuration stack and provide additional,
dedicated capacity to increase the Amazon EBS I/O. This optimization provides the best
performance for your EBS volumes by minimizing contention between Amazon EBS I/O and other
traffic from your instance.
Level of effort for the implementation plan:
Low to Medium. To establish this best practice, you must be aware of your current workload component
options that impact network performance. Gathering the components, evaluating network improvement
options, experimenting, implementing, and documenting those improvements is a low to moderate level
of effort.
Resources
Related documents:
• Amazon EBS - Optimized Instances
• Application Load Balancer
• Amazon EC2 instance network bandwidth
• EC2 Enhanced Networking on Linux
• EC2 Enhanced Networking on Windows
• EC2 Placement Groups
• Enabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux Instances
• Network Load Balancer
• Networking Products with AWS
• AWS Transit Gateway
• Transitioning to Latency-Based Routing in Amazon Route 53
• VPC Endpoints
• VPC Flow Logs
• Building a cloud CMDB
• Scaling VPN throughput using AWS Transit Gateway
Related videos:
• Connectivity to AWS and hybrid AWS network architectures (NET317-R1)
425

AWS Well-Architected Framework
Selection
• Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)
• AWS Global Accelerator
Related examples:
• AWS Transit Gateway and Scalable Security Solutions
• AWS Networking Workshops
PERF05-BP03 Choose appropriately sized dedicated connectivity or VPN for
hybrid workloads
When a common network is required to connect on-premises and cloud resources in AWS, verify that
you have adequate bandwidth to meet your performance requirements. Estimate the bandwidth and
latency requirements for your hybrid workload. These numbers will drive the sizing requirements for
your connectivity options.
Desired outcome: When deploying a workload that will need hybrid networking, you have multiple
configuration options for connectivity, such as a dedicated connection or virtual private network (VPN).
Select the appropriate connection type for each workload while verifying that you have adequate
bandwidth and encryption requirements between your location and the cloud.
Common anti-patterns:
• You fail to understand or identify all workload requirements (bandwidth, latency, jitter, encryption and
traffic needs).
• You don’t evaluate backup or parallel connectivity options.
Benefits of establishing this best practice: Selecting and configuring appropriately sized hybrid network
solutions will increase the reliability of your workload and maximize performance opportunities. By
identifying workload requirements, planning ahead, and evaluating hybrid solutions you will minimize
expensive physical network changes and operational overhead while increasing your time to market.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Develop a hybrid networking architecture based on your bandwidth requirements. Estimate the
bandwidth and latency requirements of your hybrid applications. Consider appropriate connectivity
option between using a dedicated network connection or internet-based VPN.
Dedicated connection establishes network connection over private lines. It is suitable when you need
high-bandwidth, low-latency while achieving consistent performance. VPN connection establishes
secure connection over the internet. It is suitable when you need encrypted connection using an existing
internet connection.
Based on your bandwidth requirements, a single VPN or dedicated connection might not be enough, and
you must architect a hybrid setup to permit traffic load balancing across multiple connections.
Implementation steps
1. Estimate the bandwidth and latency requirements of your hybrid applications.
a. For existing apps that are moving to AWS, leverage the data from your internal network monitoring
systems.
b. For new apps or existing apps for which you don’t have monitoring data, consult with the product
owners to derive adequate performance metrics and provide a good user experience.
426

AWS Well-Architected Framework
Selection
2. Select dedicated connection or VPN as your connectivity option. Based on all workload requirements
(encryption, bandwidth and traffic needs), you can either choose AWS Direct Connect or AWS Site-to-
Site VPN (or both). The following diagram will help you choose the appropriate connection type.
a. If you consider dedicated connection, AWS Direct Connect may be required, which offers more
predictable and consistent performance due to its private network connectivity. AWS Direct
Connect provides dedicated connectivity to the AWS environment, from 50 Mbps up to 100 Gbps,
using either dedicated connection or hosted connection. This gives you managed and controlled
latency and provisioned bandwidth so your workload can connect efficiently to other environments.
Using an AWS Direct Connect partners, you can have end-to-end connectivity from multiple
environments, providing an extended network with consistent performance. AWS offers scaling
direct connect connection bandwidth using either native 100 Gbps, Link Aggregation Group (LAG),
or BGP Equal-cost multipath (ECMP).
b. If you consider VPN connection, an AWS managed VPN is the recommended option. The AWS Site-
to-Site VPN provides a managed VPN service supporting Internet Protocol security (IPsec) protocol.
When a VPN connection is created, each VPN connection includes two tunnels for high availability.
With AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also
connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit
Gateway also allows you to scale beyond the 1.25Gbps IPsec VPN throughput limit by allowing
equal cost multi-path (ECMP) routing support over multiple VPN tunnels.
Deterministic performance flowchart.
Level of effort for the implementation plan: High. There is significant effort in evaluating workload
needs for hybrid networks and implementing hybrid networking solutions.
Resources
Related documents:
• Network Load Balancer
• Networking Products with AWS
• AWS Transit Gateway
427

AWS Well-Architected Framework
Selection
• Transitioning to latency-based Routing in Amazon Route 53
• VPC Endpoints
• VPC Flow Logs
• AWS Site-to-Site VPN
• Building a Scalable and Secure Multi-VPC AWS Network Infrastructure
• AWS Direct Connect
• Client VPN
Related videos:
• Connectivity to AWS and hybrid AWS network architectures (NET317-R1)
• Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)
• AWS Global Accelerator
• AWS Direct Connect
• Transit Gateway Connect
• VPN Solutions
• Security with VPN Solutions
Related examples:
• AWS Transit Gateway and Scalable Security Solutions
• AWS Networking Workshops
PERF05-BP04 Leverage load-balancing and encryption offloading
Use load balancers to achieve optimal performance efficiency of your target resources and improve the
responsiveness of your system.
Desired outcome: Reduce the number of computing resources to serve your traffic. Avoid resource
consumption imbalance in your targets. Offload compute-intensive tasks to the Load Balancer. Leverage
cloud elasticity and flexibility to improve performance and optimize your architecture.
Common anti-patterns:
• You don’t consider your workload requirements when choosing the load balancer type.
• You don’t leverage the load balancer features for performance optimization.
• The workload is exposed directly to the Internet without a load balancer.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Load balancers act as the entry point for your workload and from there they distribute the traffic to your
back-end targets, such as compute instances or containers. Choosing the right load balancer type is the
first step to optimize your architecture.
Start by listing your workload characteristics, such as protocol (like TCP, HTTP, TLS, or WebSockets),
target type (like instances, containers, or serverless), application requirements (like long running
connections, user authentication, or stickiness), and placement (like Region, Local Zone, Outpost, or
zonal isolation).
428

AWS Well-Architected Framework
Selection
After choosing the right load balancer, you can start leveraging its features to reduce the amount of
effort your back-end has to do to serve the traffic.
For example, using both Application Load Balancer (ALB) and Network Load Balancer (NLB), you can
perform SSL/TLS encryption offloading, which is an opportunity to avoid the CPU-intensive TLS
handshake from being completed by your targets and also to improve certificate management.
When you configure SSL/TLS offloading in your load balancer, it becomes responsible for the encryption
of the traffic from and to clients while delivering the traffic unencrypted to your back-ends, freeing up
your back-end resources and improving the response time for the clients.
Application Load Balancer can also serve HTTP2 traffic without needing to support it on your targets.
This simple decision can improve your application response time, as HTTP2 uses TCP connections more
efficiently.
Load balancers can also be used to make your architecture more flexible by distributing traffic across
different back-end types such as containers and serverless. For example, Application Load Balancer can
be configured with listener rules that forward traffic to different target groups based on the request
parameters such as header, method or pattern.
Your workload latency requirements should also be considered when defining the architecture. As an
example, if you have a latency-sensitive application, you may decide to use Network Load Balancer,
which offers extremely low latencies. Alternatively, you may decide to bring your workload closer to your
customers by leveraging Application Load Balancer in AWS Local Zones or even AWS Outposts.
Another consideration for latency-sensitive workloads is cross-zone load balancing. With cross-zone
load balancing, each load balancer node distributes traffic across the registered targets in all allowed
Availability Zones. This improves availability, although it can add a single digit millisecond to the
roundtrip latency.
Lastly, both ALB and NLB offer monitoring resources such as logs and metrics. Properly setting up
monitoring can help with gathering performance insights of your application. For example, you can use
ALB access logs to find which requests are taking longer to be answered or which back-end targets are
causing performance issues.
Implementation steps
1. Choose the right load balancer for your workload.
a. Use Application Load Balancer for HTTP/HTTPS workloads.
b. Use Network Load Balancer for non-HTTP workloads that run on TCP or UDP.
c. Use a combination of both (ALB as a target of NLB) if you want to leverage features of both
products. For example, you can do this if you want to use the static IPs of NLB together with
HTTP header based routing from ALB, or if you want to expose your HTTP workload to an AWS
PrivateLink.
d. For a full comparison of load balancers, see ELB product comparison.
2. Use SSL/TLS offloading.
a. Configure HTTPS/TLS listeners with both Application Load Balancer and Network Load Balancer
integrated with AWS Certificate Manager.
b. Note that some workloads may require end-to-end encryption for compliance reasons. In this case,
it is a requirement to allow encryption at the targets.
c. For security best practices, see SEC09-BP02 Enforce encryption in transit.
3. Select the right routing algorithm.
a. The routing algorithm can make a difference in how well-used your back-end targets are and
therefore how they impact performance. For example, ALB provides two options for routing
algorithms:
429

AWS Well-Architected Framework
Selection
b. Least outstanding requests: Use to achieve a better load distribution to your back-end targets for
cases when the requests for your application vary in complexity or your targets vary in processing
capability.
c. Round robin: Use when the requests and targets are similar, or if you need to distribute requests
equally among targets.
4. Consider cross-zone or zonal isolation.
a. Use cross-zone turned off (zonal isolation) for latency improvements and zonal failure domains. It is
turned off by default in NLB and in ALB you can turn it off per target group.
b. Use cross-zone turned on for increased availability and flexibility. By default, cross-zone is turned
on for ALB and in NLB you can turn it on per target group.
5. Turn on HTTP keep-alives for your HTTP workloads.
a. For HTTP workloads, turn on HTTP keep-alive in the web server settings for your back-end targets.
With this feature, the load balancer can reuse backend connections until the keep-alive timeout
expires, improving your HTTP request and response time and also reducing resource utilization on
your back-end targets. For detail on how to do this for Apache and Nginx, see What are the optimal
settings for using Apache or NGINX as a backend server for ELB?
6. Use Elastic Load Balancing integrations for better orchestration of compute resources.
a. Use Auto Scaling integrated with your load balancer. One of the key aspects of a performance
efficient system has to do with right-sizing your back-end resources. To do this, you can leverage
load balancer integrations for back-end target resources. Using the load balancer integration
with Auto Scaling groups, targets will be added or removed from the load balancer as required in
response to incoming traffic.
b. Load balancers can also integrate with Amazon ECS and Amazon EKS for containerised workloads.
• Use Elastic Load Balancing to distribute traffic across the instances in your Auto Scaling group
• Amazon ECS - Service load balancing
• Application load balancing on Amazon EKS
• Network load balancing on Amazon EKS
7. Monitor your load balancer to find performance bottlenecks.
a. Turn on access logs for your Application Load Balancer and Network Load Balancer.
b. The main fields to consider for ALB are request_processing_time,
request_processing_time, and response_processing_time.
c. The main fields to consider for NLB are connection_time and tls_handshake_time.
d. Be ready to query the logs when you need them. You can use Amazon Athena to query both ALB
logs and NLB Logs.
e. Create alarms for performance related metrics such as TargetResponseTime for ALB.
Resources
Related best practices:
• SEC09-BP02 Enforce encryption in transit
Related documents:
• ELB product comparison
• AWS Global Infrastructure
• Improving Performance and Reducing Cost Using Availability Zone Affinity
• Step by step for Log Analysis with Amazon Athena
• Querying Application Load Balancer logs
• Monitor your Application Load Balancers
430

AWS Well-Architected Framework
Selection
• Monitor your Network Load Balancers
Related videos:
• AWS re:Invent 2018: [REPEAT 1] Elastic Load Balancing: Deep Dive and Best Practices (NET404-R1)
• AWS re:Invent 2021 - How to choose the right load balancer for your AWS workloads
• AWS re:Inforce 2022 - How to use Elastic Load Balancing to enhance your security posture at scale
(NIS203)
• AWS re:Invent 2019: Get the most from Elastic Load Balancing for different workloads (NET407-R2)
Related examples:
• CDK and CloudFormation samples for Log Analysis with Amazon Athena
PERF05-BP05 Choose network protocols to improve performance
Assess the performance requirements for your workload, and choose the network protocols that
optimize your workload’s overall performance.
There is a relationship between latency and bandwidth to achieve throughput. For instance, if your file
transfer is using Transmission Control Protocol (TCP), higher latencies will reduce overall throughput.
There are approaches to fix this with TCP tuning and optimized transfer protocols (some approaches use
User Datagram Protocol (UDP)).
The scalable reliable datagram (SRD) protocol is a network transport protocol built by AWS for Elastic
Fabric Adapters that provides reliable datagram delivery. Unlike the TCP protocol, SRD can reorder
packets and deliver them out of order. This out of order delivery mechanism of SRD sends packets in
parallel over alternate paths, increasing throughput.
Common anti-patterns:
• Using TCP for all workloads regardless of performance requirements.
Benefits of establishing this best practice:
• Selecting the proper protocol for communication between workload components ensures that you are
getting the best performance for that workload.
• Verifying that an appropriate protocol is used for communication between users and workload
components helps improve overall user experience for your applications. For instance, by using both
TCP and UDP together, VDI workloads can take advantage of the reliability of TCP for critical data and
the speed of UDP for real-time data.
Level of risk exposed if this best practice is not established: Medium (Using an inappropriate network
protocol can lead to poor performance, such as slow response times, high latency and poor scalability)
Implementation guidance
A primary consideration for improving your workload’s performance is to understand the latency and
throughput requirements, and then choose network protocols that optimize performance.
When to consider using TCP
TCP provides reliable data delivery, and can be used for communication between workload components
where reliability and guaranteed delivery of data is important. Many web-based applications rely on TCP-
based protocols, such as HTTP and HTTPS, to open TCP sockets for communication with servers on AWS.
431

AWS Well-Architected Framework
Selection
Email and file data transfer are common applications that also make use of TCP due to TCP’s ability to
control the rate of data exchange and network congestion. Using TLS with TCP can add some overhead
to the communication, which can result in increased latency and reduced throughput. The overhead
comes mainly from the added overhead of the handshake process, which can take several round-trips to
complete. Once the handshake is complete, the overhead of encrypting and decrypting data is relatively
small.
When to consider using UDP
UDP is a connectionless-oriented protocol and is therefore suitable for applications that need fast,
efficient transmission, such as log, monitoring, and VoIP data. Also, consider using UDP if you have
workload components that respond to small queries from large numbers of clients to ensure optimal
performance of the workload. Datagram Transport Layer Security (DTLS) is the UDP equivalent of
TLS. When using DTLS with UDP, the overhead comes from encrypting and decrypting the data, as the
handshake process is simplified. DTLS also adds a small amount of overhead to the UDP packets, as it
includes additional fields to indicate the security parameters and to detect tampering.
When to consider using SRD
Scalable reliable datagram (SRD) is a network transport protocol optimized for high-throughput
workloads due to its ability to load-balancer traffic across multiple paths and quickly recover from packet
drops or link failures. SRD is therefore best used for high performance computing (HPC) workloads that
require high throughput and low latency communication between compute nodes. This might include
parallel processing tasks such as simulation, modelling, and data analysis that involve a large amount of
data transfer between nodes.
Implementation steps
1. Use the AWS Global Accelerator and AWS Transfer Family services to improve the throughput of your
online file transfer applications. The AWS Global Accelerator service helps you achieve lower latency
between your client devices and your workload on AWS. With AWS Transfer Family, you can use TCP-
based protocols such as Secure Shell File Transfer Protocol (SFTP) and File Transfer Protocol over SSL
(FTPS) to securely scale and manage your file transfers to AWS storage services.
2. Use network latency to determine if TCP is appropriate for communication between workload
components. If the network latency between your client application and server is high, then the
TCP three-way handshake can take some time, thereby impacting on the responsiveness of your
application. Metrics such as Time to First Byte (TTFB) and Round-Trip Time (RTT) can be used to
measure network latency. If your workload serves dynamic content to users, consider using Amazon
CloudFront, which establishes a persistent connection to each origin for dynamic content to eliminate
the connection setup time that would otherwise slow down each client request.
3. Using TLS with TCP or UDP can result in increased latency and reduced throughput for your workload
due to the impact of encryption and decryption. For such workloads, consider SSL/TLS offloading
on Elastic Load Balancing to improve workload performance by allowing the load balancer to handle
SSL/TLS encryption and decryption process instead of having backend instances do it. This can help
reduce the CPU utilization on the backend instances, which can improve performance and increase
capacity.
4. Use the Network Load Balancer (NLB) to deploy services that rely on the UDP protocol, such
as authentication and authorization, logging, DNS, IoT, and streaming media, to improve the
performance and reliability of your workload. The NLB distributes incoming UDP traffic across
multiple targets, allowing you to scale your workload horizontally, increase capacity, and reduce the
overhead of a single target.
5. For your High Performance Computing (HPC) workloads, consider using the Elastic Network Adapter
(ENA) Express functionality that uses the SRD protocol to improve network performance by providing
a higher single flow bandwidth (25Gbps) and lower tail latency (99.9 percentile) for network traffic
between EC2 instances.
6. Use the Application Load Balancer (ALB) to route and load balance your gRPC (Remote Procedure
Calls) traffic between workload components or between gRPC clients and services. gRPC uses the
432

AWS Well-Architected Framework
Selection
TCP-based HTTP/2 protocol for transport and it provides performance benefits such as lighter
network footprint, compression, efficient binary serialization, support for numerous languages, and
bi-directional streaming.
Resources
Related documents:
• Amazon EBS - Optimized Instances
• Application Load Balancer
• EC2 Enhanced Networking on Linux
• EC2 Enhanced Networking on Windows
• EC2 Placement Groups
• Enabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux Instances
• Network Load Balancer
• Networking Products with AWS
• Transit Gateway
• Transitioning to Latency-Based Routing in Amazon Route 53
• VPC Endpoints
• VPC Flow Logs
Related videos:
• Connectivity to AWS and hybrid AWS network architectures (NET317-R1)
• Optimizing Network Performance for Amazon EC2 Instances (CMP308-R1)
• Tuning Your Cloud: Improve Global Network Performance for Application
• Application Scaling with EFA and SRD
Related examples:
• AWS Transit Gateway and Scalable Security Solutions
• AWS Networking Workshops
PERF05-BP06 Choose your workload’s location based on network requirements
Evaluate options for resource placement to reduce network latency and improve throughput, providing
an optimal user experience by reducing page load and data transfer times.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Resources, such as Amazon EC2 instances, are placed into availability zones within AWS Regions, AWS
Local Zones, AWS Outposts, or AWS Wavelength zones. Selection of this location influences network
latency and throughput from a given user location. Edge services such as Amazon CloudFront and AWS
Global Accelerator can also be used to improve network performance by either caching content at edge
locations or providing users with an optimal path to the workload through the AWS global network.
Implementation steps
1. Choose the appropriate AWS Region or Regions for your deployment based on the following key
elements:
433

AWS Well-Architected Framework
Selection
a. Where your users are located: choosing a Region close to your workload’s users to ensure low
latency when they use the workload.
b. Where your data is located: for data-heavy applications, the major bottleneck in data transfer is
latency. Application code should run as close to the data as possible.
c. Other constraints: consider constraints such as security and compliance (for example, data
residency requirements).
2. For a given workload, if a component consists of a group of interdependent Amazon EC2 instances
requiring low-latency, consider using cluster placement groups to influence placement of those
instances to meet the requirements of the workload. Instances in the same cluster placement group
enjoy a higher per-flow throughput limit for TCP/IP traffic and are placed in the same high-bisection
bandwidth segment of the network. Cluster placement groups are recommended for applications that
benefit from low network latency, high network throughput, or both.
3. For a workload that is location-sensitive, for example with low-latency or data residency requirements,
review AWS Local Zones or AWS Outposts.
a. AWS Local Zones are a type of infrastructure deployment that places compute, storage, database,
and other select AWS services close to large population and industry centers.
b. AWS Outposts is a family of fully managed solutions delivering AWS infrastructure and services to
virtually any on-premises or edge location for a truly consistent hybrid experience.
4. Applications such as high-resolution live video streaming, high-fidelity audio, and augmented reality/
virtual reality (AR/VR) require ultra-low-latency for 5G devices. For such applications, consider AWS
Wavelength. AWS Wavelength embeds AWS compute and storage services within 5G networks,
providing mobile edge computing infrastructure for developing, deploying, and scaling ultra-low-
latency applications.
5. If you have geographically distributed users, a content distribution network (CDN) may be used
to accelerate distribution of static and dynamic web content by delivering data through globally
dispersed points of presence (PoPs). CDNs typically also provide edge computing capabilities,
performing latency sensitive operations such as HTTP header manipulations and URL rewrites and
redirects at large scale at the edge. Amazon CloudFront is a web service that speeds up distribution
of your static and dynamic web content. Use cases for CloudFront include accelerating static website
content delivery and serving video on demand or live streaming video. CloudFront can also be used to
customize the content and experience for viewers, at reduced latency.
6. Some applications require fixed entry points or higher performance by reducing first byte latency
and jitter, and increasing throughput. These applications can benefit from networking services that
provide static anycast IP addresses and TCP termination at edge locations. AWS Global Accelerator
can improve performance for your applications by up to 60% and provide quick failover for multi-
region architectures. AWS Global Accelerator provides you with static anycast IP addresses that serve
as a fixed entry point for your applications hosted in one or more AWS Regions. These IP addresses
permit traffic to ingress onto the AWS global network as close to your users as possible. AWS Global
Accelerator reduces the initial connection setup time by establishing a TCP connection between the
client and the AWS edge location closest to the client. Review the use of AWS Global Accelerator to
improve the performance of your TCP/UDP workloads and provide quick failover for multi-region
architectures.
7. If you have applications or users on-premises, you may benefit from having a dedication network
connection between your network and the cloud. A dedicated network connection can reduce the
chance of encountering congestion or unexpected increases in latency. AWS Direct Connect can
improve application performance by connecting your network directly to AWS and bypassing the
public internet. When creating a new connection, you can choose a hosted connection provided by an
AWS Direct Connect Delivery Partner, or choose a dedicated connection from AWS and deploy at over
100 AWS Direct Connect locations around the globe. You can also reduce your networking costs with
low data transfer rates out of AWS, and optionally configure a Site-to-Site VPN for failover.
8. If you configure a Site-to-Site VPN to connect to your resources within AWS, you can optionally turn
on acceleration. An accelerated Site-to-Site VPN connection uses AWS Global Accelerator to route
traffic from your on-premises network to an AWS edge location that is closest to your customer
gateway device.
434

AWS Well-Architected Framework
Selection
9. Identify which DNS routing option would optimize your workload performance by reviewing your
workload traffic and user location. Amazon Route 53 offers latency-based routing, geolocation
routing, geoproximity routing, and IP-based routing options to help you improve your workload’s
performance for a global audience.
a. Route 53 also offers low query latency for your end users. Using a global anycast network of DNS
servers around the world, Route 53 is designed to automatically answer queries from the optimal
location depending on network conditions.
Resources
Related best practices:
• COST07-BP02 Implement Regions based on cost
• COST08-BP03 Implement services to reduce data transfer costs
• REL10-BP01 Deploy the workload to multiple locations
• REL10-BP02 Select the appropriate locations for your multi-location deployment
• SUS01-BP01 Choose Regions near Amazon renewable energy projects and Regions where the grid has
a published carbon intensity that is lower than other locations (or Regions)
• SUS02-BP04 Optimize geographic placement of workloads for user locations
• SUS04-BP07 Minimize data movement across networks
Related documents:
• AWS Global Infrastructure
• AWS Local Zones and AWS Outposts, choosing the right technology for your edge workload
• Placement groups
• AWS Local Zones
• AWS Outposts
• AWS Wavelength
• Amazon CloudFront
• AWS Global Accelerator
• AWS Direct Connect
• Site-to-Site VPN
• Amazon Route 53
Related videos:
• AWS Local Zones Explainer Video
• AWS Outposts: Overview and How It Works
• AWS re:Invent 2021 - AWS Outposts: Bringing the AWS experience on premises
• AWS re:Invent 2020: AWS Wavelength: Run apps with ultra-low latency at 5G edge
• AWS re:Invent 2022 - AWS Local Zones: Building applications for a distributed edge
• AWS re:Invent 2021 - Building low-latency websites with Amazon CloudFront
• AWS re:Invent 2022 - Improve performance and availability with AWS Global Accelerator
• AWS re:Invent 2022 - Build your global wide area network using AWS
• AWS re:Invent 2020: Global traffic management with Amazon Route 53
Related examples:
435

AWS Well-Architected Framework
Selection
• AWS Global Accelerator Workshop
• Handling Rewrites and Redirects using Edge Functions
PERF05-BP07 Optimize network configuration based on metrics
Improper network configuration often affects network performance, efficiency, and cost. In common
network environments, in order to quickly complete the deployment in the early stage, the proper
network configuration is not fully considered in terms of network performance. To optimize your
network configuration, you must first have visibility and data about your network environment.
To understand how your network resources are performing, collect and analyze data to make informed
decisions about optimizing your network configuration. Measure the impact of those changes and use
the impact measurements to make future decisions.
Desired outcome: Use metrics and network monitoring tools to optimize network configuration as
workloads evolve. Cloud-based networks can be optimized quickly, so evolving your network architecture
over time is necessary to maintain performance efficiency.
Common anti-patterns:
• You assume that all performance-related issues are application-related.
• You only test your network performance from a location close to where you have deployed the
workload.
• You use default configurations for all network services.
• You overprovision the network resource to provide sufficient capacity.
Benefits of establishing this best practice: Collecting necessary metrics of your AWS network and
implementing network monitoring tools allows you to understand network performance and optimize
network configurations.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Monitoring traffic to and from VPCs, subnets, or network interfaces is crucial to understanding how to
utilize AWS network resources and how you can optimize network configurations. By using the following
tools, you can further inspect information about the traffic usage, network access and logs.
Implementation steps
1. Use Amazon VPC IP Address Manager. You can use IPAM to plan, track, and monitor IP addresses for
your AWS and on-premises workloads. This is the best practice for you for to optimize IP address
usage and allocation.
2. Turn on VPC Flow logs. Use VPC Flow Logs to capture detailed information about traffic to and from
network interfaces in your VPCs. With VPC Flow Logs, you can diagnose overly restrictive or permissive
security group rules and determine the direction of the traffic to and from the network interfaces.
Data ingestion and archival charges for vended logs apply when you publish flow logs.
3. Turn on DNS query logging. You can configure Amazon Route 53 to log information about public
or private DNS queries Route 53 receives. With DNS logs, you can optimize DNS configurations
by understanding the domain or subdomain that was requested or Route 53 EDGE locations that
responded to DNS queries.
4. Use Reachability Analyzer to analyze and debug network reachability. Reachability Analyzer is a
configuration analysis tool that allows you to perform connectivity testing between a source resource
and a destination resource in your VPCs. This tool helps you verify that your network configuration
matches your intended connectivity.
436

AWS Well-Architected Framework
Review
5. Use Network Access Analyzer to understand network access to your resources. You can use Network
Access Analyzer to specify your network access requirements and identify potential network
paths that do not meet your specified requirements. By optimizing your corresponding network
configuration, you can understand and verify the state of your network and demonstrate if your
network on AWS meets your compliance requirements.
6. Use Amazon CloudWatch and turn on the appropriate metrics for network options. Make sure
to choose the right network metric for your workload. For example, you can turn on metrics for
VPC Network Address Usage, VPC NAT Gateway, AWS Transit Gateway, VPN tunnel, AWS Network
Firewall, Elastic Load Balancing, and AWS Direct Connect. Continually monitoring metrics is a good
practice to observe and understand your network status and usage, and helps you optimize network
configuration based on your observations.
Level of effort for the implementation plan: Medium
Resources
Related documents:
• VPC Flow Logs
• Public DNS query logging
• What is IPAM?
• What is Reachability Analyzer?
• What is Network Access Analyzer?
• CloudWatch metrics for your VPCs
• Optimize performance and reduce costs for network analytics with VPC Flow Logs in Apache Parquet
format
• Monitoring your global and core networks with Amazon Cloudwatch metrics
• Continuously monitor network traffic and resources
Related videos:
• Networking best practices and tips with the Well-Architected Framework
• Monitoring and troubleshooting network traffic
Related examples:
• AWS Networking Workshops
• AWS Network Monitoring
Review
Question
• PERF 6. How do you evolve your workload to take advantage of new releases?  (p. 437)
PERF 6. How do you evolve your workload to take advantage of
new releases?
When architecting workloads, there are finite options that you can choose from. However, over time, new
technologies and approaches become available that could improve the performance of your workload.
437

AWS Well-Architected Framework
Review
Best practices
• PERF06-BP01 Stay up-to-date on new resources and services (p. 438)
• PERF06-BP02 Define a process to improve workload performance (p. 439)
• PERF06-BP03 Evolve workload performance over time (p. 440)
PERF06-BP01 Stay up-to-date on new resources and services
Evaluate ways to improve performance as new services, design patterns, and product offerings become
available. Determine which of these could improve performance or increase the efficiency of the
workload through evaluation, internal discussion, or external analysis.
Define a process to evaluate updates, new features, and services relevant to your workload. For example,
building a proof of concept that uses new technologies or consulting with an internal group. When trying
new ideas or services, run performance tests to measure the impact that they have on the performance
of the workload. Using infrastructure as code (IaC) and a DevOps culture to take advantage of the ability
to test new ideas or technologies frequently with minimal cost or risk.
Desired outcome: You have documented the inventory of components, your design pattern, and your
workload characteristics. You use that documentation to create a list of subscriptions to notify your team
on service updates, features, and new products. You have identified component stakeholders that will
evaluate the new releases and provide a recommendation for business impact and priority.
Common anti-patterns:
• You only review new options and services when your workload is not meeting performance
requirements.
• You assume all new product offerings will not be useful to your workload.
• You always choose to build as opposed to buy when improving your workload.
Benefits of establishing this best practice: By considering new services or product offerings, you can
improve the performance and efficiency of your workload, lower the cost of the infrastructure, and
reduce the effort required to maintain your services.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Define a process to evaluate updates, new features, and services from AWS. For example, building proof-
of-concepts that use new technologies. When trying new ideas or services, run performance tests to
measure the impact on the efficiency or performance of the workload. Take advantage of the flexibilfity
that you have in AWS to test new ideas or technologies frequently with minimal cost or risk.
Implementation steps
1. Document your workload solutions. Use your configuration management database (CMDB) solution to
document your inventory and categorize your services and dependencies. Use tools like AWS Config to
get a list of all services in AWS being used by your workload.
2. Use a tagging strategy to document owners for each workload component and category. For example,
if you are currently using Amazon RDS as your database solution, have your database administrator
(DBA) assigned and documented as the owner for evaluating and researching new services and
updates.
3. Identify news and update sources related to your workload components. In the Amazon RDS example
previously mentioned, the category owner should subscribe to the What’s New at AWS blog for the
products that match their workload component. You can subscribe to the RSS feed or manage your
email subscriptions. Monitor upgrades to the Amazon RDS database you use, features introduced,
438

AWS Well-Architected Framework
Review
instances released and new products like Amazon Aurora Serverless. Monitor industry blogs, products,
and vendors that the component relies on.
4. Document your process for evaluating updates and new services. Provide your category owners the
time and space needed to research, test, experiment, and validate updates and new services. Refer
back to the documented business requirements and KPIs to help prioritize which update will make a
positive business impact.
Level of effort for the implementation plan: To establish this best practice, you must be aware of your
current workload components, identify category owners and identify sources for service updates. This is
a low level of effort to start but is an ongoing process that could evolve and improve over time.
Resources
Related documents:
• AWS Blog
• What's New with AWS
Related videos:
• AWS Events YouTube Channel
• AWS Online Tech Talks YouTube Channel
• Amazon Web Services YouTube Channel
Related examples:
• AWS Github
• AWS Skill Builder
PERF06-BP02 Define a process to improve workload performance
Define a process to evaluate new services, design patterns, resource types, and configurations as they
become available. For example, run existing performance tests on new instance offerings to determine
their potential to improve your workload.
Your workload's performance has a few key constraints. Document these so that you know what kinds
of innovation might improve the performance of your workload. Use this information when learning
about new services or technology as it becomes available to identify ways to alleviate constraints or
bottlenecks.
Common anti-patterns:
• You assume your current architecture will become static and never update over time.
• You introduce architecture changes over time with no metric justification.
Benefits of establishing this best practice: By defining your process for making architectural changes,
you allow gathered data to influence your workload design over time.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Identify the key performance constraints for your workload: Document your workload’s performance
constraints so that you know what kinds of innovation might improve the performance of your workload.
439

AWS Well-Architected Framework
Review
Resources
Related documents:
• AWS Blog
• What's New with AWS
Related videos:
• AWS Events YouTube Channel
• AWS Online Tech Talks YouTube Channel
• Amazon Web Services YouTube Channel
Related examples:
• AWS Github
• AWS Skill Builder
PERF06-BP03 Evolve workload performance over time
As an organization, use the information gathered through the evaluation process to actively drive
adoption of new services or resources when they become available.
Use the information you gather when evaluating new services or technologies to drive change. As
your business or workload changes, performance needs also change. Use data gathered from your
workload metrics to evaluate areas where you can get the biggest gains in efficiency or performance, and
proactively adopt new services and technologies to keep up with demand.
Common anti-patterns:
• You assume that your current architecture will become static and never update over time.
• You introduce architecture changes over time with no metric justification.
• You change architecture just because everyone else in the industry is using it.
Benefits of establishing this best practice: To optimize your workload performance and cost, you must
evaluate all software and services available to determine the appropriate ones for your workload.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Evolve your workload over time: Use the information you gather when evaluating new services or
technologies to drive change. As your business or workload changes, performance needs also change.
Use data gathered from your workload metrics to evaluate areas where you can achieve the biggest
gains in efficiency or performance, and proactively adopt new services and technologies to keep up with
demand.
Resources
Related documents:
• AWS Blog
• What's New with AWS
440

AWS Well-Architected Framework
Monitoring
Related videos:
• AWS Events YouTube Channel
• AWS Online Tech Talks YouTube Channel
• Amazon Web Services YouTube Channel
Related examples:
• AWS Github
• AWS Skill Builder
Monitoring
Question
• PERF 7. How do you monitor your resources to verify they are performing?  (p. 441)
PERF 7. How do you monitor your resources to verify they are
performing?
System performance can degrade over time. Monitor system performance to identify degradation and
remediate internal or external factors, such as the operating system or application load.
Best practices
• PERF07-BP01 Record performance-related metrics (p. 441)
• PERF07-BP02 Analyze metrics when events or incidents occur (p. 442)
• PERF07-BP03 Establish key performance indicators (KPIs) to measure workload
performance (p. 443)
• PERF07-BP04 Use monitoring to generate alarm-based notifications (p. 445)
• PERF07-BP05 Review metrics at regular intervals (p. 446)
• PERF07-BP06 Monitor and alarm proactively (p. 447)
PERF07-BP01 Record performance-related metrics
Use a monitoring and observability service to record performance-related metrics. Examples of metrics
include record database transactions, slow queries, I/O latency, HTTP request throughput, service
latency, or other key data.
Identify the performance metrics that matter for your workload and record them. This data is an
important part of being able to identify which components are impacting overall performance or
efficiency of the workload.
Working back from the customer experience, identify metrics that matter. For each metric, identify the
target, measurement approach, and priority. Use these to build alarms and notifications to proactively
address performance-related issues.
Common anti-patterns:
• You only monitor operating system level metrics to gain insight into your workload.
• You architect your compute needs for peak workload requirements.
441

AWS Well-Architected Framework
Monitoring
Benefits of establishing this best practice: To optimize performance and resource utilization, you need
a unified operational view of your key performance indicators. You can create dashboards and perform
metric math on your data to derive operational and utilization insights.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Identify the relevant performance metrics for your workload and record them. This data helps identify
which components are impacting overall performance or efficiency of your workload.
Identify performance metrics: Use the customer experience to identify the most important metrics. For
each metric, identify the target, measurement approach, and priority. Use these data points to build
alarms and notifications to proactively address performance-related issues.
Resources
Related documents:
• CloudWatch Documentation
• Collect metrics and logs from Amazon EC2 Instances and on-premises servers with the CloudWatch
Agent
• Publish custom metrics
• Monitoring, Logging, and Performance APN Partners
• X-Ray Documentation
• Amazon CloudWatch RUM
Related videos:
• Cut through the chaos: Gain operational visibility and insight (MGT301-R1)
• Application Performance Management on AWS
• Build a Monitoring Plan
Related examples:
• Level 100: Monitoring with CloudWatch Dashboards
• Level 100: Monitoring Windows EC2 instance with CloudWatch Dashboards
• Level 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards
PERF07-BP02 Analyze metrics when events or incidents occur
In response to (or during) an event or incident, use monitoring dashboards or reports to understand and
diagnose the impact. These views provide insight into which portions of the workload are not performing
as expected.
When you write critical user stories for your architecture, include performance requirements, such as
specifying how quickly each critical story should run. For these critical stories, implement additional
scripted user journeys to ensure that you know how these stories perform against your requirement.
Common anti-patterns:
• You assume that performance events are one-time issues and only related to anomalies.
• You only evaluate existing performance metrics when responding to performance events.
442

AWS Well-Architected Framework
Monitoring
Benefits of establishing this best practice: In determine whether your workload is operating at
expected levels, you must respond to performance events by gathering additional metric data for
analysis. This data is used to understand the impact of the performance event and suggest changes to
improve workload performance.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Prioritize experience concerns for critical user stories: When you write critical user stories for your
architecture, include performance requirements, such as specifying how quickly each critical story should
run. For these critical stories, implement additional scripted user journeys to ensure that you know how
the user stories perform against your requirements.
Resources
Related documents:
• CloudWatch Documentation
• Amazon CloudWatch Synthetics
• Monitoring, Logging, and Performance APN Partners
• X-Ray Documentation
Related videos:
• Cut through the chaos: Gain operational visibility and insight (MGT301-R1)
• Optimize applications through Amazon CloudWatch RUM
• Demo of Amazon CloudWatch Synthetics
Related examples:
• Measure page load time with Amazon CloudWatch Synthetics
• Amazon CloudWatch RUM Web Client
PERF07-BP03 Establish key performance indicators (KPIs) to measure workload
performance
Identify the KPIs that quantitatively and qualitatively measures workload performance. KPIs help to
measure the health of a workload as it relates to a business goal. KPIs allow business and engineering
teams to align on the measurement of goals and strategies and how this combines to produce business
outcomes. KPIs should be revisited when business goals, strategies, or end-user requirements change.
For example, a website workload might use the page load time as an indication of overall performance.
This metric would be one of the multiple data points which measure an end user experience. In
addition to identifying the page load time thresholds, you should document the expected outcome or
business risk if the performance is not met. A long page load time would affect your end users directly,
decrease their user experience rating and might lead to a loss of customers. When you define your KPI
thresholds, combine both industry benchmarks and your end user expectations. For example, if the
current industry benchmark is a webpage loading within a two second time period, but your end users
expect a webpage to load within a one second time period, then you should take both of these data
points into consideration when establishing the KPI. Another example of a KPI might focus on meeting
internal performance needs. A KPI threshold might be established on generating sales reports within
one business day after production data has been generated. These reports might directly affect daily
decisions and business outcomes.
443

AWS Well-Architected Framework
Monitoring
Desired outcome: Establishing KPIs involve different departments and stakeholders. Your team must
evaluate your workload KPIs using real-time granular data and historical data for reference and create
dashboards that perform metric math on your KPI data to derive operational and utilization insights.
KPIs should be documented which explains the agreed upon KPIs and thresholds that support business
goals and strategies as well as mapped to metrics being monitored. The KPIs are identifying performance
requirements, reviewed intentionally and are frequently shared and understood with all teams. Risks and
tradeoffs are clearly identified and understood how business is impact within KPI thresholds are not met.
Common anti-patterns:
• You only monitor system level metrics to gain insight into your workload and don’t understand
business impacts to those metrics.
• You assume that your KPIs are already being published and shared as standard metric data.
• Defining KPIs but not sharing them with all the teams.
• Not defining a quantitative, measurable KPI.
• Not aligning KPIs with business goals or strategies.
Benefits of establishing this best practice: Identifying specific metrics which represent workload health
help to align teams on their priorities and defining successful business outcomes. Sharing those metrics
with all departments provides visibility and alignment on thresholds, expectations, and business impact.
Level of risk exposed if this best practice is not established: High
Implementation guidance
All departments and business teams impacted by the health of the workload should contribute to
defining KPIs. A single person should drive the collaboration, timelines, documentation, and information
related to an organization’s KPIs. This single threaded owner will often share the business goals and
strategies and assign business stakeholders tasks to create KPIs in their respective departments. Once
KPIs are defined, the operations team will often help define the metrics that will support and inform
the success of the different KPIs. KPIs are only effective if all team members supporting a workload are
aware of the KPIs.
Implementation steps
1. Identify and document business stakeholders.
2. Identify company goals and strategies.
3. Review common industry KPIs that align with your company goals and strategies.
4. Review end user expectations of your workload.
5. Define and document KPIs that support company goals and strategies.
6. Identify and document approved tradeoff strategies to meet the KPIs.
7. Identify and document metrics that will inform the KPIs.
8. Identify and document KPI thresholds for severity or alarm level.
9. Identify and document the risk and impact if the KPI is not met.
10Identify the frequency of review per KPI.
11Communicate KPI documentation with all teams supporting the workload.
Level of effort for the implementation guidance: Defining and communicating the KPIs is a low amount
of work. This can typically be done over a few weeks meeting with business stakeholders, reviewing
goals, strategies, and workload metrics.
Resources
Related documents:
444

AWS Well-Architected Framework
Monitoring
• CloudWatch documentation
• Monitoring, Logging, and Performance APN Partners
• X-Ray Documentation
• Using Amazon CloudWatch dashboards
• Amazon QuickSight KPIs
Related videos:
• AWS re:Invent 2019: Scaling up to your first 10 million users (ARC211-R)
• Cut through the chaos: Gain operational visibility and insight (MGT301-R1)
• Build a Monitoring Plan
Related examples:
• Creating a dashboard with Amazon QuickSight
PERF07-BP04 Use monitoring to generate alarm-based notifications
Using the performance-related key performance indicators (KPIs) that you defined, use a monitoring
system that generates alarms automatically when these measurements are outside expected boundaries.
Amazon CloudWatch can collect metrics across the resources in your architecture. You can also collect
and publish custom metrics to surface business or derived metrics. Use CloudWatch or a third-party
monitoring service to set alarms that indicate when thresholds are breached — alarms signal that a
metric is outside of the expected boundaries.
Common anti-patterns:
• You rely on staff to watch metrics and react when they see an issue.
• You rely solely on operational runbooks, when serverless workflows could be started to accomplish the
same task.
Benefits of establishing this best practice: You can set alarms and automate actions based on either
predefined thresholds, or on machine learning algorithms that identify anomalous behavior in your
metrics. These same alarms can also start serverless workflows, which can modify performance
characteristics of your workload (for example, increasing compute capacity, altering database
configuration).
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Monitor metrics: Amazon CloudWatch can collect metrics across the resources in your architecture. You
can collect and publish custom metrics to surface business or derived metrics. Use CloudWatch or a
third-party monitoring service to set alarms that indicate when thresholds are exceeded.
Resources
Related documents:
• CloudWatch Documentation
• Monitoring, Logging, and Performance APN Partners
445

AWS Well-Architected Framework
Monitoring
• X-Ray Documentation
• Using Alarms and Alarm Actions in CloudWatch
Related videos:
• AWS re:Invent 2019: Scaling up to your first 10 million users (ARC211-R)
• Cut through the chaos: Gain operational visibility and insight (MGT301-R1)
• Build a Monitoring Plan
• Using AWS Lambda with Amazon CloudWatch Events
Related examples:
• Cloudwatch Logs Customize Alarms
PERF07-BP05 Review metrics at regular intervals
As routine maintenance, or in response to events or incidents, review which metrics are collected. Use
these reviews to identify which metrics were essential in addressing issues and which additional metrics,
if they were being tracked, would help to identify, address, or prevent issues.
As part of responding to incidents or events, evaluate which metrics were helpful in addressing the issue
and which metrics could have helped that are not currently being tracked. Use this to improve the quality
of metrics you collect so that you can prevent or more quickly resolve future incidents.
Common anti-patterns:
• You allow metrics to stay in an alarm state for an extended period of time.
• You create alarms that are not actionable by an automation system.
Benefits of establishing this best practice: Continually review metrics that are being collected to ensure
that they properly identify, address, or prevent issues. Metrics can also become stale if you let them stay
in an alarm state for an extended period of time.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
Constantly improve metric collection and monitoring: As part of responding to incidents or events,
evaluate which metrics were helpful in addressing the issue and which metrics could have helped that
are not currently being tracked. Use this method to improve the quality of metrics you collect so that you
can prevent or more quickly resolve future incidents.
Resources
Related documents:
• CloudWatch Documentation
• Collect metrics and logs from Amazon EC2 Instances and on-premises servers with the CloudWatch
Agent
• Monitoring, Logging, and Performance APN Partners
• X-Ray Documentation
Related videos:
446

AWS Well-Architected Framework
Monitoring
• Cut through the chaos: Gain operational visibility and insight (MGT301-R1)
• Application Performance Management on AWS
• Build a Monitoring Plan
Related examples:
• Creating a dashboard with Amazon QuickSight
• Level 100: Monitoring with CloudWatch Dashboards
PERF07-BP06 Monitor and alarm proactively
Use key performance indicators (KPIs), combined with monitoring and alerting systems, to proactively
address performance-related issues. Use alarms to start automated actions to remediate issues where
possible. Escalate the alarm to those able to respond if automated response is not possible. For example,
you may have a system that can predict expected key performance indicators (KPI) values and alarm
when they breach certain thresholds, or a tool that can automatically halt or roll back deployments if
KPIs are outside of expected values.
Implement processes that provide visibility into performance as your workload is running. Build
monitoring dashboards and establish baseline norms for performance expectations to determine if the
workload is performing optimally.
Common anti-patterns:
• You only allow operations staff the ability to make operational changes to the workload.
• You let all alarms filter to the operations team with no proactive remediation.
Benefits of establishing this best practice: Proactive remediation of alarm actions allows support staff
to concentrate on those items that are not automatically actionable. This ensures that operations staff
are not overwhelmed by all alarms and instead focus only on critical alarms.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Monitor performance during operations: Implement processes that provide visibility into performance
as your workload is running. Build monitoring dashboards and establish a baseline for performance
expectations.
Resources
Related documents:
• CloudWatch Documentation
• Monitoring, Logging, and Performance APN Partners
• X-Ray Documentation
• Using Alarms and Alarm Actions in CloudWatch
Related videos:
• Cut through the chaos: Gain operational visibility and insight (MGT301-R1)
• Application Performance Management on AWS
• Build a Monitoring Plan
447

AWS Well-Architected Framework
Tradeoffs
• Using AWS Lambda with Amazon CloudWatch Events
Related examples:
• Cloudwatch Logs Customize Alarms
Tradeoffs
Question
• PERF 8. How do you use tradeoffs to improve performance?  (p. 448)
PERF 8. How do you use tradeoffs to improve performance?
When architecting solutions, determining tradeoffs allows you to select the more effective approach.
Often you can improve performance by trading consistency, durability, and space for time and latency.
Best practices
• PERF08-BP01 Understand the areas where performance is most critical (p. 448)
• PERF08-BP02 Learn about design patterns and services (p. 450)
• PERF08-BP03 Identify how tradeoffs impact customers and efficiency (p. 451)
• PERF08-BP04 Measure the impact of performance improvements (p. 452)
• PERF08-BP05 Use various performance-related strategies (p. 453)
PERF08-BP01 Understand the areas where performance is most critical
Understand and identify areas where increasing the performance of your workload will have a positive
impact on efficiency or customer experience. For example, a website that has a large amount of customer
interaction can benefit from using edge services to move content delivery closer to customers.
Desired outcome: Increase performance efficiency by understanding your architecture, traffic patterns,
and data access patterns, and identify your latency and processing times. Identify the potential
bottlenecks that might affect the customer experience as the workload grows. When you identify those
areas, look at which solution you could deploy to remove those performance concerns.
Common anti-patterns:
• You assume that standard compute metrics such as CPUUtilization or memory pressure are enough
to catch performance issues.
• You only use the default metrics recorded by your selected monitoring software.
• You only review metrics when there is an issue.
Benefits of establishing this best practice: Understanding critical areas of performance helps workload
owners monitor KPIs and prioritize high-impact improvements.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Set up end-to-end tracing to identify traffic patterns, latency, and critical performance areas. Monitor
your data access patterns for slow queries or poorly fragmented and partitioned data. Identify the
constrained areas of the workload using load testing or monitoring.
448

AWS Well-Architected Framework
Tradeoffs
Implementation steps
1. Set up end-to-end monitoring to capture all workload components and metrics.
• Use Amazon CloudWatch Real-User Monitoring (RUM) to capture application performance metrics
from real user client-side and frontend sessions.
• Set up AWS X-Ray to trace traffic through the application layers and identify latency between
components and dependencies. Use the X-Ray service maps to see relationships and latency
between workload components.
• Use Amazon Relational Database Service Performance Insights to view database performance
metrics and identify performance improvements.
• Use Amazon RDS Enhanced Monitoring to view database OS performance metrics.
• Collect CloudWatch metrics per workload component and service and identify which metrics impact
performance efficiency.
• Set up Amazon DevOps Guru for additional performance insights and recommendations
2. Perform tests to generate metrics, identify traffic patterns, bottlenecks, and critical performance
areas.
• Set up CloudWatch Synthetic Canaries to mimic browser-based user activities programmatically
using cron jobs or rate expressions to generate consistent metrics over time.
• Use the AWS Distributed Load Testing solution to generate peak traffic or test the workload at the
expected growth rate.
3. Evaluate the metrics and telemetry to identify your critical performance areas. Review these areas
with your team to discuss monitoring and solutions to avoid bottlenecks.
4. Experiment with performance improvements and measure those changes with data.
• Use CloudWatch Evidently to test new improvements and the performance impact to the workload.
Level of effort for the implementation plan: To establish this best practice, you must review your end-
to-end metrics and be aware of your current workload performance. This is a moderate level of effort to
set up end to end monitoring and identify your critical performance areas.
Resources
Related documents:
• Amazon Builders’ Library
• X-Ray Documentation
• Amazon CloudWatch RUM
• Amazon DevOps Guru
• CloudWatch RUM and X-Ray
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• Demo of Amazon CloudWatch Synthetics
Related examples:
• Measure page load time with Amazon CloudWatch Synthetics
• Amazon CloudWatch RUM Web Client
• X-Ray SDK for Node.js
• X-Ray SDK for Python
449

AWS Well-Architected Framework
Tradeoffs
• X-Ray SDK for Java
• X-Ray SDK for .Net
• X-Ray SDK for Ruby
• X-Ray Daemon
• Distributed Load Testing on AWS
PERF08-BP02 Learn about design patterns and services
Research and understand the various design patterns and services that help improve workload
performance. As part of the analysis, identify what you could trade to achieve higher performance.
For example, using a cache service can help to reduce the load placed on database systems. However,
caching can introduce eventual consistency and requires engineering effort to implement within business
requirements and customer expectations.
Desired outcome: Researching design patterns will lead you to choosing an architecture design that will
support the best performing system. Learn which performance configuration options are available to
you and how they could impact the workload. Optimizing the performance of your workload depends on
understanding how these options interact with your architecture and the impact they will have on both
measured performance and the performance perceived by end users.
Common anti-patterns:
• You assume that all traditional IT workload performance strategies are best suited for cloud
workloads.
• You build and manage caching solutions instead of using managed services.
• You use the same design pattern for all your workloads without evaluating which pattern would
improve the workload performance.
Benefits of establishing this best practice: By selecting the right design pattern and services for your
workload you will be optimizing your performance, improving operational excellence and increasing
reliability. The right design pattern will meet your current workload characteristics and help you scale for
future growth or changes.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Learn which performance configuration options are available and how they could impact the workload.
Optimizing the performance of your workload depends on understanding how these options interact
with your architecture, and the impact they have on measured performance and user-perceived
performance.
Implementation steps:
1. Evaluate and review design patterns that would improve your workload performance.
a. The Amazon Builders’ Library provides you with a detailed description of how Amazon builds and
operates technology. These articles are written by senior engineers at Amazon and cover topics
across architecture, software delivery, and operations.
b. AWS Solutions Library is a collection of ready-to-deploy solutions that assemble services, code, and
configurations. These solutions have been created by AWS and AWS Partners based on common
use cases and design patterns grouped by industry or workload type. For example, you can set up a
distributed load testing solution for your workload.
c. AWS Architecture Center provides reference architecture diagrams grouped by design pattern,
content type, and technology.
450

AWS Well-Architected Framework
Tradeoffs
d. AWS samples is a GitHub repository full of hands-on examples to help you explore common
architecture patterns, solutions, and services. It is updated frequently with the newest services and
examples.
2. Improve your workload to model the selected design patterns and use services and the service
configuration options to improve your workload performance.
a. Train your internal team with resources available at AWS Skills Guild.
b. Use the AWS Partner Network to provide expertise quickly and to scale your ability to make
improvements.
Level of effort for the implementation plan: To establish this best practice, you must be aware of the
design patterns and services that could help improve your workload performance. After evaluating the
design patterns, implementing the design patterns is a high level of effort.
Resources
Related documents:
• AWS Architecture Center
• AWS Partner Network
• AWS Solutions Library
• AWS Knowledge Center
• Amazon Builders’ Library
• Using load shedding to avoid overload
• Caching challenges and strategies
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• This is My Architecture
Related examples:
• AWS Samples
• AWS SDK Examples
PERF08-BP03 Identify how tradeoffs impact customers and efficiency
When evaluating performance-related improvements, determine which choices will impact your
customers and workload efficiency. For example, if using a key-value data store increases system
performance, it is important to evaluate how the eventually consistent nature of it will impact
customers.
Identify areas of poor performance in your system through metrics and monitoring. Determine how
you can make improvements, what trade-offs those improvements bring, and how they impact the
system and the user experience. For example, implementing caching data can help dramatically improve
performance but requires a clear strategy for how and when to update or invalidate cached data to
prevent incorrect system behavior.
Common anti-patterns:
• You assume that all performance gains should be implemented, even if there are tradeoffs for
implementation such as eventual consistency.
• You only evaluate changes to workloads when a performance issue has reached a critical point.
451

AWS Well-Architected Framework
Tradeoffs
Benefits of establishing this best practice: When you are evaluating potential performance-related
improvements, you must decide if the tradeoffs for the changes are consistent with the workload
requirements. In some cases, you may have to implement additional controls to compensate for the
tradeoffs.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Identify tradeoffs: Use metrics and monitoring to identify areas of poor performance in your system.
Determine how to make improvements, and how tradeoffs will impact the system and the user
experience. For example, implementing caching data can help dramatically improve performance, but
it requires a clear strategy for how and when to update or invalidate cached data to prevent incorrect
system behavior.
Resources
Related documents:
• Amazon Builders’ Library
• Amazon QuickSight KPIs
• Amazon CloudWatch RUM
• X-Ray Documentation
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• Build a Monitoring Plan
• Optimize applications through Amazon CloudWatch RUM
• Demo of Amazon CloudWatch Synthetics
Related examples:
• Measure page load time with Amazon CloudWatch Synthetics
• Amazon CloudWatch RUM Web Client
PERF08-BP04 Measure the impact of performance improvements
As changes are made to improve performance, evaluate the collected metrics and data. Use
this information to determine impact that the performance improvement had on the workload,
the workload’s components, and your customers. This measurement helps you understand the
improvements that result from the tradeoff, and helps you determine if any negative side-effects were
introduced.
A well-architected system uses a combination of performance related strategies. Determine which
strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding
data across multiple relational database systems could improve overall throughput while retaining
support for transactions and, within each shard, caching can help to reduce the load.
Common anti-patterns:
• You deploy and manage technologies manually that are available as managed services.
• You focus on just one component, such as networking, when multiple components could be used to
increase performance of the workload.
• You rely on customer feedback and perceptions as your only benchmark.
452

AWS Well-Architected Framework
Tradeoffs
Benefits of establishing this best practice: For implementing performance strategies, you must select
multiple services and features that, taken together, will allow you to meet your workload requirements
for performance.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
A well-architected system uses a combination of performance-related strategies. Determine which
strategy will have the largest positive impact on a given hotspot or bottleneck. For example, sharding
data across multiple relational database systems could improve overall throughput while retaining
support for transactions and, within each shard, caching can help to reduce the load.
Resources
Related documents:
• Amazon Builders’ Library
• Amazon CloudWatch RUM
• Amazon CloudWatch Synthetics
• Distributed Load Testing on AWS
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• Optimize applications through Amazon CloudWatch RUM
• Demo of Amazon CloudWatch Synthetics
Related examples:
• Measure page load time with Amazon CloudWatch Synthetics
• Amazon CloudWatch RUM Web Client
• Distributed Load Testing on AWS
PERF08-BP05 Use various performance-related strategies
Where applicable, use multiple strategies to improve performance. For example, using strategies like
caching data to prevent excessive network or database calls, using read-replicas for database engines to
improve read rates, sharding or compressing data where possible to reduce data volumes, and buffering
and streaming of results as they are available to avoid blocking.
As you make changes to the workload, collect and evaluate metrics to determine the impact of those
changes. Measure the impacts to the system and to the end-user to understand how your trade-offs
impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff
improves performance.
Common anti-patterns:
• You assume that workload performance is adequate if customers are not complaining.
• You only collect data on performance after you have made performance-related changes.
Benefits of establishing this best practice: To optimize performance and resource utilization, you need
a unified operational view, real-time granular data, and historical reference. You can create dashboards
453

AWS Well-Architected Framework
Cost optimization
and perform metric math on your data to derive operational and utilization insights for your workloads
as they change over time.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
Use a data-driven approach to evolve your architecture: As you make changes to the workload, collect
and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and
to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach,
such as load testing, to explore whether the tradeoff improves performance.
Resources
Related documents:
• Amazon Builders’ Library
• Best Practices for Implementing Amazon ElastiCache
• AWS Database Caching
• Amazon CloudWatch RUM
• Distributed Load Testing on AWS
Related videos:
• Introducing The Amazon Builders’ Library (DOP328)
• AWS purpose-built databases (DAT209-L)
• Optimize applications through Amazon CloudWatch RUM
Related examples:
• Measure page load time with Amazon CloudWatch Synthetics
• Amazon CloudWatch RUM Web Client
• Distributed Load Testing on AWS
Cost optimization
The Cost Optimization pillar includes the ability to run systems to deliver business value at the lowest
price point. You can find prescriptive guidance on implementation in the Cost Optimization Pillar
whitepaper.
Best practice areas
• Practice Cloud Financial Management (p. 454)
• Expenditure and usage awareness (p. 468)
• Cost-effective resources (p. 495)
• Manage demand and supply resources (p. 514)
• Optimize over time (p. 521)
Practice Cloud Financial Management
Question
• COST 1. How do you implement cloud financial management? (p. 455)
454

AWS Well-Architected Framework
Practice Cloud Financial Management
COST 1. How do you implement cloud financial management?
Implementing Cloud Financial Management helps organizations realize business value and financial
success as they optimize their cost and usage and scale on AWS.
Best practices
• COST01-BP01 Establish a cost optimization function (p. 455)
• COST01-BP02 Establish a partnership between finance and technology (p. 457)
• COST01-BP03 Establish cloud budgets and forecasts (p. 460)
• COST01-BP04 Implement cost awareness in your organizational processes (p. 461)
• COST01-BP05 Report and notify on cost optimization (p. 463)
• COST01-BP06 Monitor cost proactively (p. 464)
• COST01-BP07 Keep up-to-date with new service releases (p. 465)
• COST01-BP08 Create a cost-aware culture (p. 466)
• COST01-BP09 Quantify business value from cost optimization (p. 468)
COST01-BP01 Establish a cost optimization function
Create a team (Cloud Business Office or Cloud Center of Excellence) that is responsible for establishing
and maintaining cost awareness across your organization. The team requires people from finance,
technology, and business roles across the organization.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Establish a Cloud Business Office (CBO) or Cloud Center of Excellence (CCOE) team that is responsible
for establishing and maintaining a culture of cost awareness in cloud computing. It can be an existing
individual, a team within your organization, or a new team of key finance, technology and organization
stakeholders from across the organization.
The function (individual or team) prioritizes and spends the required percentage of their time on cost
management and cost optimization activities. For a small organization, the function might spend a
smaller percentage of time compared to a full-time function for a larger enterprise.
The function requires a multi-disciplined approach, with capabilities in project management, data
science, financial analysis, and software or infrastructure development. The function can improve
efficiencies of workloads by running cost optimizations within three different ownerships:
• Centralized: Through designated teams such as finance operations, cost optimization, CBO, or CCOE,
customers can design and implement governance mechanisms and drive best practices company-wide.
• Decentralized: Influencing technology teams to run optimizations.
• Hybrid: A combination of both centralized and decentralized teams can work together to run cost
optimizations.
The function may be measured against their ability to run and deliver against cost optimization goals
(for example, workload efficiency metrics).
You must secure executive sponsorship for this function to make changes, which is a key success factor.
The sponsor is regarded as champion for cost efficient cloud consumption, and provides escalation
support for the function to ensure that cost optimization activities are treated with the level of priority
defined by the organization. Otherwise, guidance will be ignored and cost-saving opportunities will not
be prioritized. Together, the sponsor and function ensure that your organization consumes the cloud
efficiently and continues to deliver business value.
455

AWS Well-Architected Framework
Practice Cloud Financial Management
If you have a Business, Enterprise-On-Ramp, or Enterprise Support plan, and need help to build this team
or function, reach out to Cloud Finance Management (CFM) experts through your Account team.
Implementation steps
•                                                                                                          Define key members: You need to ensure that all relevant parts of your organization contribute and
have a stake in cost management. Common teams within organizations typically include: finance,
application or product owners, management, and technical teams (DevOps). Some are engaged
full time (finance, technical), others periodically as required. Individuals or teams performing CFM
generally need the following set of skills:
• Software development skills - in the case where scripts and automation are being built out.
• Infrastructure engineering skills - to deploy scripts or automation, and understand how services or
resources are provisioned.
• Operations acumen - CFM is about operating on the cloud efficiently by measuring, monitoring,
modifying, planning and scaling efficient use of the cloud.
•                                                                                                          Define goals and metrics: The function needs to deliver value to the organization in different ways.
These goals are defined and continually evolve as the organization evolves. Common activities include:
creating and running education programs on cost optimization across the organization, developing
organization-wide standards, such as monitoring and reporting for cost optimization, and setting
workload goals on optimization. This function also needs to regularly report to the organization on the
organization's cost optimization capability.
You can define value-based key performance indicators (KPIs). KPIs can be cost-based or value-based.
When you define the KPIs, you can calculate expected cost in terms of efficiency and expected business
outcome. Value-based KPIs tie cost and usage metrics to business value drivers and help us rationalize
changes in our AWS spend. The first step to deriving value-based KPIs is working together, cross-
organizationally, to select and agree upon a standard set of KPIs.
•                                                                                                          Establish regular cadence: The group (finance, technology, and business teams) should come
together regularly to review their goals and metrics. A typical cadence involves reviewing the state
of the organization, reviewing any programs currently running, and reviewing overall financial and
optimization metrics. Then key workloads are reported on in greater detail.
During these regular meetings, you can review workload efficiency (cost) and business outcome. For
example, a 20% cost increase for a workload may align with increased customer usage. In this case,
this 20% cost increase can be interpreted as an investment. These regular cadence calls can help teams
to identify value-based KPIs that provide meaning to the entire organization.
Resources
Related documents:
• AWS CCOE Blog
• Creating Cloud Business Office
• CCOE - Cloud Center of Excellence
Related videos:
• Vanguard CCOE Success Story
Related examples:
• Using a Cloud Center of Excellence (CCOE) to Transform the Entire Enterprise
• Building a CCOE to transform the entire enterprise
•                                                                                                          7 Pitfalls to Avoid When Building CCOE
456

AWS Well-Architected Framework
Practice Cloud Financial Management
COST01-BP02 Establish a partnership between finance and technology
Involve finance and technology teams in cost and usage discussions at all stages of your cloud journey.
Teams regularly meet and discuss topics such as organizational goals and targets, current state of cost
and usage, and financial and accounting practices.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Technology teams innovate faster in the cloud due to shortened approval, procurement, and
infrastructure deployment cycles. This can be an adjustment for finance organizations previously used
to running time-consuming and resource-intensive processes for procuring and deploying capital in data
center and on-premises environments, and cost allocation only at project approval.
From a finance and procurement organization perspective, the process for capital budgeting, capital
requests, approvals, procurement, and installing physical infrastructure is one that has been learned and
standardized over decades:
• Engineering or IT teams are typically the requesters
• Various finance teams act as approvers and procurers
• Operations teams rack, stack, and hand off ready-to-use infrastructure
457

AWS Well-Architected Framework
Practice Cloud Financial Management
With the adoption of cloud, infrastructure procurement and consumption are no longer beholden to a
chain of dependencies. In the cloud model, technology and product teams are no longer just builders,
but operators and owners of their products, responsible for most of the activities historically associated
with finance and operations teams, including procurement and deployment.
All it really takes to provision cloud resources is an account, and the right set of permissions. This is also
what reduces IT and finance risk; which means teams are always a just few clicks or API calls away from
terminating idle or unnecessary cloud resources. This is also what allows technology teams to innovate
faster - the agility and ability to spin up and then tear down experiments. While the variable nature
of cloud consumption may impact predictability from a capital budgeting and forecasting perspective,
cloud provides organizations with the ability to reduce the cost of over-provisioning, as well as reduce
the opportunity cost associated with conservative under-provisioning.
Establish a partnership between key finance and technology stakeholders to create a shared
understanding of organizational goals and develop mechanisms to succeed financially in the variable
spend model of cloud computing. Relevant teams within your organization must be involved in cost and
usage discussions at all stages of your cloud journey, including:
• Financial leads: CFOs, financial controllers, financial planners, business analysts, procurement,
sourcing, and accounts payable must understand the cloud model of consumption, purchasing
options, and the monthly invoicing process. Finance needs to partner with technology teams to
create and socialize an IT value story, helping business teams understand how technology spend is
linked to business outcomes. This way, technology expenditures are viewed not as costs, but rather
as investments. Due to the fundamental differences between the cloud (such as the rate of change in
usage, pay as you go pricing, tiered pricing, pricing models, and detailed billing and usage information)
compared to on-premises operation, it is essential that the finance organization understands how
cloud usage can impact business aspects including procurement processes, incentive tracking, cost
allocation and financial statements.
458

AWS Well-Architected Framework
Practice Cloud Financial Management
• Technology leads: Technology leads (including product and application owners) must be aware of
the financial requirements (for example, budget constraints) as well as business requirements (for
example, service level agreements). This allows the workload to be implemented to achieve the
desired goals of the organization.
The partnership of finance and technology provides the following benefits:
• Finance and technology teams have near real-time visibility into cost and usage.
• Finance and technology teams establish a standard operating procedure to handle cloud spend
variance.
• Finance stakeholders act as strategic advisors with respect to how capital is used to purchase
commitment discounts (for example, Reserved Instances or AWS Savings Plans), and how the cloud is
used to grow the organization.
• Existing accounts payable and procurement processes are used with the cloud.
• Finance and technology teams collaborate on forecasting future AWS cost and usage to align and build
organizational budgets.
• Better cross-organizational communication through a shared language, and common understanding of
financial concepts.
Additional stakeholders within your organization that should be involved in cost and usage discussions
include:
• Business unit owners: Business unit owners must understand the cloud business model so that they
can provide direction to both the business units and the entire company. This cloud knowledge is
critical when there is a need to forecast growth and workload usage, and when assessing longer-term
purchasing options, such as Reserved Instances or Savings Plans.
• Engineering team: Establishing a partnership between finance and technology teams is essential
for building a cost-aware culture that encourages engineers to take action on Cloud Financial
Management (CFM). One of the common problems of CFM or finance operations practitioners and
finance teams is getting engineers to understand the whole business on cloud, follow best practices,
and take recommended actions.
• Third parties: If your organization uses third parties (for example, consultants or tools), ensure
that they are aligned to your financial goals and can demonstrate both alignment through their
engagement models and a return on investment (ROI). Typically, third parties will contribute to
reporting and analysis of any workloads that they manage, and they will provide cost analysis of any
workloads that they design.
Implementing CFM and achieving success requires collaboration across finance, technology, and business
teams, and a shift in how cloud spend is communicated and evaluated across the organization. Include
engineering teams so that they can be part of these cost and usage discussions at all stages, and
encourage them to follow best practices and take agreed-upon actions accordingly.
Implementation steps
• Define key members: Verify that all relevant members of your finance and technology teams
participate in the partnership. Relevant finance members will be those having interaction with the
cloud bill. This will typically be CFOs, financial controllers, financial planners, business analysts,
procurement, and sourcing. Technology members will typically be product and application owners,
technical managers and representatives from all teams that build on the cloud. Other members may
include business unit owners, such as marketing, that will influence usage of products, and third
parties such as consultants, to achieve alignment to your goals and mechanisms, and to assist with
reporting.
• Define topics for discussion: Define the topics that are common across the teams, or will need a
shared understanding. Follow cost from that time it is created, until the bill is paid. Note any members
459

AWS Well-Architected Framework
Practice Cloud Financial Management
involved, and organizational processes that are required to be applied. Understand each step or
process it goes through and the associated information, such as pricing models available, tiered
pricing, discount models, budgeting, and financial requirements.
• Establish regular cadence: To create a finance and technology partnership, establish a regular
communication cadence to create and maintain alignment. The group needs to come together
regularly against their goals and metrics. A typical cadence involves reviewing the state of the
organization, reviewing any programs currently running, and reviewing overall financial and
optimization metrics. Then key workloads are reported on in greater detail.
Resources
Related documents:
• AWS News Blog
COST01-BP03 Establish cloud budgets and forecasts
Adjust existing organizational budgeting and forecasting processes to be compatible with the highly
variable nature of cloud costs and usage. Processes must be dynamic using trend-based or business
driver-based algorithms, or a combination of both.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Customers use the cloud for efficiency, speed and agility, which creates a highly variable amount of cost
and usage. Costs can decrease with increases in workload efficiency, or as new workloads and features
are deployed. It is possible to see the cost increase when the workload efficiency increases, or as new
workloads and features are deployed. Or, workloads will scale to serve more of your customers, which
increases cloud usage and costs. Resources are now more readily accessible than ever before. With the
elasticity of the cloud also brings an elasticity of costs and forecasts. Existing organizational budgeting
processes must be modified to incorporate this variability.
Adjust existing budgeting and forecasting processes to become more dynamic using either a trend-based
algorithm (using historical costs as inputs), or using business-driver-based algorithms (for example, new
product launches or regional expansion), or a combination of both trend and business drivers.
Use AWS Budgets to set custom budgets at a granular level by specifying the time period, recurrence,
or amount (fixed or variable), and adding filters such as service, AWS Region, and tags. To stay informed
on the performance of your existing budgets you can create and schedule AWS Budgets Reports to
be emailed to you and your stakeholders on a regular cadence. You can also create AWS Budgets
Alerts based on actual costs, which is reactive in nature, or on forecasted costs, which provides time
to implement mitigations against potential cost overruns. You will be alerted when your cost or usage
exceeds, or if they are forecasted to exceed, your budgeted amount.
AWS gives you the flexibility to build dynamic forecasting and budgeting processes so you can stay
informed on whether costs adhere to, or exceed, budgetary limits.
Use AWS Cost Explorer to forecast costs in a defined future time range based on your past spend. AWS
Cost Explorer’s forecasting engine segments your historical data based on charge types (for example,
Reserved Instances) and uses a combination of machine learning and rule-based models to predict spend
across all charge types individually. Use AWS Cost Explorer to forecast daily (up to three months) or
monthly (up to 12 months) cloud costs based on machine learning algorithms applied to your historical
costs (trend-based).
Once you’ve determined your trend-based forecast using Cost Explorer, use the AWS Pricing Calculator to
estimate your AWS use case and future costs based on the expected usage (traffic, requests-per-second,
460

AWS Well-Architected Framework
Practice Cloud Financial Management
required Amazon Elastic Compute Cloud (Amazon EC2) instance, and so forth). You can also use it to help
you plan how you spend, find cost saving opportunities, and make informed decisions when using AWS.
Use AWS Cost Anomaly Detection to prevent or reduce cost surprises and enhance control without
slowing innovation. AWS Cost Anomaly Detection leverages advanced machine learning technologies to
identify anomalous spend and root causes, so you can quickly take action. With three simple steps, you
can create your own contextualized monitor and receive alerts when any anomalous spend is detected.
Let builders build, and let AWS Cost Anomaly Detection monitor your spend and reduce the risk of billing
surprises.
As mentioned in the Well-Architected Cost Optimization Pillar’s Finance and Technology Partnership
section, it is important to have partnership and cadences between IT, Finance and other stakeholders to
ensure that they are all using the same tooling or processes for consistency. In cases where budgets may
need to change, increasing cadence touch points can help react to those changes more quickly.
Implementation steps
• Update existing budget and forecasting processes: Implement trend-based, business driver-based, or
a combination of both in your budgeting and forecasting processes.
• Configure alerts and notifications: Use AWS Budgets Alerts and Cost Anomaly Detection.
• Perform regular reviews with key stakeholders: For example, stakeholders in IT, Finance, Platform,
and other areas of the business, to align with changes in business direction and usage.
Resources
Related documents:
• AWS Cost Explorer
• AWS Budgets
• AWS Pricing Calculator
• AWS Cost Anomaly Detection
• AWS License Manager
Related examples:
• Launch: Usage-Based Forecasting now Available in AWS Cost Explorer
• AWS Well-Architected Labs - Cost and Usage Governance
COST01-BP04 Implement cost awareness in your organizational processes
Implement cost awareness, create transparency, and accountability of costs into new or existing
processes that impact usage, and leverage existing processes for cost awareness. Implement cost
awareness into employee training.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Cost awareness must be implemented in new and existing organizational processes. It is one of the
foundational, prerequisite capabilities for other best practices. It is recommended to reuse and modify
existing processes where possible — this minimizes the impact to agility and velocity. Report cloud
costs to the technology teams and the decision makers in the business and finance teams to raise
cost awareness, and establish efficiency key performance indicators (KPIs) for finance and business
stakeholders. The following recommendations will help implement cost awareness in your workload:
461

AWS Well-Architected Framework
Practice Cloud Financial Management
• Verify that change management includes a cost measurement to quantify the financial impact of your
changes. This helps proactively address cost-related concerns and highlight cost savings.
• Verify that cost optimization is a core component of your operating capabilities. For example, you can
leverage existing incident management processes to investigate and identify root causes for cost and
usage anomalies or cost overruns.
• Accelerate cost savings and business value realization through automation or tooling. When thinking
about the cost of implementing, frame the conversation to include an return on investment (ROI)
component to justify the investment of time or money.
• Allocate cloud costs by implementing showbacks or chargebacks for cloud spend, including spend on
commitment-based purchase options, shared services and marketplace purchases to drive most cost-
aware cloud consumption.
• Extend existing training and development programs to include cost-awareness training throughout
your organization. It is recommended that this includes continuous training and certification. This will
build an organization that is capable of self-managing cost and usage.
• Take advantage of free AWS native tools such as AWS Cost Anomaly Detection, AWS Budgets, and AWS
Budgets Reports.
When organizations consistently adopt Cloud Financial Management (CFM) practices, those behaviours
become ingrained in the way of working and decision-making. The result is a culture that is more cost-
aware, from developers architecting a new born-in-the-cloud application, to finance managers analyzing
the ROI on these new cloud investments.
Implementation steps
• Identify relevant organizational processes: Each organizational unit reviews their processes
and identifies processes that impact cost and usage. Any processes that result in the creation or
termination of a resource need to be included for review. Look for processes that can support cost
awareness in your business, such as incident management and training.
• Establish self-sustaining cost-aware culture: Make sure all the relevant stakeholders align with cause-
of-change and impact as a cost so that they understand cloud cost. This will allow your organization to
establish a self-sustaining cost-aware culture of innovation.
• Update processes with cost awareness: Each process is modified to be made cost aware. The process
may require additional pre-checks, such as assessing the impact of cost, or post-checks validating that
the expected changes in cost and usage occurred. Supporting processes such as training and incident
management can be extended to include items for cost and usage.
To get help, reach out to CFM experts through your Account team, or explore the resources and related
documents below.
Resources
Related documents:
• AWS Cloud Financial Management
Related examples:
• Strategy for Efficient Cloud Cost Management
• Cost Control Blog Series #3: How to Handle Cost Shock
• A Beginner’s Guide to AWS Cost Management
462

AWS Well-Architected Framework
Practice Cloud Financial Management
COST01-BP05 Report and notify on cost optimization
Configure AWS Budgets and AWS Cost Anomaly Detection to provide notifications on cost and usage
against targets. Have regular meetings to analyze your workload's cost efficiency and to promote cost-
aware culture.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
You must regularly report on cost and usage optimization within your organization. You can implement
dedicated sessions to cost optimization, or include cost optimization in your regular operational
reporting cycles for your workloads. Use services and tools to identify and implement cost savings
opportunities. AWS Cost Explorer provides dashboards and reports. You can track your progress of cost
and usage against configured budgets with AWS Budgets Reports.
Use AWS Budgets to set custom budgets to track your costs and usage, and respond quickly to alerts
received from email or Amazon Simple Notification Service (Amazon SNS) notifications if you exceed
your threshold. Set your preferred budget period to daily, monthly, quarterly, or annually, and create
specific budget limits to stay informed on how actual or forecasted costs and usage progress toward your
budget threshold. You can also set up alerts and actions against those alerts to run automatically, or
through an approval process when a budget target is exceeded.
Implement notifications on cost and usage to ensure that changes in cost and usage can be acted upon
quickly if they are unexpected. AWS Cost Anomaly Detection allows you to reduce cost surprises and
enhance control without slowing innovation. AWS Cost Anomaly Detection identifies anomalous spend
and root causes, which helps to reduce the risk of billing surprises. With three simple steps, you can
create your own contextualized monitor and receive alerts when any anomalous spend is detected.
You can also use Amazon QuickSight with AWS Cost and Usage Report (CUR) data, to provide highly
customized reporting with more granular data. Amazon QuickSight allows you to schedule reports and
receive periodic Cost Report emails for historical cost and usage, or cost-saving opportunities.
Use AWS Trusted Advisor, which provides guidance to verify whether provisioned resources are aligned
with AWS best practices for cost optimization.
Periodically create reports containing a highlight of Savings Plans, Reserved Instances and Amazon
Elastic Compute Cloud (Amazon EC2) rightsizing recommendations from AWS Cost Explorer to start
reducing the cost associated with steady-state workloads, idle, and underutilized resources. Identify
and recoup spend associated with cloud waste for resources that are deployed. Cloud waste occurs
when incorrectly-sized resources are created, or different usage patterns are observed instead what is
expected. Follow AWS best practices to reduce your waste and optimize and save your cloud costs.
Generate reports regularly for better purchasing options for your resources to drive down unit costs
for your workloads. Purchasing options such as Savings Plans, Reserved Instances, or Amazon EC2 Spot
Instances offer the deepest cost savings for fault-tolerant workloads and allow stakeholders (business
owners, finance and tech teams) to be part of these commitment discussions.
Share the reports that contain opportunities or new release announcements that may help you to reduce
total cost of ownership (TCO) of the cloud. Adopt new services, Regions, features, solutions, or new ways
to achieve further cost reductions.
Implementation steps
• Configure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set a budget for
the overall account spend, and a budget for the workload by using tags.
• Well-Architected Labs: Cost and Governance Usage
• Report on cost optimization: Set up a regular cycle to discuss and analyze the efficiency of the
workload. Using the metrics established, report on the metrics achieved and the cost of achieving
463

AWS Well-Architected Framework
Practice Cloud Financial Management
them. Identify and fix any negative trends, and identify positive trends that you can promote across
your organization. Reporting should involve representatives from the application teams and owners,
finance, and management.
• Well-Architected Labs: Visualization
Resources
Related documents:
• AWS Cost Explorer
• AWS Trusted Advisor
• AWS Budgets
• AWS Budgets Best Practices
• Amazon CloudWatch
• AWS CloudTrail
• Amazon S3 Analytics
• AWS Cost and Usage Report
Related examples:
• Well-Architected Labs: Cost and Governance Usage
• Well-Architected Labs: Visualization
• Key ways to start optimizing your AWS cloud costs
COST01-BP06 Monitor cost proactively
Implement tooling and dashboards to monitor cost proactively for the workload. Regularly review the
costs with configured tools or out of the box tools, do not just look at costs and categories when you
receive notifications. Monitoring and analyzing costs proactively helps to identify positive trends and
allows you to promote them throughout your organization.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
It is recommended to monitor cost and usage proactively within your organization, not just when there
are exceptions or anomalies. Highly visible dashboards throughout your office or work environment
ensure that key people have access to the information they need, and indicate the organization’s
focus on cost optimization. Visible dashboards allow you to actively promote successful outcomes and
implement them throughout your organization.
Create a daily or frequent routine to use AWS Cost Explorer or any other dashboard such as Amazon
QuickSight to see the costs and analyze proactively. Analyze AWS service usage and costs at the AWS
account-level, workload-level, or specific AWS service-level with grouping and filtering, and validate
whether they are expected or not. Use the hourly- and resource-level granularity and tags to filter
and identify incurring costs for the top resources. You can also build your own reports with the Cost
Intelligence Dashboard, an Amazon QuickSight solution built by AWS Solutions Architects, and compare
your budgets with the actual cost and usage.
Implementation steps
• Report on cost optimization: Set up a regular cycle to discuss and analyze the efficiency of the
workload. Using the metrics established, report on the metrics achieved and the cost of achieving
464

AWS Well-Architected Framework
Practice Cloud Financial Management
them. Identify and fix any negative trends, and identify positive trends to promote across your
organization. Reporting should involve representatives from the application teams and owners,
finance, and management.
• Create and activate daily granularity AWS Budgets for the cost and usage to take timely actions to
prevent any potential cost overruns:  AWS Budgets allow you to configure alert notifications, so you
stay informed if any of your budget types fall out of your pre-configured thresholds. The best way to
leverage AWS Budgets is to set your expected cost and usage as your limits, so that anything above
your budgets can be considered overspend.
• Create AWS Cost Anomaly Detection for cost monitor:  AWS Cost Anomaly Detection uses advanced
Machine Learning technology to identify anomalous spend and root causes, so you can quickly take
action. It allows you to configure cost monitors that define spend segments you want to evaluate
(for example, individual AWS services, member accounts, cost allocation tags, and cost categories),
and lets you set when, where, and how you receive your alert notifications. For each monitor, attach
multiple alert subscriptions for business owners and technology teams, including a name, a cost
impact threshold, and alerting frequency (individual alerts, daily summary, weekly summary) for each
subscription.
• Use AWS Cost Explorer or integrate your AWS Cost and Usage Report (CUR) data with Amazon
QuickSight dashboards to visualize your organization’s costs: AWS Cost Explorer has an easy-to-use
interface that lets you visualize, understand, and manage your AWS costs and usage over time. The
Cost Intelligence Dashboard is a customizable and accessible dashboard to help create the foundation
of your own cost management and optimization tool.
Resources
Related documents:
• AWS Budgets
• AWS Cost Explorer
• Daily Cost and Usage Budgets
• AWS Cost Anomaly Detection
Related examples:
• Well-Architected Labs: Visualization
• Well-Architected Labs: Advanced Visualization
• Well-Architected Labs: Cloud Intelligence Dashboards
• Well-Architected Labs: Cost Visualization
• AWS Cost Anomaly Detection Alert with Slack
COST01-BP07 Keep up-to-date with new service releases
Consult regularly with experts or AWS Partners to consider which services and features provide lower
cost. Review AWS blogs and other information sources.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
AWS is constantly adding new capabilities so you can leverage the latest technologies to experiment
and innovate more quickly. You may be able to implement new AWS services and features to increase
cost efficiency in your workload. Regularly review AWS Cost Management, the AWS News Blog, the
AWS Cost Management blog, and What’s New with AWS for information on new service and feature
465

AWS Well-Architected Framework
Practice Cloud Financial Management
releases. What's New posts provide a brief overview of all AWS service, feature, and Region expansion
announcements as they are released.
Implementation steps
• Subscribe to blogs: Go to the AWS blogs pages and subscribe to the What's New Blog and other
relevant blogs. You can sign up on the communication preference page with your email address.
• Subscribe to AWS News: Regularly review the AWS News Blog and What’s New with AWS for
information on new service and feature releases. Subscribe to the RSS feed, or with your email to
follow announcements and releases.
• Follow AWS Price Reductions: Regular price cuts on all our services has been a standard way for AWS
to pass on the economic efficiencies to our customers gained from our scale. As of April 2022, AWS
has reduced prices 115 times since it was launched in 2006. If you have any pending business decisions
due to price concerns, you can review them again after price reductions and new service integrations.
You can learn about the previous price reductions efforts, including Amazon Elastic Compute Cloud
(Amazon EC2) instances, in the price-reduction category of the AWS News Blog.
• AWS events and meetups: Attend your local AWS summit, and any local meetups with other
organizations from your local area. If you cannot attend in person, try to attend virtual events to hear
more from AWS experts and other customers’ business cases.
• Meet with your account team: Schedule a regular cadence with your account team, meet with them
and discuss industry trends and AWS services. Speak with your account manager, Solutions Architect,
and support team.
Resources
Related documents:
• AWS Cost Management
• What’s New with AWS
• AWS News Blog
Related examples:
• Amazon EC2 - 15 Years of Optimizing and Saving Your IT Costs
• AWS News Blog - Price Reduction
COST01-BP08 Create a cost-aware culture
Implement changes or programs across your organization to create a cost-aware culture. It is
recommended to start small, then as your capabilities increase and your organization’s use of the cloud
increases, implement large and wide ranging programs.
Level of risk exposed if this best practice is not established: Low
Implementation guidance
A cost-aware culture allows you to scale cost optimization and Cloud Financial Management (financial
operations, cloud center of excellence, cloud operations teams, and so on) through best practices that
are performed in an organic and decentralized manner across your organization. Cost awareness allows
you to create high levels of capability across your organization with minimal effort, compared to a strict
top-down, centralized approach.
Creating cost awareness in cloud computing, especially for primary cost drivers in cloud computing,
allows teams to understand expected outcomes of any changes in cost perspective. Teams who access
466

AWS Well-Architected Framework
Practice Cloud Financial Management
the cloud environments should be aware of pricing models and the difference between traditional on-
premesis datacenters and cloud computing.
The main benefit of a cost-aware culture is that technology teams optimize costs proactively and
continually (for example, they are considered a non-functional requirement when architecting new
workloads, or making changes to existing workloads) rather than performing reactive cost optimizations
as needed.
Small changes in culture can have large impacts on the efficiency of your current and future workloads.
Examples of this include:
• Giving visibility and creating awareness in engineering teams to understand what they do, and what
they impact in terms of cost.
• Gamifying cost and usage across your organization. This can be done through a publicly visible
dashboard, or a report that compares normalized costs and usage across teams (for example, cost-per-
workload and cost-per-transaction).
• Recognizing cost efficiency. Reward voluntary or unsolicited cost optimization accomplishments
publicly or privately, and learn from mistakes to avoid repeating them in the future.
• Creating top-down organizational requirements for workloads to run at pre-defined budgets.
• Questioning business requirements of changes, and the cost impact of requested changes to the
architecture infrastructure or workload configuration to make sure you pay only what you need.
• Making sure the change planner is aware of expected changes that have a cost impact, and that they
are confirmed by the stakeholders to deliver business outcomes cost-effectively.
Implementation steps
• Report cloud costs to technology teams: To raise cost awareness, and establish efficiency KPIs for
finance and business stakeholders.
• Inform stakeholders or team members about planned changes: Create an agenda item to discuss
planned changes and the cost-benefit impact on the workload during weekly change meetings.
• Meet with your account team: Establish a regular meeting cadence with your account team, and
discuss industry trends and AWS services. Speak with your account manager, architect, and support
team.
• Share success stories: Share success stories about cost reduction for any workload, AWS account, or
organization to create a positive attitude and encouragement around cost optimization.
• Training: Ensure technical teams or team members are trained for awareness of resource costs on AWS
Cloud.
• AWS events and meetups: Attend local AWS summits, and any local meetups with other organizations
from your local area.
• Subscribe to blogs: Go to the AWS blogs pages and subscribe to the What's New Blog and other
relevant blogs to follow new releases, implementations, examples, and changes shared by AWS.
Resources
Related documents:
• AWS Blog
• AWS Cost Management
• AWS News Blog
Related examples:
• AWS Cloud Financial Management
467

AWS Well-Architected Framework
Expenditure and usage awareness
• AWS Well-Architected Labs: Cloud Financial Management
COST01-BP09 Quantify business value from cost optimization
Quantifying business value from cost optimization allows you to understand the entire set of benefits
to your organization. Because cost optimization is a necessary investment, quantifying business value
allows you to explain the return on investment to stakeholders. Quantifying business value can help you
gain more buy-in from stakeholders on future cost optimization investments, and provides a framework
to measure the outcomes for your organization’s cost optimization activities.
Level of risk exposed if this best practice is not established: Medium
Implementation guidance
In addition to reporting savings from cost optimization, it is recommended that you quantify the
additional value delivered. Cost optimization benefits are typically quantified in terms of lower
costs per business outcome. For example, you can quantify On-Demand Amazon Elastic Compute
Cloud(Amazon EC2) cost savings when you purchase Savings Plans, which reduce cost and maintain
workload output levels. You can quantify cost reductions in AWS spending when idle Amazon EC2
instances are terminated, or unattached Amazon Elastic Block Store (Amazon EBS) volumes are deleted.
The benefits from cost optimization, however, go above and beyond cost reduction or avoidance.
Consider capturing additional data to measure efficiency improvements and business value.
Implementation steps
• Executing cost optimization best practices: For example, resource lifecycle management reduces
infrastructure and operational costs and creates time and unexpected budget for experimentation.
This increases organization agility and uncovers new opportunities for revenue generation.
• Implementing automation: For example, Auto Scaling, which ensures elasticity at minimal effort,
and increases staff productivity by eliminating manual capacity planning work. For more details on
operational resiliency, refer to the Well-Architected Reliability Pillar whitepaper.
• Forecasting future AWS costs: Forecasting helps finance stakeholders to set expectations with other
internal and external organization stakeholders, and helps improve your organization’s financial
predictability. AWS Cost Explorer can be used to perform forecasting for your cost and usage.
Resources
Related documents:
• AWS Blog
• AWS Cost Management
• AWS News Blog
• Well-Architected Reliability Pillar whitepaper
• AWS Cost Explorer
Expenditure and usage awareness
Questions
• COST 2. How do you govern usage? (p. 469)
• COST 3. How do you monitor usage and cost? (p. 480)
• COST 4. How do you decommission resources? (p. 489)
468

AWS Well-Architected Framework
Expenditure and usage awareness
COST 2. How do you govern usage?
Establish policies and mechanisms to verify that appropriate costs are incurred while objectives are
achieved. By employing a checks-and-balances approach, you can innovate without overspending.
Best practices
• COST02-BP01 Develop policies based on your organization requirements (p. 469)
• COST02-BP02 Implement goals and targets (p. 472)
• COST02-BP03 Implement an account structure (p. 473)
• COST02-BP04 Implement groups and roles (p. 477)
• COST02-BP05 Implement cost controls (p. 477)
• COST02-BP06 Track project lifecycle (p. 479)
COST02-BP01 Develop policies based on your organization requirements
This best practice was updated with new guidance on July 13th, 2023.
Develop policies that define how resources are managed by your organization and inspect them
periodically. Policies should cover the cost aspects of resources and workloads, including creation,
modification, and decommissioning over a resource’s lifetime.
Level of risk exposed if this best practice is not established: High
Implementation guidance
Understanding your organization’s costs and drivers is critical for managing your cost and usage
effectively and identifying cost reduction opportunities. Organizations typically operate multiple
workloads run by multiple teams. These teams can be in different organization units, each with its own
revenue stream. The capability to attribute resource costs to the workloads, individual organization,
or product owners drives efficient usage behaviour and helps reduce waste. Accurate cost and usage
monitoring helps you understand how optimized a workload is, as well as how profitable organization
units and products are. This knowledge allows for more informed decision making about where to
allocate resources within your organization. Awareness of usage at all levels in the organization is key to
driving change, as change in usage drives changes in cost. Consider taking a multi-faceted approach to
becoming aware of your usage and expenditures.
The first step in performing governance is to use your organization’s requirements to develop policies
for your cloud usage. These policies define how your organization uses the cloud and how resources
are managed. Policies should cover all aspects of resources and workloads that relate to cost or usage,
including creation, modification, and decommissioning over a resource’s lifetime. Verify that policies
and procedures are followed and implemented for any change in a cloud environment. During your IT
change management meetings, raise questions to find out the cost impact of planned changes, whether
increasing or decreasing, the business justification, and the expected outcome.
Policies should be simple so that they are easily understood and can be implemented effectively
throughout the organization. Policies also need to be easy to follow and interpret (so they are used) and
specific (no misinterpretation between teams). Moreover, they need to be inspected periodically (like our
mechanisms) and updated as customers business conditions or priorities change, which would make the
policy outdated.
Start with broad, high-level policies, such as which geographic Region to use or times of the day that
resources should be running. Gradually refine the policies for the various organizational units and
469
