Checklist - Testing for performance
efficiency
Article • 05/30/2023

Is the application tested for performance, scalability, and resiliency?
Performance testing helps to maintain systems properly and fix defects before problems
reach system users. It's part of the Performance Efficiency pillar in the Microsoft Azure
Well-Architected Framework.
Performance testing is the superset of both load testing and stress testing. The primary
goal of performance testing is to validate benchmark behavior for the application.
Load Testing validates application scalability by rapidly or gradually increasing the load
on the application until it reaches a threshold.
Stress Testing is a type of negative testing that involves various activities to overload
existing resources and remove components. This testing lets you understand overall
resiliency and how the application responds to issues.
Use the following checklist to review your application architecture from a performance
testing standpoint.

Performance testing
Ensure solid performance testing with shared team responsibility. Successfully
implementing meaningful performance tests requires many resources. It's not just
a single developer or QA analyst running some tests on their local machine.
Instead, performance tests need a test environment (also known as a test bed) that
tests can be executed against without interfering with production environments
and data. Performance testing requires input and commitment from developers,
architects, database administrators, and network administrators.
Capacity planning. When performance testing, the business must communicate
any fluctuation in expected load. Load can be impacted by:
World events, such as political, economic, or weather changes.
Marketing initiatives, such as sales or promotions.
Seasonal events, such as holidays.
Test variations of load prior to events, including unexpected ones, to ensure that
your application can scale. Additionally, you should ensure that all regions can

adequately scale to support total load, should one region fail.
Identify a path forward to leveraging existing tests or the creation of new tests.
There are different types of performance testing: load testing, stress testing, API
testing, client-side/browser testing, and so on. It's important that you understand
and articulate the different types of tests, along with their advantages and
disadvantages, to the customer.
Perform testing in all stages in the development and deployment life cycle.
Application code, infrastructure automation, and fault tolerance should all be
tested. This step ensures that the application performs as expected in every
situation. You want to test early enough in the application life cycle to catch and fix
errors. Errors are cheaper to repair when caught early and can be expensive or
impossible to fix later. To learn more, reference Testing your application and Azure
environment.
Avoid experiencing poor performance with testing. Two subsets of performance
testing—load testing and stress testing—can determine the upper limit and
maximum point of failure, respectively, of the application's capacity. By performing
these tests, you can determine the necessary infrastructure to support the
anticipated workloads.
Plan for a load buffer to accommodate random spikes without overloading the
infrastructure. For example, if a normal system load is 100,000 requests per
second, the infrastructure should support 100,000 requests at 80% of total capacity
(for example, 125,000 requests per second). If you expect that the application
continues to sustain 100,000 requests per second, and the current Stock Keeping
Unit (SKU) introduces latency at 65,000 requests per second, you'll most likely need
to upgrade your product to the next higher SKU. If there's a secondary region,
you'll need to ensure that it also supports the higher SKU.
Test failover in multiregions. Test the amount of time it would take for users to be
rerouted to the paired region so that the region doesn't fail. Typically, a planned
test failover can help determine how much time would be required to fully scale to
support the redirected load.
Configure the environment based on testing results to sustain performance
efficiency. Scale out or scale in to handle increases and decreases in load. For
example, you might know that you encounter high levels of traffic during the day
and low levels on weekends. You can configure the environment to scale out for
increases in load or scale in for decreases before the load actually changes.

Testing tools
Choose testing tools based on the type of performance testing you are
attempting to execute. There are various performance testing tools available for
Azure DevOps. Some tools, like JMeter, only perform testing against endpoints and
test HTTP statuses. Other tools, such as K6 and Selenium, can perform tests that
also check data quality and variations. Azure Load Testing lets you create a load
test by using an existing JMeter script, and monitor client-side and server-side
metrics to identify performance bottlenecks. Application Insights, while not
necessarily designed to test server load, can test the performance of an application
within the user's browser.
Carry out performance profiling and load testing during development, as part of
test routines, and before final release to ensure the application performs and
scales as required. This testing should occur on the same type of hardware as the
production platform, and with the same types of data, quantities of data, and user
load as it encounters in production.
Determine if it is better to use automated or manual testing. Testing can be
automated or manual. Automating tests is the best way to make sure that they're
executed. Depending on how frequently tests are performed, they're typically
limited in duration and scope. Manual testing is run much less frequently.
Cache data to improve performance, scalability, and availability. The more data
that you have, the greater the benefits of caching become. Caching typically works
well with data that is immutable or that changes infrequently.
Decide how you'll handle local development and testing when some static
content is expected to be served from a content delivery network (CDN). You
could predeploy the content to the CDN as part of your build script. Or, use
compile directives or flags to control how the application loads the resources. For
example, in debug mode, the application could load static resources from a local
folder. In release mode, the application would use the CDN.
Simulate different workloads on your application and measure application
performance for each workload. This technique is the best way to figure out what
resources you need to host your application. Use performance indicators to assess
whether your application is performing as expected or not.

Recommendation
Define a testing strategy. For more information, see Testing.

Next steps
Performance testing

Performance testing
Article • 05/30/2023

Performance testing helps to maintain systems properly and fix defects before problems
reach system users. It helps maintain the efficiency, responsiveness, scalability, and
speed of applications when compared with business requirements. When done
effectively, performance testing should give you the diagnostic information necessary to
eliminate bottlenecks, which lead to poor performance. A bottleneck occurs when data
flow is either interrupted or stops due to insufficient capacity to handle the workload.
To avoid experiencing poor performance, commit time and resources to testing system
performance. Two subsets of performance testing—load testing and stress testing—can
determine the upper (close to capacity) limit and maximum (point of failure) limit,
respectively, of the application's capacity. By performing these tests, you can determine
the necessary infrastructure to support the anticipated workloads.
A best practice is to plan for a load buffer to accommodate random spikes without
overloading the infrastructure. For example, if a normal system load is 100,000 requests
per second, the infrastructure should support 100,000 requests at 80% of total capacity
(that is, 125,000 requests per second). If you anticipate that the application will continue
to sustain 100,000 requests per second, and the current Stock Keeping Unit (SKU)
introduces latency at 65,000 requests per second, consider upgrading your product to
the next higher SKU. If there's a secondary region, you must ensure that it also supports
the higher SKU.
Depending on the scale of your performance test, you must plan for and maintain a
testing infrastructure. You can use a cloud-based tool, such as Azure Load Testing, to
abstract the infrastructure needed to run your performance tests.

Establish baselines
Baselines help to determine the current efficiency state of your application and its
supporting infrastructure. Baselines can provide good insights for improvements and
determine if the application is meeting business goals. Baselines can be created for any
application regardless of its maturity.
First, establish performance baselines for your application. Then, establish a regular
cadence for running the tests. Run the test as part of a scheduled event or part of a
continuous integration (CI) build pipeline.

No matter when you establish the baseline, measure performance against that baseline
during continued development. When code or infrastructure changes, the effect on
performance can be actively measured.

Load testing
Load testing measures system performance as the workload increases. It identifies
where and when your application breaks so you can fix the issue before shipping to
production. It does so by testing system behavior under typical and heavy loads.
Load testing takes places in stages of load. These stages are usually measured by virtual
users (VUs) or simulated requests, and the stages happen over given intervals. Load
testing provides insights into how and when your application needs to scale to continue
meeting your SLA to your customers, both internal and external. Load testing can also
be useful for determining latency across distributed applications and microservices.
Consider the following key points for load testing:
Know the Azure service limits: Different Azure services have soft and hard limits
associated with them. The terms soft limit and hard limit describe the current,
adjustable service limit (soft limit) and the maximum limit (hard limit). Understand
the limits for the services you consume so that you aren't blocked if you need to
exceed them. For a list of the most common Azure limits, see Azure subscription
and service limits, quotas, and constraints.
The ResourceLimits

sample shows how to query the limits and quotas for

commonly used resources.
Measure typical loads: Knowing the typical and maximum loads on your system
helps you understand when something is operating outside of its designed limits.
Monitor traffic to understand application behavior.
Understand application behavior under various scales: Load test your application
to understand how it performs at various scales. First, test to see how the
application performs under a typical load. Then, test to see how it performs under
load using different scaling operations. To get more insight into how to evaluate
your application as the amount of traffic sent to it increases, see Autoscale best
practices.

Stress testing

Unlike load testing, which ensures that a system can handle what it's designed to
handle, stress testing focuses on overloading the system until it breaks. A stress test
determines how stable a system is and its ability to withstand extreme increases in load.
For example, it can test the maximum number of requests from another service that a
system can handle at a given time before performance is compromised and fails. Find
this maximum to understand what kind of load the current environment can adequately
support.
Determine the maximum demand you want to place on memory, CPU, and disk IOPS.
After a stress test has been performed, you know the maximum supported load and an
operational margin. It's best to choose an operational threshold so that scaling can be
performed before the threshold has been reached.
Once you determine an acceptable operational margin and response time under typical
loads, verify that the environment is configured adequately. To verify the configuration,
make sure the SKUs that you selected are based on the desired margins. Be careful to
stay as close as possible to your margins. Allocating too much can increase costs and
maintenance unnecessarily. Allocating too few can result in a poor user experience.
In addition to stress testing through increased load, you can stress test by reducing
resources to identify what happens when the machine runs out of memory. You can also
stress test by increasing latency (for example, the database takes 10 times longer to
reply, writes to storage takes 10 times longer, and so on).

Multi-region testing
A multi-region architecture can provide higher availability than deploying to a single
region. If a regional outage affects the primary region, you can use Front Door to use
the secondary region. This architecture can also help if an individual subsystem of the
application fails.
Test the amount of time it would take for users to be rerouted to the paired region so
that the region doesn't fail. To learn more about routing, see Front Door routing
methods. Typically, a planned test failover can help determine how much time would be
required to fully scale to support the redirected load.

Configure the environment based on testing
results
Once you have performed testing and found an acceptable operational margin and
response under increased levels of load, configure the environment to sustain

performance efficiency. Scale out or scale in to handle increases and decreases in load.
For example, you might know that you encounter high levels of traffic during the day
and low levels on weekends. You can configure the environment to scale out for
increases in load or scale in for decreases before the load actually changes.
For more information on autoscaling, see Design for scaling in the Performance
Efficiency pillar.
７ Note
Ensure that a rule has been configured to scale the environment back down once
load drops below the set thresholds. This action helps save you money.

Next steps
Testing tools

Load Testing
Article • 05/30/2023

Testing at expected peak load
Load test your application at the expected peak load to ensure there are no challenges
with performance or stability when operating at full capacity. Review Changes to load
test functionality in Visual Studio and cloud load testing in Azure DevOps to get an
overview of the Microsoft provided load testing tools, along with a comprehensive list of
third-party tools that you can use.

Azure service limits
Different Azure services have soft and hard limits associated with them. Understand the
limits for the services you consume so that you aren't blocked if you need to exceed
them. Review Azure subscription and service limits, quotas, and constraints to get a list
of the most common Azure limits.
The ResourceLimits

sample shows how to query the limits and quotas for

commonly used resources.

Understanding application behavior under load
Load test your application to understand how it performs at various scales. Review
Autoscale best practices to get more insight into how you can evaluate your application
as the amount of traffic sent to it increases.

Measuring typical loads
Knowing the typical and maximum loads on your system helps you understand when
something is operating outside of its designed limits. Monitor traffic to your application
to understand user behavior.

Caching
Applications should implement a strategy that helps to ensure that the data in the cache
is as up to date as possible, but can also detect and handle situations that arise when

the data in the cache has become stale. Review the Cache-Aside pattern to learn how to
load data on demand into a cache from a data store. Doing so can improve
performance and also helps to maintain consistency between data held in the cache and
data in the underlying data store.

Availability of SKUs
Certain Azure SKUs are only available in certain regions. Understand which SKUs are
available in the regions you operate in so you can plan accordingly. Read about global
infrastructure services

.

Related sections
Performance tuning scenario: Event streaming with Azure Functions

Testing strategies
Article • 05/30/2023

There are multiple stages in the development and deployment life cycle in which tests
can be performed. Application code, infrastructure automation, and fault tolerance
should all be tested. Testing in various stages can ensure that the application performs
as expected in every situation. You want to test early enough in the application life cycle
to catch and fix errors. Errors are cheaper to repair when caught early and can be
expensive or impossible to fix later.
Testing can be automated or manual. Automating tests is the best way to make sure that
they're executed. Depending on how frequently tests are performed, they're typically
limited in duration and scope. Manual testing is run much less frequently. For a list of
tests that you should consider while developing and deploying applications, see Testing
your application and Azure environment.

Identify baselines and goals for performance
Knowing where you are (baseline) and where you want to be (goal) makes it easier to
plan how to get there. Established baselines and goals help you to stay on track and
measure progress. Testing might also uncover a need to perform more testing on areas
for which you might not have planned.
Baselines can vary based on connections or platforms used for accessing the application.
It can be important to establish baselines that address the different connections,
platforms, and elements such as time of day, or weekday versus weekend.
There are many types of goals when determining baselines for application performance.
Some examples are the time it takes to render a page, or a desired number of
transactions if your site conducts e-commerce. The following list shows some examples
of questions that can help you determine goals.
What are your baselines and goals for:
Establishing an initial connection to a service?
An API endpoint complete response?
Server response times?
Latency between systems/microservices?
Database queries?

Caching data
Caching can dramatically improve performance, scalability, and availability. The more
data that you have, the greater the benefits of caching become. Caching typically works
well with data that is immutable or that changes infrequently. Examples include
reference information such as product and pricing information in an e-commerce
application, or shared static resources that are costly to construct. Some or all of this
data can be loaded into the cache at application startup to minimize demand on
resources and to improve performance.
Use performance testing and usage analysis to determine whether prepopulating or ondemand loading of the cache, or a combination of both, is appropriate. The decision
should be based on the volatility and usage pattern of the data. Cache utilization and
performance analysis are important in applications that encounter heavy loads and must
be highly scalable.
To learn more about how to use caching as a solution in testing, see Caching.

Use Azure Cache for Redis to cache data
Azure Cache for Redis is a caching service that can be accessed from any Azure
application, whether the application is implemented as a cloud service, a website, or
inside an Azure virtual machine. Caches can be shared by client applications that have
the appropriate access key. It's a high-performance caching solution that provides
availability, scalability, and security.
To learn more about using Azure Cache for Redis, see Considerations for implementing
caching in Azure.

Content delivery network
Content delivery networks (CDNs) are typically used to deliver static content such as
images, style sheets, documents, client-side scripts, and HTML pages. The major
advantages of using a CDN are lower latency and faster delivery of content to users,
regardless of their geographical location in relation to the datacenter where the
application is hosted. CDNs can help to reduce load on a web application because the
application doesn't have to service requests for the content that is hosted in the CDN.
Using a CDN is a good way to minimize the load on your application and maximize
availability and performance. Consider adopting this strategy for all of the appropriate
content and resources your application uses.

Decide how to handle local development and testing when some static content is served
from a CDN. For example, you could predeploy the content to the CDN as part of your
build script. Alternatively, use compile directives or flags to control how the application
loads the resources. For example, in debug mode, the application could load static
resources from a local folder. In release mode, the application would use the CDN.
To learn more about CDNs, see Best practices for using content delivery networks
(CDNs).

Benchmark testing
Benchmarking is the process of simulating different workloads on your application and
measuring application performance for each workload. It's the best way to figure out
what resources you need to host your application.
Use performance indicators to assess whether your application is performing as
expected or not.
For workloads running on virtual machines, take into consideration VM sizes and disk
sizes when benchmarking, since you might hit a particular bottleneck. The Optimize
IOPS, throughput, and latency table offers further guidance.
Tools such as Azure Load Testing can help you simulate load and different usage
patterns. The simulation can help you prepare for particular scenarios that are relevant
to your organization or industry, for example, how a promotion or flash sale might affect
an online store.

Metrics
Metrics measure trends over time. They're available for interactive analysis in the Azure
portal with Azure Monitor metrics explorer. Metrics also can be added to an Azure
dashboard for visualization in combination with other data and used for near-real time
alerting.
Performance testing lets you see specific details on the processing capabilities of
applications. You'll most likely want a monitoring tool that lets you discover proactively
if the issues you find through testing are appearing in both your infrastructure and
applications. Azure Monitor Metrics is a feature of Azure Monitor that collects metrics
from monitored resources into a time series database.
With Azure Monitor, you can collect, analyze, and act on telemetry from your cloud and
on-premises environments. It helps you understand how applications are performing

and identifies issues affecting them and the resources they depend on.
For a list of Azure metrics, see Supported metrics with Azure Monitor.

Next steps
Performance monitoring

Monitor the performance of a cloud
application
Article • 05/04/2023

Troubleshooting an application's performance requires monitoring and reliable
investigation. Issues in performance can arise from database queries, connectivity
between services, under-provisioned resources, or memory leaks in code.
Continuously monitoring services and checking the health state of current workloads is
key in maintaining the overall performance of the workload. An overall monitoring
strategy considers these factors:
Scalability
Resiliency of the infrastructure, application, and dependent services
Application and infrastructure performance

Checklist
How are you monitoring to ensure the workload is scaling appropriately?
＂ Enable and capture telemetry throughout your application to build and visualize
end-to-end transaction flows for the application.
＂ See metrics from Azure services, such as CPU and memory utilization, bandwidth
information, current storage utilization, and more.
＂ Use resource and platform logs to get information about what events occur and
under which conditions.
＂ For scalability, look at the metrics to determine how to provision resources
dynamically and scale with demand.
＂ In the collected logs and metrics, look for signs that might make a system or its
components suddenly become unavailable.
＂ Use log aggregation technology to gather information across all application
components.
＂ Store logs and key metrics of critical components for statistical evaluation and
predicting trends.
＂ Identify antipatterns in the code.

In this section
Follow these questions to assess the workload at a deeper level.

Assessment

Description

Are application logs and events correlated
across all application components?

Correlate logs and events for subsequent
interpretation. This correlation gives you
visibility into end-to-end transaction flows.

Are you collecting Azure Activity Logs within
the log aggregation tool?

Collect platform metrics and logs to get
visibility into the health and performance of
services that are part of the architecture.

Are application and resource-level logs

Implement a unified solution to aggregate and

aggregated in a single data sink, or is it
possible to cross-query events at both levels?

query application and resource-level logs, such
as Azure Log Analytics.

Azure services
The monitoring operations should utilize Azure Monitor

. You can analyze data, set up

alerts, get end-to-end views of your applications, and use machine learning–driven
insights to identify and resolve problems quickly. Export logs and metrics to services
such as Azure Log Analytics or an external service like Splunk. Also, application
technologies such as Application Insights can enhance the telemetry coming out of
applications.

Next section
Based on insights gained through monitoring, optimize your code. One option might be
to consider other Azure services that might be more appropriate for your objectives.
Optimize

Related links
Back to the main article

Application profiling considerations for
performance monitoring
Article • 05/19/2023

Continuously monitor the application with Application Performance Monitoring (APM)
technology, such as Application Insights. This technology can help you manage the
performance and availability of the application, aggregating application level logs and
events for subsequent interpretation.

Key points
Enable instrumentation and collect data using Application Insights.
Use distributed tracing to build and visualize end-to-end transaction flows for
the application.
Separate logs and events of a noncritical environment from a production
environment.
Include end-to-end transaction times for key technical functions.
Correlate application log events across critical system flows.

Application logs
Are application logs collected from different application environments?
Application logs support the end-to-end application lifecycle. Logging is essential in
understanding how the application operates in various environments and what events
occur and under which conditions.
Collect application logs and events across all application environments. Use a sufficient
degree of separation and filtering to ensure noncritical environments aren't mixed with
production log interpretation. Also, corresponding log entries across the application
should capture a correlation ID for their respective transactions.
Are log messages captured in a structured format?
Structured format in a well-known schema can help expedite parsing and analyzing logs.
Structured data can be indexed, queried, and reported without complexity.
Also, application events should be captured as a structured data type with machinereadable data points rather than unstructured string types.

Application instrumentation
Do you have detailed instrumentation in the application code?
Instrumentation of your code allows precise detection of underperforming pieces when
load or stress tests are applied. It's critical to have this data available to improve and
identify performance opportunities in the application code.
Use an APM such as Application Insights to continuously improve performance and
usability. You need to enable Application Insights by installing an instrumentation
package. The service provides extensive telemetry out of the box. You can customize
what is captured for greater visibility. After it's enabled, metrics and logs related to the
performance and operations are collected. View and analyze the captured data in Azure
Monitor.

Distributed tracing
Events coming from different application components or different component tiers of
the application should be correlated to build end-to-end transaction flows. Use
distributed tracing to build and visualize flows for the application. For example, this is
often achieved by using consistent correlation IDs transferred between components
within a transaction.
Are application events correlated across all application components?
Event correlation between the layers of the application provides the ability to connect
tracing data of the complete application stack. Once this connection is made, you can
see a complete picture of where time is spent at each layer. You can then query the
repositories of tracing data in correlation to a unique identifier that represents a
completed transaction that has flowed through the system.
For more information, see Distributed tracing.

Critical targets
Is it possible to evaluate critical application performance targets and nonfunctional
requirements (NFRs)?
Application level metrics should include end-to-end transaction times of key technical
functions, such as database queries, response times for external API calls, failure rates of
processing steps, and more.

Is the end-to-end performance of critical system flows monitored?
It should be possible to correlate application log events across critical system flows, such
as user sign-in, to fully assess the health of key scenarios in the context of targets and
NFRs.

Cost optimization
If you're using Application Insights to collect instrumentation data, there are cost
considerations. For more information, see Manage usage and costs for Application
Insights .

Next
Monitor infrastructure

Related links
Distributed tracing
Application Insights
Azure Monitor
Back to the main article

Analyze infrastructure metrics and logs
Article • 05/04/2023

Performance issues can occur because of interaction between the application and other
services in the architecture. For example, issues in database queries, connectivity
between services, and under-provisioned resources are all common causes for
inefficiencies.
The practice of continuous monitoring must include analysis of platform metrics and
logs to get visibility into the health and performance of services that are part of the
architecture.

Key points
View platform metrics to get visibility into the health and performance of
Azure services.
Use log data to get visibility into the operations and events of the
management plane.
Track events from internal dependencies.
Check the health of external dependencies, such as an API service.

Platform metrics
Metrics are numerical values that are collected at regular intervals and describe some
aspect of a system at a particular time. View the platform metrics that are generated by
the services used in the architecture. Each Azure service has a set of metrics that's
unique to the functionality of the resource. These metrics give you visibility into their
health and performance. There's no added configuration for Azure resources. You can
also define custom metrics for an Azure service using the custom metrics API.
Azure Monitor Metrics is a feature of Azure Monitor that collects numeric data from
monitored resources into a time series database. To learn more about Azure Monitor
Metrics, see What can you do with Azure Monitor Metrics?.
If your application is running in Azure Virtual Machines, configure Azure Diagnostics
extension to send guest OS performance metrics to Azure Monitor. Guest OS metrics
include performance counters that track guest CPU percentage or memory usage, both
of which are frequently used for autoscaling or alerting.
For more information, see Supported metrics with Azure Monitor.

Also, use technology-specific tools for the services used in the architecture. For example,
use network traffic capturing tools, such as Azure Network Watcher.
One of the challenges to metric data is that it often has limited information to provide
context for collected values. Azure Monitor addresses this challenge with multidimensional metrics. These metrics are name-value pairs that carry more data to
describe the metric value. To learn about multi-dimensional metrics and an example for
network throughput, see multi-dimensional metrics.

Platform logs
Azure provides various operational logs from the platform and the resources. These logs
provide insight into what events occurred, what changes were made to the resource,
and more. These logs are useful in tracking operations. For example, you can track
scaling events to check if autoscaling is working as expected.
Azure Monitor Logs can store various different data types each with their own structure.
You can also perform complex analysis on logs data using log queries, which can't be
used for analysis of metrics data. Azure Monitor Logs is capable of supporting near realtime scenarios, making them useful for alerting and fast detection of issues. To learn
more about Azure Monitor Logs, see What can you do with Azure Monitor Logs?.
Are you collecting Azure activity logs within the log aggregation tool?
Azure activity logs provide audit information about when an Azure resource is modified,
such as when a virtual machine is started or stopped. This information is useful for the
interpretation and troubleshooting of issues. It provides transparency around
configuration changes that can be mapped to adverse performance events.
Are logs available for critical internal dependencies?
To build a robust application health model, ensure there's visibility into the operational
state of critical internal dependencies.
In Azure Monitor, enable Azure resource logs so that you have visibility into operations
that were done within an Azure resource. Similar to platform metrics, resource logs vary
by the Azure service and resource type.
For example, for services such as a shared network virtual appliance (NVA) or Express
Route connection, monitor the network performance. Azure monitor can also help
diagnose networking related issues. You can trigger a packet capture, diagnose routing
issues, analyze network security group flow logs, and gain visibility and control over
your Azure network.

Here are some tools you can use:
Network performance monitor
Service connectivity monitor
ExpressRoute monitor
Also, data from network traffic capturing tools, such as Network Watcher, can be helpful.
Are critical external dependencies monitored?
Monitor critical external dependencies, such as an API service, to ensure operational
visibility of performance. For example, a probe can be used to measure the latency of an
external API.

Cost optimization for monitoring
Azure Monitor billing model is based on consumption. Azure creates metered instances
that track usage to calculate your bill. Pricing depends on the metrics, alerting,
notifications, Log Analytics, and Application Insights.
For information about usage and estimated costs, see Monitoring usage and estimated
costs in Azure Monitor.
You can also use the pricing calculator

to determine your pricing. The pricing

calculator helps you estimate your likely costs based on your expected use.

Next
Data analysis considerations

Related links
Supported metrics with Azure Monitor
Network performance monitor
Azure Network Watcher
Back to the main article

Performance data integration
Article • 05/19/2023

Performance testing and investigation should be based on data captured from
repeatable processes. To understand how an application's performance is affected by
code and infrastructure changes, retain data for analysis. Also, it's important to measure
how performance has changed over time, not just compared to the last measurement
taken.
This article describes some considerations and tools you can use to aggregate data for
troubleshooting and analyzing performance trends.

Key points
Analyze performance data holistically to detect fault types, bottleneck
regressions, and health states.
Use log aggregation technologies to consolidate data into a single workspace
and analyze using a sophisticated query language.
Retain data in a time-series database to predict performance issues before
they occur.
Balance the retention policy and service pricing plans with the cost expectation
of the organization.

Data interpretation
The overall performance can be impacted by both application-level issues and resourcelevel failures. It's vital that all data is correlated and evaluated together to optimize the
detection of issues and troubleshooting of detected issues. This approach helps
distinguish between transient and nontransient faults.
Use a holistic approach to quantify what healthy and unhealthy states represent across
all application components. We recommend that a traffic light model is used to indicate
a healthy state. For example, use a green light to show that key nonfunctional
requirements and targets are fully satisfied and resources are optimally used. For
example, a healthy state can mean that 95% of requests are processed in <= 500 ms
with AKS node utilization at x%, and so on. Also, an Application Map can help spot
performance bottlenecks or failure hotspots across components of a distributed
application.

Analyze long-term operational data to get historical context and detect if there have
been any regressions. For example, check the average response times to see if they've
been slowly increasing over time and getting closer to the maximum target.

Aggregated view
Log aggregation technologies should be used to collate logs and metrics across all
application components, including infrastructural components for later evaluation.
Resources can include Azure infrastructure as a service (IaaS) and platform as a service
(PaaS) services and third-party appliances, such as firewalls or antimalware solutions
used in the application. For example, if Azure Event Hubs is used, the diagnostic settings
should be configured to push logs and metrics to the data sink.
Azure Monitor can collect and organize log and performance data from monitored
resources. Data is consolidated into an Azure Log Analytics workspace so they can be
analyzed together using a sophisticated query language that can quickly analyze
millions of records. Splunk is another popular choice.
How is aggregated monitoring enforced?
All application resources should be configured to route diagnostic logs and metrics to
the chosen log aggregation technology. Use Azure Policy to ensure the consistent use of
diagnostic settings across the application and to enforce the desired configuration for
each Azure service.

Long-term data
Store long-term operational data to understand the history of application performance.
This data store is important for analyzing performance trends and regressions.
Are long-term trends analyzed to predict performance issues before they occur?
It's often helpful to store such data in a time-series database (TSDB) and then view the
data from an operational dashboard. An Azure Data Explorer cluster

is a powerful

TSDB that can store any schema of data, including performance test metrics. Grafana ,
an open-source platform for observability dashboards, can then be used to query your
Azure Data Explorer cluster to view performance trends in your application.
Have retention times been defined for logs and metrics, with housekeeping
mechanisms configured?

Clear retention times should be defined to allow for suitable historic analysis and to
control storage costs. Suitable housekeeping tasks should also be used to archive data
to cheaper storage or aggregate data for long-term trend analysis.

Cost optimization
While correlating data is recommended, there are cost implications to storing long-term
data. For example, Azure Monitor is capable of collecting, indexing, and storing massive
amounts of data in a Log Analytics workspace. The cost of a Log Analytics workspace is
based on amount of storage, retention period, and the plan. For more information
about how you can balance cost, see Manage usage and costs with Azure Monitor Logs.
If you're using Application Insights to collect instrumentation data, there are cost
considerations. For more information, see Manage usage and costs for Application
Insights.

Next
Scalability and reliability considerations

Related links
Application Map
Azure Policy
Azure Data Explorer cluster
Manage usage and costs with Azure Monitor Logs
Manage usage and costs for Application Insights
Back to the main article

Monitor performance for scalability and
reliability
Article • 05/19/2023

The overall monitoring strategy should take into consideration scalability and reliability
of the infrastructure, application, and dependent services.

Scalability
A goal of analyzing metrics is to determine the thresholds for scale up, scale out, scale
in, and scale down. The ability to scale dynamically is one of the biggest values of
moving to the cloud.
Know the minimum number of instances that should run at any given time.
Determine the best metrics for your solution on which to base your auto scaling
rules.
Configure the auto scaling rules for those services that include it.
Create alert rules for the services that can be scaled manually.
For scalability, look at the metrics to determine how to provision resources dynamically
and scale with demand.

Reliability
Monitor your application for early warning signs that might require proactive
intervention. Tools that assess the overall health of the application and its dependencies
help you to recognize quickly when a system or its components suddenly become
unavailable. Use them to implement an early warning system.
Monitoring systems should capture comprehensive details so that applications can be
restored efficiently. Then, if necessary, designers and developers can modify the system
to prevent the situation from recurring.
The raw data for monitoring can come from various sources, including:
Application logs, such as those produced by Application Insights.
Operating system performance metrics collected by Azure monitoring agents.
Azure resources, including metrics collected by Azure Monitor.
Azure Service Health, which offers a dashboard to help you track active events.
Azure AD logs built into the Azure platform.

To learn more, see Monitoring health for reliability.

Next section
Based on insights gained through monitoring, optimize your code. One option might be
to consider other Azure services that might be more appropriate for your objectives.
Optimize

Related links
Back to the main article

Sustain performance efficiency over
time
Article • 05/19/2023

Performance is an indication of the responsiveness of a system to execute any action
within a given time interval. As workloads and requirements change over time, you need
to continuously ensure that systems don't start to under-perform. This article explains
some steps you can take to help make sure your systems are continuously running at an
optimum level.

Optimize autoscaling
Autoscaling is the process of dynamically allocating resources to match performance
requirements. As the volume of work grows, an application may need more resources to
maintain the desired performance levels and satisfy service-level agreements (SLAs). As
demand slackens, and the extra resources are no longer needed, they can be
deallocated to minimize costs.
To prevent a system from attempting to scale out excessively, consider limiting the
maximum number of instances that can be automatically added. Doing so helps you
avoid the costs associated with running potentially many thousands of instances. Most
autoscaling mechanisms allow you to specify the minimum and maximum number of
instances for a rule. Also consider gracefully degrading the functionality that the system
provides if the maximum number of instances have been deployed and the system is
still overloaded.
But autoscaling might not always be the most appropriate mechanism to handle a
sudden burst in workload. It takes time to provision and start new instances of a service
or add resources to a system, and the peak demand may have passed by the time these
extra resources have been made available. In this scenario, it may be better to throttle
the service. For more information, see Throttling pattern.
If you do need the capacity to process all requests when the volume fluctuates rapidly,
you can use an aggressive autoscaling strategy. Aggressive autoscaling does increase
costs, but it adds the extra instances you need more quickly. You can also use a
scheduled policy that starts a sufficient number of instances to meet the maximum load
before that load is expected.

Separate out critical workload

Some data is accessed more frequently than other data. If you store data in its own
partition depending on its usage pattern, your system can run more efficiently.
Operations that affect more than one partition can run in parallel.
Partitioning data can improve the availability of applications by ensuring that the entire
dataset doesn't constitute a single point of failure and that subsets of the dataset can be
managed independently. For this reason, you might want to separate out critical
workload from noncritical. If one instance fails, only the data in that partition is
unavailable. Operations on other partitions can continue. For managed PaaS data stores,
this consideration is less relevant, because these services are designed with built-in
redundancy.
To learn more, see Data partitioning.

Right-size your resources
Changes to the resources that support a workload may affect the architecture of the
workload. When this situation happens, other considerations are required to minimize
the effect on end users and business functions. One of these considerations is rightsizing, which is about controlling cost by continuously monitoring and adjusting size of
your instances to meet needs. Cost shouldn't necessarily be the main decision-making
factor. Choosing the least expensive option could expose the workload to performance
and availability risks.
For example, when you consider pricing and sizing resources hosted in Azure, rightsizing virtual machines (VMs) is best practice. Choosing the right storage type for data
can save your organization several thousands of dollars every month. Microsoft offers
many options, and each VM type has specific features and different combinations of
CPU, memory, and disks.
To learn more about right sizing best practices with VMs, see Best practice: Right-size
VMs.

For accountability purposes
Identify right-size opportunities by reviewing your current resource utilization and
performance requirements across the environment. Then, modify each resource to use
the smallest instance or SKU that can support the performance requirements of each
resource.
For other best practices, see Best practices by team and accountability.

For operational cost management purposes
Review your environment's current resource utilization and performance requirements.
Then, identify resources that have remained underutilized (generally more than 90 days).
Also, right-size provisioned SKUs by modifying underutilized resources to use the
smallest instance or SKU that can support the performance requirements of each
resource. Finally, right-size redundancy. If the resource doesn't require a high degree of
redundancy, remove geo-redundant storage.
For other best practices, see Operational cost management best practices.

Remove antipatterns
A performance antipattern is a common practice that is likely to cause scalability
problems when an application is under pressure. Some of the following common
antipatterns can occur when a system offloads too much processing to a data store:
Moving resource-intensive tasks onto background threads.
Continually sending many small network requests.
Failing to cache data.
Antipatterns can cause decreased response time, high latency, slow I/O calls, and other
performance issues.
Many factors can cause an antipattern to occur. Sometimes an application inherits a
design that worked on-premises, but doesn't scale in the cloud. Or, an application might
start with a clean design, but as new features are added, one or more of these
antipatterns can appear.
Removing antipatterns can improve performance efficiency. But removal isn't a straightforward task, because sometimes the problem only manifests under certain
circumstances. Instrumentation and logging are key to finding the root cause, but you
also have to know what to look for. To learn more about common antipatterns and how
to identify and fix them, see Catalog of antipatterns.

Partition data to optimize performance
Article • 05/19/2023

Data partitioning can optimize performance, improve scalability, and reduce contention
by lowering the taxation of database operations. It can also provide a mechanism for
dividing data by usage pattern. For example, you can archive older data in cheaper data
storage. Data partitioning involves conversations and planning between developers and
database administrators.
For more reasons why you might want to partition, see Why partition data?

Determine acceptable performance
optimization
There's almost no limit to how much an application can be performance tuned. How do
you know when you have tuned an application enough? To find your limit, use the 80/20
rule. Generally, 80% of the application can be optimized by focusing on just 20%. A
company typically sees a diminishing return on further optimization after 20%. The
question you should answer is how much of the remaining 80% of the application is
worth optimizing for the business. For example, how much does optimizing the
remaining 80% help the business reach its goals of customer acquisition, retention,
sales, etc.? The business must determine its own realistic definition of "acceptable."

Types of partitioning
You can partition to hold a specific subset of the data, like all the orders for a set of
customers. You can hold a subset of fields that are divided according to their pattern of
use, like frequently accessed fields versus less frequently accessed fields. Or you can
aggregate data according to how it's used by each bounded context in the system, like
how an e-commerce system might store product inventory and invoice data in two
different partitions.
To learn more about the main types of partitioning, see Horizontal, vertical, and
functional data partitioning.

Strategies for data partitioning
Partitioning adds complexity to the design and development of your system. Consider
partitioning as a fundamental part of system design even if the system initially contains

only a single partition. If you address partitioning as an afterthought, it's more
challenging because you already have a live system to maintain:
Data access logic needs to be modified.
Large quantities of existing data might need to be migrated to distribute it across
partitions.
Users expect to be able to continue using the system during the migration.
Different strategies are used to partition data in various Azure data stores to help
improve performance. To learn more, see the following data storage links details.
Partitioning Azure SQL Databases
Partitioning Azure table storage
Partitioning Azure blob storage
Partitioning Azure storage queues
Partitioning Azure Service Bus
Partitioning Azure Cosmos DB
Partitioning Azure Search
Partitioning Azure Cache for Redis
Partitioning Azure Service Fabric
Partitioning Azure Event Hubs
To learn about these strategies, see Application design considerations.

Next steps
Sustain

Cache data for performance
optimization
Article • 05/19/2023

Caching is a strategy where you store a copy of the data in front of the main data store.
Advantages of caching include faster response times and the ability to serve data
quickly, which can improve user experience. The cache store is typically located closer to
the consuming client than the main store.
Caching is most effective when a client instance repeatedly reads the same data,
especially if all the following conditions apply to the original data store:
It remains relatively static.
It's slow compared to the speed of the cache.
It's subject to a high level of contention.
It's far away when network latency can cause access to be slow.
Caching can dramatically improve performance, scalability, and availability. The more
data that you have and the larger the number of users that need to access this data, the
greater the benefits of caching become. You get this benefit because caching reduces
the latency and contention that's associated with handling large volumes of concurrent
requests in the original data store.
Incorporating appropriate caching can also help reduce latency by eliminating repetitive
calls to microservices, APIs, and data repositories. The key to using a cache effectively
lies in determining the most appropriate data to cache and caching it at the appropriate
time. Data can be added to the cache on demand the first time it's retrieved by an
application. Using this method means that the application needs to fetch the data only
once from the data store, and it means that subsequent access can be satisfied by using
the cache. To learn more, see Determine how to cache data effectively.
For details, see Caching.

Azure Cache for Redis
Azure Cache for Redis improves the performance and scalability of an application. It
processes large volumes of application requests by keeping frequently accessed data in
the server memory that can be written to and read from quickly. Based on the Redis
software, Azure Cache for Redis brings critical low-latency and high-throughput data
storage to modern applications.

Azure Cache for Redis also improves application performance by supporting common
application architecture patterns. Some of the most common patterns include data
cache and content cache. For the most common patterns and their descriptions, see
Common application architecture patterns.

Azure Content Delivery Network (CDN)
A content delivery network (CDN) is a distributed network of servers that can efficiently
deliver web content to users. A CDN stores cached content on edge servers in point-ofpresence (POP) locations that are close to end users to minimize latency. To learn more
about CDN, see What is a content delivery network on Azure?

Next
Partition

Performance efficiency patterns
Article • 05/26/2023

Performance efficiency is the ability of your workload to meet the demands placed on it
by users in an efficient manner. You need to anticipate increases in load to meet
business requirements. To achieve performance efficiency, it's important to consider
how your application scales and to implement platform as a service (PaaS) offerings that
have built-in scaling operations.
Performance efficiency depends on the system's ability to handle load increases without
impacting performance, or for available resources to be readily increased. Performance
efficiency concerns not just compute instances, but other elements such as data storage,
messaging infrastructure, and application architecture.
The following table summarizes architectural design patterns that relate to performance
efficiency.
Pattern

Summary

Cache-aside

Load data on demand into a cache from a data store.

Choreography

Have each component of the system participate in the decision-making
process about the workflow of a business transaction, instead of relying on
a central point of control.

Command query
responsibility

Segregate operations that read data from operations that update data by
using separate interfaces.

segregation (CQRS)
Event sourcing

Use an append-only store to record the full series of events that describe
actions taken on data in a domain.

Deployment stamps

Deploy multiple independent copies of application components, including
data stores.

Geodes

Deploy backend services into a set of geographical nodes, each of which
can service any client request in any region.

Index table

Create indexes over the fields in data stores that are frequently referenced
by queries.

Materialized view

Generate prepopulated views over the data in one or more data stores
when the data isn't ideally formatted for a required query.

Priority queue

Prioritize requests sent to services so that requests with a higher priority
are received and processed more quickly than requests with a lower
priority.

Pattern

Summary

Queue-based load
leveling

Use a queue that acts as a buffer between a task and a service that it
invokes in order to smooth intermittent heavy loads.

Sharding

Divide a data store into a set of horizontal partitions or shards.

Static content

Deploy static content to a cloud-based storage service that can deliver the

hosting

assets directly to the client.

Throttling

Control the consumption of resources used by an instance of an
application, an individual tenant, or an entire service.

Performance efficiency checklist
Article • 05/19/2023

Performance efficiency, one of the pillars of the Microsoft Azure Well-Architected
Framework, is the ability of your workload to scale to meet the demands placed on it by
users in an efficient manner. Use this checklist to review your application architecture
from a performance efficiency standpoint.

Application design
Design for scaling. Scaling allows applications to react to variable load by
increasing and decreasing the number of roles, queues, and other services they
use. The application must be designed with scaling in mind.
The application and the services it uses must be stateless, so requests can be
routed to any instance and adding or removing specific instances doesn't
adversely affect users. Implement configuration, autodetection, or load balancing,
so as services are added or removed, the application can do the necessary routing.
For example, a web application might use a set of queues in a round-robin
approach to route requests to background services running in worker roles. The
web application must be able to detect changes in the number of queues to
successfully route requests and balance the load on the application.
You can scale outbound connectivity to the internet with Azure NAT Gateway.
Azure NAT Gateway provides a scalable, reliable, and secure way to connect
outbound traffic to the internet, and helps prevent connection failures caused by
SNAT exhaustion.
Partition the workload. Design parts of the process to be discrete and
decomposable. Minimize the size of each part, and follow the usual rules for
separation of concerns and the single responsibility principle. These practices allow
the component parts to be distributed to maximize use of each compute unit, such
as a role or database server.
Partitioning the workload also makes it easier to scale the application by adding
instances of specific resources. For complex domains, consider adopting a
microservices architecture.
Scale as a unit. Plan for more resources to accommodate growth. For each
resource, know the upper scaling limits, and use sharding or decomposition to go

beyond these limits. Scaling as a unit also makes operations less prone to negative
impact from resource limitations in other parts of the overall system.
To make scale-out operations easier, determine the scale units for the system in
well-defined sets of resources. For example, adding x number of web and worker
roles might require y added queues and z storage accounts to handle the added
workload. So a scale unit could consist of x web and worker roles, y queues, and z
storage accounts.
Design the application to easily scale by adding one or more scale units. Consider
using the deployment stamps pattern to deploy scale units.
Take advantage of platform autoscaling. If the hosting platform, such as Azure,
supports autoscaling, prefer it to custom or third-party mechanisms unless the
built-in mechanism can't fulfill your requirements. Schedule scaling rules where
possible to ensure resources are available without a startup delay. Add reactive
autoscaling to the rules where appropriate to cope with unexpected changes in
demand. For more information, see Autoscaling guidance.
７ Note
You can use autoscaling operations and add custom counters to rules in the
older Azure Classic deployment model. For more information, see Increase a
VM-family vCPU quota for the Classic deployment model.
Avoid client affinity. Where possible, ensure that the application doesn't require
affinity. Requests can then be routed to any instance, and the number of instances
is irrelevant. You also avoid the overhead of storing, retrieving, and maintaining
state information for each user.
Offload CPU-intensive and I/O-intensive tasks as background tasks. If a request
to a service is expected to take a long time to run or consume many resources,
offload the processing for this request to a separate task. Depending on hosting
platform, use worker roles or background jobs to run these tasks. This strategy
enables the service to continue receiving requests and remain responsive. For
more information, see Background jobs guidance.
Distribute the workload for background tasks. If there are many background
tasks, or the tasks require substantial time or resources, spread the work across
multiple compute units, such as worker roles or background jobs. For one possible
solution, see the Competing consumers pattern.

Move toward a shared-nothing architecture. A shared-nothing architecture uses
independent, self-sufficient nodes that have no single point of contention like
shared services or storage. In theory, such a system can scale almost indefinitely.
While a fully shared-nothing approach isn't practical for most applications, it might
provide opportunities to design for better performance. For example, avoiding the
use of server-side session state, client affinity, and data partitioning are good
examples of moving toward a shared-nothing architecture.

Data management
Use data partitioning. Divide the data across multiple databases and database
servers. You can design the application to use data storage services that provide
partitioning transparently, such as Azure SQL Database elastic pools and Azure
Table Storage. This approach can help maximize performance and allow easier
scaling. Benefits of partitioning include better query performance and availability,
simpler scalability, and easier management.
Different partitioning techniques include horizontal, vertical, and functional. You
can combine these techniques to achieve maximum benefits. Match different data
store types to different data types, and choose data store types that are optimized
for specific types of data. For example, you can use table storage, a document
database, or a column-family data store instead of, or along with, a relational
database. For more information, see Data partitioning guidance.
Design for eventual consistency. Eventual consistency improves scalability by
reducing or removing the time needed to synchronize related data partitioned
across multiple stores. The cost is that data isn't always consistent when it's read,
and some write operations might cause conflicts. Eventual consistency is ideal for
situations where the same data is read frequently but written infrequently. For
more information, see the Data consistency primer.
Reduce chatty interactions between components and services. Avoid designing
interactions in which an application is required to make multiple calls to a service,
each of which returns a small amount of data. Instead use a single call that can
return all the data. For example, use stored procedures in databases to encapsulate
complex logic, reduce the number of round trips, and help prevent resource
locking.
Where possible, combine several related operations into a single request when the
call is to a service or component that has noticeable latency. This practice makes it
easier to monitor performance and optimize complex operations.

Use queues to level the load for high velocity data writes. Surges in demand can
overwhelm a service and cause escalating failures. The queue-based load leveling
pattern can help prevent this situation. This pattern uses a queue as a buffer
between a task and a service that it invokes. The queue can smooth intermittent
heavy loads that might otherwise cause the service to fail or the task to time out.
Minimize the load on the data store. The data store is a costly resource, is often a
processing bottleneck, and isn't easy to scale out. It's easier to scale out the
application than the data store, so you should attempt to do as much as possible
of the compute-intensive processing within the application.
Where possible, remove logic like processing XML documents or JSON objects
from the data store, and do the processing within the application. For example,
avoid passing XML to the database, other than as an opaque string for storage.
Serialize or deserialize the XML within the application layer and pass it in a form
that's native to the data store.
Minimize the volume of data retrieved. Retrieve only the data you require by
specifying columns and using criteria to select rows. Use table value parameters
and the appropriate isolation level. Use mechanisms such as entity tags to avoid
retrieving data unnecessarily.
Aggressively use caching. Use caching wherever possible to reduce the load on
resources and services that generate or deliver data. Caching is best suited to data
that's relatively static, or that requires considerable processing to get. Where
appropriate, caching should occur in each layer of the application, including data
access and user interface generation. For more information, see Caching guidance.
Handle data growth and retention. The amount of data stored by an application
grows over time. This growth increases storage costs and data access latency,
affecting application throughput and performance. If possible, periodically archive
some of the old data that's no longer accessed. Or, move rarely accessed data into
long-term storage that's more cost effective, even if access latency is higher.
Optimize Data Transfer Objects (DTOs) by using an efficient binary format. DTOs
are passed between the layers of an application many times. Minimizing their size
reduces the load on resources and the network. Balance the savings with the
overhead of converting the data to the required format in each location where it's
used. To encourage easy reuse of a component, adopt a format that has the
maximum interoperability.
Set cache control. To minimize processing load, design and configure the
application to use output caching or fragment caching where possible.

Enable client side caching. Web applications should enable cache settings on
content that can be cached, which are often disabled by default. Configure the
server to deliver the appropriate cache control headers to enable caching of
content on proxy servers and clients.
Use Azure Blob Storage and the Azure Content Delivery Network to reduce the
load on the application. Consider storing static or relatively static public content,
such as images, resources, scripts, and style sheets, in blob storage. This approach
relieves the application from the load of dynamically generating this content for
each request.
Also consider using the Content Delivery Network to cache this content and deliver
it to clients. Using the Content Delivery Network can improve performance at the
client because the content is delivered from the geographically closest datacenter
that contains a Content Delivery Network cache. For more information, see Best
practices for using content delivery networks.
Optimize and tune SQL queries and indexes. Some T-SQL statements or
constructs might have an adverse effect on performance that can be reduced by
optimizing the code in a stored procedure. For example, avoid converting
datetime types to varchar before comparing with a datetime literal value. Use
date/time comparison functions instead.
Lack of appropriate indexes can also slow query execution. If you use an
object/relational mapping framework, understand how it works and how it might
affect performance of the data access layer. For more information, see Query
tuning.
Denormalize data. Data normalization helps to avoid duplication and
inconsistency. However, data normalization imposes an overhead that can affect
performance. Overhead includes maintaining multiple indexes, checking for
referential integrity, performing multiple accesses of small chunks of data, and
joining tables to reassemble the data. Consider whether some added storage
volume and duplication is acceptable to reduce the load on the data store.
Also consider if the application itself, which is typically easier to scale, can be relied
on to take over tasks such as managing referential integrity to reduce the load on
the data store. For more information, see Data partitioning guidance.

Implementation

Avoid performance antipatterns. Review the performance antipatterns for cloud
applications for common practices that often cause performance problems when
applications are under pressure.
Use asynchronous calls. If possible, use asynchronous code to access resources or
services that might be limited by I/O or network bandwidth or have a noticeable
latency. Asynchronous calls avoid locking the calling thread.
Avoid locking resources, and use an optimistic approach instead. Locking access
to resources such as storage or other services that have noticeable latency is a
primary cause of poor performance. Always use optimistic approaches to
managing concurrent operations like writing to storage. Use storage layer features
to manage conflicts. In distributed applications, data might be only eventually
consistent.
Compress highly compressible data over high latency, low bandwidth networks.
In most web applications, HTTP responses to client requests are the largest volume
of data generated by the application and passed over the network. HTTP
compression can reduce this volume, especially for static content.
Compressing dynamic content can reduce cost and load on the network, but
applies a slightly higher load on the server. In more generalized environments,
data compression can reduce the volume of data transmitted and minimize
transfer time and costs, but the compression and decompression processes incur
overhead.
７ Note
Compression should be used only when there's a demonstrable gain in
performance. Other serialization methods like JSON or binary encodings
might reduce payload size and have less impact on performance, whereas
XML is likely to increase it.
Minimize the time that connections and resources are in use. Maintain
connections and resources only for as long as you need to use them. Open
connections as late as possible, and allow them to be returned to the connection
pool as soon as possible. Acquire resources as late as possible, and dispose of
them as soon as possible.
Minimize the number of connections required. Service connections absorb
resources. Limit the number of required service connections, and ensure that
existing connections are reused whenever possible. For example, after

authentication, use impersonation where appropriate to run code as a specific
identity. Impersonation helps make the best use of the connection pool by reusing
connections.
７ Note
APIs for some services automatically reuse connections, if service-specific
guidelines are followed. Understand the conditions that enable connection
reuse for each service that your application uses.
Send requests in batches to optimize network use. For example, send and read
messages in batches when you access a queue, and do multiple reads or writes as
a batch when you access storage or a cache. Batching can help to maximize
efficiency of the services and data stores by reducing the number of calls across
the network.
Avoid requirements to store server-side session state. Server-side session state
management typically requires client affinity, or routing each request to the same
server instance, which affects the system's ability to scale. Where possible, design
clients to be stateless with respect to the servers that they use. However, if the
application must maintain session state, store sensitive data or large volumes of
per-client data in a distributed server-side cache that all instances of the
application can access.
Optimize table storage schemas. When using table stores that require the table
and column names to be passed and processed with every query, such as Table
Storage, consider using shorter names to reduce overhead. However, don't
sacrifice readability or manageability by using overly compact names.
Create resource dependencies during deployment or at application startup.
Avoid repeated calls to methods that test the existence of a resource and then
create the resource if it doesn't exist. Methods such as
CloudTable.CreateIfNotExists and CloudQueue.CreateIfNotExists in the Azure

Storage client library follow this pattern. These methods can impose considerable
overhead if they're invoked before each access to a storage table or storage
queue.
Instead, create the required resources when the application is deployed, or when it
first starts. A single call to CreateIfNotExists for each resource in the startup code
for a web or worker role is acceptable. However, be sure to handle exceptions that
might arise if your code attempts to access a resource that doesn't exist. In these

situations, you should log the exception, and possibly alert an operator that a
resource is missing.
Under some circumstances, it might be appropriate to create the missing resource
as part of the exception handling code. Use this approach with caution, because
the nonexistence of the resource might indicate a programming error, a misspelled
resource name, or some other infrastructure-level issue.
Use lightweight frameworks. Carefully choose the APIs and frameworks you use to
minimize resource usage, execution time, and overall load on the application. For
example, using a web API to handle service requests can reduce the application
footprint and increase execution speed. But a web API might not be suitable for
advanced scenarios where the added capabilities of Windows Communication
Foundation are required.
Minimize the number of service accounts. For example, use a specific account to
access resources or services that impose a limit on connections or perform better
when fewer connections are maintained. This approach is common for services
such as databases, but can affect accurate auditing due to the impersonation of
the original user.
Carry out performance profiling and load testing during development, as part of
test routines, and before final release to ensure the application performs and
scales as required. This testing should occur on the same type of hardware as the
production platform. Use the same types and quantities of data and user load as in
production. For more information, see Test the performance of a cloud service.

Next steps
Tradeoffs for performance efficiency

Tradeoffs for performance efficiency
Article • 05/26/2023

As you design a workload, consider tradeoffs between performance efficiency and other
aspects of the design, such as cost optimization, operational excellence, reliability, and
security.

Performance efficiency vs. cost optimization
Cost can increase as a result of increasing performance. When you optimize for
performance, consider the following factors to manage costs:
Avoid estimating workload costs at consistently high utilization. Smooth out the
peaks to get a consistent flow of compute and data. Consumption-based pricing is
more expensive than the equivalent provisioned pricing. Ideally, use manual
scaling and autoscaling to find the right balance. Scaling up is usually more
expensive than scaling out.
Costs increase directly with number of regions. Locating resources in cheaper
regions doesn't negate the cost of network ingress and egress, or degraded
application performance because of increased latency.
Every render cycle of a payload consumes both compute and memory. You can use
caching to reduce load on servers, and save with precanned storage and
bandwidth costs. The savings can be dramatic, especially for static content services.
While caching can reduce cost, there are some performance tradeoffs. For
example, Azure Traffic Manager pricing is based on the number of Domain Name
Service (DNS) queries that reach the service. You can reduce that number through
caching, and configure how often the cache is refreshed. But relying on a cache
that isn't frequently updated causes longer user failover times if an endpoint is
unavailable.
Using dedicated resources to batch process long running jobs increases the cost.
You can lower costs by provisioning Azure Spot Virtual Machines , but be
prepared for the job to be interrupted every time Azure evicts the virtual machine
(VM).
For other cost considerations, see the Cost optimization pillar.

Performance efficiency vs. operational
excellence
As you determine how to design your workload to meet the demands placed on it by
users in an efficient manner, consider operations. Operations processes keep an
application running in production. To achieve operational excellence with these
processes, use the following guidelines:
Ensure that deployments remain reliable and predictable.
Automate deployments to reduce the chance of human error.
Make the deployment process fast and routine, so it doesn't slow down the release
of new features or bug fixes.
Be able to quickly roll back or roll forward if an update has problems.
For other operational considerations, see the Operational excellence pillar.

Automate performance testing
Automated performance testing is an operational process that can help to identify
performance issues early. A serious performance issue can impact a deployment as
severely as a bug in the code. Automated functional tests can prevent application bugs,
but they might not detect performance problems.
Define acceptable performance goals for metrics such as latency, load times, and
resource usage. Include automated performance tests in your release pipeline to make
sure the application meets those goals.

Do fast builds
Another operational excellence process is making sure that your product is in a
deployable state through a fast build process. Builds provide crucial information about
the status of your application.
The following practices promote faster builds:
Select the right size of VMs.
Ensure that the build server is located near the source and target locations to
considerably reduce build duration.
Scale out build servers.
Optimize the build.

For more information, see Performance considerations for your deployment
infrastructure.

Monitor performance
As you consider performance improvements, monitoring should be done to verify that
your application is running correctly. Monitoring should include the application,
platform, and networking. For more information, see Monitor operations of cloud
applications.

Performance efficiency vs. reliability
Acknowledge up front that failures happen. Instead of trying to completely prevent
failures, the goal is to minimize the effects of a single failing component.
Reliable applications are resilient and highly available (HA). Resiliency allows systems to
recover gracefully from failures and continue to function with minimal downtime and
data loss before full recovery. HA systems run as designed in a healthy state with no
significant downtime. Maintaining reliability lets you maintain performance efficiency.
For reliability, consider the following guidelines:
Use the circuit breaker pattern to provide stability and minimize performance
impact while the system recovers from a failure.
Segregate read and write interfaces by using the command query responsibility
segregation (CQRS) pattern to achieve the scale and performance needed for your
solution.
Try to achieve higher availability by adopting an eventual consistency model. For
more information about selecting the correct data store, see Use the best data
store for your data.
If your application requires more storage accounts than are currently available in
your subscription, create a new subscription with more storage accounts. For more
information, see Scalability and performance targets.
Avoid scaling up and down. Instead, select a tier and instance size that meet your
performance requirements under typical load, and then scale out the instances to
handle changes in traffic volume. Scaling up or down might trigger an application
restart.

Don't use the same storage account for logs and application data. Create a
separate storage account for logs to help prevent logging from reducing
application performance.
Monitor performance. Use a performance monitoring service such as Application
Insights or New Relic

to monitor application performance and behavior under

load. Performance monitoring gives you real-time insight into the application, and
helps you diagnose issues and perform root-cause failure analysis.
For resiliency, availability, and reliability considerations, see the Reliability pillar.

Performance efficiency vs. security
If performance is so poor that the data is unusable, you can consider the data
inaccessible. You need to do whatever you can do from a security perspective to make
sure that your services have optimal uptime and performance.
A popular and effective method for enhancing availability and performance is load
balancing. Load balancing is a method of distributing network traffic across servers that
are part of a service. Load balancing improves performance because the processor,
network, and memory overhead for serving requests are distributed across all the loadbalanced servers. Employ load balancing whenever you can, as appropriate for your
services. For information about load balancing scenarios, see Optimize uptime and
performance.
Consider how the following security measures impact performance:
To optimize performance and maximize availability, application code should first
try to get OAuth access tokens silently from a cache before attempting to acquire a
token from the identity provider. OAuth is a technological standard that allows you
to securely share information between services without exposing your password.
Make sure to integrate critical security alerts and logs into security information and
event management (SIEM) systems without introducing a high volume of lowvalue data. Low-value data can increase SIEM costs and false positives and
decrease performance. For more information, see Security logs and alerts using
Azure services.
Use Azure Active Directory Connect (Azure AD Connect) to synchronize your onpremises directory with your cloud directory. Various factors affect the
performance of Azure AD Connect. Ensure Azure AD Connect has enough capacity
to keep underperforming systems from impeding security and productivity.

Large or complex organizations that provision more than 100,000 objects should
follow the recommendations to optimize their Azure AD Connect implementation.
For more information, see What is hybrid identity with Azure Active Directory?
To gain access to real-time performance information at the packet level, use packet
capture to set alerts.
For other security considerations, see the Security pillar.

Azure VMware Solution workload
documentation
Relocate legacy application virtual machines to Azure VMware Solution as a staging area
for the first phase of your migration and modernization strategy.

Get started

ｅ

OVERVIEW

What is an Azure VMware Solution workload?

ｐ

CONCEPT

Design principles
Integration with Azure landing zones

Design areas

ｐ

CONCEPT

Infrastructure
Applications
Networking
Monitoring
Security
Operations

Reference examples

Ｙ

ARCHITECTURE

Baseline Azure VMware Solution reference architecture

Azure VMware Solution landing zone accelerator

Reference implementations

｀

DEPLOY

Azure VMware Solution implementation options

Learn

ｄ

TRAINING

Introduction to Azure VMware Solution
Migrate VMware resources on-premises to Azure VMware Solution
Run VMware resources on Azure VMware Solution

Assessment

ｃ

HOW-TO GUIDE

Azure VMware Solution assessment tool

Azure VMware Solution workloads
Article • 08/15/2023

This guidance is intended for workload owners, technical stakeholders, and business
stakeholders. Specifically, this guidance is appropriate for people who play an integral
role in harnessing, developing, and upholding applications throughout the Azure
VMware Solution private cloud lifecycle. This documentation provides prescriptive
guidance and best practices for key design areas that have a technical foundation in
Azure Well-Architected Framework pillars. The recommendations center on Azure
VMware Solution, a managed service for migrating on-premises VMware vSphere
workloads to Azure.
You can use this workload documentation as your go-to resource for optimizing the
lifecycle of particular applications in Azure VMware Solution.

What is an Azure VMware Solution workload?
VMware is a leading provider of virtualization and cloud computing software and
services. VMware collaborates with Microsoft Azure to offer Azure VMware Solution.
This solution seamlessly integrates the VMware vSphere hypervisor with Azure
dedicated BareMetal infrastructure. When you use this integration, you can take
advantage of Azure native resources like virtual machines (VMs), storage disks, and
network components. Azure VMware Solution also provides a way for workloads to
migrate to Azure with minimal environmental modifications.

When you use this unique hybrid environment, you can retain certain VMware vSphere
workloads on-premises. But you also have the flexibility of extending to the cloud,
where you can benefit from the following advantages:
Infrastructure that's suited for cloud bursting scenarios
Azure native integrations
Disaster recovery locations
Azure VMware Solution offers a consistent VMware solution infrastructure within Azure.
When you employ this service, your team can use familiar VMware solution processes,
skills, and tools like VMware vSphere, VMware HCX, and VMware NSX-T Data Center.
You can also streamline the management of your cloud-based workloads.

The Well-Architected Framework approach
A well-architected workload is structured to meet specific performance, reliability,
security, and cost optimization objectives. By following architectural principles and
guidelines that are specific to Azure VMware Solution, you can enhance end-user
experiences and deliver consistency and reliability. This guidance is tailored to address
one Azure VMware Solution workload at a time. This approach optimizes specific
applications in Azure VMware Solution, such as three-tier apps or virtual desktops.

The Well-Architected Framework pillars also aim to involve modularity, separation of
roles, and a way to improve operational productivity. This approach results in
application workloads that avoid unnecessary complexities and unforeseen costs.
Consider making your Azure VMware Solution application well architected for the
following reasons:
Reliability and availability. Your application is more resilient to failure when you
take advantage of fault tolerance mechanisms and redundancy measures.
Cost optimization. Efficiently using Azure resources such as VMs helps you to
effectively manage expenses without compromising performance.
Streamlined operations and simplified management. You perform operations
such as updates, troubleshooting, and monitoring more efficiently when you use
streamlined processes.
Future-proofing and integration with Azure services. When your application is
well architected, it's well equipped to adapt to future changes and take advantage
of new offerings in Azure and Azure VMware Solution.
If you don't apply these principles to your Azure VMware Solution application, you
might encounter a range of operational, performance, and security challenges. These
challenges can significantly impede your application's success and limit your
organization's ability to take full advantage of Azure VMware Solution benefits.
Azure VMware Solution operates as a hybrid solution. As a result, an inadequately
designed application can lead to intricate management challenges and complications in
integrating with other Azure services. Poorly architected applications can suffer from
many disadvantages:
Performance degradation
Inadequate scalability
Heightened security vulnerabilities
Escalated costs from inefficient resource utilization
These shortcomings can prevent your business from fully harnessing the opportunities
that Azure VMware Solution and Azure offer. They can also hinder support for critical
initiatives.

Choose a migration approach
Common approaches for migrating or modernizing to the cloud are rehosting,
refactoring, rearchitecting, and rebuilding. Each approach requires careful consideration.

The modernization approach

, or updating current apps and data to a cloud-first

model, can meet business needs at reduced costs. Consider modernizing based on the
purpose of the application, life expectancy, supportability, cost, and service-level
agreements (SLAs).
You can run multiple workloads in Azure VMware Solution. But it's important to assess
whether there are Azure native offerings that are a better fit or more cost-effective than
instantiating another Azure VMware Solution node.
For help with determining potential placement for migrated servers, see the application
assessment results.

What are the key design areas?
Azure VMware Solution uses VMware and Microsoft propriety services. The following
design areas focus on the technical decision points for infrastructure components that
are part of a workload and their interactivity with the shared services.
Design area

Summary

Application

Optimize the performance, security, and manageability of application platforms

platform

that run on Azure VMware Solution. Efficiently manage VM sizing, application
deployment, orchestration, connectivity, and access controls.

Infrastructure

Manage the underlying infrastructure of a workload. Examples include rightsizing VMs, designing for fault tolerance, defining traffic flow, and applying
security best practices such as encryption and access controls.

Monitoring

Gain insights into application behavior by monitoring resource utilization,
health, availability, and capacity expansion. Use visualization tools to track
trends and anomalies.

Networking

Enable seamless connectivity and communication between resources. Efficiently
allocate IP addresses to avoid conflicts. Isolate workloads for enhanced security
and performance. Design for redundancy and high availability.

Operations

Ensure that day-to-day management and maintenance of your Azure VMware
Solution environment remains secure, efficient, and reliable over time.

Security

Implement security measures that help protect your workload from threats.

Example workloads
Many organizations run business applications, enterprise software, and legacy
applications in on-premises VMware vSphere environments. When you use Azure

VMware Solution to bring these applications to the cloud, you benefit from Azure global
infrastructure and service offerings. Examples of cloud applications include traditional
three-tier web-based applications and virtual desktops.
For more information, see the following reference architectures:
About Azure VMware Solution
Deploy Horizon on Azure VMware Solution

Azure landing zones
In an enterprise setup, your workload shares platform resources that central teams
manage. The design areas can provide points of integration with those shared resources.
For a description of that integration, see Integrate an Azure VMware Solution workload
with Azure landing zones.
Examples that demonstrate architectural best practices back up this guidance. Use the
following implementations as a starting point for your workload:
Baseline architecture for Azure VMware Solution. This article focuses on the
networking design area. It includes various scenarios, such as handling internet
ingress and egress traffic and handling traffic to an on-premises datacenter.
Azure VMware Solution landing zone accelerator.
An open-source collection of Azure Resource Manager and Bicep templates is available.
It represents the strategic design path and target technical state for an Azure VMware
Solution deployment.

Assessment
Use the assessment tool to evaluate your design choices.
Assessment

Next steps
Start by reviewing design principles.
Design principles

Azure VMware Solution design
principles
Article • 08/15/2023

Guidance about well-architected Azure VMware Solution workloads is built on the Azure
Well-Architected Framework and its five pillars of architectural excellence. The following
table lists each pillar and a summary of its goals.
WellArchitected
Framework
pillar

Summary

Reliability

An Azure VMware Solution workload requires platform resiliency and high
availability to protect critical business data. You can use the Well-Architected
Framework to assess hybrid workloads that run in Azure VMware Solution and
have an application footprint that extends to Azure native services.

Security

This pillar is concerned with implementing measures that help protect your
workload from threats. Examples include adding multiple security layers to your
applications, including identity and access management (IAM), input validation,
data sovereignty, and encryption for distributed denial-of-service (DDoS)
mitigation. Other measures include blocking bad actors, preventing data
exfiltration, and providing protection from operating system vulnerabilities.

Cost
optimization

A well-architected application deployment meets performance expectations
while reducing the total cost of ownership.

Performance
efficiency

When your application demonstrates performance efficiency, it can optimize
disk operations and has sufficient resources to scale with demand. It also
minimizes network latency to make communication between the components
of a distributed workload efficient. This pillar also includes load balancing,
geographic placement, and caching mechanisms.

Operational

This pillar focuses on smooth operations and maximizing the value you derive

excellence

from VMware deployments. These goals are achievable by combining
automation, monitoring, security measures, disaster recovery planning,
performance optimization, and effective documentation.

Reliability
Because failures can occur on-premises and in the cloud, it's important to focus on
resilience and availability when you design an Azure VMware Solution workload.
Resiliency refers to recovering from failures and maintaining functionality.

Availability ensures uninterrupted uptime. High availability minimizes application
downtime during critical maintenance activities. It also enhances recovery from
incidents like virtual machine (VM) crashes, backend updates, extended
downtimes, and ransomware attacks.
Achieving reliability requires a comprehensive approach that spans architecture,
operational procedures, automation, monitoring, regular testing, and validation.
Define service-level agreements (SLAs) to help ensure the reliability of your
workload.
Scale workloads vertically by selecting a VM SKU that's appropriate for your
workload's resources such as CPU and memory. Scale horizontally by adding VM
instances.
Design for high availability by implementing redundancy and failover mechanisms
to help ensure continuous operations. Distribute traffic by considering strategies
such as load balancing to distribute traffic across multiple back-end sources.
Implement fault-tolerant storage to help ensure data integrity and availability by
using technologies such as redundant array of independent disks (RAID)
configurations or distributed storage systems.
Understand backup and restore capabilities for data recovery in case of accidental
deletion, corruption, or other data loss scenarios. Documenting application
dependencies and backup and restore procedures also helps to streamline
recovery processes.

Security
In a shared-responsibility model:
Organizations are primarily responsible for managing and operating workloads.
Microsoft manages the physical and virtual infrastructure of Azure VMware
Solution.
We strongly recommend that you regularly assess the services and technologies to
ensure that your security posture adapts to the evolving threat landscape. Establishing a
clear understanding of the shared responsibility model when you collaborate with
vendors to implement suitable security measures is also essential.
You can employ several methods to secure your Azure VMware Solution environment:
Implement network isolation by using segments, virtual LANs, and network
security groups for Azure native services.
Manage patches effectively.

Conduct environmental audits regularly.
Monitor security with a security information and event management (SIEM)
solution like Microsoft Sentinel.
Implement encryption for data at rest and in transit.
Use robust IAM practices:
Enforce multi-factor authentication.
Integrate your workload with Azure Active Directory (Azure AD).
Implement the principle of least privilege, and use role-based access control to
assign roles.

Cost optimization
Microsoft and VMware make significant investments in:
The fast evolution of hardware.
The VMware hypervisor.
Azure native services.
These investments help provide more value for less. For example, they help suppress
egress charges out of the software-defined datacenter (SDDC). They also incorporate
enterprise licensing of several services into base costs.
The capabilities of Azure hardware expand frequently. As a result, opportunities
regularly come up for Azure VMware Solution workloads to optimize costs, eliminate
waste, and improve technologies such as purchasing reservations.
Consider creating a plan for each of your Azure VMware Solution applications and its
integration services. Each plan should contain the objectives and motivations for the
workload. Organizational objectives and investment priorities should drive cost
optimization initiatives for your application, application platform, and data platform.

Performance efficiency
This pillar focuses on optimizing workloads that run on Azure-dedicated hardware. That
focus includes many aspects:
Allocating resources, which includes the ability to adjust CPU, storage, memory,
and network resources to meet application demands.
Continuously monitoring performance and detecting anomalies by using tools
such as VMware vRealize Operations Manager.
Configuring specific settings at the application layer to improve overall
performance, such as:

Query optimization at the data tier.
In-memory caching of your application.
The configuration of HTTP headers and the web tier.
Considering each of these aspects helps you build a well-architected application that
delivers a consistent, cohesive user experience.

Operational excellence
Operational excellence involves fully using the capabilities of Azure VMware Solution.
Another part of operational excellence is adhering to well-architected best practices to
secure, optimize, and scale workloads. This practice involves:
Having processes in place for installing up-to-date patches and upgrades.
Maintaining governance and compliance.
Analyzing the performance and health of your environment.
Following comprehensive processes.
Maintaining documentation that captures:
Troubleshooting procedures.
Disaster recovery plans.
Remediation guidance on how to accelerate the process of resolving problems.
These steps help teams to collaborate in a way that's efficient and transparent.

Next steps
The design principles that are incorporated into Azure VMware Solution guidance cover
specific design domains. Each design domain offers focused guidance that helps you
quickly access the information that you need for enhanced productivity in a minimal
amount of time. Consider the headings as navigational tools that guide you in the right
direction for networking, core infrastructure, the application platform, monitoring,
security, and operational procedures.
Start by reviewing the design considerations for Azure infrastructure that are needed to
support a workload.
Infrastructure

Infrastructure and provisioning
considerations for Azure VMware
Solution workloads
Article • 08/15/2023

This article discusses the infrastructure design area of an Azure VMware Solution
offering, which refers to the foundational layer. This layer supports the compute,
storage, and networking capabilities that you need to run VMware vSphere workloads
efficiently and reliably. This article also explains how to use the VMware softwaredefined datacenter (SDDC) stack for resilience, security, scalability, automation, and
disaster recovery. The SDDC stack includes:
VMware ESXi.
VMware vCenter Server.
VMware NSX-T Data Center.
VMware vSAN.
VMware HCX (optional).
VMware SRM (optional).

Calculate business targets
Impact: Reliability, Performance efficiency, Operational excellence
Availability targets such as service-level agreements (SLAs) for Azure VMware Solution
applications should be defined for your platform and in place.
Use recovery targets like a recovery time objective (RTO) to identify how long Azure
VMware Solution can be unavailable. Use a recovery point objective (RPO) to identify
how much data loss is acceptable during a disaster.
Consider the following points when you design your solution:
Critical path dependencies. Not all components of a solution are equally critical.
Clearly differentiate between the dependencies that can take down the system and
the dependencies that might lead to a degraded experience. The design should
harden the resiliency of critical components to minimize the impact of outages.
Scaling out and scaling in on demand. The environment should be able to expand
and contract based on load. Handle these operations through automation. User
input should be kept to a minimum to avoid typical errors caused by humans.

Recommendations
Establish an SLA that's appropriate for your platform.
Define RTO and RPO targets.
Design critical components to be resilient.
Use automation for scaling in and out.

Consider workload resources and scale
requirements
Impact: Performance efficiency, Operational excellence
Before you deploy a workload, you need to have a general understanding of the
components that are required to support Azure VMware Solution. To achieve this
understanding, you need to carefully consider your workload's characteristics, resources,
and scale requirements. These factors make up the overall cluster design. Also, ensure
that Azure VMware Solution is the right choice for your infrastructure deployment. There
are scenarios where migrating workloads to an infrastructure as a service (IaaS) or
platform as a service (PaaS) solution in Azure is more cost-effective and performant than
migrating to Azure VMware Solution.

Recommendations
Assess Azure native solutions before moving to Azure VMware Solution.
Use the Azure VMware Solution deployment planning guide checklist and read the
Azure VMware Solution documentation.
Set up criteria to determine which workloads to move to Azure VMware Solution
and which to move to Azure native solutions. Consider costs, the ability to reassign
IP capacity, and usage patterns in these criteria.

Use thick and thin provisioning
Impact: Performance efficiency
When you provision infrastructure in the SDDC, the primary focus is on the hosts, which
are the underlying compute and storage for the virtual machines (VMs). In Azure
VMware Solution, you can choose either thin provisioning or thick provisioning for the
virtual disks of your VMs. The choice that you make depends on your specific storage
requirements, performance considerations, and anticipated VM growth. Thin
provisioning optimizes storage usage by allocating storage on demand. This approach

minimizes unused space. In contrast, from the start, thick provisioning uses the full
amount of storage space that you allocate and reserve for each VM.

Recommendations
If storage efficiency is a priority and you want to minimize unused storage
consumption, use thin provisioning.
If your application requires consistent and high-performance storage access,
consider thick provisioning.

Plan capacity and resource usage
Impact: Performance efficiency
Before you deploy an application in Azure VMware Solution, it's crucial to ensure proper
sizing and capacity planning. Specifically, consider scalability requirements, growth
projections, and performance considerations.

Recommendations
Before you migrate to Azure, use Azure Migrate to get insights into resource usage
and recommendations for sizing.
Analyze resource utilization patterns over a specific time frame to help establish
baseline usage, identify peak periods, and predict resource spikes.
Create a dependency map that outlines the components on your critical path.
Actively maintain the map, and regularly check it for changes in the solution.

Select regions
Impact: Performance efficiency, Security
To ensure that users are near your solution, carefully consider which regions to select.
Having users physically close to your peering location minimizes latency and helps you
meet requirements. For example, if you use VMware HCX, there's a requirement that the
roundtrip latency must be less than 150 ms.
It's also important to consider regulatory requirements during region selection.
Regulatory requirements and data residency restrictions can vary among regions. When
you use Azure geo-replicated storage, you can take advantage of paired regions. Under
this concept, each Azure region is paired with another region within the same

geography. Data that's replicated resides within the same geography as its pair for tax
and law enforcement jurisdiction purposes.

Recommendations
Select a region that's close to your users or other Azure services to help minimize
latency.
Understand the cloud shared responsibility

model for industry or region-based

regulatory compliance.
Ensure that your data remains in the correct geopolitical zone when you use Azure
data services.

Use a scalable design
Impact: Reliability, Performance efficiency
When you size for an application, size the VM to handle the workload at peak
performance. During an outage, the application should also be able to operate with
reduced functionality or degraded performance. To prepare for a failure event, design
for resilience. Design your system to respond to outages and deliver reliability even
when regional, zonal, service, or component failures impact critical application
functionality. Vertical scaling is the ability of a VM to add resources to individual hosts.
Vertical scaling requires picking the right SKU, powering the host down, and adding
resources from a VMware ESXi host that has those resources available.
The downtime that's associated with vertical scaling can disrupt your business, so
consider horizontal scaling in your workload design. Horizontal scaling is the ability to
dynamically span your workload across multiple VMs. Horizontal scaling typically
involves using VMware vSphere features like resource allocation settings, VM templates,
cloning, or dynamic resource allocation techniques. For example, if you want to
distribute traffic across three separate VMs, place those VMs on three separate hosts for
high availability.
Clustering is concerned with creating logical groupings of hosts to provide advanced
management and availability features. After you provision your hosts, you can create
and configure VMware vSphere clusters within the Azure VMware Solution environment
to manage VMs and provide compute capabilities.

Recommendations

Use the custom GitHub autoscale add-on for Azure VMware Solution

to define

performance metrics to use for scale-in or scale-out operations in Azure VMware
Solution cluster nodes.
Use placement policy affinity rules to help ensure resource availability. Configuring
affinity rules gives administrators control over VM placement. The VMs can then be
distributed according to specific requirements, performance considerations,
availability needs, or licensing constraints.

Implement high availability
Impact: Reliability, Performance efficiency
When you deploy VMs with high availability or clustering within Azure VMware Solution,
we recommend that you create anti-affinity rules to keep the VMs apart and on separate
hosts. A stretched cluster primarily relates to computing resource distribution across
fault domains or availability zones. Another aspect to consider is the latency of
connecting to workloads. Some workloads might not be sensitive to latency.

Recommendations
Use stretched clusters for high availability.
Colocate the application and service tiers by ensuring that the application,
database, and storage tiers are in the same availability zone.
Choose the Azure region for deploying an Azure VMware Solution cluster carefully.
Consider the proximity to your users or other resources, network connectivity
options, and latency requirements.

Use affinity rules
Impact: Reliability
If one host experiences an issue or failure, anti-affinity rules enforce distribution across
multiple hosts. This distribution helps to limit the impact of the failure and maintain the
availability of applications and services.

Recommendations
When a low-latency communication path is required between VMs, use placement
policy affinity rules to keep the VMs on the same host.

Use placement policy VM-VM affinity when VMs that support your application
need fault tolerance, or when you want to optimize host performance through
resource distribution.
For VMs that are deployed with high availability or clustering within Azure VMware
Solution, create placement policy VM-VM anti-affinity rules to keep those VMs
apart and on separate hosts.

Deploy VMware vSAN
Impact: Reliability, Performance efficiency
To design a well-architected Azure VMware Solution with storage, you need to plan for
adequate data protection and redundancy.
An Azure VMware Solution virtual storage area network (vSAN) uses local storage
resources from the VMware ESXi hosts within an Azure VMware Solution cluster to
create a distributed, shared-storage infrastructure. The vSAN provisioning must
adequately meet current and future storage needs. You can use storage area network
(SAN) storage policies to define the characteristics and behaviors of the storage that
your VMs use. You can use policies to configure data protection, performance, and
space efficiency settings according to the specific requirements of your workloads. The
default storage policy in Azure VMware Solution is redundant. As a result, if your
machines require copying data to extra vSAN nodes, create another policy to ensure
that the data meets your enhanced redundancy requirements.

Recommendations
Determine your failures to tolerate (FTT) level, or your failure tolerance, based on
your desired resiliency.
Choose a redundant array of independent disks (RAID) configuration that helps
ensure data availability and protection against host and disk failures.

Use Azure NetApp Files
Impact: Reliability, Performance efficiency
When you plan to exceed the storage in your private cloud, Azure NetApp Files in Azure
VMware Solution is another solution that expands disk allocation and provides a highperformance, low-latency, scalable storage platform. Azure NetApp Files dynamically
adjusts the storage capacity and performance tiers based on your workload needs. As a

result, the Azure VMware Solution environment can scale as your storage requirements
grow.
Ensure that Azure services such as Azure NetApp Files that interact with Azure VMware
Solution are in the same zone that Azure VMware Solution is deployed in. If all or part of
the application is highly sensitive to latency, it might mandate component colocation. In
that case, the applicability of multi-region and multi-zone strategies is limited. But
colocation reduces latency, so applications respond more quickly. An example is when
you use a datastore that's based on Azure NetApp Files where colocation is critical for
disk expansion.

Recommendations
Consider using Azure NetApp Files or a third-party solution as an extra datastore
for Azure VMware Solution.
Take an application assessment to help determine your optimal combination of
Azure VMware Solution nodes and external storage solutions like Azure NetApp
Files.

Recommendations
Have a procedure in place to help ensure no data is lost during a node
replacement process.

Establish your baseline performance
Impact: Operational excellence
Establishing a performance baseline provides insight into the capabilities of Azure
VMware Solution and helps you identify performance constraints.

Recommendations
Use tools to benchmark your existing environment before you migrate to the
Azure VMware Solution private cloud. Some common utilities that you can use to
establish baseline performance include VMware vRealize Operations, Perfmon, and
iostat

.

Use a performance-based assessment when you estimate Azure VMware Solution
private cloud capacity.

Use debugging and troubleshooting tools
Impact: Operational excellence
Having a systematic approach to identifying, troubleshooting, and fixing problems in the
SDDC leads to faster resolution times. Operations teams must be able to define the
problem or symptom that the workload is experiencing and the scope of the issue. They
also need to be able to collect information, including error messages, logs, and any
specific conditions or actions that trigger the issue.
For detailed coverage of infrastructure monitoring, see Monitoring considerations for
Azure VMware Solution workloads.

Recommendations
Familiarize yourself with the following debugging and troubleshooting tools. These
tools are indispensable for identifying potential performance bottlenecks quickly.
Azure Network Watcher
Kusto Query Language
PsPing

Next steps
Now that you've examined the underlying Azure VMware Solution platform, investigate
the application platform, which includes databases, VMs, operating systems, and
configurations.
Application platform
Use the assessment tool to evaluate your design choices.
Assessment

Application considerations for Azure
VMware Solution workloads
Article • 08/15/2023

This article discusses the application platform design area of an Azure VMware Solution
workload. This area covers the specific tasks and responsibilities that are associated with
deploying, configuring, and maintaining applications that you host in an Azure VMware
Solution environment. An application owner is responsible for the applications in an
Azure VMware Solution environment. This individual or team manages aspects that are
related to the deployment, configuration, monitoring, and maintenance of the
applications.
Key objectives of a well-architected application include:
Designing for scale. Gracefully handle higher user demands and concurrent
transactions without degradation or service interruption.
Performance. Deliver fast, low-latency response times, and efficiently manage
resource utilization.
Reliability and resiliency. Design redundant, fault-tolerant patterns to help ensure
that your application remains responsive and recovers quickly from failures.
This article aims to provide developers, architects, and application owners with best
practices that are specific to Azure VMware Solution. These practices can help you build
applications that remain robust, secure, scalable, and maintainable throughout their
lifecycle.

Design for scalability and efficient resource
distribution
Impact: Reliability, Performance efficiency, Security
This section covers the effective allocation and utilization of computing resources across
virtual machines (VMs) and workloads within the Azure VMware Solution private cloud.
These resources can include CPU, memory, storage, and network resources. This section
also explores the implementation of responsive scaling techniques. You can use these
techniques to dynamically adapt resource provisioning to accommodate fluctuations in
demand. The primary objective is to achieve optimal resource utilization by mitigating
underutilization and overprovisioning that can result in inefficiencies and escalated
expenses.

Use fault domains
Fault domains in Azure VMware Solution represent logical groupings of resources within
a stretched cluster. These resources share a common physical fault domain. Fault
domains help improve availability in various failure scenarios. Organizing resources into
fault domains helps ensure that critical components of an application are spread across
multiple failure domains.
By placing VMs and other resources into separate fault domains, the application team
helps ensure that an application remains available during a datacenter or infrastructure
failure. For example, you might separate VMs into fault domains that are spread across
geographically distributed datacenters. Then your application can remain operational if
one datacenter experiences a complete failure.
Application teams should consider defining VM-VM affinity and anti-affinity rules that
are based on fault domains. VM-VM affinity rules place critical VMs in the same fault
domain to ensure that they aren't spread across multiple datacenters. Anti-affinity rules
prevent related VMs from being placed together within the same fault domain. This
practice helps ensure redundancy.

Use VM-VM anti-affinity policies in three-tier applications
In Azure VMware Solution, a use case for VM-VM anti-affinity policies involves a threetier application. The goal is to enhance each application tier's high availability, fault
tolerance, and resiliency. To achieve this goal, you can use anti-affinity policies to spread
the tiers across different hosts within the Azure VMware Solution private cloud.
To implement this use case, create a distributed, fault-tolerant architecture by using VMVM anti-affinity policies across three tiers. This setup improves the availability of the
entire application and helps ensure that the failure of a single host doesn't disrupt the
entire tier or the entire application.
For example, in the front-end web tier that serves user requests, you can apply VM-VM
anti-affinity policies to spread the web servers across different physical hosts. This
approach helps improve high availability and fault tolerance. Similarly, you can use antiaffinity measures to help safeguard the application servers in the business layer and to
bolster data resiliency within the database layer.

Recommendations
Create maps that show VM interdependencies, communication, and usage patterns
to ensure that proximity is a requirement.
Determine whether placement policy VM-VM affinity helps meet performance
metrics or service-level agreements (SLAs).
Design for high availability by implementing placement VM-VM anti-affinity
policies to protect against host failures and distribute your application across
multiple hosts.
To avoid overprovisioning, distribute your workload across small VMs rather than a
few large VMs.
Regularly monitor, review, and fine-tune affinity policies to identify potential
resource contention. Adapt these policies over time as needed.

Use VM-host affinity policies for performance isolation
Some workloads that run VMs in different application tiers perform better when they're
colocated. This use case often occurs when applications require:
Performance isolation for resource-intensive, high-performance compute
workloads.
Compliance with licensing agreements. For instance, system specifications might
require a mapping that associates a VM with a specific set of cores to maintain
compliance with Windows or SQL Server licensing.
Regulatory compliance and data integrity to ensure that VMs that belong to
specific security domains or data classifications are confined to specific hosts or a

subset of hosts within the cluster.
A simplified network configuration that places VMs on the same host. In this case,
the network configuration between tiers is simplified. The tiers share the same
network connectivity and don't require extra network hops.
If it's essential to maintain the colocation of application tiers, you can opt for VM-host
affinity policies to ensure that the tiers are deployed on the same host and within the
same availability zone.

７ Note
The platform team is responsible for setting up VM placement, host affinity rules,
and resource pooling. But the application team should understand each
application's performance requirements to make sure that application needs are
met.
Application teams must comprehensively evaluate VM placement and engage in
meticulous planning. VM placement can present potential challenges like resource
imbalances and uneven workload distribution. These situations can lead to adverse
effects on performance and resource optimization. Also, placing all workloads in one
availability zone can create a single point of failure in a disaster. Consider replicating
your configuration across multiple availability zones to enhance datacenter resilience
during a failure.

Recommendations

Carefully plan how you use placement policy VM-host affinity policies. Consider
alternative solutions when possible, such as load balancing, resource pools in
VMware vSphere, distributed databases, containerization, and availability zones.
Regularly monitor resource utilization and performance to identify any imbalances
or issues.
Opt for a VM placement strategy that's balanced and flexible. This approach helps
you maximize resource utilization while maintaining high availability and ensuring
compliance with licensing requirements.
Test and validate your placement policy VM-host affinity configurations to ensure
that they align with your application's specific requirements and that they don't
negatively impact overall performance and resilience.

Distribute traffic by using an application or network load
balancer
Besides the use of placement policies, load balancing is also a critical component of
modern applications for helping to ensure:
Efficient resource distribution.
Increased application availability.
Optimal application performance.
Load balancing meets these criteria while maintaining flexibility for workload scaling and
management.
After you deploy applications on VMs, consider using a load balancing tool such as
Azure Application Gateway to create back-end pools. Application Gateway is a managed
web traffic load balancer and application delivery service that can manage and optimize
incoming HTTP and HTTPS traffic to web applications. As an entry point for web traffic,
Application Gateway offers various types of functionality. Examples include TLS/SSL
termination, URL-based routing, session affinity, and web application firewall
capabilities.

After the resources in your back-end pools are available, create listeners to specify ports
and routing rules for incoming requests. Then you can create health probes to monitor
the health of your VMs and to indicate when to remove unhealthy back-end resources
from rotation.

Implement TLS/SSL termination and certificate management
TLS/SSL encryption needs to be enforced for all communication between your
application and users' browsers. This encryption helps protect session data from
eavesdropping and man-in-the-middle attacks. If your application requires TLS/SSL
termination, configure the necessary TLS/SSL certificate in Application Gateway to
offload TLS/SSL processing from your back-end VMs.
After you generate TLS/SSL certificates, place them in a service like Azure Key Vault that
helps you store and access them securely. Use PowerShell, the Azure CLI, or tools such
as Azure Automation to update and renew certificates.

Manage APIs
Azure API Management helps you securely publish internally and externally deployed
API endpoints. An example of an endpoint is a back-end API that's in an Azure VMware
Solution private cloud behind a load balancer or Application Gateway. API Management
helps you manage the methods and behaviors of your API, such as by applying security
policies to enforce authentication and authorization. API Management can also route
API requests to your back-end services through Application Gateway.
In the following diagram, traffic from consumers travels to an API Management public
endpoint. The traffic is then forwarded to back-end APIs that run on Azure VMware
Solution.

Recommendations
To enhance the security and performance of your Azure VMware Solution
applications, use Application Gateway with Azure VMware Solution back ends to
distribute traffic to your application endpoints.
Ensure that there's connectivity between the back-end segments that host Azure
VMware Solution and the subnet that contains Application Gateway or the load
balancer.
Configure health proves to monitor the health of your back-end instances.
Offload TLS/SSL termination to Application Gateway to reduce processing
overhead on the back-end VMs.
Securely store TLS/SSL keys in vaults.
Streamline processes by automating tasks such as certificate updates and renewals.

Optimize stretched clusters to strengthen
business continuity and disaster recovery
readiness
Impact: Reliability

Stretched clusters provide VMware clusters with high availability and disaster recovery
capabilities across multiple geographically distributed datacenters.
The following setup supports active-active architectures. The virtual storage area
network (vSAN) spans two datacenters. A third availability zone maps to a vSAN witness
to serve as a quorum for split-brain scenarios.

Distributing the application across multiple availability zones and regions helps to
ensure continuous availability during datacenter failures. Deploy the application tiers
and data tiers across both datacenters, and turn on synchronous replication.

Configure fault tolerance and failure to tolerate (FTT)
policies
Your application's total usable capacity depends on several variables. Examples include
your redundant array of independent disks (RAID) configuration, the value of your
failures to tolerate attribute, and the failure to tolerate (FTT) policies that control the

number of failures your storage system can tolerate. Application teams need to
determine the level of redundancy that's needed for the application. It's also important
to note that higher FTT values improve data resiliency but increase storage overhead.

Recommendations
Deploy your application across shared storage so that the VM data remains
consistent across the stretched cluster. Turn on synchronous replication.

Configure fault domains to define how stretched clusters should respond in a
failure scenario.
Implement automatic failover and failback to minimize manual intervention during
failover and recovery events.

Configure data synchronization and storage policies
Data synchronization methods are important when your application relies on stateful
data and databases to ensure consistency and availability during a disaster. Data
synchronization helps provide high availability and fault tolerance for critical VMs that
run applications.
An application owner can define a storage policy to help ensure that:
Critical VMs that run an application receive the required level of data redundancy
and performance.
The VMs are positioned to take advantage of the high availability capabilities of
the stretched cluster in Azure VMware Solution.
Example policies can involve the following factors:
The vSAN configuration. Use a VMware vSAN with a stretched cluster across
availability zones.
The number of failures to tolerate. Set the policy to tolerate at least one or more
failures. For example, use a RAID-1 layout.
Performance. Configure performance-related settings to optimize IOPS and latency
for critical VMs.
Affinity rules. Set up affinity rules to help ensure VMs or groups of VMs are placed
in separate hosts or fault domains within the stretched cluster to maximize
availability during datacenter failures.
Backup and replication. Specify integration with backup and replication solutions
to ensure that VM data is regularly backed up and replicated to a secondary
location for extra data protection.

Recommendations
Define data storage policies in the vSAN to specify the redundancy and
performance that's required for various VM disks.
Configure applications to run in an active-active or active-passive configuration so
that critical application components can fail during datacenter failures.
Understand each application's network requirements. Applications that run across
availability zones can incur higher latency than applications with traffic within an

availability zone. Design your application to tolerate this latency.
Run performance tests on your placement policies to evaluate their impact on your
application.

Next steps
Now that you've examined the application platform, see how to establish connectivity,
create perimeters for your workload, and evenly distribute traffic to your application
workloads.
Networking
Use the assessment tool to evaluate your design choices.
Assessment

Networking considerations for Azure
VMware Solution workloads
Article • 08/15/2023

This article discusses the networking design area of an Azure VMware Solution
workload. Well-architected networks are critical for enabling connectivity, optimizing
response times, distributing traffic, and helping to ensure continuous availability of
workloads in Azure VMware Solution.

Distribute loads for high availability
Impact: Reliability, Performance efficiency
To achieve scalability and optimize performance, you should distribute traffic across
destinations within your Azure VMware Solution infrastructure. When you load balance
Azure VMware Solution traffic, you can distribute it through various algorithms, such as
algorithms that consider performance and weight. Distributing incoming traffic
improves the scale and reliability of your workload applications. If your application
spans multiple software-defined datacenters (SDDCs), an Azure load balancer can
distribute traffic across all your environments.

Recommendations
Use a load balancer such as VMware NSX Advanced Load Balancer for even
distribution of traffic. Support internal-facing and external-facing application
gateways. Use the load balancer for routing, application delivery, and TLS
termination.
For workloads that extend into Azure, use Azure Application Gateway. This load
balancer distributes traffic to enhance application performance. Application
Gateway also offers intrusion detection and prevention system (IDPS) and Azure
Web Application Firewall capabilities. Azure Load Balancer can distribute traffic
across zones to provide high availability and fault tolerance for workloads that
span multiple Azure availability zones.

Minimize distance in global distribution
Impact: Reliability, Performance efficiency, Cost optimization

For applications with a global presence, it's important to minimize the distance between
instances and users. You can achieve this objective by routing traffic to the nearest
Azure VMware Solution SDDC. If you use a traffic manager, you can also potentially
lower outbound data transfer costs. Specifically, you can send users to the nearest Azure
VMware Solution deployment or edge location. Reducing the distance that data travels
helps you avoid long data transfers.

Recommendations
For applications that span multiple regions, consider deploying a Domain Name
System–based global traffic load balancing solution such as Azure Traffic Manager.
Create traffic-routing profiles. Configure various routing methods such as prioritybased routing, weighted round-robin, performance-based routing, or geographicbased routing.

Deliver content
Impact: Performance, Cost optimization
High-traffic applications require optimal retrieval of content. Optimization techniques
such as compression and HTTP accelerators can improve the retrieval performance of
assets within your Azure VMware Solution environment. You can use compression
techniques on files before you transmit them. This practice can reduce costs by reducing
the amount of data that you transmit. A content delivery network caches frequently
accessed content. As a result, using a content delivery network with Azure VMware
Solution can help you optimize retrieval and distribution. Content delivery networks can
also help you save on costs in other ways. Because these networks cache date at edge
locations that are close to users, they reduce the distance that data travels.

Recommendations
Use Azure Content Delivery Network to improve responsiveness and reduce
latency for users who access your applications and websites.
Use compression to minimize the payload of static assets.
Use caching solutions like Redis or Azure Cache for Redis to cache frequently
accessed data.

Use a firewall for internet-facing workloads
Impact: Security

Front-facing workloads in Azure VMware Solution are mapped to a public IP address. As
a result, they can be exposed to the internet, and they accept incoming connections
from external sources. This association with your virtual machine (VM) or load balancer
poses risks to your workloads.

Recommendations
For internet-facing applications, use a firewall such as Azure Firewall or a certified
third-party NVA to inspect Azure VMware Solution traffic to the internet and Azure.
Make sure that your firewall has rules and access control lists to restrict and filter
inbound traffic.

Secure traffic between internal workloads
Impact: Security
The network is a critical perimeter in the Azure VMware Solution environment. Networks
help control access, protect data, and mitigate threats. Robust network security
measures help ensure the availability and resiliency of Azure VMware Solution
workloads. For example, implementing network isolation through segmentation and the
use of virtual LANs can help prevent unauthorized access between the components of
your Azure VMware Solution environment. You can use network security groups to
isolate and protect traffic within your workload virtual networks.

Recommendations
Create network segments for your Azure VMware Solution workloads.
Create firewall rules within VMware NSX-T Data Center.

Design IP addressing schemes for growth
Impact: Security, Operational excellence
You can use an intentional subnet security strategy to design Azure VMware Solution
and cloud virtual networks for growth. This design involves strategically organizing IP
address allocation. You also need to use an IP addressing tool and enforce allocation.
Besides a /22 RFC-1918 address range, workload segments have separate,
nonconflicting classless inter-domain routing (CIDR) ranges. Plan to have enough IP
addresses for VMs, public IP addresses, and load balancers.

Recommendations
Ensure your IP address range is large enough to accommodate all current and
future Azure VMware Solution workloads.
Use a spreadsheet or IP address management (IPAM) tool to efficiently organize
available IP addresses, track IP address usage, and help avoid IP address conflicts.
Plan for potential increases in devices, segments, or subnets. Use an IP addressing
scheme that can handle demand increases.
Use Dynamic Host Configuration Protocol (DHCP) for dynamic IP address
assignment.

Next steps
Now that you've examined networking in Azure VMware Solution, investigate best
practices for monitoring your infrastructure and application.
Monitoring
Use the assessment tool to evaluate your design choices.
Assessment

Monitoring considerations for Azure
VMware Solution workloads
Article • 08/15/2023

This article discusses the monitoring design area of an Azure VMware Solution
workload. This area focuses on observability best practices. The guidance is intended for
an operations team. Microsoft, VMware, and third parties provide various tools that you
can use to monitor your infrastructure and application. This article lists those options.
Each option offers monitoring solutions with varying degrees of licensing costs,
integration options, monitoring scope, and support. Carefully review the applicable
terms and conditions before using the tools.

Collect infrastructure data
Impact: Operational excellence
Monitoring your workload involves collecting data from Azure VMware Solution
infrastructure and various VMware solution components. Azure VMware Solution is
integrated with the VMware software-defined datacenter (SDDC), which runs several
VMware solution native components such as VMware Aria. You can use this suite of
tools, including VMware Aria Operations, to manage various aspects of your
infrastructure.
Another tool at your disposal is VMware vSphere Health Status for Azure VMware
Solution. This tool helps ensure that proactive issue detection and remediation are
continually performed in your Azure VMware Solution environment. In particular, this
tool finds misconfigurations in the VMware vSphere infrastructure and detects
performance bottlenecks. It also provides insight into resource utilization and overall
environmental health performance.
VMware Aria Operations for Networks helps you achieve comprehensive network
visibility, streamline troubleshooting processes, and optimize network performance.

Recommendations
Configure VMware vSphere Health Status

to get a high-level view of the Azure

VMware Solution private cloud health status.
Use third-party tools like VMware Aria Suite

for enhanced visibility and analytics

of Azure VMware Solution private cloud network infrastructure.

Use Azure native monitoring tools such as:
Azure Monitor.
VMware Aria Operations for operational monitoring.
Azure Policy and its associated dashboard for compliance monitoring.
Microsoft Defender for Cloud and Microsoft Sentinel for security monitoring.

Manage logs and archives
Impact: Operational excellence
To get health data from VMware solution components, you need access to logs that the
VMware syslog service collects. Examples of solution components include VMware ESXi,
VMware vSAN, VMware NSX-T Data Center, and VMware vCenter Server. Logs from
these components are available through Azure VMware Solution infrastructure. A Log
Analytics agent or extension sends guest logs at the virtual machine (VM) level to Log
Analytics. Within Azure VMware Solution, you can send the Azure VMware Solution logs
to an Azure native storage blob. To send logs to a storage blob, you can set up
forwarders from a centralized syslog server, or you can configure the blob as a
destination in Azure Monitor. It's also possible to use an Azure native tool such as Azure
Logic Apps or Azure Functions to forward logs. You can use these tools to create
listeners for incoming logs from Azure VMware Solution and to send the logs to a
storage blob.
Archiving logs is a strategy for keeping your storage costs down. Azure Storage blobs
and Log Analytics can transfer logs for long-term archival. Using a storage blob is the
less expensive option. But Log Analytics has advanced integrations for alerting,
visualization, querying, and gaining machine learning–based insights. Consider your
budget, functional use cases, and long-term use cases when you choose a solution.

Recommendations
Collect logs from the VMware syslog service to get health data from VMware
solution components such as VMware ESXi, VMware vSAN, VMware NSX-T Data
Center, and VMware vCenter Server.
Configure tools such as VMware Aria Operations for Logging to collect various
logs for querying, analyzing, and reporting capabilities.
Configure retention durations for sending logs to long-term storage to reduce
query time and save on storage costs.

Monitor the guest operating system

Impact: Operational excellence
Within the guest operating system, metrics are available for disk usage, application
performance, system resource utilization, and user activity. Consider using Azure Arc for
Azure VMware Solution (preview) to manage VMware infrastructure resources in Azure.
For more information, see Deploy Azure Arc for Azure VMware Solution.

Recommendations
Enable guest management and install Azure extensions after your private cloud is
enabled by Azure Arc for servers or Azure Arc for Azure VMware Solution
(preview).
Install extra agents to collect data to enable guest management and monitoring on
Azure VMware Solution guest VMs.

Implement security monitoring
Impact: Security, Operational excellence
Security monitoring is critical for detecting and responding to anomalous activities.
Workloads that run in an Azure VMware Solution private cloud need comprehensive
security monitoring that spans networks, Azure resources, and the Azure VMware
Solution private cloud itself. You can centralize security events by deploying a Microsoft
Sentinel workspace. By using this integration, the operation team can view, analyze, and
detect security incidents in the context of a broader organizational threat landscape.

Recommendations
Enable Defender for Cloud on the Azure subscription that you use to deploy the
Azure VMware Solution private cloud. Ensure that in the Defender for Cloud plan,
the Cloud Workload Protection (CWP) setting has a value of ON for servers.
Audit actions that privileged users take on the Azure VMware Solution private
cloud. For more information, see Audit activity history for group assignments in
privileged identity management.
Integrate Microsoft Sentinel with Defender for Cloud. Enable its data collector for
security events and connect it with Defender for Cloud.
Use security monitoring solutions from validated partners in Azure VMware
Solution.

Monitor and analyze networks

Impact: Security, Operational excellence
The process of network monitoring inspects all the traffic that comes into and goes out
of the Azure VMware Solution private cloud. In Azure VMware Solution, network security
operates at the network and host layers.

Recommendations
Capture and monitor network firewall logs that are deployed in the Azure VMware
Solution private cloud. Also monitor logs that are deployed in Azure when your
application extends to Azure native devices such as Azure Firewall or Azure
Application Gateway. For more information, see Firewall integration in Azure
VMware Solution

.

Use Azure Firewall Workbook or similar tools to monitor common metrics and logs
that are related to firewall devices.
Correlate logs from multiple security vectors such as identity, networking, and
infrastructure vectors.

Configure and streamline alerts
Impact: Operational excellence, Cost optimization
When you run workloads in the Azure VMware Solution private cloud, you need to
effectively monitor workload performance. For example, you should capture logs,
metrics, and trace requests for your application and infrastructure layers.
Alerts can help you respond to changes in your performance baseline. You can also use
alerts to provide information about necessary maintenance or configuration changes.
For instance, you can receive notifications when a key expires, a connection is lost, or
there's a risk of exceeding a resource's capacity.
To make alerts effective, configure them to notify accountable teams when certain
conditions are met. Also consider consolidating alerts to reduce the number of
individual notifications that are sent:
Instead of issuing an alert for every machine that's low on space, consider
consolidating alerts by hosts, resource groups, or clusters.
Use this approach also with host issues, CPU, and storage spikes.
Base alerts on time windows. For example, if a host issues alerts for a short time,
you can suppress the alerts according to a defined time threshold. For instance,
you can send an alert only after five minutes have passed.

Recommendations
Discuss and establish baselines that are based on performance data.
Define relevant alert criteria such as thresholds, severity levels, or specific
conditions.
Use the VMware vSphere events and alarms subsystem

to monitor VMware

vSphere and set up triggers.
Configure Azure alerts in Azure VMware Solution to respond to events in real time.
Ensure that alerts are configured so that VMware vSAN datastore slack space is
maintained at the levels that your service-level agreement (SLA) mandates.
Configure resource health alerts to get the real-time health status of the Azure
VMware Solution private cloud.
Use application performance monitoring (APM) tools to gain performance insights
at the application code level.
Use a combination of monitoring techniques such as synthetic transactions,
heartbeat monitoring, and endpoint monitoring.
Prioritize alerts based on their impact on operations or the criticality of the
affected systems. Fine-tune alerts to trigger only meaningful events.
To reduce noise and effectively manage alerts, use methods for reducing the
number of individual notifications that are issued.
To minimize alert fatigue, employ a mechanism to notify key stakeholders only
about significant events.
Use notification channels such as SMS, email, push notifications, and collaboration
platforms such as Microsoft Teams to ensure that alerts are delivered effectively.

Manage costs
Impact: Cost optimization, Operational excellence
Cost monitoring refers to the ability to keep track of costs that are associated with the
Azure VMware Solution private cloud.

Recommendations
Use the VMware vSphere events and alarms subsystem to monitor VMware
vSphere and set up triggers.
Configure Azure alerts that are based on Log Analytics queries for Azure VMware
Solution. These alerts help your operation team respond to expected and
unexpected events in real time.

Use troubleshooting and debugging tools
Impact: Cost optimization, Operational excellence
To efficiently debug and troubleshoot your application, you need logs, metrics, and
associated information. This information includes event activities so that you can
identify, analyze, and establish connections between events.

Recommendations
Configure your system to forward logs from the Azure VMware Solution syslog
service to Log Analytics. Forward all relevant logs, metrics, and diagnostic
information.
Configure a server agent that's enabled by Azure Arc on guest VMs that run inside
the Azure VMware Solution private cloud.

Use dashboards
Impact: Operational excellence
Application dashboards help you visualize and monitor an application's performance,
health, and other metrics:
Monitoring reports in dashboards help you do root-cause analysis and
troubleshooting quickly. Operations teams can use these dashboards to view all
key resources that make up Azure VMware Solution in a single pane.
Dashboard metrics provide insight into how code and infrastructure changes
impact application behavior.
Visuals help customer support teams understand the impact of changes,
performance, and availability issues on an application.
Performance indicators benefit executive leadership and business stakeholders.
These tools inform decisions that align an application's performance with business
objectives. For example, an executive can monitor commitment to customers by
reviewing metrics such as service availability, incident resolution times, and
average response times. These metrics help ensure that the organization delivers
services according to its SLA.
Besides offering insight, dashboards can also promote transparency and encourage
collaboration, for instance, when you grant appropriate stakeholders access to
application dashboards. This act cultivates a shared comprehension of an application's
performance. This practice also enables an organization to make informed decisions. As

a result, stakeholders can focus on pursuing crucial initiatives that propel the business
forward.

Recommendations
Build an application dashboard with Application Insights or Grafana. Connect the
dashboard to relevant data sources that store metrics from your Azure VMware
Solution environment.
Create an Azure workbook as a central repository for commonly run queries,
metrics, and interactive reports.
Ensure that your data source aligns with security and compliance requirements.
Define access controls and permissions such as user authentication and role-based
access control. Ensure that each stakeholder has appropriate access that's based
on their role.
Conduct periodic access reviews to check that user access is up to date and
aligned with current roles and responsibilities.

Next steps
Now that you've looked at observability best practices in Azure VMware Solution,
explore mechanisms, tools, and perimeters that you can use to further secure workloads
in the SDDC.
Security
Use the assessment tool to evaluate your design choices.
Assessment

Security considerations for Azure
VMware Solution workloads
Article • 08/15/2023

This article discusses the security design area of an Azure VMware Solution workload.
The discussion covers various measures for helping to secure and protect Azure VMware
Solution workloads. These measures help protect infrastructure, data, and applications.
This approach to security is a holistic one that aligns with an organization's core
priorities.
Securing Azure VMware Solution requires a shared responsibility model, where
Microsoft Azure and VMware are both responsible for certain aspects of security. To
implement appropriate security measures, ensure a clear understanding of the shared
responsibility model and collaboration between IT teams, VMware, and Microsoft.

Manage compliance and governance
Impact: Security, Operational excellence
It's important to detect noncompliant servers. You can use Azure Arc for this purpose.
Azure Arc extends Azure management capabilities and Azure services to on-premises or
multicloud environments. By providing centralized management and governance for
servers, Azure Arc gives you a single-pane view for applying updates and hot fixes. The
result is a consistent experience for managing components from Azure, on-premises
systems, and Azure VMware Solution.

Recommendations
Configure Azure VMware Solution guest virtual machines (VMs) as Azure Arc–
enabled servers. For methods that you can use to connect machines, see Azure
connected machine agent deployment options.
Deploy a certified third-party solution or Azure Arc for Azure VMware Solution
(preview).
Use Azure Policy for Azure Arc–enabled servers to audit and enforce security
controls on Azure VMware Solution guest VMs.

Protect the guest operating system
Impact: Security

If you don't patch and regularly update your operating system, you make it susceptible
to vulnerabilities and put your entire platform at risk. When you apply patches regularly,
you keep your system up to date. When you also use an endpoint protection solution,
you help to prevent common attack vectors from targeting your operating system. It's
also important to regularly perform vulnerability scans and assessments. These tools
help you identify and remediate security weaknesses and vulnerabilities.
Microsoft Defender for Cloud offers unique tools that provide advanced threat
protection across Azure VMware Solution and on-premises VMs, including:
File integrity monitoring.
Fileless attack detection.
Operating system patch assessment.
Security misconfiguration assessment.
Endpoint protection assessment.

Recommendations
Install an Azure security agent on Azure VMware Solution guest VMs through
Azure Arc for servers to monitor them for security configurations and
vulnerabilities.
Configure Azure Arc machines to automatically create an association with the
default data collection rule for Defender for Cloud.
On the subscription that you use to deploy and run the Azure VMware Solution
private cloud, use a Defender for Cloud plan that includes protection for servers.
If you have guest VMs with extended security benefits in the Azure VMware
Solution private cloud, deploy security updates regularly. Use the Volume
Activation Management Tool to deploy these updates.

Encrypt data
Impact: Security, Operational excellence
Data encryption is an important aspect of protecting your Azure VMware Solution
workload from unauthorized access and preserving the integrity of sensitive data.
Encryption includes data at rest on the systems and data in transit.

Recommendations
Encrypt VMware vSAN datastores with customer-managed keys to encrypt data at
rest.

Use native encryption tools such as BitLocker to encrypt guest VMs.
Use native database encryption options for databases that run on Azure VMware
Solution private cloud guest VMs. For instance, you can use transparent data
encryption (TDE) for SQL Server.
Monitor database activities for suspicious activity. You can use native database
monitoring tools like SQL Server Activity Monitor for this purpose.

Implement network security
Impact: Operational excellence
A goal of network security is to prevent unauthorized access to Azure VMware Solution
components. One method for achieving this goal is to implement boundaries via
network segmentation. This practice helps to isolate your applications. As part of
segmentation, a virtual LAN operates at your data-link layer. That virtual LAN provides
physical separation of your VMs by partitioning the physical network into logical ones to
separate traffic.
Segments are then created to provide advanced security capabilities and routing. For
example, the application, web, and database tier can have separate segments in a threetier architecture. The application can add a level of micro-segmentation by using
security rules to restrict network communication between the VMs in each segment.

The tier-1 routers are positioned in front of the segments. These routers provide routing
capabilities within the software-defined datacenter (SDDC). You can deploy multiple tier1 routers to segregate different sets of segments or to achieve a specific routing. For
example, say you'd like to restrict East-West traffic that flows to and from your
production, development, and testing workloads. You can use distributed level-1 tiers to
segment and filter that traffic based on specific rules and policies.

Recommendations
Use network segments to logically separate and monitor components.
Use micro-segmentation capabilities that are native to VMware NSX-T Data Center
to restrict network communication between application components.
Use a centralized routing appliance to secure and optimize routing between
segments.
Use staggered tier-1 routers when network segmentation is driven by
organizational security or networking policies, compliance requirements, business
units, departments, or environments.

Use an intrusion detection and prevention
system (IDPS)
Impact: Security
An IDPS can help you detect and prevent network-based attacks and malicious activity
in your Azure VMware Solution environment.

Recommendations
Use the VMware NSX-T Data Center distributed firewall for help with detecting
malicious patterns and malware in East-West traffic between your Azure VMware
Solution components.
Use an Azure service such as Azure Firewall or a certified third-party NVA that runs
in Azure or in Azure VMware Solution.

Use role-based access control (RBAC) and
multifactor authentication
Impact: Security, Operational excellence
Identity security helps control access to Azure VMware Solution private cloud workloads
and the applications that run on them. You can use RBAC to assign roles and
permissions that are appropriate for specific users and groups. These roles and
permissions are granted based on the principle of least privilege.
You can enforce multifactor authentication for user authentication to provide an extra
layer of security against unauthorized access. Various multifactor authentication
methods, such as mobile push notifications, offer a convenient user experience and also
help to ensure strong authentication. You can integrate Azure VMware Solution with
Azure Active Directory (Azure AD) to centralize user management and take advantage of
Azure AD advanced security features. Examples of features include privileged identity
management, multifactor authentication, and conditional access.

Recommendations
Use Azure AD privileged identity management to allow time-bound access to the
Azure portal and control pane operations. Use privileged identity management
audit history to track operations that highly privileged accounts perform.
Reduce the number of Azure AD accounts that can:
Access the Azure portal and APIs.
Navigate to the Azure VMware Solution private cloud.
Read VMware vCenter Server and VMware NSX-T Data Center admin accounts.
Rotate local cloudadmin account credentials for VMware vCenter Server and
VMware NSX-T Data Center to prevent the misuse and abuse of these
administrative accounts. Use these accounts only in break glass scenarios. Create
server groups and users for VMware vCenter Server, and assign them identities
from external identity sources. Use these groups and users for specific VMware
vCenter Server and VMware NSX-T Data Center operations.

Use a centralized identity source for configuring authentication and authorization
services for guest VMs and applications.

Monitor security and detect threats
Impact: Security, Operational excellence
Security monitoring and threat detection involve detecting and responding to changes
in the security posture of Azure VMware Solution private cloud workloads. It's important
to follow industry best practices and comply with regulatory requirements, including:
The Health Insurance Portability and Accountability Act (HIPAA).
Payment Card Industry Data Security Standards (PCI DSS).
You can use a security information and event management (SIEM) tool or Microsoft
Sentinel to aggregate, monitor, and analyze security logs and events. This information
helps you detect and respond to potential threats. Regularly conducting audit reviews
also helps you avert threats. When you regularly monitor your Azure VMware Solution
environment, you're in a better position to ensure it aligns with security standards and
policies.

Recommendations
Automate responses to recommendations from Defender for Cloud by using the
following Azure policies:
Workflow automation for security alerts
Workflow automation for security recommendations
Workflow automation for regulatory compliance changes
Deploy Microsoft Sentinel and set the destination to a Log Analytics workspace to
collect logs from Azure VMware Solution private cloud guest VMs.
Use a data connector to connect Microsoft Sentinel and Defender for Cloud.
Automate threat responses by using Microsoft Sentinel playbooks and Azure
Automation rules.

Establish a security baseline
Impact: Security
The Microsoft cloud security benchmark provides recommendations about how you can
secure your cloud solutions on Azure. This security baseline applies controls that are
defined by Microsoft cloud security benchmark version 1.0 to Azure Policy.

Recommendations
To help protect your workload, apply the recommendations that are given in Azure
security baseline for Azure VMware Solution.

Next steps
Now that you've looked at best practices for securing Azure VMware Solution,
investigate operational management procedures for achieving business excellence.
Operations
Use the assessment tool to evaluate your design choices.
Assessment

Operations considerations for Azure
VMware Solution workloads
Article • 08/15/2023

This article discusses the operations design area for Azure VMware Solution. The aim of
this article is to build an operating model for Azure VMware Solution and the
applications within the WMware software-defined datacenter (SDDC). Standard
operating procedures (SOPs) are documented processes for managing a workload. Each
Azure VMware Solution workload should have SOPs to govern operations. To stay
aligned with business objectives and to help prevent drift from best practices, use SOPs
in a continuous cycle of assessment and health checks that you run on your Azure
VMware Solution workload.

Track application dependencies
Impact: Operational excellence
IT teams continuously look to optimize the deployment, management, and maintenance
of applications, sites, and services. This practice helps to ensure high performance,
reliability, scalability, and security. Optimizing involves understanding how applications
flow inside the Azure VMware Solution platform. It also involves examining external
dependencies and relationships that are outside the private cloud. A dependency map is
a valuable tool for developers, application architects, and IT teams as they seek to
understand the structure and behavior of applications. Having insight into application
components such as software and infrastructure, services, and external dependencies
provides a visual way to understand data flows, functionality, and API calls.

Recommendations
Use Application Insights to track dependencies such as databases, API calls, and
external services.
Use the service map feature of Azure Monitor to automatically discover and
visualize different application and infrastructure components.
Use third-party tools like New Relic and Datadog to discover and map
dependencies.
Use custom scripts or third-party configuration management tools that track the
automation and deployment of dependencies.

Use automation, version control systems, and
blue-green deployments
Impact: Operational excellence
Organizations can decrease time to market and benefit from improved collaboration
and software quality by adopting DevOps practices. For example, automation can
expedite the deployment and maintenance of applications.
When you use infrastructure as code (IaC) to organize infrastructure deployments, you
can benefit from improved efficiency in your infrastructure provisioning. IaC can also
facilitate the adoption of DevOps principles in infrastructure management.
You can use IaC to create several resources in Azure VMware Solution. Examples
include the entire private cloud or individual components like clusters, network
appliances, and storage. Tools such as Azure Resource Manager, Bicep, Terraform,
the Azure CLI, and PowerShell automate the provisioning and configuration of
resources in Azure VMware Solution.
When you use IaC, you can update your infrastructure by modifying code. This
approach reduces the time and effort that are required for manual configuration
and provisioning.
The output that's returned from deploying IaC can serve as documentation to help
maintain and provide extra visibility into the state and configuration of provisioned
resources.
Version control systems provide a way for you to manage your code and use versioning
to track and roll back changes as needed.
When you need to update application code across servers, blue-green deployments can
be helpful in many ways:
They aid in managing the lifecycle of an application from development to
production.
They help give customers a consistent web experience when updates and patches
are being applied.
They use weighted algorithms to distribute traffic only to healthy servers during
maintenance.
Azure VMware Solution doesn't offer methods that a cloud-native application offers for
achieving blue-green deployments. But these deployments are still possible in Azure
VMware Solution:

Before you make changes to your application configuration, take snapshots of
your environment.
Use version control to ensure that you can return to a last-known good state.
Consider creating a staging environment that mirrors production and deploys
updates before you go live.
From the staging environment, perform rolling updates to a subset of servers and
test your application.
You can reduce manual effort, minimize errors, and improve resource usage by
automating routine tasks like provisioning, scaling, and patching. DevOps
methodologies are an important element of a well-architected solution for streamlining
operations, saving time, and helping teams focus on value-added activities.

Recommendations
Use IaC to deploy and provision infrastructure in a way that's repeatable, auditable,
and consistent.
Automate expansion and contraction by using IaC.
Use version control systems to track changes, collaborate, and roll back code to
previous versions as needed.
Take advantage of the blue-green concept by creating a staging environment that
mirrors production and test environments before you go live.
Maintain the last good state of your application by using snapshots, cloning your
disks, and having version-controlled code.

Define roles and processes
Impact: Operational excellence
Well-defined roles and responsibilities help ensure clarity, accountability, and effective
management of a well-architected Azure VMware Solution workload. Having a defined
set of standards and structured processes and knowing who runs them leads to efficient
operations and helps IT organizations align their technical offerings with business
objectives and strategies. As the Azure VMware Solution environment grows and
evolves, well-defined roles and responsibilities lead to easier task delegation and the
potential to scale the solution without disruption. The result is a better experience for
the application's users.
It's important to have a culture of continual improvement that focuses on efficient dayto-day operations for applications in the private cloud. Examples of operations include
maintaining service-level agreements (SLAs), maintaining availability, having the

capacity to minimize service disruptions, and having a smooth delivery. For instance,
Azure VMware Solution makes it possible to expand an environment with minimal user
input. If you manually expand your contract, you should document who performs the
associated activities and how to carry them out. Azure VMware Solution operators
should ensure node reservation is available for expanding the environment as needed.
For example, designate individuals who are responsible for identifying underutilized or
idle resources. Provide those individuals with a process for right-sizing virtual machines
(VMs) to reduce unnecessary costs.
Application teams and developers should define coding guidelines for code structure,
exceptions, and error handling. You should also have methods for tracking changes,
such as regular code reviews, API documentation, and regular code refactoring. When
you use version control, enforce best practices such as branching, commit messages,
and approval workflows. These practices can help make your code consistent, easy to
debug, and maintainable.

Recommendations
Work with the cloud center of excellence (CCoE) team to understand the standards
and guidelines for compliance, security, application architecture, and operational
processes.
Have a security and compliance team focus on security policies that are specific to
Azure VMware Solution. Also have the team perform risk assessments and ensure
compliance with regulatory requirements.
Adopt a framework such as the Information Technology Infrastructure Library (ITIL).
Or use International Organization for Standardization (ISO) practices to map dayto-day operations, processes, and activities. These practices can result in faster
knowledge transfers, continuous improvements, and improved change
management.
Define coding standards and implement security practices during application
development.

Use tagging strategies and best practices
Impact: Operational excellence
You can use a tagging strategy for chargeback and resource tracking. Tags are key-pair
values that you define at the resource level and apply during provisioning. You can use
IaC to create, update, and destroy guest VMs. Tags and IaC work together with
configuration management tools. You can use tags in the following areas:

Environments. You can apply tags like production, QA, or dev test to identify
resources.
Cost centers, for tracking resource costs and expenses.
SLAs, to prioritize SLA requirements of resources.
Lifecycles. You can label applications as active, archived, or retired.
Criticality, by labeling resources based on their business impact and significance.
As part of your governance and compliance strategy, a group should be responsible for
identifying resources without tags. That group can combine automation, auditing, and
processes to help identify and remediate resources that don't meet tagging compliance
policies.
These tagging considerations are general. It's important that your tagging strategy
supports effective resource categorization, resource lifecycle management, and
reporting within Azure VMware Solution.

Recommendations
Apply tags for resource management by using an organizational taxonomy to
identify workloads and infrastructure. The taxonomy should include the host,
business, owner, and environment.
Use appropriate tooling to maintain and apply tags programmatically during
provisioning.
Use tags that align with your organization's compliance and governance initiatives,
such as SLAs, chargeback policies, and lifecycle management practices.
Have processes in place to identify and remedy resources that don't adhere to
tagging requirements.

Establish incident response teams
Impact: Operational excellence
To track workload status in a private cloud, it's essential to monitor metrics such as CPU
usage, operating system logs, and security alerts. To ensure the effectiveness of your
alerting system, you need to evaluate several key operational aspects. Specifically, check
that:
All critical components are identified, such as databases, network devices, and
storage.
Thresholds are set appropriately.
Alerts are specific and actionable.
The right people receive alerts.

There isn't a substantial amount of noise and false positives.
Adequate escalation procedures are in place.
Before an incident or outage, it's crucial to establish a well-defined notification process
to ensure timely communication. Identifying the relevant personnel responsible for
resolution is vital. A dedicated remediation team can include operations personnel,
application owners, and DevOps experts who possess the expertise that's needed to
resolve issues quickly. The operations team must be aware of the appropriate individuals
to involve in triaging each problem.
An incident response team can effectively coordinate responses by maintaining a
comprehensive distribution list. This list should include key stakeholders from businesscritical departments and designated escalation contacts. Business stakeholders must be
informed of any potential impact on operations that result from an incident. The
assigned escalation contacts should be individuals who are capable of making decisions
or escalating issues to higher levels for guidance.
Regularly reviewing the distribution list is essential to ensure its accuracy and alignment
with current roles and responsibilities. Reviews ensure that key stakeholders are
promptly informed about significant events that occur in Azure VMware Solution.
An IT service management (ITSM) solution can map events to tasks. For example, an
Azure native ITSM might use Azure DevOps to manage tasks. It might use Azure
Automation for automating IT processes and Azure Logic Apps for building workflows.
The result is a customized solution for problem management in Azure VMware Solution.

Recommendations
Define the appropriate recipients for Azure VMware Solution alerts and incidents.
Clearly define escalation contacts who should be reachable and authorized to
make decisions or escalate issues.
Identify key business stakeholders or representatives to ensure visibility into any
potential impact and to provide guidance.
Have a remediation team in place that consists of administrators, infrastructure
engineers, and personnel who have the expertise that's needed to address and
resolve issues.
Integrate alerts with an ITSM like Azure DevOps, JIRA, or ServiceNow.

Document procedures
Impact: Reliability

It's important to have a clear understanding of the backup and recovery infrastructure
that exists in your environment. To configure a backup solution, you first need to define
backup targets for your infrastructure. You should back up your applications, databases,
and assets in blob storage or an Azure backup vault. You should also designate owners
who are responsible for backing up and restoring your application.

Recommendations
Clearly document your backup and recovery infrastructure.
Clearly document your backup and recovery procedures.

Implement backup and restore solutions
Impact: Reliability
The SDDC should protect against data loss, minimize downtime, and maintain the
continuity of operations when there are unexpected disruptions or disasters.
For business continuity, you need to implement robust data protection to help ensure
the availability, integrity, and recoverability of your VMs and the critical data within the
Azure VMware Solution environment. The backup tools need to be in place, and you
must also confirm that they work. A key principle of Azure VMware Solution is to
provide independent software vendor (ISV) technology support that's validated with
Azure VMware Solution. Understanding the partners and options that are available to
you is critical to your backup success.

Recommendations
Use backup solutions that Microsoft supports, such as Microsoft Azure Backup
Server, or approved third-party vendors.

Use Azure Site Recovery
Impact: Reliability
Azure Site Recovery is a disaster recovery solution that's designed to minimize
downtime of the VMs in an Azure VMware Solution environment when there's a
disaster. Azure Site Recovery automates and orchestrates failover and failback. Built-in
nondisruptive testing helps ensure your recovery time objectives (RTOs) are met. Azure
Site Recovery simplifies management through automation and helps ensure fast and
highly predictable recovery times.

Recommendations
In a prolonged regional outage, protect your workloads by replicating them to an
alternate Azure region.
Configure Azure Site Recovery to send backups to an alternate region.

Rotate secrets
Impact: Security
It's more challenging for attackers to access or misuse encrypted data if they don't have
access to encryption keys. You should securely store keys, secrets, and certificates, and
you should rotate them frequently. Comprehensive steps for securing and maintaining
data integrity include:
Encrypting data.
Storing keys securely.
Encrypting data at the application level before you transmit data.

Recommendations
Use Azure Key Vault to store encryption keys.

Next steps
Now that you've looked at operational management procedures, see how to integrate
an Azure VMware Solution workload with Azure landing zones.
Landing zone integration
Use the assessment tool to evaluate your design choices.
Assessment

Integrate an Azure VMware Solution
workload with Azure landing zones
Article • 08/15/2023

Each organization manages workloads and operates its cloud environment uniquely. The
common cloud operating models are decentralized, centralized, enterprise, and
distributed.
The most important difference between the various models is the level of ownership. In
the decentralized model, workload owners have autonomy without any central IT
oversight for governance. For example, they manage their own networking, monitoring,
and identity requirements. At the other end of the spectrum is the centralized model,
where the workload owners adhere to the governance requirements that central IT
teams set.
For a detailed discussion of the models, see Review and compare common cloud
operating models.
As a workload owner, you should understand the operating model that your
organization uses. That choice influences the technical decisions that you're accountable
for and the technical requirements that you drive to your central teams.
To make the most of Azure VMware Solution features and capabilities, you should take
advantage of the best practices that apply to organizations. The platform provides
adaptability and flexibility, which helps your Azure VMware Solution environment
accommodate future growth.

Azure landing zones
An Azure landing zone is a conceptual architecture that depicts the overall cloud
footprint for an organization. It has multiple subscriptions, each with a unique purpose.
Central teams own some of the subscriptions, such as Azure platform landing zones.
To familiarize yourself with the concept of Azure landing zones, see What is an Azure
landing zone?.
） Important
Azure VMware Solution has specific considerations and requirements, especially
ones that are related to integration with Azure services. The Azure VMware Solution

landing zone accelerator and the Azure Well-Architected Framework guidance for
Azure VMware Solution aim to highlight these necessary customizations. These
resources also incorporate cloud-adoption framework perspectives for a holistic
approach to cloud readiness.



Platform landing zones
Sometimes an Azure VMware Solution private cloud is deployed before a workload gets
migrated. Other times, the private cloud is deployed on a workload. In both cases, the
private cloud needs to interact with multiple external services. Central teams might own
some of these services as part of platform landing zones. Examples of these services
include domain resolution, network connectivity, and security services. Interaction with
these external services is a foundational concern. To be fully functional, workloads that
are deployed on an Azure VMware Solution private cloud need the platform team and
the workload team to share the same responsibility mindset.
For a demonstration of the platform landing zones that you need for an Azure VMware
Solution workload to run, see Azure landing zone review for Microsoft Azure VMware
Solution. This article describes a solid platform foundation that accelerates migration
from an on-premises VMware environment to an Azure VMware Solution private cloud.

Application landing zones

There's a separate subscription, which is also known as an Azure application landing
zone, that's intended for workload owners. This application landing zone is where you
deploy your VMware workload. It has access to platform landing zones that provide the
basic infrastructure that you need to run your workload. Examples include networking,
identity access management, policy, and monitoring infrastructure.
Guidance about application landing zones applies to Azure VMware Solution workloads.
For more information, see Platform landing zones vs. application landing zones. This
guidance includes recommendations for efficiently governing and managing your
workload.
For a demonstration of an application landing zone for an Azure VMware Solution
workload, see the baseline reference architecture in Example architectures for Azure
VMware Solutions. Initially, workload density and maturity are minimal inside an Azure
VMware Solution private cloud. The density and maturity are expected to increase after
the initial accelerator deployment. This guidance is applicable when the density and
maturity of the private cloud start to increase.

Design area integration
This section highlights the solid foundation that the platform provides. The discussion
also covers the areas of shared responsibility between the platform team and the
workload team.

Platform responsibilities
The Azure VMware Solution platform team ensures that the infrastructure is ready for
application teams to build. Some common tasks include:
Requesting capacity by ensuring that the Azure VMware Solution softwaredefined datacenter (SDDC) activation has occurred and has defined the regions,
nodes, and network settings. The platform team then allocates compute resources,
resource pools, virtual storage area network (vSAN) storage, and clustering.
Designing to meet recovery point objective (RPO) and recovery time objective
(RTO) targets by strategically building infrastructure to meet service-level
agreements (SLAs).
Securing and optimizing connectivity to on-premises systems, Azure, and the
internet. This task includes routing, setting up firewall entries, and managing
centralized network appliances.
Managing Azure integrations, such as integrations with Azure DNS, Azure Backup,
Azure Monitor, Log Analytics, Azure Active Directory (Azure AD), and Azure Key

Vault.

Shared responsibilities
The workload team and the platform team have distinct responsibilities. But both teams
often work closely together to help ensure workload availability and recoverability. The
teams coordinate efforts for the overall success of the workloads that run in Azure
VMware Solution. Effective collaboration between the platform and application teams is
vital for successfully deploying cloud-based applications.
The design areas of the platform and application landing zones are tightly coupled.
For a description of changes in platform resources that are needed for a workload,
see Azure landing zone review for Microsoft Azure VMware Solution.
For a description of the technical specification of a workload, see What are the key
design areas?.

Design area - Infrastructure
Backup and disaster recovery is an area of infrastructure design that the application
and platform teams both have roles in implementing.
The Azure VMware Solution platform team sets up infrastructure-level backups
and replication for virtual machines (VMs) and Azure VMware Solution
components.
The application team is responsible for application-level backups and data
recovery procedures.
In some organizations, some operations are shared responsibilities. The following table
lists examples:
Platform team responsibilities

Workload team responsibilities

- Infrastructure backups. Implement backups
for Azure VMware Solution components, VMs,

- Data backup. Back up application-specific
data and databases to a storage location by

and core infrastructure.
- Configuration of VM image backups. Quickly
recover infrastructure from formats such as VM

using an agent-based, VMware-compatible
backup solution.
- Application configurations. Manage

disks (VMDKs) during failures.
- Disaster recovery planning. Define
mechanisms for site failover, data replication,

application configurations, settings, and
libraries that the application needs to operate.
- Task prioritization. Discern important tasks

and maintaining RPO and RTO targets for the
Azure VMware Solution infrastructure.

from nonessential tasks.
- Data restore and recovery. Restore
application data from backups periodically.

Platform team responsibilities

Workload team responsibilities
Ensure that the application returns to a
functional state in real-world scenarios.

Design area - Networking
DNS resolution is a key concept in the networking design area.
The DNS configuration in Azure VMware Solution involves mapping host names to IP
addresses. This mapping establishes connectivity between VMs and services within
Azure VMware Solution and the broader network. The following table lists DNS
responsibilities:
Platform team responsibilities

Workload team responsibilities

- Spin up VMs as domain
controllers.

- Configure host names.
- Manage application time to live (TTL) settings.

- Create private DNS zones.
- Manage domain names.
- Configure reverse DNS lookup.

- Manage internal DNS resolution.
- Manage DNS monitoring and up-down alerts that affect the
application.

Design area - Operations
Key management is an important area in operations.
The application and platform teams both have responsibilities in key and password
management. The roles that they play help ensure the security and access control of the
applications that run in Azure VMware Solution. The following table lists the differences
in the teams' responsibilities:
Platform team responsibilities

Workload team responsibilities

- Infrastructure key management. Manage encryption
key and infrastructure-level data such as encrypted
disks and VM templates.

- Manage application-specific
credentials and keys such as ones that
are used for accessing APIs, databases,

- Infrastructure credentials. Manage Azure VMware
Solution administrative credentials for components
like VMware vCenter Server and VMware ESXi hosts.

and secrets.
- Implement regular password rotation
and credential expiry policies to prevent

- Platform access control. Define user roles and
permissions in the Azure VMware Solution
environment.

unauthorized access.
- Ensure that application credentials are
stored securely and aren't hard-coded in

- Key Vault: Create the Key Vault instance, configure
policies to protect the vault, manage infrastructure
and platform secrets, and manage encryption and

application code or configuration files.
- Define access policies for key vaults
that are specific to the application or

decryption operations.

Platform team responsibilities

services
that
require
access to those
Workload
team
responsibilities
secrets.

Next steps
Use the assessment tool to evaluate your design choices.
Assessment

Well-architected assessment for Azure
VMware Solution workloads
Article • 08/15/2023

The Azure well-architected VMware workload assessment is a review tool for selfassessing the readiness of your Azure VMware Solution workload in production.
Running an on-premises VMware technology stack on Azure can be a complex process.
The assessment is organized so that you can methodically check your alignment with
best practices for the Azure Well-Architected Framework pillars.
For best results, the team that completes the assessment should be well versed in the
architecture of your workload. That team should also have a strong understanding of
cloud principles and patterns. These roles include but aren't limited to cloud architects,
operators, and DevOps engineers.
The assessment is a set of questions that are based on the Azure VMware Solution
design areas as a way of checking the foundational design choices of your workload's
architecture and your end-to-end operational approach.

These questions are designed to help benchmark your workload's maturity and
alignment with the recommended approach for operating an Azure VMware Solution
workload. The outcome of the assessment is the delivery of technical recommendations
and documentation that provide guidance on how to implement a highly reliable
solution on Azure.

Azure VMware Solution review tool

Next steps
See the following reference architectures, which describe design choices for productionready implementations:
Baseline Azure VMware Solution reference architecture
Azure VMware Solution landing zone accelerator

Mission-critical workload
documentation
Learn about building highly reliable workloads on Microsoft Azure. The articles provide
architectural guidance and a prescriptive approach for designing, building, and
operating mission-critical workloads. For a given set of business requirements, an
application should always be operational and available. While there are many
approaches to achieving high reliability, one of the goals is to accelerate adoption
toward cloud native solutions to derive maximum value from the Microsoft Cloud.

Get started

ｅ

OVERVIEW

What is a mission-critical workload?
Design methodology

ｐ

CONCEPT

Architecture pattern
Design principles
Cross-cutting concerns

Design areas

ｐ

CONCEPT

Application design
Application platform
Data platform
Networking and connectivity
Health modeling
Deployment and testing
Security

Operational procedures

Reference examples

Ｙ

ARCHITECTURE

Baseline reference architecture
Baseline with network controls
Baseline in Azure landing zones
Baseline with App Services

Reference implementations

｀

DEPLOY

Mission-Critical Online
Mission-Critical Connected

Learn

ｄ

TRAINING

Challenge Project - Design a mission-critical web application
Design a health model for your mission-critical workload
Continuously validate and test mission-critical workloads

Video library

ｑ

VIDEO

What is a mission-critical workload?
Global distribution
Define a health model

Continuously validate your workload
Use multiple subscriptions
Development environments
Zero downtime deployments
Integration with Azure landing zones

ｑ

VIDEO

Demo - Continuous validation with Azure Load Test and Azure Chaos Studio
Demo - Ephemeral dev environments and automated feature validation
Demo - Monitoring and health modeling

Assessment

ｃ

HOW-TO GUIDE

Mission-critical assessment tool

Industry solutions

Ｙ

ARCHITECTURE

Carrier-grade for telecommunications

Architect perspectives

ｑ

VIDEO

Azure Friday - Health modeling for mission-critical workloads on Azure
Azure Friday - Continuously validate and test your mission-critical Azure workloads
Azure Friday - Deploy your mission-critical workload in an Azure landing zone
Azure Enablement Show - Designing a mission-critical workload on Azure

Azure Enablement Show - Integrating a mission-critical workload with Azure landing zones

Mission-critical workloads
Article • 02/01/2023

This section strives to address the challenges of designing mission-critical workloads on
Azure. The guidance is based on lessons learned from reviewing numerous customer
applications and first-party solutions. This section provides actionable and authoritative
guidance that applies Well-Architected best practices as the technical foundation for
building and operating a highly reliable solution on Azure at-scale.

What is a mission-critical workload?
The term workload refers to a collection of application resources that support a common
business goal or the execution of a common business process, with multiple services,
such as APIs and data stores, working together to deliver specific end-to-end
functionality.
The term mission-critical refers to a criticality scale that covers significant financial cost
(business-critical) or human cost (safety-critical) associated with unavailability or
underperformance.
A mission-critical workload therefore describes a collection of application resources,
which must be highly reliable on the platform. The workload must always be available,
resilient to failures, and operational.

Video: Mission-critical workloads on Azure
https://www.microsoft.com/en-us/videoplayer/embed/RE52ryK?postJsllMsg=true

What are the common challenges?
Microsoft Azure makes it easy to deploy and manage cloud solutions. However, building
mission-critical workloads that are highly reliable on the platform remains a challenge
for these main reasons:
Designing a reliable application at scale is complex. It requires extensive platform
knowledge to select the right technologies and optimally configure them to deliver
end-to-end functionality.
Failure is inevitable in any complex distributed system, and the solution must
therefore be architected to handle failures with correlated or cascading impact.

This is a change in mindset for many developers and architects entering the cloud
from an on-premises environment; reliability engineering is no longer an
infrastructure subject, but should be a first-class concern within the application
development process.
Operationalizing mission-critical workloads requires a high degree of engineering
rigor and maturity throughout the end-to-end engineering lifecycle as well as the
ability to learn from failure.

Is mission-critical only about reliability?
While the primary focus of mission-critical workloads is Reliability, other pillars of the
Well-Architected Framework are equally important when building and operating a
mission-critical workload on Azure.
Security: how a workload mitigates security threats, such as Distributed Denial of
Service (DDoS) attacks, will have a significant bearing on overall reliability.
Operational Excellence: how a workload is able to effectively respond to
operational issues will have a direct impact on application availability.
Performance Efficiency: availability is more than simple uptime, but rather a
consistent level of application service and performance relative to a known healthy
state.
Achieving high reliability imposes significant cost tradeoffs, which may not be justifiable
for every workload scenario. It is therefore recommended that design decisions be
driven by business requirements.

What are the key design areas?
Mission-critical guidance within this series is composed of architectural considerations
and recommendations orientated around these key design areas.

The design areas are interrelated and decisions made within one area can impact or
influence decisions across the entire design. We recommend that readers familiarize
themselves with these design areas, reviewing provided considerations and
recommendations to better understand the consequences of encompassed decisions.
For example, to define a target architecture it's critical to determine how best to monitor
application health across key components. In this instance, the reader should review the
health modeling design area, using the outlined recommendations to help drive
decisions.
Design area

Summary

Application
design

The use of a scale-unit architecture in the context of building a highly reliable
application. Also explores the cloud application design patterns that allow for
scaling, and error handling.

Application
platform

Decision factors and recommendations related to the selection, design, and
configuration of an appropriate application hosting platform, application
dependencies, frameworks, and libraries.

Data
platform

Choices in data store technologies, informed by evaluating the required—volume,
velocity, variety, veracity.

Networking
and

Network topology concepts at an application level, considering requisite
connectivity and redundant traffic management. Critical recommendations

connectivity

intended to inform the design of a secure and scalable global network topology.

Design area

Summary

Health
modeling
and

Processes to define a robust health model, mapping quantified application health
states through observability and operational constructs to achieve operational
maturity.

observability
Deployment
and testing

Eradicate downtime and maintain application health for deployment operations,
providing key considerations and recommendations intended to inform the
design of optimal CI/CD pipelines for a mission-critical application.

Security

Protect the application against threats intended to directly or indirectly
compromise its reliability.

Operational
procedures

Adoption of DevOps and related deployment methods is used to drive effective
and consistent operational procedures.

Illustrative examples
The guidance provided within this series is based on a solution-orientated approach to
illustrate key design considerations and recommendations. There are several reference
implementations available that can be used as a basis for further solution development.
Baseline architecture of an internet-facing application—Provides a foundation for
building a cloud-native, highly scalable, internet-facing application on Microsoft
Azure. The workload is accessed over a public endpoint and doesn't require private
network connectivity to a surrounding organizational technical estate.
Refer to the implementation: Mission-Critical Online
Baseline architecture of an internet-facing application with network controls—
Extends the baseline architecture with strict network controls in place to prevent
unauthorized public access from the internet to any of the workload resources.
Baseline architecture in an Azure landing zone—Provides a foundation for building
a corporate-connected cloud-native application on Microsoft Azure using existing
network infrastructure and private endpoints. The workload requires private
connectivity to other organizational resources and takes a dependency on preprovided Virtual Networks for connectivity to other organizational resources. This
use case is intended for scenarios that require integration with a broader
organizational technical estate for either public-facing or internal-facing
workloads.
Refer to the implementation: Mission-Critical Connected

Industry scenarios
The mission-critical guidance within this series forms an industry agnostic design
methodology which can be applied across a multitude of different industry contexts. The
following list provides specific examples where the mission-critical design methodology
has been applied and tailored to a particular industry scenario.
Carrier-grade within the telecommunications industry
A carrier-grade workload pivots on both business-critical and safety-critical aspects,
where there's a fundamental requirement to be operational with only minutes or even
seconds of downtime per calendar year. Failure to achieve this uptime requirement can
result in extensive loss of life, incur significant fines, or contractual penalties.

Next step
Start by reviewing the design methodology for mission-critical application scenarios.
Design methodology

Design methodology for mission-critical
workloads on Azure
Article • 03/15/2023

Building a mission-critical application on any cloud platform requires significant
technical expertise and engineering investment, particularly since there's significant
complexity associated with:
Understanding the cloud platform,
Choosing the right services and composition,
Applying the correct service configuration,
Operationalizing utilized services, and
Constantly aligning with the latest best practices and service roadmaps.
This design methodology strives to provide an easy to follow design path to help
navigate this complexity and inform design decisions required to produce an optimal
target architecture.

1—Design for business requirements
Not all mission-critical workloads have the same requirements. Expect that the review
considerations and design recommendations provided by this design methodology will
yield different design decisions and trade-offs for different application scenarios.

Select a reliability tier
Reliability is a relative concept and for any workload to be appropriately reliable it
should reflect the business requirements surrounding it. For example, a mission-critical
workload with a 99.999% availability Service Level Objective (SLO) requires a much
higher level of reliability than another less critical workload with an SLO of 99.9%.
This design methodology applies the concept of reliability tiers expressed as availability
SLOs to inform required reliability characteristics. The table below captures permitted
error budgets associated with common reliability tiers.
Reliability Tier
(Availability SLO)

Permitted
Downtime (Week)

Permitted Downtime
(Month)

Permitted Downtime
(Year)

99.9%

10 minutes, 4
seconds

43 minutes, 49
seconds

8 hours, 45 minutes,
56 seconds

Reliability Tier
(Availability SLO)

Permitted
Downtime (Week)

Permitted Downtime
(Month)

Permitted Downtime
(Year)

99.95%

5 minutes, 2 seconds

21 minutes, 54
seconds

4 hours, 22 minutes,
58 seconds

99.99%

1 minutes

4 minutes 22 seconds

52 minutes, 35
seconds

99.999%

6 seconds

26 seconds

5 minutes, 15 seconds

99.9999%

<1 second

2 seconds

31 seconds

） Important
Availability SLO is considered by this design methodology to be more than simple
uptime, but rather a consistent level of application service relative to a known
healthy application state.
As an initial exercise, readers are advised to select a target reliability tier by determining
how much downtime is acceptable? The pursuit of a particular reliability tier will
ultimately have a significant bearing on the design path and encompassed design
decisions, which will result in a different target architecture.
This image shows how the different reliability tiers and underlying business
requirements influence the target architecture for a conceptual reference
implementation, particularly concerning the number of regional deployments and
utilized global technologies.

Recovery Time Objective (RTO) and Recovery Point Objective (RPO) are further critical
aspects when determining required reliability. For instance, if you're striving to achieve
an application RTO of less than a minute then back-up based recovery strategies or an
active-passive deployment strategy are likely to be insufficient.

2—Evaluate the design areas using the design
principles
At the core of this methodology lies a critical design path comprised of:
Foundational design principles
Fundamental design area with heavily interrelated and dependent design
decisions.
The impact of decisions made within each design area will reverberate across other
design areas and design decisions. Review the provided considerations and
recommendations to better understand the consequences of encompassed decisions,
which may produce trade-offs within related design areas.
For example, to define a target architecture it's critical to determine how best to monitor
application health across key components. We highly recommend that you review the
health modeling design area, using the outlined recommendations to help drive
decisions.

3—Deploy your first mission-critical application
Refer to these reference architectures that describe the design decisions based on this
methodology.
Baseline architecture of an internet-facing application
Baseline architecture of an internet-facing application with network controls
 Tip

The architecture is backed by Mission-Critical Online

implementation that

illustrates the design recommendations.
Production-grade artifacts Every technical artifact is ready for use in production
environments with all end-to-end operational aspects considered.

Rooted in real-world experiences All technical decisions are guided by experiences of
Azure customers and lessons learned from deploying those solutions.
Azure roadmap alignment The mission-critical reference architectures have their own
roadmap that is aligned with Azure product roadmaps.

4—Integrate your workload in Azure landing
zones
Azure landing zone subscriptions provide shared infrastructure for enterprise
deployments that need centralized governance.
https://learn-video.azurefd.net/vod/player?id=9e05a6bd-7d10-4a83-9436370a75dc1919&locale=en-us&embedUrl=%2Fazure%2Fwell-architected%2Fmissioncritical%2Fmission-critical-design-methodology
It's crucial to evaluate which connectivity use case is required by your mission-critical
application. Azure landing zones support two main archetypes separated into different
Management Group scopes: Online or Corp. as shown in this image.

Online subscription
A mission-critical workload operates as an independent solution, without any direct
corporate network connectivity to the rest of the Azure landing zone architecture. The
application will be further safeguarded through the policy-driven governance and will
automatically integrate with centralized platform logging through policy.

The baseline architecture and Mission-Critical Online

implementation align with the

Online approach.

Corp. subscription
When deployed in a Corp. subscription a mission-critical workload depends on the
Azure landing zone to provide connectivity resources. This approach allows integration
with other applications and shared services. You'll need to design around some
foundational resources, which will exist up-front as part of the shared-service platform.
For example, the regional deployment stamp should no longer encompass an
ephemeral Virtual Network or Azure Private DNS Zone because these will exist in the
Corp. subscription.
To get started with this use case, we recommend the baseline architecture in an Azure
landing zone reference architecture.
 Tip

The preceding architecture is backed by Mission-Critical Connected
implementation.

5—Deploy a sandbox application environment
In parallel to design activities, it's highly recommended that a sandbox application
environment is established using the Mission-Critical reference implementations.
This provides hands-on opportunities to validate design decisions by replicating the
target architecture, allowing for design uncertainty to be quickly assessed. If applied
correctly with representative requirement coverage, most problematic issues likely to
hinder progress can be uncovered and subsequently addressed.

6—Continuously evolve with Azure roadmaps
Application architectures established using this design methodology must continue to
evolve in alignment with Azure platform roadmaps to support optimized sustainability.

Next step
Review the design principles for mission-critical application scenarios.

Design principles

Design principles of a mission-critical
workload
Article • 02/01/2023

The mission-critical design methodology is underpinned by five key design principles
which serve as a compass for subsequent design decisions across the critical design
areas. We highly recommend that you familiarize yourselves with these principles to
better understand their impact and the trade-offs associated with non-adherence.
） Important
This article is part of the Azure Well-Architected mission-critical workload series. If
you aren't familiar with this series, we recommend you start with what is a missioncritical workload?

These mission-critical design principles resonate and extend the quality pillars of the
Azure Well-Architected Framework—Reliability, Security, Cost Optimization, Operational
Excellence, and Performance Efficiency.

Reliability
Maximum reliability - Fundamental pursuit of the most reliable solution, ensuring
trade-offs are properly understood.

Design
principle

Considerations

Active/Active
design

To maximize availability and achieve regional fault tolerance, solution
components should be distributed across multiple Availability Zones and Azure
regions using an active/active deployment model where possible.

Blast radius
reduction

Failure is impossible to avoid in a highly distributed multi-tenant cloud
environment like Azure. By anticipating failures and correlated impact, from

and fault
isolation

individual components to entire Azure regions, a solution can be designed and
developed in a resilient manner.

Observe
application
health

Before issues impacting application reliability can be mitigated, they must first be
detected and understood. By monitoring the operation of an application relative
to a known healthy state it becomes possible to detect or even predict reliability
issues, allowing for swift remedial action to be taken.

Drive
automation

One of the leading causes of application downtime is human error, whether that
is due to the deployment of insufficiently tested software or misconfiguration. To
minimize the possibility and impact of human errors, it's vital to strive for
automation in all aspects of a cloud solution to improve reliability; automated
testing, deployment, and management.

Design for
self-healing

Self healing describes a system's ability to deal with failures automatically
through pre-defined remediation protocols connected to failure modes within
the solution. It's an advanced concept that requires a high level of system
maturity with monitoring and automation, but should be an aspiration from
inception to maximize reliability.

Complexity

Avoid unnecessary complexity when designing the solution and all operational

avoidance

processes to drive reliability and management efficiencies, minimizing the
likelihood of failures.

Performance Efficiency
Sustainable performance and scalability - Design for scalability across the end-to-end
solution without performance bottlenecks.
Design

Considerations

principle
Design for

Scale-out is a concept that focuses on a system's ability to respond to demand

scale-out

through horizontal growth. This means that as traffic grows, more resource units
are added in parallel instead of increasing the size of the existing resources. A
systems ability to handle expected and unexpected traffic increases through scaleunits is essential to overall performance and reliability by further reducing the
impact of a single resource failure.

Design

Considerations

principle
Automation

Scale operations throughout the solution should be fully automated to minimize

for

the performance and availability impact from unexpected or expected increases in

hyperscale

traffic, ensuring the time it takes to conduct scale operations is understood and
aligned with a model for application health.

Continuous
validation

Automated testing should be performed within CI/CD processes to drive
continuous validation for each application change. Load testing against a

and testing

performance baseline with synchronized chaos experimentation should be
included to validate existing thresholds, targets, and assumptions, as well as
helping to quickly identify risks to resiliency and availability. Such testing should
be conducted within staging and testing environments, but also optionally within
development environments. It can also be beneficial to run a subset of tests
against the production environment, particularly in conjunction with a blue/green
deployment model to validate new deployment stamps before receiving
production traffic.

Reduce

Using managed compute services and containerized architectures significantly

overhead

reduces the ongoing administrative and operational overhead of designing,

with
managed

operating, and scaling applications by shifting infrastructure deployment and
maintenance to the managed service provider.

compute
services
Baseline

Performance testing with detailed telemetry from every system component allows

performance
and identify

for the identification of bottlenecks within the system, including components that
need to be scaled in relation to other components, and this information should be

bottlenecks

incorporated into a capacity model.

Model

A capacity model enables planning of resource scale levels for a given load profile,

capacity

and additionally exposes how system components perform in relation to each
other, therefore enabling system-wide capacity allocation planning.

Operational Excellence
Operations by design - Engineered to last with robust and assertive operational
management.
Design
principle

Considerations

Loosely

Loose coupling enables independent and on-demand testing, deployments, and

coupled
components

updates to components of the application while minimizing inter-team
dependencies for support, services, resources, or approvals.

Design
principle

Considerations

Automate

Fully automated build and release processes reduce the friction and increase the

build and
release
processes

velocity of deploying updates, bringing repeatability and consistency across
environments. Automation shortens the feedback loop from developers pushing
changes to getting insights on code quality, test coverage, resiliency, security,
and performance, which increases developer productivity.

Developer
agility

Continuous Integration and Continuous Deployment (CI/CD) automation enables
the use of short-lived development environments with lifecycles tied to that of an
associated feature branch, which promotes developer agility and drives validation
as early as possible within the engineering cycle to minimize the engineering cost
of bugs.

Quantify

Full diagnostic instrumentation of all components and resources enables ongoing

operational
health

observability of logs, metrics and traces, but also facilitates health modeling to
quantify application health in the context to availability and performance
requirements.

Rehearse
recovery and
practice

Business Continuity (BC) and Disaster Recovery (DR) planning and practice drills
are essential and should be conducted frequently, since learnings can iteratively
improve plans and procedures to maximize resiliency in the event of unplanned

failure

downtime.

Embrace
continuous
operational

Prioritize routine improvement of the system and user experience, using a health
model to understand and measure operational efficiency with feedback
mechanisms to enable application teams to understand and address gaps in an

improvement

iterative manner.

Security
Always secure - Design for end-to-end security to maintain application stability and
ensure availability.
Design
principle

Considerations

Monitor the

Correlate security and audit events to model application health and identify

security of the
entire solution
and plan

active threats. Establish automated and manual procedures to respond to
incidents using Security Information and Event Management (SIEM) tooling for
tracking.

incident
responses

Design

Considerations

principle
Model and test
against
potential

Ensure appropriate resource hardening and establish procedures to identify
and mitigate known threats, using penetration testing to verify threat
mitigation, as well as static code analysis and code scanning.

threats
Identify and
protect
endpoints

Monitor and protect the network integrity of internal and external endpoints
through security capabilities and appliances, such as firewalls or web
application firewalls. Use industry standard approaches to protect against
common attack vectors like Distributed Denial-Of-Service (DDoS) attacks, such
as SlowLoris.

Protect against

Identify and mitigate code-level vulnerabilities, such as cross-site scripting or

code level
vulnerabilities

SQL injection, and incorporate security patching into operational lifecycles for
all parts of the codebase, including dependencies.

Automate and
use least

Drive automation to minimize the need for human interaction and implement
least privilege across both the application and control plane to protect against

privilege

data exfiltration and malicious actor scenarios.

Classify and
encrypt data

Classify data according to risk and apply industry standard encryption at rest
and in transit, ensuring keys and certificates are stored securely and managed
properly.

Cost Optimization
There are obvious cost tradeoffs associated with introducing greater reliability, which
should be carefully considered in the context of workload requirements.
Maximizing reliability can impact the overall financial cost of the solution. For example,
the duplication of resources and the distribution of resources across regions to achieve
high availability has clear cost implications. To avoid excess costs, don't over-engineer or
over-provision beyond the relevant business requirements.
Also, there is added cost associated with engineering investment in fundamental
reliability concepts, such as embracing infrastructure as code, deployment automation,
and chaos engineering. This comes at a cost in terms of both time and effort, which
could be invested elsewhere to deliver new application functionality and features.

Cloud native design
Azure-native managed services - Azure-native managed services are prioritized
due to their lower administrative and operational overhead as well as tight

integration with consistent configuration and instrumentation across the
application stack.
Roadmap alignment - Incorporate upcoming new and improved Azure service
capabilities as they become Generally Available (GA) to stay close to the leading
edge of Azure.
Embrace preview capabilities and mitigate known gaps - While Generally
Available (GA) services are prioritized for supportability, Azure service previews are
actively explored for rapid incorporation, providing technical and actionable
feedback to Azure product groups to address gaps.
Azure landing zone alignment - Deployable within an Azure landing zone and
aligned to the Azure landing zone design methodology, but also fully functional
and deployable in a bare environment outside of a landing zone.

Next step
Review cross-cutting concerns associated with mission-critical workloads.
Cross-cutting concerns

Architecture pattern for mission-critical
workloads on Azure
Article • 12/16/2022

This article presents a key pattern for mission-critical architectures on Azure. Apply this
pattern when you start your design process, and then select components that are best
suited for your business requirements. The article recommends a north star design
approach and includes other examples with common technology components.
We recommend that you evaluate the key design areas, define the critical user and
system flows that use the underlying components, and develop a matrix of Azure
resources and their configuration while keeping in mind the following characteristics.
Characteristic

Considerations

Lifetime

What's the expected lifetime of the resource, relative to other resources in
the solution? Should the resource outlive or share the lifetime with the
entire system or region, or should it be temporary?

State

What impact will the persisted state at this layer have on reliability or
manageability?

Reach

Is the resource required to be globally distributed? Can the resource
communicate with other resources, located globally or within that region?

Dependencies

What are the dependencies on other resources?

Scale limits

What is the expected throughput for that resource? How much scale is
provided by the resource to fit that demand?

Availability/disaster
recovery

What is the impact on availability from a disaster at this layer? Would it
cause a systemic outage or only a localized capacity or availability issue?

） Important
This article is part of the Azure Well-Architected mission-critical workload series. If
you aren't familiar with this series, we recommend you start with what is a missioncritical workload?

Core architecture pattern

Global resources
Certain resources are globally shared by resources deployed within each region.
Common examples are resources that are used to distribute traffic across multiple
regions, store permanent state for the whole application, and monitor resources for
them.
Characteristic

Considerations

Lifetime

These resources are expected to be long living (non-ephemeral). Their
lifetime spans the life of the system or longer. Often the resources are
managed with in-place data and control plane updates, assuming they
support zero-downtime update operations.

Characteristic

Considerations

State

Because these resources exist for at least the lifetime of the system, this
layer is often responsible for storing global, geo-replicated state.

Reach

The resources should be globally distributed and replicated to the regions
that host those resources. It’s recommended that these resources
communicate with regional or other resources with low latency and the
desired consistency.

Dependencies

The resources should avoid dependencies on regional resources because
their unavailability can be a cause for global failure. For example, certificates
or secrets kept in a single vault could have global impact if there's a
regional failure where the vault is located.

Scale limits

Often these resources are singleton instances in the system, and they
should be able to scale such that they can handle throughput of the system
as a whole.

Availability/disaster

Regional and stamp resources can use global resources. It's critical that

recovery

global resources are configured with high availability and disaster recovery
for the health of the whole system.

Regional stamp resources
The stamp contains the application and resources that participate in completing
business transactions. A stamp typically corresponds to a deployment to an Azure
region. Although a region can have more than one stamp.
Characteristic

Considerations

Lifetime

The resources are expected to have a short life span (ephemeral) with the
intent that they can get added and removed dynamically while regional
resources outside the stamp continue to persist. The ephemeral nature is
needed to provide more resiliency, scale, and proximity to users.

State

Because stamps are ephemeral and will be destroyed with each
deployment, a stamp should be stateless as much as possible.

Reach

Can communicate with regional and global resources. However,
communication with other regions or other stamps should be avoided.

Dependencies

The stamp resources must be independent. They're expected to have
regional and global dependencies but shouldn't rely on components in
other stamps in the same or other regions.

Characteristic

Considerations

Scale limits

Throughput is established through testing. The throughput of the overall
stamp is limited to the least performant resource. Stamp throughput needs
to estimate the high-level of demand caused by a failover to another stamp.

Availability/disaster
recovery

Because of the temporary nature of stamps, disaster recovery is done by
redeploying the stamp. If resources are in an unhealthy state, the stamp, as
a whole, can be destroyed and redeployed.

Regional resources
A system can have resources that are deployed in region but outlive the stamp
resources. For example, observability resources that monitor resources at the regional
level, including the stamps.
Characteristic

Consideration

Lifetime

The resources share the lifetime of the region and out live the stamp resources.

State

State stored in a region can't live beyond the lifetime of the region. If state needs
to be shared across regions, consider using a global data store.

Reach

The resources don't need to be globally distributed. Direct communication with
other regions should be avoided at all cost.

Dependencies

The resources can have dependencies on global resources, but not on stamp
resources because stamps are meant to be short lived.

Scale limits

Determine the scale limit of regional resources by combining all stamps within
the region.

Baseline architectures for mission-critical
workloads
These baseline examples serve as the recommended north star architecture for missioncritical applications. The baseline strongly recommends containerization and using a
container orchestrator for the application platform. The baseline uses Azure Kubernetes
Service (AKS).
Refer to Well-Architected mission-critical workloads: Containerization.

Baseline with
Baseline
architecture
If you're just
starting your
mission-critical
journey, use this
architecture as a
reference. The
workload is
accessed over a
public endpoint
and doesn't
require private
network
connectivity to
other company
resources.

network controls
This architecture
builds on the
baseline
architecture. The
design is extended
to provide strict
network controls
to prevent
unauthorized
public access from
the internet to the
workload
resources.

Baseline in Azure
landing zones
This architecture is
appropriate if
you're deploying
the workload in an
enterprise setup
where integration
within a broader
organization is
required. The
workload uses
centralized shared
services, needs onpremises
connectivity, and
integrates with
other workloads
within the
enterprise. It's
deployed in an
Azure landing zone
subscription that
inherits from the
Corp. management
group.

Design areas
We recommend that you use the provided design guidance to navigate the key design
decisions to reach an optimal solution. For information, see What are the key design
areas?

Next step

Review the best practices for designing mission-critical application scenarios.
Application design

Cross-cutting concerns of missioncritical workloads on Azure
Article • 02/01/2023

There are several cross-cutting concerns that traverse the key design areas. This article
contextualizes these cross-cutting concerns for subsequent consideration within each
design area.
） Important
This article is part of the Azure Well-Architected mission-critical workload series. If
you aren't familiar with this series, we recommend you start with what is a missioncritical workload?

Scale limits
Azure applies various limits or quotas to ensure a consistent level of service for all
customers. Examples of these limits include restrictions on the number of deployable
resources within a single subscription, and restrictions to network and query
throughput.
Service limits may have a significant bearing on a large mission-critical workload.
Consider the limits of the services used in the target architecture carefully to ensure
sustainable scale. Otherwise, you may hit one or more of these limits as the workload
grows.
） Important
Limits and quotas may change as the platform evolves. Be sure to check the current
limits at Azure subscription and service limits, quotas, and constraints.

Recommendations
Employ a scale unit approach for resource composition, deployment, and
management.
Use subscriptions as scale units, scaling out resources and subscriptions as
required.
Ensure scale limits are considered as part of capacity planning.

If available, use data about existing application environments to explore which
limits might be encountered.

Automation
A holistic approach to automation of deployment and management activities can
maximize the reliability and operability of the workload.

Recommendations
Automate continuous integration and continuous delivery (CI/CD) pipelines for all
application components.
Automate application management activities, such as patching and monitoring.
Use declarative management semantics, such as Infrastructure as code (IaC),
instead of over imperative approaches.
Prioritize templating over scripting. Defer to scripting only when using templates
isn't possible.

Azure roadmap alignment
Azure is constantly evolving through frequent updates to services, features, and regional
availability. It's important to align the target architecture with Azure platform roadmaps
to inform an optimal application trajectory. For example, making sure that the required
services and features are available within the chosen deployment regions.
Refer to Azure updates

for the latest information about new services and features.

Recommendations
Align with Azure engineering roadmaps and regional rollout plans.
Unblock with preview services or by taking dependencies on the Azure platform
roadmap.
Only take a dependency on committed services and features; validate roadmap
dependencies with Microsoft engineering product groups.

Next step
Explore the design areas that provide critical considerations and recommendations for
building a mission-critical workload.

Architecture pattern

Application design of mission-critical
workloads on Azure
Article • 04/10/2023

When you design an application, both functional and non-functional application
requirements are critical. This design area describes architecture patterns and scaling
strategies that can help make your application resilient to failures.
） Important
This article is part of the Azure Well-Architected Framework mission-critical
workload series. If you aren't familiar with this series, we recommend that you start
with What is a mission-critical workload?.

Scale-unit architecture
All functional aspects of a solution must be capable of scaling to meet changes in
demand. We recommend that you use a scale-unit architecture to optimize end-to-end
scalability through compartmentalization, and also to standardize the process of adding
and removing capacity. A scale unit is a logical unit or function that can be scaled
independently. A unit can be made up of code components, application hosting
platforms, the deployment stamps that cover the related components, and even
subscriptions to support multi-tenant requirements.
We recommend this approach because it addresses the scale limits of individual
resources and the entire application. It helps with complex deployment and update
scenarios because a scale unit can be deployed as one unit. Also, you can test and
validate specific versions of components in a unit before directing user traffic to it.
Suppose your mission-critical application is an online product catalog. It has a user flow
for processing product comments and ratings. The flow uses APIs to retrieve and post
comments and ratings, and supporting components like an OAuth endpoint, datastore,
and message queues. The stateless API endpoints represent granular functional units
that must adapt to changes on demand. The underlying application platform must also
be able to scale accordingly. To avoid performance bottlenecks, the downstream
components and dependencies must also scale to an appropriate degree. They can
either scale independently, as separate scale units, or together, as part of a single logical
unit.

Example scale units
The following image shows the possible scopes for scale units. The scopes range from
microservice pods to cluster nodes and regional deployment stamps.



Design considerations
Scope. The scope of a scale unit, the relationship between scale units, and their
components should be defined according to a capacity model. Take into
consideration non-functional requirements for performance.
Scale limits. Azure subscription scale limits and quotas might have a bearing on
application design, technology choices, and the definition of scale units. Scale units
can help you bypass the scale limits of a service. For example, if an AKS cluster in
one unit can have only 1,000 nodes, you can use two units to increase that limit to
2,000 nodes.
Expected load. Use the number of requests for each user flow, the expected peak
request rate (requests per second), and daily/weekly/seasonal traffic patterns to
inform core scale requirements. Also factor in the expected growth patterns for
both traffic and data volume.

Acceptable degraded performance. Determine whether a degraded service with
high response times is acceptable under load. When you're modeling required
capacity, the required performance of the solution under load is a critical factor.
Non-functional requirements. Technical and business scenarios have distinct
considerations for resilience, availability, latency, capacity, and observability.
Analyze these requirements in the context of key end-to-end user flows. You'll
have relative flexibility in the design, decision making, and technology choices at a
user-flow level.

Design recommendations
Define the scope of a scale unit and the limits that will trigger the unit to scale.
Ensure that all application components can scale independently or as part of a
scale unit that includes other related components.
Define the relationship between scale units, based on a capacity model and nonfunctional requirements.
Define a regional deployment stamp to unify the provisioning, management, and
operation of regional application resources into a heterogenous but
interdependent scale unit. As load increases, extra stamps can be deployed, within
the same Azure region or different ones, to horizontally scale the solution.
Use an Azure subscription as the scale unit so that scale limits within a single
subscription don't constrain scalability. This approach applies to high-scale
application scenarios that have significant traffic volume.
Model required capacity around identified traffic patterns to make sure sufficient
capacity is provisioned at peak times to prevent service degradation. Alternatively,
optimize capacity during off-peak hours.
Measure the time required to do scale-out and scale-in operations to ensure that
the natural variations in traffic don't create an unacceptable level of service
degradation. Track the scale operation durations as an operational metric.
７ Note
When you deploy in an Azure landing zone, ensure that the landing zone
subscription is dedicated to the application to provide a clear management
boundary and to avoid the Noisy Neighbor antipattern.

Global distribution
It's impossible to avoid failure in any highly distributed environment. This section
provides strategies to mitigate many fault scenarios. The application must be able to
withstand regional and zonal failures. It must be deployed in an active/active model so
that the load is distributed among all regions.
Watch this video to get an overview of how to plan for failures in mission-critical
applications and maximize resiliency:
https://learn-video.azurefd.net/vod/player?id=7cea20d8-8265-4c5c-aaba5e174731c2e3&locale=en-us&embedUrl=%2Fazure%2Fwell-architected%2Fmissioncritical%2Fmission-critical-application-design

Design considerations
Redundancy. Your application must be deployed to multiple regions. Additionally,
within a region, we strongly recommend that you use availability zones to allow for
fault tolerance at the datacenter level. Availability zones have a latency perimeter
of less than 2 milliseconds between availability zones. For workloads that are
"chatty" across zones, this latency can introduce a performance penalty and incur
bandwidth charges for interzone data transfer.
Active/active model. An active/active deployment strategy is recommended
because it maximizes availability and provides a higher composite service-level
agreement (SLA). However, it can introduce challenges around data
synchronization and consistency for many application scenarios. Address the
challenges at a data platform level while considering the trade-offs of increased
cost and engineering effort.
An active/active deployment across multiple cloud providers is a way to potentially
mitigate dependency on global resources within a single cloud provider. However,
a multicloud active/active deployment strategy introduces a significant amount of
complexity around CI/CD. Also, given the differences in resource specifications and
capabilities among cloud providers, you'd need specialized deployment stamps for
each cloud.
Geographical distribution. The workload might have compliance requirements for
geographical data residency, data protection, and data retention. Consider whether
there are specific regions where data must reside or where resources need to be
deployed.

Request origin. The geographic proximity and density of users or dependent
systems should inform design decisions about global distribution.
Connectivity. How the workload is accessed by users or external systems will
influence your design. Consider whether the application is available over the public
internet or private networks that use either VPN or Azure ExpressRoute circuits.
For design recommendations and configuration choices at the platform level, see
Application platform: Global distribution.

Loosely coupled event-driven architecture
Coupling enables interservice communication via well-defined interfaces. A loose
coupling allows an application component to operate independently. A microservices
architecture style is consistent with mission-critical requirements. It facilitates high
availability by preventing cascading failures.
For loose coupling, we strongly recommend that you incorporate event-driven design.
Asynchronous message processing through an intermediary can build resiliency.


In some scenarios, applications can combine loose and tight coupling, depending on
business objectives.

Design considerations
Runtime dependencies. Loosely coupled services shouldn't be constrained to use
the same compute platform, programming language, runtime, or operating
system.

Scaling. Services should be able to scale independently. Optimize the use of
infrastructure and platform resources.
Fault tolerance. Failures should be handled separately and shouldn’t affect client
transactions.
Transactional integrity. Consider the effect of data creation and persistence that
happens in separate services.
Distributed tracing. End-to-end tracing might require complex orchestration.

Design recommendations
Align microservice boundaries with critical user flows.
Use event-driven asynchronous communication where possible to support
sustainable scale and optimal performance.
Use patterns like Outbox and Transactional Session to guarantee consistency so
that every message is processed correctly.

Example: Event-driven approach
The Mission-Critical Online

reference implementation uses microservices to process a

single business transaction. It applies write operations asynchronously with a message
broker and worker. Read operations are synchronous, with the result directly returned to
the caller.



Resiliency patterns and error handling in
application code
A mission-critical application must be designed to be resilient so that it addresses as
many failure scenarios as possible. This resiliency maximizes service availability and
reliability. The application should have self-healing capabilities, which you can
implement by using design patterns like Retries with Backoff and Circuit Breaker.
For non-transient failures that you can't fully mitigate in application logic, the health
model and operational wrappers need to take corrective action. Application code must
incorporate proper instrumentation and logging to inform the health model and
facilitate subsequent troubleshooting or root cause analysis as required. You need to
implement distributed tracing to provide the caller with a comprehensive error message
that includes a correlation ID when a failure occurs.
Tools like Application Insights can help you query, correlate, and visualize application
traces.

Design considerations
Proper configurations. It's not uncommon for transient problems to cause
cascading failures. For example, retry without appropriate back-off exacerbates the

problem when a service is being throttled. You can space retry delays linearly or
increase them exponentially to back off through growing delays.
Health endpoints. You can expose functional checks within application code by
using health endpoints that external solutions can poll to retrieve application
component health status.

Design recommendations
Here are some common software engineering patterns for resilient applications:
Pattern

Summary

QueueBased
Load

Introduces a buffer between consumers and requested resources to ensure
consistent load levels. As consumer requests are queued, a worker process handles
them against the requested resource at a pace that's set by the worker and by the

Leveling

requested resource's ability to process the requests. If consumers expect replies to
their requests, you need to implement a separate response mechanism. Apply a
prioritized order so that the most important activities are performed first.

Circuit
Breaker

Provides stability by either waiting for recovery or quickly rejecting requests rather
than blocking while waiting for an unavailable remote service or resource. This
pattern also handles faults that might take a variable amount of time to recover
from when a connection is made to a remote service or resource.

Bulkhead

Attempts to partition service instances into groups based on load and availability
requirements, isolating failures to sustain service functionality.

Saga

Manages data consistency across microservices that have independent datastores
by ensuring that services update each other through defined event or message
channels. Each service performs local transactions to update its own state and
publishes an event to trigger the next local transaction in the saga. If a service
update fails, the saga runs compensating transactions to counteract preceding
service update steps. Individual service update steps can themselves implement
resiliency patterns, such as retry.

Health
Endpoint
Monitoring

Implements functional checks in an application that external tools can access
through exposed endpoints at regular intervals. You can interpret responses from
the endpoints by using key operational metrics to inform application health and
trigger operational responses, like raising an alert or performing a compensating
rollback deployment.

Pattern

Summary

Retry

Handles transient failures elegantly and transparently.
- Cancel if the fault is unlikely to be transient and is unlikely to succeed if the
operation is attempted again.
- Retry if the fault is unusual or rare and the operation is likely to succeed if
attempted again immediately.
- Retry after a delay if the fault is caused by a condition that might need a short
time to recover, like network connectivity or high-load failures. Apply a suitable
back-off strategy as retry delays increase.

Throttling

Controls the consumption of resources used by application components, protecting
them from becoming over-encumbered. When a resource reaches a load threshold,
it defers lower-priority operations and degrading non-essential functionality so that
essential functionality can continue until sufficient resources are available to return
to normal operation.

Here are some additional recommendations:
Use vendor-provided SDKs, like the Azure SDKs, to connect to dependent services.
Use built-in resiliency capabilities instead of implementing custom functionality.
Apply a suitable back-off strategy when retrying failed dependency calls to avoid a
self-inflicted DDoS scenario.
Define common engineering criteria for all application microservice teams to drive
consistency and speed in the use of application-level resiliency patterns.
Implement resiliency patterns by using proven standardized packages, like Polly
for C# or Sentinel

for Java.

Use correlation IDs for all trace events and log messages to link them to a given
request. Return correlation IDs to the caller for all calls, not just failed requests.
Use structured logging for all log messages. Select a unified operational data sink
for application traces, metrics, and logs to enable operators to easily debug
problems. For more information, see Collect, aggregate, and store monitoring data
for cloud applications.
Ensure that operational data is used together with business requirements to inform
an application health model.

Programming language selection
It's important to select the right programming languages and frameworks. These
decisions are often driven by the skill sets or standardized technologies in the

organization. However, it's essential to evaluate the performance, resilience, and overall
capabilities of various languages and frameworks.

Design considerations
Development kit capabilities. There are differences in the capabilities that are
offered by Azure service SDKs in various languages. These differences might
influence your choice of an Azure service or programming language. For example,
if Azure Cosmos DB is a feasible option, Go might not be an appropriate
development language because there's no first-party SDK.
Feature updates. Consider how often the SDK is updated with new features for the
selected language. Commonly used SDKs, like .NET and Java libraries, are updated
frequently. Other SDKs or SDKs for other languages might be updated less
frequently.
Multiple programming languages or frameworks. You can use multiple
technologies to support various composite workloads. However, you should avoid
sprawl because it introduces management complexity and operational challenges.
Compute option. Legacy or proprietary software might not run in PaaS services.
Also, you might not be able to include legacy or proprietary software in containers.

Design recommendations
Evaluate all relevant Azure SDKs for the capabilities you need and your chosen
programming languages. Verify alignment with non-functional requirements.
Optimize the selection of programming languages and frameworks at the
microservice level. Use multiple technologies as appropriate.
Prioritize the .NET SDK to optimize reliability and performance. .NET Azure SDKs
typically provide more capabilities and are updated frequently.

Next step
Review the considerations for the application platform.
Application platform

Application platform considerations for
mission-critical workloads on Azure
Article • 02/01/2023

Azure provides many compute services for hosting highly available applications. The
services differ in capability and complexity. We recommend that you choose services
based on:
Non-functional requirements for reliability, availability, performance, and security.
Decision factors like scalability, cost, operability, and complexity.
The choice of an application hosting platform is a critical decision that affects all other
design areas. For example, legacy or proprietary development software might not run in
PaaS services or containerized applications. This limitation would influence your choice
of compute platform.
A mission-critical application can use more than one compute service to support
multiple composite workloads and microservices, each with distinct requirements.
This design area provides recommendations related to compute selection, design, and
configuration options. We also recommend that you familiarize yourself with the
Compute decision tree.
） Important
This article is part of the Azure Well-Architected Framework mission-critical
workload series. If you aren't familiar with this series, we recommend that you start
with What is a mission-critical workload?.

Global distribution of platform resources
A typical pattern for a mission-critical workload includes global resources and regional
resources.
Azure services, which aren't constrained to a particular Azure region, are deployed or
configured as global resources. Some use cases include distributing traffic across
multiple regions, storing permanent state for a whole application, and caching global
static data. If you need to accommodate both a scale-unit architecture and global
distribution, consider how resources are optimally distributed or replicated across Azure
regions.

Other resources are deployed regionally. These resources, which are deployed as part of
a deployment stamp, typically correspond to a scale unit. However, a region can have
more than one stamp, and a stamp can have more than one unit. The reliability of
regional resources is crucial because they're responsible for running the main workload.
The following image shows the high-level design. A user accesses the application via a
central global entry point that then redirects requests to a suitable regional deployment
stamp:

The mission-critical design methodology requires a multi-region deployment. This
model ensures regional fault tolerance, so that the application remains available even
when an entire region goes down. When you design a multi-region application, consider
different deployment strategies, like active/active and active/passive, together with
application requirements, because there are significant trade-offs for each approach. For
mission-critical workloads, we strongly recommend the active/active model.
Not every workload supports or requires running multiple regions simultaneously. You
should weigh specific application requirements against trade-offs to determine an
optimal design decision. For certain application scenarios that have lower reliability
targets, active/passive or sharding can be suitable alternatives.
Availability zones can provide highly available regional deployments across different
datacenters within a region. Nearly all Azure services are available in either a zonal
configuration, where the service is delegated to a specific zone, or a zone-redundant
configuration, where the platform automatically ensures that the service spans across
zones and can withstand a zone outage. These configurations provide fault tolerance up
to the datacenter level.

Design considerations
Regional and zonal capabilities. Not all services and capabilities are available in
every Azure region. This consideration could affect the regions you choose. Also,
availability zones aren't available in every region.
Regional pairs. Azure regions are grouped into regional pairs that consist of two
regions in a single geography. Some Azure services use paired regions to ensure
business continuity and to provide a level of protection against data loss. For
example, Azure geo-redundant storage (GRS) replicates data to a secondary paired
region automatically, ensuring that data is durable if the primary region isn't
recoverable. If an outage affects multiple Azure regions, at least one region in each
pair is prioritized for recovery.
Data consistency. For consistency challenges, consider using a globally distributed
data store, a stamped regional architecture, and a partially active/active
deployment. In a partial deployment, some components are active across all
regions while others are located centrally within the primary region.
Safe deployment. The Azure safe deployment practice (SDP) framework

ensures

that all code and configuration changes (planned maintenance) to the Azure
platform undergo a phased rollout. Health is analyzed for degradation during the
release. After canary and pilot phases complete successfully, platform updates are
serialized across regional pairs, so only one region in each pair is updated at a
given time.
Platform capacity. Like any cloud provider, Azure has finite resources.
Unavailability can be the result of capacity limitations in regions. If there's a
regional outage, there's an increase in demand for resources as the workload
attempts to recover within the paired region. The outage might create a capacity
problem, where supply temporarily doesn't meet demand.

Design recommendations
Deploy your solution in at least two Azure regions to help protect against regional
outages. Deploy it in regions that have the capabilities and characteristics that the
workload requires. The capabilities should meet performance and availability
targets while fulfilling data residency and retention requirements.
For example, some data compliance requirements might constrain the number of
available regions and potentially force design compromises. In such cases, we
strongly recommend that you add extra investment in operational wrappers to

predict, detect, and respond to failures. Suppose you're constrained to a
geography with two regions, and only one of those regions supports availability
zones (3 + 1 datacenter model). Create a secondary deployment pattern using
fault domain isolation to allow both regions to be deployed in an active
configuration, and ensure that the primary region houses multiple deployment
stamps.
If suitable Azure regions don't all offer capabilities that you need, be prepared to
compromise on the consistency of regional deployment stamps to prioritize
geographical distribution and maximize reliability. If only a single Azure region is
suitable, deploy multiple deployment stamps (regional scale units) in the selected
region to mitigate some risk, and use availability zones to provide datacenter-level
fault tolerance. However, such a significant compromise in geographical
distribution dramatically constrains the attainable composite SLA and overall
reliability.
） Important
For scenarios that target an SLO that's greater than or equal to 99.99%, we
recommend a minimum of three deployment regions to maximize the
composite SLA and overall reliability. Calculate the composite SLA for all user
flows. Ensure that the composite SLA is aligned with business targets.
For high-scale application scenarios that have significant volumes of traffic, design
the solution to scale across multiple regions to navigate potential capacity
constraints within a single region. Additional regional deployment stamps will
achieve a higher composite SLA. Using global resources constrains the increase in
composite SLA that you achieve by adding more regions.
Define and validate your recovery point objectives (RPO) and recovery time
objectives (RTO).
Within a single geography, prioritize the use of regional pairs to benefit from SDP
serialized rollouts for planned maintenance and regional prioritization for
unplanned maintenance.
Geographically colocate Azure resources with users to minimize network latency
and maximize end-to-end performance.
You can also use solutions like a Content Delivery Network (CDN) or edge
caching to drive optimal network latency for distributed user bases. For more
information, see Global traffic routing, Application delivery services, and
Caching and static content delivery.

Align current service availability with product roadmaps when you choose
deployment regions. Some services might not be immediately available in every
region.

Containerization
A container includes application code and the related configuration files, libraries, and
dependencies that the application needs to run. Containerization provides an
abstraction layer for application code and its dependencies and creates separation from
the underlying hosting platform. The single software package is highly portable and can
run consistently across various infrastructure platforms and cloud providers. Developers
don't need to rewrite code and can deploy applications faster and more reliably.
） Important
We recommend that you use containers for mission-critical application packages.
They improve infrastructure utilization because you can host multiple containers on
the same virtualized infrastructure. Also, because all software is included in the
container, you can move the application across various operating systems,
regardless of runtimes or library versions. Management is also easier with
containers than it is with traditional virtualized hosting.
Mission-critical applications need to scale fast to avoid performance bottlenecks.
Because container images are pre-built, you can limit startup to occur only during
bootstrapping of the application, which provides rapid scalability.

Design considerations
Monitoring. It can be difficult for monitoring services to access applications that
are in containers. You typically need third-party software to collect and store
container state indicators like CPU or RAM usage.
Security. The hosting platform OS kernel is shared across multiple containers,
creating a single point of attack. However, the risk of host VM access is limited
because containers are isolated from the underlying operating system.
State. Although it's possible to store data in a running container's file system, the
data won't persist when the container is re-created. Instead, persist data by
mounting external storage or using an external database.

Design recommendations
Containerize all application components. Use container images as the primary
model for application deployment packages.
Prioritize Linux-based container runtimes when possible. The images are more
lightweight, and new features for Linux nodes/containers are released frequently.
Make containers immutable and replaceable, with short lifecycles.
Be sure to gather all relevant logs and metrics from the container, container host,
and underlying cluster. Send the gathered logs and metrics to a unified data sink
for further processing and analysis.
Store container images in Azure Container Registry

. Use geo-replication to

replicate container images across all regions. Enable Microsoft Defender for
container registries to provide vulnerability scanning for container images. Make
sure access to the registry is managed by Azure Active Directory (Azure AD).

Container hosting and orchestration
Several Azure application platforms can effectively host containers. There are
advantages and disadvantages associated with each of these platforms. Compare the
options in the context of your business requirements. However, always optimize
reliability, scalability, and performance. For more information, see these articles:
Compute decision tree
Container option comparisons
） Important
Azure Kubernetes Service (AKS)

should be your first choice of orchestrator when

it meets your requirements. Azure Container Apps is another option. Although
Azure App Service

isn't an orchestrator, as a low-friction container platform, it's

still a feasible alternative to AKS.

Design considerations and recommendations for Azure Kubernetes
Service
AKS, a managed Kubernetes service, enables quick cluster provisioning without
requiring complex cluster administration activities and offers a feature set that includes

advanced networking and identity capabilities. For a complete set of recommendations,
see Azure Well-Architected Framework review - AKS.
） Important
There are some foundational configuration decisions that you can't change without
re-deploying the AKS cluster. Examples include the choice between public and
private AKS clusters, enabling Azure Network Policy, Azure AD integration, and the
use of managed identities for AKS instead of service principals.

Reliability
AKS manages the native Kubernetes control plane. If the control plane isn't available, the
workload experiences downtime. Take advantage of the reliability features offered by
AKS:
Deploy AKS clusters across different Azure regions as a scale unit to maximize
reliability and availability. Use availability zones to maximize resilience within an
Azure region by distributing AKS control plane and agent nodes across physically
separate datacenters. However, if colocation latency is a problem, you can do AKS
deployment within a single zone or use proximity placement groups to minimize
internode latency.
Use the AKS Uptime SLA for production clusters to maximize Kubernetes API
endpoint availability guarantees.

Scalability
Take into account AKS scale limits, like the number of nodes, node pools per cluster, and
clusters per subscription.
If scale limits are a constraint, take advantage of the scale-unit strategy, and
deploy more units with clusters.
Enable cluster autoscaler to automatically adjust the number of agent nodes in
response to resource constraints.
Use the horizontal pod autoscaler to adjust the number of pods in a deployment
based on CPU utilization or other metrics.
For high scale and burst scenarios, consider using virtual nodes for extensive and
rapid scale.

Define pod resource requests and limits in application deployment manifests. If
you don't, you might experience performance problems.

Isolation
Maintain boundaries between the infrastructure used by the workload and system tools.
Sharing infrastructure might lead to high-resource utilization and noisy neighbor
scenarios.
Use separate node pools for system and workload services. Dedicated node pools
for workload components should be based on requirements for specialized
infrastructure resources like high-memory GPU VMs. In general, to reduce
unnecessary management overhead, avoid deploying large numbers of node
pools.
Use taints and tolerations to provide dedicated nodes and limit resource-intensive
applications.
Evaluate application affinity and anti-affinity requirements and configure the
appropriate colocation of containers on nodes.

Security
Default vanilla Kubernetes requires significant configuration to ensure a suitable security
posture for mission-critical scenarios. AKS addresses various security risks out of the
box. Features include private clusters, auditing and logging into Log Analytics, hardened
node images, and managed identities.
Apply configuration guidance provided in the AKS security baseline.
Use AKS features for handling cluster identity and access management to reduce
operational overhead and apply consistent access management.
Use managed identities instead of service principals to avoid management and
rotation of credentials. You can add managed identities at the cluster level. At the
pod level, you can use managed identities via Azure AD workload identity.
Use Azure AD integration for centralized account management and passwords,
application access management, and enhanced identity protection. Use
Kubernetes RBAC with Azure AD for least privilege, and minimize granting
administrator privileges to help protect configuration and secrets access. Also, limit
access to the Kubernetes cluster configuration file by using Azure role-based

access control. Limit access to actions that containers can perform, provide the
least number of permissions, and avoid the use of root privilege escalation.

Upgrades
Clusters and nodes need to be upgraded regularly. AKS supports Kubernetes versions in
alignment with the release cycle of native Kubernetes.
Subscribe to the public AKS Roadmap

and Release Notes

on GitHub to stay

up-to-date on upcoming changes, improvements, and, most importantly,
Kubernetes version releases and deprecations.
Apply the guidance provided in the AKS checklist

to ensure alignment with best

practices.
Be aware of the various methods supported by AKS for updating nodes and/or
clusters. These methods can be manual or automated. You can use Planned
Maintenance to define maintenance windows for these operations. New images
are released weekly. AKS also supports auto-upgrade channels for automatically
upgrading AKS clusters to newer versions of Kubernetes and/or newer node
images when they're available.

Networking
Evaluate the network plugins that best fit your use case. Determine whether you need
granular control of traffic between pods. Azure supports kubenet, Azure CNI, and bring
your own CNI for specific use cases.
Prioritize the use of Azure CNI after assessing network requirements and the size of the
cluster. Azure CNI enables the use of Azure or Calico network policies for controlling
traffic within the cluster.

Monitoring
Your monitoring tools should be able to capture logs and metrics from running pods.
You should also gather information from the Kubernetes Metrics API to monitor the
health of running resources and workloads.
Use Azure Monitor and Application Insights to collect metrics, logs, and
diagnostics from AKS resources for troubleshooting.
Enable and review Kubernetes resource logs.

Configure Prometheus metrics in Azure Monitor. Container insights in Monitor
provides onboarding, enables monitoring capabilities out of the box, and enables
more advanced capabilities via built-in Prometheus support.

Governance
Use policies to apply centralized safeguards to AKS clusters in a consistent way. Apply
policy assignments at a subscription scope or higher to drive consistency across
development teams.
Control which functions are granted to pods, and whether running contradicts
policy, by using Azure Policy. This access is defined through built-in policies
provided by the Azure Policy Add-on for AKS.
Establish a consistent reliability and security baseline for AKS cluster and pod
configurations by using Azure Policy.
Use the Azure Policy Add-on for AKS to control pod functions, like root privileges,
and to disallow pods that don't conform to policy.
７ Note
When you deploy into an Azure landing zone, the Azure policies to help you ensure
consistent reliability and security should be provided by the landing zone
implementation.
The mission-critical reference implementations provide a suite of baseline policies
to drive recommended reliability and security configurations.

Design considerations and recommendations for Azure App Service
For web and API-based workload scenarios, App Service

might be a feasible

alternative to AKS. It provides a low-friction container platform without the complexity
of Kubernetes. For a complete set of recommendations, see Reliability considerations for
App Service and Operational excellence for App Service.

Reliability
Evaluate the use of TCP and SNAT ports. TCP connections are used for all outbound
connections. SNAT ports are used for outbound connections to public IP addresses.
SNAT port exhaustion is a common failure scenario. You should predictively detect this
problem by load testing while using Azure Diagnostics to monitor ports. If SNAT errors

occur, you need to either scale across more or larger workers or implement coding
practices to help preserve and reuse SNAT ports. Examples of coding practices that you
can use include connection pooling and the lazy loading of resources.
TCP port exhaustion is another failure scenario. It occurs when the sum of outbound
connections from a given worker exceeds capacity. The number of available TCP ports
depends on the size of the worker. For recommendations, see TCP and SNAT ports.

Scalability
Plan for future scalability requirements and application growth so that you can apply
appropriate recommendations from the start. By doing so, you can avoid technical
migration debt as the solution grows.
Enable autoscale to ensure that adequate resources are available to service
requests. Evaluate per-app scaling for high-density hosting on App Service.
Be aware that App Service has a default, soft limit of instances per App Service
plan.
Apply autoscale rules. An App Service plan scales out if any rule within the profile
is met but only scales in if all rules within the profile are met. Use a scale-out and
scale-in rule combination to ensure that autoscale can take action to both scale
out and scale in. Understand the behavior of multiple scaling rules in a single
profile.
Be aware that you can enable per-app scaling at the level of the App Service plan
to allow an application to scale independently from the App Service plan that hosts
it. Apps are allocated to available nodes via a best-effort approach for an even
distribution. Although an even distribution isn't guaranteed, the platform ensures
that two instances of the same app aren't hosted on the same instance.

Monitoring
Monitor application behavior and get access to relevant logs and metrics to ensure that
your application works as expected.
You can use diagnostic logging to ingest application-level and platform-level logs
into Log Analytics, Azure Storage, or a third-party tool via Azure Event Hubs.
Application performance monitoring with Application Insights provides deep
insights into application performance.

Mission-critical applications must have the ability to self-heal if there are failures.
Enable Auto Heal

to automatically recycle unhealthy workers.

You need to use appropriate health checks to assess all critical downstream
dependencies, which helps to ensure overall health. We strongly recommend that
you enable Health Check

to identify non-responsive workers.

Deployment
To work around the default limit of instances per App Service plan, deploy App Service
plans in multiple scale units in a single region. Deploy App Service plans in an
availability zone configuration

to ensure that worker nodes are distributed across

zones within a region. Consider opening a support ticket to increase the maximum
number of workers to twice the instance count that you need to serve normal peak load.

Container registry
Container registries host images that are deployed to container runtime environments
like AKS. You need to configure your container registries for mission-critical workloads
carefully. An outage shouldn't cause delays in pulling images, especially during scaling
operations. The following considerations and recommendations focus on Azure
Container Registry and explore the trade-offs that are associated with centralized and
federated deployment models.

Design considerations
Format. Consider using a container registry that relies on the Docker-provided
format and standards for both push and pull operations. These solutions are
compatible and mostly interchangeable.
Deployment model. You can deploy the container registry as a centralized service
that's consumed by multiple applications within your organization. Or you can
deploy it as a dedicated component for a specific application workload.
Public registries. Container images are stored in Docker Hub or other public
registries that exist outside of Azure and a given virtual network. This isn't
necessarily a problem, but it can lead to various issues that are related to service
availability, throttling, and data exfiltration. For some application scenarios, you
need to replicate public container images in a private container registry to limit
egress traffic, increase availability, or avoid potential throttling.

Design recommendations
Use container registry instances that are dedicated to the application workload.
Avoid creating a dependency on a centralized service unless organizational
availability and reliability requirements are fully aligned with the application.
In the recommended core architecture pattern, container registries are global
resources that are long living. Consider using a single global container registry per
environment. For example, use a global production registry.
Ensure that the SLA for public registry is aligned with your reliability and security
targets. Take special note of throttling limits for use cases that depend on Docker
Hub.
Prioritize Azure Container Registry

for hosting container images.

Design considerations and recommendations for Azure Container
Registry
This native service provides a range of features, including geo-replication, Azure AD
authentication, automated container building, and patching via Container Registry tasks.

Reliability
Configure geo-replication to all deployment regions to remove regional dependencies
and optimize latency. Container Registry supports high availability through georeplication to multiple configured regions, providing resiliency against regional outages.
If a region becomes unavailable, the other regions continue to serve image requests.
When the region is back online, Container Registry recovers and replicates changes to it.
This capability also provides registry colocation within each configured region, reducing
network latency and cross-region data transfer costs.
In Azure regions that provide availability zone support, the Premium Container Registry
tier supports zone redundancy to provide protection against zonal failure. The Premium
tier also supports private endpoints to help prevent unauthorized access to the registry,
which can lead to reliability issues.
Host images close to the consuming compute resources, within the same Azure regions.

Image locking
Images can get deleted, as a result of, for example, manual error. Container Registry
supports locking an image version or a repository to prevent changes or deletions.

When a previously deployed image version is changed in place, same-version
deployments might provide different results before and after the change.
If you want to protect the Container Registry instance from deletion, use resource locks.

Tagged images
Tagged Container Registry images are mutable by default, which means that the same
tag can be used on multiple images pushed to the registry. In production scenarios, this
can lead to unpredictable behavior that could affect application uptime.

Identity and access management
Use Azure AD integrated authentication to push and pull images instead of relying on
access keys. For enhanced security, fully disable the use of the admin access key.

Serverless compute
Serverless computing provides resources on demand and eliminates the need to
manage infrastructure. The cloud provider automatically provisions, scales, and manages
the resources required to run deployed application code. Azure provides several
serverless compute platforms:
Azure Functions. When you use Azure Functions, application logic is implemented
as distinct blocks of code, or functions, that run in response to events, like an HTTP
request or queue message. Each function scales as necessary to meet demand.
Azure Logic Apps. Logic Apps is best suited for creating and running automated
workflows that integrate various apps, data sources, services, and systems. Like
Azure Functions, Logic Apps uses built-in triggers for event-driven processing.
However, instead of deploying application code, you can create logic apps by
using a graphical user interface that supports code blocks like conditionals and
loops.
Azure API Management . You can use API Management to publish, transform,
maintain, and monitor enhanced-security APIs by using the Consumption tier.
Power Apps and Power Automate. These tools provide a low-code or no-code
development experience, with simple workflow logic and integrations that are
configurable through connections in a user interface.
For mission-critical applications, serverless technologies provide simplified development
and operations, which can be valuable for simple business use cases. However, this

simplicity comes at the cost of flexibility in terms of scalability, reliability, and
performance, and that's not viable for most mission-critical application scenarios.
The following sections provide design considerations and recommendations for using
Azure Functions and Logic Apps as alternative platforms for non-critical workflow
scenarios.

Design considerations and recommendations for Azure Functions
Mission-critical workloads have critical and non-critical system flows. Azure Functions is
a viable choice for flows that don't have the same stringent business requirements as
critical system flows. It's well suited for event-driven flows that have short-lived
processes because functions perform distinct operations that run as fast as possible.
Choose an Azure Functions hosting option that's appropriate for the application's
reliability tier. We recommend the Premium plan because it allows you to configure
compute instance size. The Dedicated plan is the least serverless option. It provides
autoscale, but these scale operations are slower than those of the other plans. We
recommend that you use the Premium plan to maximize reliability and performance.
There are some security considerations. When you use an HTTP trigger to expose an
external endpoint, use a web application firewall (WAF) to provide a level of protection
for the HTTP endpoint from common external attack vectors.
We recommend the use of private endpoints for restricting access to private virtual
networks. They can also mitigate data exfiltration risks, like malicious admin scenarios.
You need to use code scanning tools on Azure Functions code and integrate those tools
with CI/CD pipelines.

Design considerations and recommendations for Azure Logic Apps
Like Azure Functions, Logic Apps uses built-in triggers for event-driven processing.
However, instead of deploying application code, you can create logic apps by using a
graphical user interface that supports blocks like conditionals, loops, and other
constructs.
Multiple deployment modes are available. We recommend the Standard mode to ensure
a single-tenant deployment and mitigate noisy neighbor scenarios. This mode uses the
containerized single-tenant Logic Apps runtime, which is based on Azure Functions. In
this mode, the logic app can have multiple stateful and stateless workflows. You should
be aware of the configuration limits.

Constrained migrations via IaaS
Many applications that have existing on-premises deployments use virtualization
technologies and redundant hardware to provide mission-critical levels of reliability.
Modernization is often hindered by business constraints that prevent full alignment with
the cloud-native baseline (North Star) architecture pattern that's recommended for
mission-critical workloads. That's why many applications adopt a phased approach, with
initial cloud deployments using virtualization and Azure Virtual Machines as the primary
application hosting model. The use of IaaS virtual machines might be required in certain
scenarios:
Available PaaS services don't provide the required performance or level of control.
The workload requires operating system access, specific drivers, or network and
system configurations.
The workload doesn't support running in containers.
There's no vendor support for third-party workloads.
This section focuses on the best ways to use Azure Virtual Machines and associated
services to maximize the reliability of the application platform. It highlights key aspects
of the mission-critical design methodology that transpose cloud-native and IaaS
migration scenarios.

Design considerations
The operational costs of using IaaS virtual machines are significantly higher than
the costs of using PaaS services because of the management requirements of the
virtual machines and the operating systems. Managing virtual machines
necessitates the frequent rollout of software packages and updates.
Azure provides capabilities to increase the availability of virtual machines:
Availability sets can help protect against network, disk, and power failures by
distributing virtual machines across fault domains and update domains.
Availability zones can help you achieve even higher levels of reliability by
distributing VMs across physically separated datacenters within a region.
Virtual Machine Scale Sets provide functionality for automatically scaling the
number of virtual machines in a group. They also provide capabilities for
monitoring instance health and automatically repairing unhealthy instances.

Design recommendations
） Important

Use PaaS services and containers when possible to reduce operational complexity
and cost. Use IaaS virtual machines only when you need to.
Right-size VM SKU sizes to ensure effective resource utilization.
Deploy three or more virtual machines across availability zones to achieve
datacenter-level fault tolerance.
If you're deploying commercial off-the-shelf software, consult the software
vendor and test adequately before deploying the software into production.
For workloads that can't be deployed across availability zones, use availability sets
that contain three or more VMs.
Consider availability sets only if availability zones don't meet workload
requirements, such as for chatty workloads with low latency requirements.
Prioritize the use of Virtual Machine Scale Sets for scalability and zone redundancy.
This point is particularly important for workloads that have varying loads. For
example, if the number of active users or requests per second is a varying load.
Don't access individual virtual machines directly. Use load balancers in front of
them when possible.
To protect against regional outages, deploy application virtual machines across
multiple Azure regions.
See the networking and connectivity design area for details about how to
optimally route traffic between active deployment regions.
For workloads that don't support multi-region active/active deployments, consider
implementing active/passive deployments by using hot/warm standby virtual
machines for regional failover.
Use standard images from Azure Marketplace rather than custom images that
need to be maintained.
Implement automated processes to deploy and roll out changes to virtual
machines, avoiding any manual intervention. For more information, see IaaS
considerations in the Operational procedures design area.
Implement chaos experiments to inject application faults into virtual machine
components, and observe the mitigation of faults. For more information, see
Continuous validation and testing.
Monitor virtual machines and ensure that diagnostic logs and metrics are ingested
into a unified data sink.

Implement security practices for mission-critical application scenarios, when
applicable, and the Security best practices for IaaS workloads in Azure.

Next step
Review the considerations for the data platform.
Data platform

Data platform considerations for
mission-critical workloads on Azure
Article • 02/01/2023

The selection of an effective application data platform is a further crucial decision area,
which has far-reaching implications across other design areas. Azure ultimately offers a
multitude of relational, non-relational, and analytical data platforms, which differ greatly
in capability. It's therefore essential that key non-functional requirements be fully
considered alongside other decision factors such as consistency, operability, cost, and
complexity. For example, the ability to operate in a multi-region write configuration will
have a critical bearing on suitability for a globally available platform.
This design area expands on application design, providing key considerations and
recommendations to inform the selection of an optimal data platform.
） Important
This article is part of the Azure Well-Architected mission-critical workload series. If
you aren't familiar with this series, we recommend you start with what is a missioncritical workload?

The Four Vs of Big Data
The 'Four Vs of Big Data' provide a framework to better understand requisite
characteristics for a highly available data platform, and how data can be used to
maximize business value. This section will therefore explore how the Volume, Velocity,
Variety, and Veracity characteristics can be applied at a conceptual level to help design a
data platform using appropriate data technologies.
Volume: how much data is coming in to inform storage capacity and tiering
requirements -that is the size of the dataset.
Velocity: the speed at which data is processed, either as batches or continuous
streams -that is the rate of flow.
Variety: the organization and format of data, capturing structured, semi-structured,
and unstructured formats -that is data across multiple stores or types.
Veracity: includes the provenance and curation of considered data sets for
governance and data quality assurance -that is accuracy of the data.

Design Considerations
Volume
Existing (if any) and expected future data volumes based on forecasted data
growth rates aligned with business objectives and plans.
Data volume should encompass the data itself and indexes, logs, telemetry, and
other applicable datasets.
Large business-critical and mission-critical applications typically generate and
store high volumes (GB and TB) on a daily basis.
There can be significant cost implications associated with data expansion.
Data volume may fluctuate due to changing business circumstances or
housekeeping procedures.
Data volume can have a profound impact on data platform query performance.
There can be a profound impact associated with reaching data platform volume
limits.
Will it result in downtime? and if so, for how long?
What are the mitigation procedures? and will mitigation require application
changes?
Will there be a risk of data loss?
Features such as Time to Live (TTL) can be used to manage data growth by
automatically deleting records after an elapsed time, using either record creation
or modification.
For example, Azure Cosmos DB provides an in-built TTL capability.
Velocity
The speed with which data is emitted from various application components, and
the throughput requirements for how fast data needs to be committed and
retrieved are critical to determining an optimal data technology for key workload
scenarios.
The nature of throughput requirements will differ by workload scenario, such as
those that are read-heavy or write-heavy.
For example, analytical workloads will typically need to cater to a large read
throughput.
What is the required throughput? And how is throughput expected to grow?
What are the data latency requirements at P50/P99 under reference load levels?
Capabilities such as supporting a lock-free design, index tuning, and consistency
policies are critical to achieving high-throughput.

Optimizing configuration for high throughput incurs trade-offs, which should be
fully understood.
Load-levelling persistence and messaging patterns, such as CQRS and Event
Sourcing, can be used to further optimize throughput.
Load levels will naturally fluctuate for many application scenarios, with natural
peaks requiring a sufficient degree of elasticity to handle variable demand while
maintaining throughput and latency.
Agile scalability is key to effectively support variable throughput and load levels
without overprovisioning capacity levels.
Both read and write throughput must scale according to application
requirements and load.
Both vertical and horizontal scale operations can be applied to respond to
changing load levels.
The impact of throughput dips can vary based on workload scenario.
Will there be connectivity disruption?
Will individual operations return failure codes while the control plane continues to
operate?
Will the data platform activate throttling, and if so for how long?
The fundamental application design recommendation to use an active-active
geographical distribution introduces challenges around data consistency.
There's a trade-off between consistency and performance with regard to full
ACID transactional semantics and traditional locking behavior.
Minimizing write latency will come at the cost of data consistency.
In a multi-region write configuration, changes will need to be synchronized and
merged between all replicas, with conflict resolution where required, and this may
impact performance levels and scalability.
Read-only replicas (intra-region and inter-region) can be used to minimize
roundtrip latency and distributing traffic to boost performance, throughput,
availability, and scalability.
A caching layer can be used to increase read throughput to improve user
experience and end-to-end client response times.
Cache expiration times and policies need to be considered to optimize data
recentness.
Variety

The data model, data types, data relationships, and intended query model will
strongly affect data platform decisions.
Does the application require a relational data model or can it support a variableschema or non-relational data model?
How will the application query data? And will queries depend on database-layer
concepts such as relational joins? Or does the application provide such semantics?
The nature of datasets considered by the application can be varied, from
unstructured content such as images and videos, or more structured files such as
CSV and Parquet.
Composite application workloads will typically have distinct datasets and
associated requirements.
In addition to relational or non-relational data platforms, graph or key-value data
platforms may also be suitable for certain data workloads.
Some technologies cater to variable-schema data models, where data items are
semantically similar and/or stored and queried together but differ structurally.
In a microservice architecture, individual application services can be built with
distinct scenario-optimized datastores rather than depending on a single
monolithic datastore.
Design patterns such as SAGA can be applied to manage consistency and
dependencies between different datastores.
Inter-database direct queries can impose co-location constraints.
The use of multiple data technologies will add a degree of management
overhead to maintain encompassed technologies.
The feature-sets for each Azure service differ across languages, SDKs, and APIs,
which can greatly impact the level of configuration tuning that can be applied.
Capabilities for optimized alignment with the data model and encompassed data
types will strongly influence data platform decisions.
Query layers such as stored procedures and object-relational mappers.
Language-neutral query capability, such as a secured REST API layer.
Business continuity capabilities, such as backup and restore.
Analytical datastores typically support polyglot storage for various types of data
structures.
Analytical runtime environments, such as Apache Spark, may have integration
restrictions to analyze polyglot data structures.
In an enterprise context, the use of existing processes and tooling, and the
continuity of skills, can have a significant bearing on the data platform design and

selection of data technologies.
Veracity
Several factors must be considered to validate the accuracy of data within an
application, and the management of these factors can have a significant bearing
on the design of the data platform.
Data consistency.
Platform security features.
Data governance.
Change management and schema evolution.
Dependencies between datasets.
In any distributed application with multiple data replicas there's a trade-off
between consistency and latency, as expressed in the CAP

and PACELC

theorems.
When readers and writers are distinctly distributed, an application must choose
to return either the fastest-available version of a data item, even if it out of date
compared to a just-completed write (update) of that data item in another
replica, or the most up-to-date version of the data item, which may incur
additional latency to determine and obtain the latest state.
Consistency and availability can be configured at platform level or at individual
data request level.
What is the user experience if data was to be served from a replica closest to the
user which doesn't reflect the most recent state of a different replica? i.e. can the
application support possibly serving out-of-date data?
In a multi-region write context, when the same data item is changed in two
separate write-replicas before either change can be replicated, a conflict is created
which must be resolved.
Standardized conflict resolution policies, such as "Last Write Wins", or a custom
strategy with custom logic can be applied.
The implementation of security requirements may adversely impact throughput or
performance.
Encryption at-rest can be implemented in the application layer using client-side
encryption and/or the data layer using server-side encryption if necessary.
Azure supports various encryption models, including server-side encryption that
uses service-managed keys, customer-managed keys in Key Vault, or customermanaged keys on customer-controlled hardware.

With client-side encryption, keys can be managed in Key Vault or another secure
location.
MACsec (IEEE 802.1AE MAC) data-link encryption is used to secure all traffic
moving between Azure datacenters on the Microsoft backbone network.
Packets are encrypted and decrypted on the devices before being sent,
preventing physical 'man-in-the-middle' or snooping/wiretapping attacks.
Authentication and authorization to the data plane and control plane.
How will the data platform authenticate and authorize application access and
operational access?
Observability through monitoring platform health and data access.
How will alerting be applied for conditions outside acceptable operational
boundaries?

Design Recommendations
Volume
Ensure future data volumes associated with organic growth aren't expected to
exceed data platform capabilities.
Forecast data growth rates aligned to business plans and use established rates
to inform ongoing capacity requirements.
Compare aggregate and per-data record volumes against data platform limits.
If there's a risk of limits being reached in exceptional circumstances, ensure
operational mitigations are in place to prevent downtime and data loss.
Monitor data volume and validate it against a capacity model, considering scale
limits and expected data growth rates.
Ensure scale operations align with storage, performance, and consistency
requirements.
When a new scale-unit's introduced, underlying data may need to be replicated
which will take time and likely introduce a performance penalty while replication
occurs. So ensure these operations are performed outside of critical business
hours if possible.
Define application data tiers to classify datasets based on usage and criticality to
facilitate the removal or offloading of older data.
Consider classifying datasets into 'hot', 'warm', and 'cold' ('archive') tiers.
For example, the foundational reference implementations use Azure Cosmos
DB to store 'hot' data that is actively used by the application, while Azure
Storage is used for 'cold' operations data for analytical purposes.

Configure housekeeping procedures to optimize data growth and drive data
efficiencies, such as query performance, and managing data expansion.
Configure Time-To-Live (TTL) expiration for data that is no-longer required and
has no long-term analytical value.
Validate that old data can be safely tiered to secondary storage, or deleted
outright, without an adverse impact to the application.
Offload non-critical data to secondary cold storage, but maintain it for analytical
value and to satisfy audit requirements.
Collect data platform telemetry and usage statistics to enable DevOps teams to
continually evaluate housekeeping requirements and 'right-size' datastores.
In-line with a microservice application design, consider the use of multiple
different data technologies in-parallel, with optimized data solutions for specific
workload scenarios and volume requirements.
Avoid creating a single monolithic datastore where data volume from expansion
can be hard to manage.
Velocity
The data platform must inherently be designed and configured to support highthroughput, with workloads separated into different contexts to maximize
performance using scenario optimized data solutions.
Ensure read and write throughput for each data scenario can scale according to
expected load patterns, with sufficient tolerance for unexpected variance.
Separate different data workloads, such as transactional and analytical
operations, into distinct performance contexts.
Load-level through the use of asynchronous non-blocking messaging, for example
using the CQRS or Event Sourcing patterns.
There might be latency between write requests and when new data becomes
available to read, which may have an impact on the user experience.
This impact must be understood and acceptable in the context of key
business requirements.
Ensure agile scalability to support variable throughput and load levels.
If load levels are highly volatile, consider overprovisioning capacity levels to
ensure throughput and performance is maintained.
Test and validate the impact to composite application workloads when
throughput can't be maintained.
Prioritize Azure-native data services with automated scale-operations to facilitate a
swift response to load-level volatility.
Configure autoscaling based on service-internal and application-set thresholds.

Scaling should initiate and complete in timeframes consistent with business
requirements.
For scenarios where manual interaction is necessary, establish automated
operational 'playbooks' that can be triggered rather than conducting manual
operational actions.
Consider whether automated triggers can be applied as part of subsequent
engineering investments.
Monitor application data read and write throughput against P50/P99 latency
requirements and align to an application capacity model.
Excess throughput should be gracefully handled by the data platform or
application layer and captured by the health model for operational representation.
Implement caching for 'hot' data scenarios to minimize response times.
Apply appropriate policies for cache expiration and house-keeping to avoid
runaway data growth.
Expire cache items when the backing data changes.
If cache expiration is strictly Time-To-Live (TTL) based, the impact and
customer experience of serving outdated data needs to be understood.
Variety
In alignment with the principle of a cloud- and Azure-native design, it's highly
recommended to prioritize managed Azure services to reduce operational and
management complexity, and taking advantage of Microsoft's future platform
investments.
In alignment with the application design principle of loosely coupled microservice
architectures, allow individual services to use distinct data stores and scenariooptimized data technologies.
Identify the types of data structure the application will handle for specific
workload scenarios.
Avoid creating a dependency on a single monolithic datastore.
Consider the SAGA design pattern where dependencies between datastores
exist.
Validate that required capabilities are available for selected data technologies.
Ensure support for required languages and SDK capabilities. Not every
capability is available for every language/SDK in the same fashion.
Veracity

Adopt a multi-region data platform design and distribute replicas across regions
for maximum reliability, availability, and performance by moving data closer to
application endpoints.
Distribute data replicas across Availability Zones (AZs) within a region (or use
zone-redundant service tiers) to maximize intra-region availability.
Where consistency requirements allow for it, use a multi-region write data platform
design to maximize overall global availability and reliability.
Consider business requirements for conflict resolution when the same data item
is changed in two separate write replicas before either change can be replicated
and thus creating a conflict.
Use standardized conflict resolution policies such as "Last one wins" where
possible
If a custom strategy with custom logic is required, ensure CI/CD DevOps
practices are applied to manage custom logic.
Test and validate backup and restore capabilities, and failover operations through
chaos testing within continuous delivery processes.
Run performance benchmarks to ensure throughput and performance
requirements aren't impacted by the inclusion of required security capabilities,
such as encryption.
Ensure continuous delivery processes consider load testing against known
performance benchmarks.
When applying encryption, it's strongly recommended to use service-managed
encryption keys as a way of reducing management complexity.
If there's a specific security requirement for customer-managed keys, ensure
appropriate key management procedures are applied to ensure availability,
backup, and rotation of all considered keys.
７ Note
When integrating with a broader organizational implementation, it's critical that an
application centric approach be applied for the provisioning and operation of data
platform components in an application design.
More specifically, to maximize reliability it's critical that individual data platform
components appropriately respond to application health through operational
actions which may include other application components. For example, in a
scenario where additional data platform resources are needed, scaling the data
platform along with other application components according to a capacity model

will likely be required, potentially through the provision of additional scale units.
This approach will ultimately be constrained if there's a hard dependency of a
centralized operations team to address issues related to the data platform in
isolation.
Ultimately, the use of centralized data services (that is Central IT DBaaS) introduces
operational bottlenecks that significantly hinder agility through a largely
uncontextualized management experience, and should be avoided in a missioncritical or business-critical context.

Additional references
Additional data-platform guidance is available within the Azure Application Architecture
Guide.
Azure Data Store Decision Tree
Criteria for choosing a Data Store
Non-Relational Data Stores
Relational OLTP Data Stores

Globally distributed multi-region write
datastore
To fully accommodate the globally distributed active-active aspirations of an application
design, it's strongly recommended to consider a distributed multi-region write data
platform, where changes to separate writeable replicas are synchronized and merged
between all replicas, with conflict resolution where required.
） Important
The microservices may not all require a distributed multi-region write datastore, so
consideration should be given to the architectural context and business
requirements of each workload scenario.
Azure Cosmos DB provides a globally distributed and highly available NoSQL datastore,
offering multi-region writes and tunable consistency out-of-the-box. The design
considerations and recommendations within this section will therefore focus on optimal
Azure Cosmos DB usage.

Design considerations
Azure Cosmos DB
Azure Cosmos DB stores data within Containers, which are indexed, row-based
transactional stores designed to allow fast transactional reads and writes with
response times on the order of milliseconds.
Azure Cosmos DB supports multiple different APIs with differing feature sets, such
as SQL, Cassandra, and MongoDB.
The first-party Azure Cosmos DB for NoSQL provides the richest feature set and
is typically the API where new capabilities will become available first.
Azure Cosmos DB supports Gateway and Direct connectivity modes, where Direct
facilitates connectivity over TCP to backend Azure Cosmos DB replica nodes for
improved performance with fewer network hops, while Gateway provides HTTPS
connectivity to frontend gateway nodes.
Direct mode is only available when using the Azure Cosmos DB for NoSQL and
is currently only supported on .NET and Java SDK platforms.
Within Availability Zone enabled regions, Azure Cosmos DB offers Availability Zone
(AZ) redundancy support for high availability and resiliency to zonal failures within
a region.
Azure Cosmos DB maintains four replicas of data within a single region, and when
Availability Zone (AZ) redundancy is enabled, Azure Cosmos DB ensures data
replicas are placed across multiple AZs to protect against zonal failures.
The Paxos consensus protocol is applied to achieve quorum across replicas
within a region.
An Azure Cosmos DB account can easily be configured to replicate data across
multiple regions to mitigate the risk of a single region becoming unavailable.
Replication can be configured with either single-region writes or multi-region
writes.
With single region writes, a primary 'hub' region is used to serve all writes
and if this 'hub' region becomes unavailable, a failover operation must occur
to promote another region as writable.
With multi-region writes, applications can write to any configured
deployment region, which will replicate changes between all other regions. If
a region is unavailable then the remaining regions will be used to serve write
traffic.

In a multi-region write configuration, update (insert, replace, delete) conflicts can
occur where writers concurrently update the same item in multiple regions.
Azure Cosmos DB provides two conflict resolution policies, which can be applied to
automatically address conflicts.
Last Write Wins (LWW) applies a time-synchronization clock protocol using a
system-defined timestamp _ts property as the conflict resolution path. If of a
conflict the item with the highest value for the conflict resolution path becomes
the winner, and if multiple items have the same numeric value then the system
selects a winner so that all regions can converge to the same version of the
committed item.
With delete conflicts, the deleted version always wins over either insert or
replace conflicts regardless of conflict resolution path value.
Last Write Wins is the default conflict resolution policy.
When using Azure Cosmos DB for NoSQL, a custom numerical property such
as a custom timestamp definition can be used for conflict resolution.
Custom resolution policies allow for application-defined semantics to reconcile
conflicts using a registered merge stored procedure that is automatically
invoked when conflicts are detected.
The system provides exactly once guarantee for the execution of a merge
procedure as part of the commitment protocol.
A custom conflict resolution policy is only available with Azure Cosmos DB
for NoSQL and can only be set at container creation time.
In a multi-region write configuration, there's a dependency on a single Azure
Cosmos DB 'hub' region to perform all conflict resolutions, with the Paxos
consensus protocol applied to achieve quorum across replicas within the hub
region.
The platform provides a message buffer for write conflicts within the hub region
to load level and provide redundancy for transient faults.
The buffer is capable of storing a few minutes worth of write updates
requiring consensus.
The strategic direction of the Azure Cosmos DB platform is to remove this single
region dependency for conflict resolution in a multi-region write configuration,
utilizing a 2-phase Paxos approach to attain quorum at a global level and within a
region.
The primary 'hub' region is determined by the first region that Azure Cosmos DB is
configured within.

A priority ordering is configured for additional satellite deployment regions for
failover purposes.
The data model and partitioning across logical and physical partitions plays an
important role in achieving optimal performance and availability.
When deployed with a single write region, Azure Cosmos DB can be configured for
automatic failover based on a defined failover priority considering all read region
replicas.
The RTO provided by the Azure Cosmos DB platform is ~10-15 minutes, capturing
the elapsed time to perform a regional failover of the Azure Cosmos DB service if a
catastrophic disaster impacting the hub region.
This RTO is also relevant in a multi-region write context given the dependency
on a single 'hub' region for conflict resolution.
If the 'hub' region becomes unavailable, writes made to other regions will fail
after the message buffer fills since conflict resolution won't be able to occur
until the service fails over and a new hub region is established.
The strategic direction of the Azure Cosmos DB platform is to reduce the RTO to ~5
minutes by allowing partition level failovers.
Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) are
configurable via consistency levels, with a trade-off between data durability and
throughput.
Azure Cosmos DB provides a minimum RTO of 0 for a relaxed consistency level
with multi-region writes or an RPO of 0 for strong consistency with single-write
region.
Azure Cosmos DB offers a 99.999% SLA

for both read and write availability for

Database Accounts configured with multiple Azure regions as writable.
The SLA is represented by the Monthly Uptime Percentage, which is calculated
as 100% - Average Error Rate.
The Average Error Rate is defined as the sum of Error Rates for each hour in the
billing month divided by the total number of hours in the billing month, where
the Error Rate is the total number of Failed Requests divided by Total Requests
during a given one-hour interval.
Azure Cosmos DB offers a 99.99% SLA for throughput, consistency, availability, and
latency for Database Accounts scoped to a single Azure region when configured
with any of the five Consistency Levels.

A 99.99% SLA also applies to Database Accounts spanning multiple Azure
regions configured with any of the four relaxed Consistency Levels.
There are two types of throughput that can be provisioned in Azure Cosmos DB,
standard and autoscale, which are measured using Request Units per second
(RU/s).
Standard throughput allocates resources required to guarantee a specified RU/s
value.
Standard is billed hourly for provisioned throughput.
Autoscale defines a maximum throughput value, and Azure Cosmos DB will
automatically scale up or down depending on application load, between the
maximum throughput value and a minimum of 10% of the maximum
throughput value.
Autoscale is billed hourly for the maximum throughput consumed.
Static provisioned throughput with a variable workload may result in throttling
errors, which will impact perceived application availability.
Autoscale protects against throttling errors by enabling Azure Cosmos DB to
scale up as needed, while maintaining cost protection by scaling back down
when load decreases.
When Azure Cosmos DB is replicated across multiple regions, the provisioned
Request Units (RUs) are billed per region.
There's a significant cost delta between a multi-region-write and single-regionwrite configuration which in many cases may make a multi-master Azure Cosmos
DB data platform cost prohibitive.
Single Region
Read/Write

Single Region Write - Dual Region
Read

Dual Region
Read/Write

1 RU

2 RU

4 RU

The delta between single-region-write and multi-region-write is actually less than
the 1:2 ratio reflected in the table above. More specifically, there's a cross-region
data transfer charge associated with write updates in a single-write configuration,
which isn't captured within the RU costs as with the multi-region write
configuration.
Consumed storage is billed as a flat rate for the total amount of storage (GB)
consumed to host data and indexes for a given hour.

Session is the default and most widely used consistency level since data is

received in the same order as writes.
Azure Cosmos DB supports authentication via either an Azure Active Directory
identity or Azure Cosmos DB keys and resource tokens, which provide overlapping
capabilities.

It's possible to disable resource management operations using keys or resource
tokens to limit keys and resource tokens to data operations only, allowing for finegrained resource access control using Azure Active Directory Role-Based Access
Control (RBAC).
Restricting control plane access via keys or resource tokens will disable control
plane operations for clients using Azure Cosmos DB SDKs and should therefore
be thoroughly evaluated and tested.
The disableKeyBasedMetadataWriteAccess setting can be configured via ARM
Template IaC definitions, or via a Built-In Azure Policy .
Azure Cosmos DB Azure Active Directory RBAC support applies to account and
resource control plane management operations.
Application administrators can create role assignments for users, groups, service
principals or managed identities to grant or deny access to resources and
operations on Azure Cosmos DB resources.
There are several Built-in RBAC Roles available for role assignment, and custom
RBAC roles can also be used to form specific privilege combinations.
Cosmos DB Account Reader enables read-only access to the Azure Cosmos
DB resource.
DocumentDB Account Contributor enables management of Azure Cosmos
DB accounts including keys and role assignments, but doesn't enable dataplane access.

Cosmos DB Operator, which is similar to DocumentDB Account Contributor,
but doesn't provide the ability to manage keys or role assignments.
Azure Cosmos DB resources (accounts, databases, and containers) can be
protected against incorrect modification or deletion using Resource Locks.
Resource Locks can be set at the account, database, or container level.
A Resource Lock set at on a resource will be inherited by all child resources. For
example, a Resource Lock set on the Azure Cosmos DB account will be inherited
by all databases and containers within the account.
Resource Locks only apply to control plane operations and do not prevent data
plane operations, such as creating, changing, or deleting data.
If control plane access isn't restricted with disableKeyBasedMetadataWriteAccess ,
then clients will be able to perform control plane operations using account keys.
The Azure Cosmos DB change feed provides a time-ordered feed of changes to
data in an Azure Cosmos DB container.
The change feed only includes insert and update operations to the source Azure
Cosmos DB container; it doesn't include deletes.
The change feed can be used to maintain a separate data store from the primary
Container used by the application, with ongoing updates to the target data store
fed by the change feed from the source Container.
The change feed can be used to populate a secondary store for additional data
platform redundancy or for subsequent analytical scenarios.
If delete operations routinely affect the data within the source Container, then the
store fed by the change feed will be inaccurate and unreflective of deleted data.
A Soft Delete pattern can be implemented so that data records are included in
the change feed.
Instead of explicitly deleting data records, data records are updated by
setting a flag (e.g. IsDeleted ) to indicate that the item is considered deleted.
Any target data store fed by the change feed will need to detect and process
items with a deleted flag set to True; instead of storing soft-deleted data
records, the existing version of the data record in the target store will need to
be deleted.
A short Time-To-Live (TTL) is typically used with the soft-delete pattern so that
Azure Cosmos DB automatically deletes expired data, but only after it's reflected
within the change feed with the deleted flag set to True.
Accomplishes the original delete intent whilst also propagating the delete
through the change feed.

Azure Cosmos DB can be configured as an analytical store, which applies a column
format for optimized analytical queries to address the complexity and latency
challenges that occur with the traditional ETL pipelines.
Azure Cosmos DB automatically backs up data at regular intervals without
affecting the performance or availability, and without consuming RU/s.
Azure Cosmos DB can be configured according to two distinct backup modes.
Periodic is the default backup mode for all accounts, where backups are taken at
a periodic interval and the data is restored by creating a request with the
support team.
The default periodic backup retention period is 8 hours and the default
backup interval is fourhours, which means only the latest two backups are
stored by default.
The backup interval and retention period are configurable within the account.
The maximum retention period extends to a month with a minimum
backup interval of one hour.
A role assignment to the Azure "Cosmos DB Account Reader Role" is
required to configure backup storage redundancy.
Two backup copies are included at no extra cost, but additional backups
incur additional costs.
By default, periodic backups are stored within separate Geo-Redundant
Storage (GRS) that isn't directly accessible.
Backup storage exists within the primary 'hub' region and is replicated to
the paired region through underlying storage replication.
The redundancy configuration of the underlying backup storage account
is configurable to Zone-Redundant Storage or Locally-Redundant Storage.
Performing a restore operation requires a Support Request since customers
can't directly perform a restore.
Before opening a support ticket, the backup retention period should be
increased to at least seven days within eight hours of the data loss event.
A restore operation creates a new Azure Cosmos DB account where data is
recovered.
An existing Azure Cosmos DB account can't be used for Restore
By default, a new Azure Cosmos DB account named
<Azure_Cosmos_account_original_name>-restored<n> will be used.

This name can be adjusted, such as by reusing the existing name if the
original account was deleted.
If throughput is provisioned at the database level, backup and restore will
happen at the database level
It's not possible to select a subset of containers to restore.

Continuous backup mode allows for a restore to any point of time within the
last 30 days.
Restore operations can be performed to return to a specific point in time
(PITR) with a one-second granularity.
The available window for restore operations is up to 30 days.
It's also possible to restore to the resource instantiation state.
Continuous backups are taken within every Azure region where the Azure
Cosmos DB account exists.
Continuous backups are stored within the same Azure region as each
Azure Cosmos DB replica, using Locally-Redundant Storage (LRS) or Zone
Redundant Storage (ZRS) within regions that support Availability Zones.
A self-service restore can be performed using the Azure portal or IaC artifacts
such as ARM templates.
There are several limitations with Continuous Backup.
The continuous backup mode isn't currently available in a multi-regionwrite configuration.
Only Azure Cosmos DB for NoSQL and Azure Cosmos DB for MongoDB
can be configured for Continuous backup at this time.
If a container has TTL configured, restored data that has exceeded its TTL
may be immediately deleted
A restore operation creates a new Azure Cosmos DB account for the pointin-time restore.
There's an additional storage cost for Continuous backups and restore
operations.
Existing Azure Cosmos DB accounts can be migrated from Periodic to Continuous,
but not from Continuous to Periodic; migration is one-way and not reversible.
Each Azure Cosmos DB backup is composed of the data itself and configuration
details for provisioned throughput, indexing policies, deployment region(s), and
container TTL settings.
Backups don't contain firewall settings, virtual network access control lists,
private endpoint settings, consistency settings (an account is restored with
session consistency), stored procedures, triggers, UDFs, or multi-region settings.
Customers are responsible for redeploying capabilities and configuration
settings. These aren't restored via Azure Cosmos DB backup.
Azure Synapse Link analytical store data is also not included in Azure Cosmos
DB backups.
It's possible to implement a custom backup and restore capability for scenarios
where Periodic and Continuous approaches aren't a good fit.

A custom approach introduces significant costs and additional administrative
overhead, which should be understood and carefully assessed.
Common restore scenarios should be modeled, such as the corruption or
deletion of an account, database, container, on data item.
Housekeeping procedures should be implemented to prevent backup sprawl.
Azure Storage or an alternative data technology can be used, such an
alternative Azure Cosmos DB container.
Azure Storage and Azure Cosmos DB provide native integrations with Azure
services such as Azure Functions and Azure Data Factory.
The Azure Cosmos DB documentation denotes two potential options for
implementing custom backups.
Azure Cosmos DB change feed to write data to a separate storage facility.
An Azure function or equivalent application process uses the change feed
processor to bind to the change feed and process items into storage.
Both continuous or periodic (batched) custom backups can be implemented
using the change feed.
The Azure Cosmos DB change feed doesn't yet reflect deletes, so a soft-delete
pattern must be applied using a boolean property and TTL.
This pattern won't be required when the change feed provides full-fidelity
updates.
Azure Data Factory Connector for Azure Cosmos DB (Azure Cosmos DB for
NoSQL or MongoDB API connectors) to copy data.
Azure Data Factory (ADF) supports manual execution and Schedule, Tumbling
window, and Event-based triggers.
Provides support for both Storage and Event Grid.
ADF is primarily suitable for periodic custom backup implementations due to
its batch-oriented orchestration.
It's less suitable for continuous backup implementations with frequent
events due to the orchestration execution overhead.
ADF supports Azure Private Link for high network security scenarios
Azure Cosmos DB is used within the design of many Azure services, so a significant
regional outage for Azure Cosmos DB will have a cascading effect across various
Azure services within that region. The precise impact to a particular service will
heavily depend on how the underlying service design uses Azure Cosmos DB.

Design Recommendations
Azure Cosmos DB

Use Azure Cosmos DB as the primary data platform where requirements allow.
For mission-critical workload scenarios, configure Azure Cosmos DB with a write
replica inside each deployment region to reduce latency and provide maximum
redundancy.
Configure the application to prioritize the use of the local Azure Cosmos DB
replica for writes and reads to optimize application load, performance, and
regional RU/s consumption.
The multi-region-write configuration comes at a significant cost and should be
prioritized only for workload scenarios requiring maximum reliability.
For less-critical workload scenarios, prioritize the use of single-region-write
configuration (when using Availability Zones) with globally distributed read
replicas, since this offers a high level of data platform reliability (99.999% SLA for
read-, 99.995% SLA for write-operations) at a more compelling price-point.
Configure the application to use the local Azure Cosmos DB read replica to
optimize read performance.
Select an optimal 'hub' deployment region where conflict resolution will occur in a
multi-region-write configuration, and all writes will be performed in a singleregion-write configuration.
Consider distance relative to other deployment regions and associated latency
in selecting a primary region, and requisite capabilities such as Availability
Zones support.
Configure Azure Cosmos DB with Availability Zone (AZ) redundancy in all
deployment regions with AZ support, to ensure resiliency to zonal failures within a
region.
Use Azure Cosmos DB for NoSQL since it offers the most comprehensive feature
set, particularly where performance tuning is concerned.
Alternative APIs should primarily be considered for migration or compatibility
scenarios.
When using alternative APIs, validate that required capabilities are available
with the selected language and SDK to ensure optimal configuration and
performance.
Use the Direct connection mode to optimize network performance through direct
TCP connectivity to backend Azure Cosmos DB nodes, with a reduced number of
network 'hops'.
The Azure Cosmos DB SLA is calculated by averaging failed requests, which may not
directly align with a 99.999% reliability tier error budget. When designing for

99.999% SLO, it's therefore vital to plan for regional and multi-region Azure Cosmos
DB write unavailability, ensuring a fallback storage technology is positioned if a
failure, such as a persisted message queue for subsequent replay.
Define a partitioning strategy across both logical and physical partitions to
optimize data distribution according to the data model.
Minimize cross-partition queries.
Iteratively test and validate the partitioning strategy to ensure optimal
performance.
Select an optimal partition key.
The partition key can't be changed after it has been created within the
collection.
The partition key should be a property value that doesn't change.
Select a partition key that has a high cardinality, with a wide range of possible
values.
The partition key should spread RU consumption and data storage evenly
across all logical partitions to ensure even RU consumption and storage
distribution across physical partitions.
Run read queries against the partitioned column to reduce RU consumption and
latency.
Indexing is also crucial for performance, so ensure index exclusions are used to
reduce RU/s and storage requirements.
Only index those fields that are needed for filtering within queries; design
indexes for the most-used predicates.
Leverage the built-in error handling, retry, and broader reliability capabilities of the
Azure Cosmos DB SDK.
Implement retry logic within the SDK on clients.
Use service-managed encryption keys to reduce management complexity.
If there's a specific security requirement for customer-managed keys, ensure
appropriate key management procedures are applied, such as backup and
rotation.
Disable Azure Cosmos DB key-based metadata write access

by applying the

built-in Azure Policy.
Enable Azure Monitor to gather key metrics and diagnostic logs, such as
provisioned throughput (RU/s).

Route Azure Monitor operational data into a Log Analytics workspace dedicated
to Azure Cosmos DB and other global resources within the application design.
Use Azure Monitor metrics to determine if application traffic patterns are
suitable for autoscale.
Evaluate application traffic patterns to select an optimal option for provisioned
throughput types.
Consider auto-scale provisioned throughput to automatically level-out
workload demand.
Evaluate Microsoft performance tips for Azure Cosmos DB to optimize client-side
and server-side configuration for improved latency and throughput.
When using AKS as the compute platform: For query-intensive workloads, select an
AKS node SKU that has accelerated networking enabled to reduce latency and CPU
jitters.
For single write region deployments, it's strongly recommended to configure Azure
Cosmos DB for automatic failover.
Load-level through the use of asynchronous non-blocking messaging within
system flows, which write updates to Azure Cosmos DB.
Consider patterns such as Command and Query Responsibility Segregation and
Event Sourcing.
Configure the Azure Cosmos DB account for continuous backups to obtain a fine
granularity of recovery points across the last 30 days.
Consider the use of Azure Cosmos DB backups in scenarios where contained
data or the Azure Cosmos DB account is deleted or corrupted.
Avoid the use of a custom backup approach unless absolutely necessary.
It's strongly recommended to practice recovery procedures on non-production
resources and data, as part of standard business continuity operation preparation.
Define IaC artifacts to re-establish configuration settings and capabilities of an
Azure Cosmos DB backup restore.
Evaluate and apply the Azure Security Baseline control guidance for Azure Cosmos
DB Backup and Recovery.
BR-1: Ensure regular automated backups
BR-3: Validate all backups including customer-managed keys
BR-4, Mitigate risk of lost keys

For analytical workloads requiring multi-region availability, use the Azure Cosmos
DB Analytical Store, which applies a column format for optimized analytical
queries.

Relational data technologies
For scenarios with a highly relational data model or dependencies on existing relational
technologies, the use of Azure Cosmos DB in a multi-region write configuration might
not be directly applicable. In such cases, it's vital that used relational technologies are
designed and configured to uphold the multi-region active-active aspirations of an
application design.
Azure provides many managed relational data platforms, including Azure SQL Database
and Azure Database for common OSS relational solutions, including MySQL,
PostgreSQL, and MariaDB. The design considerations and recommendations within this
section will therefore focus on the optimal usage of Azure SQL Database and Azure
Database OSS flavors to maximize reliability and global availability.

Design considerations
Whilst relational data technologies can be configured to easily scale read
operations, writes are typically constrained to go through a single primary
instance, which places a significant constraint on scalability and performance.
Sharding can be applied to distribute data and processing across multiple identical
structured databases, partitioning databases horizontally to navigate platform
constraints.
For example, sharding is often applied in multi-tenant SaaS platforms to isolate
groups of tenants into distinct data platform constructs.
Azure SQL Database
Azure SQL Database provides a fully managed database engine that is always
running on the latest stable version of the SQL Server database engine and
underlying Operating System.
Provides intelligent features such as performance tuning, threat monitoring, and
vulnerability assessments.
Azure SQL Database provides built-in regional high availability and turnkey georeplication to distribute read-replicas across Azure regions.
With geo-replication, secondary database replicas remain read-only until a
failover is initiated.

Up to four secondaries are supported in the same or different regions.
Secondary replicas can also be used for read-only query access to optimize read
performance.
Failover must be initiated manually but can be wrapped in automated
operational procedures.
Azure SQL Database provides Auto Failover Groups, which replicates databases to
a secondary server and allows for transparent failover if a failure.
Auto-failover groups support geo-replication of all databases in the group to
only one secondary server or instance in a different region.
Auto-failover groups aren't currently supported in the Hyperscale service tier.
Secondary databases can be used to offload read traffic.
Premium or Business Critical service tier database replicas can be distributed across
Availability Zones at no extra cost.
The control ring is also duplicated across multiple zones as three gateway rings
(GW).
The routing to a specific gateway ring is controlled by Azure Traffic Manager.
When using the Business Critical tier, zone redundant configuration is only
available when the Gen5 compute hardware is selected.
Azure SQL Database offers a baseline 99.99% availability SLA across all of its
service tiers, but provides a higher 99.995% SLA for the Business Critical or
Premium tiers in regions that support availability zones.
Azure SQL Database Business Critical or Premium tiers not configured for Zone
Redundant Deployments have an availability SLA of 99.99%.
When configured with geo-replication, the Azure SQL Database Business Critical
tier provides a Recovery Time Objective (RTO) of 30 seconds for 100% of deployed
hours.
When configured with geo-replication, the Azure SQL Database Business Critical
tier has a Recovery point Objective (RPO) of 5 seconds for 100% of deployed
hours.
Azure SQL Database Hyperscale tier, when configured with at least two replicas,
has an availability SLA of 99.99%.
Compute costs associated with Azure SQL Database can be reduced using a
Reservation Discount.
It's not possible to apply reserved capacity for DTU-based databases.

