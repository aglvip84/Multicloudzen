Point-in-time restore can be used to return a database and contained data to an
earlier point in time.
Geo-restore can be used to recover a database from a geo-redundant backup.
Azure Database For PostgreSQL
Azure Database For PostgreSQL is offered in three different deployment options:
Single Server, SLA 99.99%
Flexible Server, which offers Availability Zone redundancy, SLA 99.99%
Hyperscale (Citus), SLA 99.95% when High Availability mode is enabled.
Hyperscale (Citus) provides dynamic scalability through sharding without
application changes.
Distributing table rows across multiple PostgreSQL servers is key to ensure
scalable queries in Hyperscale (Citus).
Multiple nodes can collectively hold more data than a traditional database, and
in many cases can use worker CPUs in parallel to optimize costs.
Autoscale

can be configured through runbook automation to ensure elasticity in

response to changing traffic patterns.
Flexible server provides cost efficiencies for non-production workloads through the
ability to stop/start the server, and a burstable compute tier that is suitable for
workloads that don't require continuous compute capacity.
There's no additional charge for backup storage for up to 100% of total
provisioned server storage.
Additional consumption of backup storage is charged according to consumed
GB/month.
Compute costs associated with Azure Database for PostgreSQL can be reduced
using either a Single Server Reservation Discount or Hyperscale (Citus) Reservation
Discount.

Design Recommendations
Consider sharding to partition relational databases based on different application
and data contexts, helping to navigate platform constraints, maximize scalability
and availability, and fault isolation.
This recommendation is particularly prevalent when the application design
considers three or more Azure regions since relational technology constraints
can significantly hinder globally distributed data platforms.

Sharding isn't appropriate for all application scenarios, so a contextualized
evaluation is required.
Prioritize the use of Azure SQL Database where relational requirements exist due
to its maturity on the Azure platform and wide array of reliability capabilities.
Azure SQL Database
Use the Business-Critical service tier to maximize reliability and availability,
including access to critical resiliency capabilities.
Use the vCore based consumption model to facilitate the independent selection of
compute and storage resources, tailored to workload volume and throughput
requirements.
Ensure a defined capacity model is applied to inform compute and storage
resource requirements.
Consider Reserved Capacity to provide potential cost optimizations.
Configure the Zone-Redundant deployment model to spread Business Critical
database replicas within the same region across Availability Zones.
Use Active Geo-Replication to deploy readable replicas within all deployment
regions (up to four).
Use Auto Failover Groups to provide transparent failover to a secondary region,
with geo-replication applied to provide replication to additional deployment
regions for read optimization and database redundancy.
For application scenarios limited to only two deployment regions, the use of
Auto Failover Groups should be prioritized.
Consider automated operational triggers, based on alerting aligned to the
application health model, to conduct failovers to geo-replicated instances if a
failure impacting the primary and secondary within the Auto Failover Group.
） Important
For applications considering more than four deployment regions, serious
consideration should be given to application scoped sharding or refactoring the
application to support multi-region write technologies, such as Azure Cosmos DB.
However, if this isn't feasible within the application workload scenario, it's advised
to elevate a region within a single geography to a primary status encompassing a
geo-replicated instance to more evenly distributed read access.

Configure the application to query replica instances for read queries to optimize
read performance.
Use Azure Monitor and Azure SQL Analytics for near real-time operational insights
in Azure SQL DB for the detection of reliability incidents.
Use Azure Monitor to evaluate usage for all databases to determine if they have
been sized appropriately.
Ensure CD pipelines consider load testing under representative load levels to
validate appropriate data platform behavior.
Calculate a health metric for database components to observe health relative to
business requirements and resource utilization, using monitoring and alerts to
drive automated operational action where appropriate.
Ensure key query performance metrics are incorporated so swift action can be
taken when service degradation occurs.
Optimize queries, tables, and databases using Query Performance Insights and
common performance recommendations provided by Microsoft.
Implement retry logic using the SDK to mitigate transient errors impacting Azure
SQL Database connectivity.
Prioritize the use of service-managed keys when applying server-side Transparent
Data Encryption (TDE) for at-rest encryption.
If customer-managed keys or client-side (AlwaysEncrypted) encryption is
required, ensure keys are appropriately resilient with backups and automated
rotation facilities.
Consider the use of point-in-time restore as an operational playbook to recover
from severe configuration errors.
Azure Database For PostgreSQL
Flexible Server is recommended to use it for business critical workloads due to its
Availability Zone support.
When using Hyperscale (Citus) for business critical workloads, enable High
Availability mode to receive the 99.95% SLA guarantee.
Use the Hyperscale (Citus) server configuration to maximize availability across
multiple nodes.
Define a capacity model for the application to inform compute and storage
resource requirements within the data platform.

Consider the Hyperscale (Citus) Reservation Discount to provide potential cost
optimizations.

Caching for Hot Tier Data
An in-memory caching layer can be applied to enhance a data platform by significantly
increasing read throughput and improving end-to-end client response times for hot tier
data scenarios.
Azure provides several services with applicable capabilities for caching key data
structures, with Azure Cache for Redis positioned to abstract and optimize data platform
read access. This section will therefore focus on the optimal usage of Azure Cache for
Redis in scenarios where additional read performance and data access durability is
required.

Design Considerations
A caching layer provides additional data access durability since even if an outage
impacting the underlying data technologies, an application data snapshot can still
be accessed through the caching layer.
In certain workload scenarios, in-memory caching can be implemented within the
application platform itself.
Azure Cache for Redis
Redis cache is an open source NoSQL key-value in-memory storage system.
The Enterprise and Enterprise Flash tiers can be deployed in an active-active
configuration across Availability Zones within a region and different Azure regions
through geo-replication.
When deployed across at least three Azure regions and three or more
Availability Zones in each region, with active geo-replication enabled for all
Cache instances, Azure Cache for Redis provides an SLA of 99.999% for
connectivity to one regional cache endpoint.
When deployed across three Availability Zones within a single Azure region a
99.99% connectivity SLA is provided.
The Enterprise Flash tier runs on a combination of RAM and flash non-volatile
memory storage, and while this introduces a small performance penalty it also
enables very large cache sizes, up to 13TB with clustering.

With geo-replication, charges for data transfer between regions will also be
applicable in addition to the direct costs associated with cache instances.
The Scheduled Updates feature doesn't include Azure updates or updates applied
to the underlying VM operating system.
There will be an increase in CPU utilization during a scale-out operation while data
is migrated to new instances.

Design Recommendations
Consider an optimized caching layer for 'hot' data scenarios to increase read
throughput and improve response times.
Apply appropriate policies for cache expiration and housekeeping to avoid
runaway data growth.
Consider expiring cache items when the backing data changes.
Azure Cache for Redis
Use the Premium or Enterprise SKU to maximize reliability and performance.
For scenarios with extremely large data volumes, the Enterprise Flash tier should
be considered.
For scenarios where only passive geo-replication is required, the Premium tier
can also be considered.
Deploy replica instances using geo-replication in an active configuration across all
considered deployment regions.
Ensure replica instances are deployed across Availability Zones within each
considered Azure region.
Use Azure Monitor to evaluate Azure Cache for Redis.
Calculate a health score for regional cache components to observe health
relative to business requirements and resource utilization.
Observe and alert on key metrics such as high CPU, high memory usage, high
server load, and evicted keys for insights when to scale the cache.
Optimize connection resilience by implementing retry logic, timeouts, and using a
singleton implementation of the Redis connection multiplexer.
Configure scheduled updates to prescribe the days and times that Redis Server
updates are applied to the cache.

Analytical Scenarios
It's increasingly common for mission-critical applications to consider analytical scenarios
as a means to drive additional value from encompassed data flows. Application and
operational (AIOps) analytical scenarios therefore form a crucial aspect of highly reliable
data platform.
Analytical and transactional workloads require different data platform capabilities and
optimizations for acceptable performance within their respective contexts.
Description

Analytical

Transactional

Use Case

Analyze very large volumes of
data ("big data")

Process very large volumes of individual
transactions

Optimized for

Read queries and
aggregations over many

Near real-time Create/Read/Update/Delete
(CRUD) queries over few records

records
Key

- Consolidate from data

- Data source of record for application

Characteristics

sources of record
- Column-based storage
- Distributed storage

- Row-based Storage
- Contiguous storage
- Symmetrical processing

- Parallel processing
- Denormalized
- Low concurrency reads and
writes

- Normalized
- High concurrency reads and writes, index
updates
- Optimize for fast data access with in-memory

- Optimize for storage volume
with compression

storage

Azure Synapse provides an enterprise analytical platform that brings together relational
and non-relational data with Spark technologies, using built-in integration with Azure
services such as Azure Cosmos DB to facilitate big data analytics. The design
considerations and recommendations within this section will therefore focus on optimal
Azure Synapse and Azure Cosmos DB usage for analytical scenarios.

Design Considerations
Traditionally, large-scale analytical scenarios are facilitated by extracting data into a
separate data platform optimized for subsequent analytical queries.
Extract, Transform, and Load (ETL) pipelines are used to extract data will
consume throughput and impact transactional workload performance.
Running ETL pipelines infrequently to reduce throughput and performance
impacts will result in analytical data that is less up-to-date.

ETL pipeline development and maintenance overhead increases as data
transformations become more complex.
For example, if source data is frequently changed or deleted, ETL pipelines
must account for those changes in the target data for analytical queries
through an additive/versioned approach, dump and reload, or in-place
changes on the analytical data. Each of these approaches will have derivative
impact, such as index re-creation or update.
Azure Cosmos DB
Analytical queries run on Azure Cosmos DB transactional data will typically
aggregate across partitions over large volumes of data, consuming significant
Request Unit (RU) throughput, which can impact the performance of surrounding
transactional workloads.
The Azure Cosmos DB Analytical Store provides a schematized, fully isolated
column-oriented data store that enables large-scale analytics on Azure Cosmos DB
data from Azure Synapse without impact to Azure Cosmos DB transactional
workloads.
When an Azure Cosmos DB Container is enabled as an Analytical Store, a new
column store is internally created from the operational data in the Container.
This column store is persisted separately from the row-oriented transaction
store for the container.
Create, Update and Delete operations on the operational data are automatically
synced to the analytical store, so no change feed or ETL processing is required.
Data sync from the operational to the analytical store doesn't consume
throughput Request Units (RUs) provisioned on the Container or Database.
There's no performance impact on transactional workloads. Analytical Store
doesn't require allocation of additional RUs on an Azure Cosmos DB Database
or Container.
Auto-Sync is the process where operational data changes are automatically
synced to the Analytical Store. Auto-Sync latency is usually less than two (2)
minutes.
Auto-Sync latency can be up to five (5) minutes for a Database with shared
throughput and a large number of Containers.
As soon as Auto-Sync completes, the latest data can be queried from Azure
Synapse.
Analytical Store storage uses a consumption-based pricing model

that

charges for volume of data and number of read and write operations. Analytical
store pricing is separate from transactional store pricing.

Using Azure Synapse Link, the Azure Cosmos DB Analytical Store can be queried
directly from Azure Synapse. This enables no-ETL, Hybrid Transactional-Analytical
Processing (HTAP) from Synapse, so that Azure Cosmos DB data can be queried
together with other analytical workloads from Synapse in near real-time.
The Azure Cosmos DB Analytical Store isn't partitioned by default.
For certain query scenarios, performance will improve by partitioning Analytical
Store data using keys that are frequently used in query predicates.
Partitioning is triggered by a job in Azure Synapse that runs a Spark notebook
using Synapse Link, which loads the data from the Azure Cosmos DB Analytical
Store and writes it into the Synapse partitioned store in the primary storage
account of the Synapse workspace.
Azure Synapse Analytics SQL Serverless pools can query the Analytical Store
through automatically updated views or via SELECT / OPENROWSET commands.
Azure Synapse Analytics Spark pools can query the Analytical Store through
automatically updated Spark tables or the spark.read command.
Data can also be copied from the Azure Cosmos DB Analytical Store into a
dedicated Synapse SQL pool using Spark, so that provisioned Azure Synapse SQL
pool resources can be used.
Azure Cosmos DB Analytical Store data can be queried with Azure Synapse Spark.
Spark notebooks allow for Spark dataframe combinations to aggregate and
transform Azure Cosmos DB analytical data with other data sets, and use other
advanced Synapse Spark functionality including writing transformed data to
other stores or training AIOps Machine Learning models.

The Azure Cosmos DB change feed can also be used to maintain a separate
secondary data store for analytical scenarios.
Azure Synapse
Azure Synapse brings together analytics capabilities including SQL data
warehousing, Spark big data, and Data Explorer for log and time series analytics.
Azure Synapse uses linked services to define connections to other services, such
as Azure Storage.
Data can be ingested into Synapse Analytics via Copy activity from supported
sources. This permits data analytics in Synapse without impacting the source
data store, but adds time, cost and latency overhead due to data transfer.
Data can also be queried in-place in supported external stores, avoiding the
overhead of data ingestion and movement. Azure Storage with Data Lake Gen2
is a supported store for Synapse and Log Analytics exported data can be
queried via Synapse Spark .
Azure Synapse Studio unites ingestion and querying tasks.
Source data, including Azure Cosmos DB Analytical Store data and Log Analytics
Export data, are queried and processed in order to support business intelligence
and other aggregated analytical use cases.

Design Recommendations
Ensure analytical workloads don't impact transactional application workloads to
maintain transactional performance.
Application Analytics
Use Azure Synapse Link with Azure Cosmos DB Analytical Store to perform
analytics on Azure Cosmos DB operational data by creating an optimized data
store, which won't impact transactional performance.
Enable Azure Synapse Link on Azure Cosmos DB accounts.

Create a container enabled for Analytical Store, or enable an existing Container
for Analytical Store.
Connect the Azure Synapse workspace to the Azure Cosmos DB Analytical Store
to enable analytical workloads in Azure Synapse to query Azure Cosmos DB
data. Use a connection string with a read-only Azure Cosmos DB key.
Prioritize Azure Cosmos DB Analytical Store with Azure Synapse Link instead of
using the Azure Cosmos DB change feed to maintain an analytical data store.
The Azure Cosmos DB change feed may be suitable for very simple analytical
scenarios.
AIOps and Operational Analytics
Create a single Azure Synapse workspace with linked services and data sets for
each source Azure Storage account where operational data from resources are sent
to.
Create a dedicated Azure Storage account and use it as the workspace primary
storage account to store the Synapse workspace catalog data and metadata.
Configure it with hierarchical namespace to enable Azure Data Lake Gen2.
Maintain separation between the source analytical data and Synapse workspace
data and metadata.
Do not use one of the regional or global Azure Storage accounts where
operational data is sent.

Next step
Review the considerations for networking considerations.
Network and connectivity

Networking and connectivity for
mission-critical workloads on Azure
Article • 02/01/2023

Networking is a fundamental area for a mission-critical application, given the
recommended globally distributed active-active design approach.
This design area explores various network topology topics at an application level,
considering requisite connectivity and redundant traffic management. More specifically,
it highlights critical considerations and recommendations intended to inform the design
of a secure and scalable global network topology for a mission-critical application.
） Important
This article is part of the Azure Well-Architected mission-critical workload series. If
you aren't familiar with this series, we recommend you start with what is a missioncritical workload?

Global traffic routing
The use of multiple active regional deployment stamps requires a global routing service
to distribute traffic to each active stamp.
Azure Front Door

, Azure Traffic Manager , and Azure Standard Load Balancer

provide the needed routing capabilities to manage global traffic across a multi-region
application.
Alternatively, third-party globally routing technologies can be considered. These options
can almost seamlessly be swapped in to replace or extend the use of Azure-native
global routing services. Popular choices include routing technologies by CDN providers.
This section explores the key differences Azure routing services to define how each can
be used to optimize different scenarios.

Design considerations
A routing service bound to a single region represents a single-point-of-failure and
a significant risk with regard to regional outages.

If the application workload encompasses client control, such as with mobile or
desktop client applications, it's possible to provide service redundancy within client
routing logic.
Multiple global routing technologies, such as Azure Front Door and Azure Traffic
Manager, can be considered in parallel for redundancy, with clients configured
to fail over to an alternative technology when certain failure conditions are met.
The introduction of multiple global routing services introduces significant
complexities around edge caching and Web Application Firewall capabilities,
and certificate management for SSL offload and application validation for
ingress paths.
Third-party technologies can also be considered, providing global routing
resiliency to all levels of Azure platform failures.
Capability disparity between Azure Front Door and Traffic Manager means that if
the two technologies are positioned alongside one another for redundancy, a
different ingress path or design changes would be required to ensure a consistent
and acceptable level of service is maintained.
Azure Front Door and Azure Traffic Manager are globally distributed services with
built-in multi-region redundancy and availability.
Hypothetical failure scenarios of a scale large enough to threaten the global
availability of these resilient routing services presents a broader risk to the
application in terms of cascading and correlated failures.
Failure scenarios of this scale are only feasibly caused by shared foundational
services, such as Azure DNS or Azure AD, which serve as global platform
dependencies for almost all Azure services. If a redundant Azure technology
is applied, it's likely that the secondary service will also be experiencing
unavailability or a degraded service.
Global routing service failure scenarios are highly likely to significantly impact
many other services used for key application components through
interservice dependencies. Even if a third-party technology is used, the
application will likely be in an unhealthy state due to the broader impact of
the underlying issue, meaning that routing to application endpoints on Azure
will provide little value anyway.
Global routing service redundancy provides mitigation for an extremely small
number of hypothetical failure scenarios, where the impact of a global outage is
constrained to the routing service itself.
To provide broader redundancy to global outage scenarios, a multi-cloud activeactive deployment approach can be considered. A multi-cloud active-active
deployment approach introduces significant operational complexities, which pose

significant resiliency risks, likely far outweighing the hypothetical risks of a global
outage.
For scenarios where client control isn't possible, a dependency must be taken on a
single global routing service to provide a unified entry point for all active
deployment regions.
When used in isolation they represent a single-point-of-failure at a service level
due to global dependencies, even though built-in multi-region redundancy and
availability are provided.
The SLA provided by the selected global routing service represents the
maximum attainable composite SLA, regardless of how many deployment
regions are considered.
When client control isn't possible, operational mitigations can be considered to
define a process for migrating to a secondary global routing service if a global
outage disables the primary service. Migrating from one global routing service to
another is typically a lengthy process lasting several hours, particularly where DNS
propagation is considered.
Some third-party global routing services provide a 100% SLA. However, the historic
and attainable SLA provided by these services is typically lower than 100%.
While these services provide financial reparations for unavailability, it comes of
little significance when the impact of unavailability is significant, such as with
safety-critical scenarios where human life is ultimately at stake. Technology
redundancy or sufficient operational mitigations should therefore still be
considered even when the advertised legal SLA is 100%.
Azure Front Door
Azure Front Door provides global HTTP/S load balancing and optimized
connectivity using the Anycast protocol with split TCP to take advantage of the
Microsoft global backbone network.
A number of connections are maintained for each of the backend endpoints.
Incoming client requests are first terminated at the edge node closest to the
originating client.
After any required traffic inspection, requests are either forwarded over the
Microsoft backbone to the appropriate backend using existing connections, or
served from the internal cache of an edge node.
This approach is very efficient in spreading high traffic volumes over the
backend connections.
Provides a built-in cache that serves static content from edge nodes. In many use
cases, this can also eliminate the need for a dedicated Content Delivery Network

(CDN).
Azure Web Application Firewall (WAF) can be used on Azure Front Door, and since
it's deployed to Azure network edge locations around the globe, every incoming
request delivered by Front Door in inspected at the network edge.
Azure Front Door protects application endpoints against DDoS attacks using Azure
DDoS protection Basic. Azure DDoS Standard provides additional and more
advanced protection and detection capabilities and can be added as an additional
layer to Azure Front Door.
Azure Front Door offers a fully managed certificate service. Enables TLS connection
security for endpoints without having to manage the certificate lifecycle.
Azure Front Door Premium supports private endpoints, enabling traffic to flow
from the internet directly onto Azure virtual networks. This would eliminate the
need of using public IPs on the VNet for making the backends accessible via Azure
Front Door Premium.
Azure Front Door relies on health probes and backend health endpoints (URLs)
that are called on an interval basis to return an HTTP status code reflecting if the
backend is operating normally, with an HTTP 200 (OK) response reflecting a
healthy status. As soon as a backend reflects an unhealthy status, from the
perspective of a certain edge node, that edge node will stop sending requests
there. Unhealthy backends are therefore transparently removed from traffic
circulation without any delay.
Supports HTTP/S protocols only.
The Azure Front Door WAF and Application Gateway WAF provide a slightly
different feature set, though both support built-in and custom rules and can be set
to operate in either detection mode or prevention mode.
The Front Door backend IP space may change, but Microsoft will ensure
integration with Azure IP Ranges and Service Tags

. It's possible to subscribe to

Azure IP Ranges and Service Tags to receive notifications about any changes or
updates.
Azure Front Door supports various load distribution configurations:
Latency-based: the default setting that routes traffic to the "closest" backend
from the client; based on request latency.
Priority-based: useful for active-passive setups, where traffic must always be
sent to a primary backend unless it's not available.

Weighted: applicable for canary deployments in which a certain percentage of
traffic is sent to a specific backend. If multiple backends have the same weights
assigned, latency-based routing is used.
By default Azure Front Door uses latency-based routing that can lead to situations
where some backends get a lot more incoming traffic than others, depending on
where clients originate from.
If a series of client requests must be handled by the same backend, Session Affinity
can be configured on the frontend. It uses a client-side cookie to send subsequent
requests to the same backend as the first request, provided the backend is still
available.
Azure Traffic Manager
Azure Traffic Manager is a DNS redirection service.
The actual request payload isn't processed, but instead Traffic Manager returns
the DNS name of one of the backends it the pool, based on configured rules for
the selected traffic routing method.
The backend DNS name is then resolved to its final IP address that is
subsequently directly called by the client.
The DNS response is cached and reused by the client for a specified Time-To-Live
(TTL) period, and requests made during this period will go directly to the backend
endpoint without Traffic Manager interaction. Eliminates the extra connectivity step
that provides cost benefits compared to Front Door.
Since the request is made directly from the client to the backend service, any
protocol supported by the backend can be leveraged.
Similar to Azure Front Door, Azure Traffic Manager also relies on health probes to
understand if a backend is healthy and operating normally. If another value is
returned or nothing is returned, the routing service recognizes ongoing issues and
will stop routing requests to that specific backend.
However, unlike with Azure Front Door this removal of unhealthy backends isn't
instantaneous since clients will continue to create connections to the unhealthy
backend until the DNS TTL expires and a new backend endpoint is requested
from the Traffic Manager service.
In addition, even when the TTL expires, there no guarantee that public DNS
servers will honor this value, so DNS propagation can actually take much longer
to occur. This means that traffic may continue to be sent to the unhealthy
endpoint for a sustained period of time.

Azure Standard Load Balancer
） Important
Cross-Region Standard Load Balancer is available in preview with technical
limitations. This option isn't recommended for mission-critical workloads.

Design recommendations
Use Azure Front Door as the primary global traffic routing service for HTTP/S
scenarios. Azure Front Door is strongly advocated for HTTP/S workloads as it
provides optimized traffic routing, transparent fail over, private backend endpoints
(with the Premium SKU), edge caching and integration with Web Application
Firewall (WAF).
For application scenarios where client control is possible, apply client side routing
logic to consider failover scenarios where the primary global routing technology
fails. Two or more global routing technologies should be positioned in parallel for
added redundancy, if single service SLA isn't sufficient. Client logic is required to
route to the redundant technology in case of a global service failure.
Two distinct URLs should be used, with one applied to each of the different
global routing services to simplify the overall certificate management
experience and routing logic for a failover.
Prioritize the use of third-party routing technologies as the secondary failover
service, since this will mitigate the largest number of global failure scenarios
and the capabilities offered by industry leading CDN providers will allow for a
consistent design approach.
Consideration should also be given to directly routing to a single regional
stamp rather than a separate routing service. While this will result in a degraded
level of service, it represents a far simpler design approach.
This image shows a redundant global load balancer configuration with client failover
using Azure Front Door as primary global load balancer.

） Important
To truly mitigate the risk of global failures within the Azure platform, a multi-cloud
active-active deployment approach should be considered, with active deployment
stamps hosted across two or more cloud providers and redundant third-party
routing technologies used for global routing.
Azure can effectively be integrated with other cloud platforms. However, it's
strongly recommended not to apply a multi-cloud approach because it introduces
significant operational complexity, with different deployment stamp definitions and
representations of operational health across the different cloud platforms. This
complexity in-turn introduces numerous resiliency risks within the normal operation
of the application, which far outweigh the hypothetical risks of a global platform
outage.
Although not recommended, for HTTP(s) workloads using Azure Traffic Manager
for global routing redundancy to Azure Front Door, consider offloading Web
Application Firewall (WAF) to Application Gateway for acceptable traffic flowing
through Azure Front Door.
This will introduce an additional failure point to the standard ingress path, an
additional critical-path component to manage and scale, and will also incur
additional costs to ensure global high-availability. It will, however, greatly
simplify the failure scenario by providing consistency between the acceptable
and not acceptable ingress paths through Azure Front Door and Azure Traffic

Manager, both in terms of WAF execution but also private application
endpoints.
The loss of edge caching in a failure scenario will impact overall performance,
and this must be aligned with an acceptable level of service or mitigating design
approach. To ensure a consistent level of service, consider offloading edge
caching to a third-party CDN provider for both paths.
It's recommended to consider a third-party global routing service in place of two Azure
global routing services. This provides the maximum level of fault mitigation and a more
simple design approach since most industry leading CDN providers offer edge
capabilities largely consistent with that offered by Azure Front Door.
Azure Front Door
Use the Azure Front Door managed certificate service to enable TLS connections,
and remove the need to manage certificate lifecycles.
Use the Azure Front Door Web Application Firewall (WAF) to provide protection at
the edge from common web exploits and vulnerabilities, such as SQL injection.
Use the Azure Front Door built-in cache to serve static content from edge nodes.
In most cases this will also eliminate the need for a dedicated Content Delivery
Network (CDN).
Configure the application platform ingress points to validate incoming requests
through header based filtering using the X-Azure-FDID to ensure all traffic is
flowing through the configured Front Door instance. Consider also configuring IP
ACLing using Front Door Service Tags to validate traffic originates from the Azure
Front Door backend IP address space and Azure infrastructure services. This will
ensure traffic flows through Azure Front Door at a service level, but header based
filtering will still be required to ensure the use of a configured Front Door instance.
Define a custom TCP health endpoint to validate critical downstream dependencies
within a regional deployment stamp, including data platform replicas, such as
Azure Cosmos DB in the example provided by the foundational reference
implementation. If one or more dependencies becomes unhealthy, the health
probe should reflect this in the response returned so that the entire regional stamp
can be taken out of circulation.
Ensure health probe responses are logged and ingest all operational data exposed
by Azure Front Door into the global Log Analytics workspace to facilitate a unified
data sink and single operational view across the entire application.

Unless the workload is extremely latency sensitive, spread traffic evenly across all
considered regional stamps to most effectively use deployed resources.
To achieve this, set the "Latency Sensitivity (Additional Latency)" parameter to a
value that is high enough to cater for latency differences between the different
regions of the backends. Ensure a tolerance that is acceptable to the application
workload regarding overall client request latency.
Don't enable Session Affinity unless it's required by the application, since it can
have a negative impact the balance of traffic distribution. With a fully stateless
application, if the recommended mission-critical application design approach is
followed, any request could be handled by any of the regional deployments.
Azure Traffic Manager
Use Traffic Manager for non HTTP/S scenarios as a replacement to Azure Front
Door. Capability differences will drive different design decisions for cache and WAF
capabilities, and TLS certificate management.
WAF capabilities should be considered within each region for the Traffic Manager
ingress path, using Azure Application Gateway.
Configure a suitably low TTL value to optimize the time required to remove an
unhealthy backend endpoint from circulation in the event that backend becomes
unhealthy.
Similar to with Azure Front Door, a custom TCP health endpoint should be defined
to validate critical downstream dependencies within a regional deployment stamp,
which should be reflected in the response provided by health endpoints.
However, for Traffic Manager additional consideration should be given to service
level regional fail over. such as 'dog legging', to mitigate the potential delay
associated with the removal of an unhealthy backend due to dependency failures,
particularly if it's not possible to set a low TTL for DNS records.
Consideration should be given to third-party CDN providers in order to achieve
edge caching when using Azure Traffic Manager as a primary global routing
service. Where edge WAF capabilities are also offered by the third-party service,
consideration should be given to simplify the ingress path and potentially remove
the need for Application Gateway.

Application delivery services

The network ingress path for a mission-critical application must also consider
application delivery services to ensure secure, reliable, and scalable ingress traffic.
This section builds on global routing recommendations by exploring key application
delivery capabilities, considering relevant services such as Azure Standard Load Balancer,
Azure Application Gateway, and Azure API Management.

Design considerations
TLS encryption is critical to ensure the integrity of inbound user traffic to a
mission-critical application, with TLS Offloading applied only at the point of a
stamp's ingress to decrypt incoming traffic. TLS Offloading Requires the private key
of the TLS certificate to decrypt traffic.
A Web Application Firewall provides protection against common web exploits and
vulnerabilities, such as SQL injection or cross site scripting, and is essential to
achieve the maximum reliability aspirations of a mission-critical application.
Azure WAF provides out-of-the-box protection against the top 10 OWASP
vulnerabilities using managed rule sets.
Custom rules can also be added to extend the managed rule set.
Azure WAF can be enabled within either Azure Front Door, Azure Application
Gateway, or Azure CDN (currently in public preview).
The features offered on each of the services differ slightly. For example, the
Azure Front Door WAF provides rate limiting, geo-filtering and bot
protection, which aren't yet offered within the Application Gateway WAF.
However, they all support both built-in and custom rules and can be set to
operate in detection mode or prevention mode.
The roadmap for Azure WAF will ensure a consistent WAF feature set is
provided across all service integrations.
Third-party WAF technologies such as NVAs and advanced ingress controllers
within Kubernetes can also be considered to provide requisite vulnerability
protection.
Optimal WAF configuration typically requires fine tuning, regardless of the
technology used.
Azure Front Door
Azure Front Door only accepts HTTP and HTTPS traffic, and will only process
requests with a known Host header. This protocol blocking helps to mitigate

volumetric attacks spread across protocols and ports, and DNS amplification and
TCP poisoning attacks.
Azure Front Door is a global Azure resource so configuration is deployed globally
to all edge locations.
Resource configuration can be distributed at a massive scale to handle
hundreds of thousands of requests per second.
Updates to configuration, including routes and backend pools, are seamless and
won't cause any downtime during deployment.
Azure Front Door provides both a fully managed certificate service and a bringyour-own-certificate method for the client-facing SSL certificates. The fully
managed certificate service provides a simplified operational approach and helps
to reduce complexity in the overall design by performing certificate management
within a single area of the solution.
Azure Front Door auto-rotates "Managed" certificates at least 60 days ahead of
certificate expiration to protect against expired certificate risks. If self-managed
certificates are used, updated certificates should be deployed no later than 24
hours prior to expiration of the existing certificate, otherwise clients may receive
expired certificate errors.
Certificate updates will only result in downtime if Azure Front Door is switched
between "Managed" and "Use Your Own Certificate".
Azure Front Door is protected by Azure DDoS Protection Basic, which is integrated
into Front Door by default. This provides always-on traffic monitoring, real-time
mitigation, and also defends against common Layer 7 DNS query floods or Layer
3/4 volumetric attacks.
These protections help to maintain Azure Front Door availability even when
faced with a DDoS attack. Distributed Denial of Service (DDoS) attacks can
render a targeted resource unavailable by overwhelming it with illegitimate
traffic.
Azure Front Door also provides WAF capabilities at a global traffic level, while
Application Gateway WAF must be provided within each regional deployment
stamp. Capabilities include firewall rulesets to protect against common attacks,
geo-filtering, address blocking, rate limiting, and signature matching.
Azure Load Balancer
The Azure Basic Load Balancer SKU isn't backed by an SLA and has several
capability constraints compared to the Standard SKU.

Design recommendations
Perform TLS Offloading in as few places as possible in order to maintain security
whilst simplifying the certificate management lifecycle.
Use encrypted connections (e.g. HTTPS) from the point where TLS offloading
occurs to the actual application backends. Application endpoints won't be visible
to end users, so Azure-managed domains, such as azurewebsites.net or
cloudapp.net , can be used with managed certificates.

For HTTP(S) traffic, ensure WAF capabilities are applied within the ingress path for
all publicly exposed endpoints.
Enable WAF capabilities at a single service location, either globally with Azure Front
Door or regionally with Azure Application Gateway, since this simplifies
configuration fine tuning and optimizes performance and cost.
Configure WAF in Prevention mode to directly block attacks. Only use WAF in
Detection mode (i.e. only logging but not blocking suspicious requests) when the
performance penalty of Prevention mode is too high. The implied additional risk
must be fully understood and aligned to the specific requirements of the workload
scenario.
Prioritize the use of Azure Front Door WAF since it provides the richest Azurenative feature set and applies protections at the global edge, which simplifies the
overall design and drives further efficiencies.
Use Azure API Management only when exposing a large number of APIs to
external clients or different application teams.
Use the Azure Standard Load Balancer SKU for any internal traffic distribution
scenario within micros-service workloads.
Provides an SLA of 99.99% when deployed across Availability Zones.
Provides critical capabilities such as diagnostics or outbound rules.
Use Azure DDoS Network Protection to help protect public endpoints hosted
within each application virtual network.

Caching and static content delivery
Special treatment of static content like images, JavaScript, CSS and other files can have a
significant impact on the overall user experience as well as on the overall cost of the
solution. Caching static content at the edge can speed up the client load times which

results in a better user experience and can also reduce the cost for traffic, read
operations and computing power on backend services involved.

Design considerations
Not all content that a solution makes available over the Internet is generated
dynamically. Applications serve both static assets (images, JavaScript, CSS,
localization files, etc.) and dynamic content.
Workloads with frequently accessed static content benefit greatly from caching
since it reduces the load on backend services and reduces content access latency
for end users.
Caching can be implemented natively within Azure using either Azure Front Door
or Azure Content Delivery Network (CDN).
Azure Front Door provides Azure-native edge caching capabilities and routing
features to divide static and dynamic content.
By creating the appropriate routing rules in Azure Front Door, /static/*
traffic can be transparently redirected to static content.
More complex caching scenarios can be implemented using the Azure CDN
service to establish a full-fledged content delivery network for significant static
content volumes.
The Azure CDN service will likely be more cost effective, but does not provide
the same advanced routing and Web Application Firewall (WAF) capabilities
which are recommended for other areas of an application design. It does,
however, offer further flexibility to integrate with similar services from thirdparty solutions, such as Akamai and Verizon.
When comparing the Azure Front Door and Azure CDN services, the following
decision factors should be explored:
Can required caching rules be accomplished using the rules engine.
Size of the stored content and the associated cost.
Price per month for the execution of the rules engine (charged per request
on Azure Front Door).
Outbound traffic requirements (price differs by destination).

Design recommendations
Generated, static content like sized copies of image files that never or only rarely
change can benefit from caching as well. Caching can be configured based on URL
parameters and with varying caching duration.
Separate the delivery of static and dynamic content to users and deliver relevant
content from a cache to reduce load on backend services optimize performance

for end-users.
Given the strong recommendation (Network and connectivity design area) to use
Azure Front Door for global routing and Web Application Firewall (WAF) purposes,
it's recommended to prioritize the use of Azure Front Door caching capabilities
unless gaps exist.

Virtual network integration
A mission-critical application will typically encompass requirements for integration with
other applications or dependent systems, which could be hosted on Azure, another
public cloud, or on-premises data centers. This application integration can be
accomplished using public-facing endpoints and the internet, or private networks
through network-level integration. Ultimately, the method by which application
integration is achieved will have a significant impact on the security, performance, and
reliability of the solution, and strongly impacting design decisions within other design
areas.
A mission-critical application can be deployed within one of three overarching network
configurations, which determines how application integration can occur at a network
level.
1. Public application without corporate network connectivity.
2. Public application with corporate network connectivity.
3. Private application with corporate network connectivity.
Ｕ Caution
When deploying within an Azure landing zone, configuration 1. should be deployed
within an Online Landing Zone, while both 2) and 3) should be deployed within a
Corp. Connected Landing Zone to facilitate network-level integration.
This section explores these network integration scenarios, layering in the appropriate
use of Azure Virtual Networks and surrounding Azure networking services to ensure
integration requirements are optimally satisfied.

Design Considerations
No virtual networks

The simplest design approach is to not deploy the application within a virtual
network.
Connectivity between all considered Azure services will be provided entirely
through public endpoints and the Microsoft Azure backbone. Connectivity
between public endpoints hosted on Azure will only traverse the Microsoft
backbone and won't go over the public internet.
Connectivity to any external systems outside Azure will be provided by the
public internet.
This design approach adopts "identity as a security perimeter" to provide access
control between the various service components and dependent solution. This may
be an acceptable solution for scenarios that are less sensitive to security. All
application services and dependencies are accessible through a public endpoint
leaves them vulnerable to additional attack vectors orientated around gaining
unauthorized access.
This design approach is also not applicable for all Azure services, since many
services, such as AKS, have a hard requirement for an underlying virtual network.

Isolated virtual networks
To mitigate the risks associated with unnecessary public endpoints, the application
can be deployed within a standalone network that isn't connected to other
networks.
Incoming client requests will still require a public endpoint to be exposed to the
internet, however, all subsequent communication can be within the virtual network
using private endpoints. When using Azure Front Door Premium, it's possible to
route directly from edge nodes to private application endpoints.
While private connectivity between application components will occur over virtual
networks, all connectivity with external dependencies will still rely on public
endpoints.
Connectivity to Azure platform services can be established with Private
Endpoints if supported. If other external dependencies exist on Azure, such as
another downstream application, connectivity will be provided through public
endpoints and the Microsoft Azure backbone.
Connectivity to any external systems outside Azure would be provided by the
public internet.
For scenarios where there are no network integration requirements for external
dependencies, deploying the solution within an isolated network environment

provides maximum design flexibility. No addressing and routing constraints
associated with broader network integration.
Azure Bastion is a fully platform-managed PaaS service that can be deployed on a
virtual network and provides secure RDP/SSH connectivity to Azure VMs. When
you connect via Azure Bastion, virtual machines don't need a public IP address.
The use of application virtual networks introduces significant deployment
complexities within CI/CD pipelines, since both data plane and control plane access
to resources hosted on private networks is required to facilitate application
deployments.
Secure private network path must be established to allow CI/CD tooling to
perform requisite actions.
Private build agents can be deployed within application virtual networks to
proxy access to resources secured by the virtual network.

Connected virtual networks
For scenarios with external network integration requirements, application virtual
networks can be connected to other virtual networks within Azure, another cloud
provider, or on-premises networks using a variety of connectivity options. For
example, some application scenarios might consider application-level integration
with other line-of-business applications hosted privately within an on-premises
corporate network.
The application network design must align with the broader network architecture,
particularly concerning topics such as addressing and routing.
Overlapping IP address spaces across Azure regions or on-premises networks will
create major contention when network integration is considered.
A virtual network resource can be updated to consider additional address space,
however, when a virtual network address space of a peered network changes a
sync on the peering link is required , which will temporarily disable peering.
Azure reserves five IP addresses within each subnet, which should be considered
when determining appropriate sizes for application virtual networks and
encompassed subnets.
Some Azure services require dedicated subnets, such as Azure Bastion, Azure
Firewall, or Azure Virtual Network Gateway. The size of these service subnets is
very important, since they should be large enough to support all current
instances of the service considering future scale requirements, but not so large
as to unnecessarily waste addresses.

When on-premises or cross-cloud network integration is required, Azure offers two
different solutions to establish a secure connection.
An ExpressRoute circuit can be sized to provide bandwidths up to 100 Gbps.
A Virtual Private Network (VPN) can be sized to provide aggregated bandwidth
up to 10 Gbps in hub and spoke networks, and up to 20 Gbps in Azure Virtual
WAN.
７ Note
When deploying within an Azure landing zone, be aware that any required
connectivity to on-premises networks should be provided by the landing zone
implementation. The design can use ExpressRoute and other virtual networks in
Azure using either Virtual WAN or a hub-and-spoke network design.
The inclusion of additional network paths and resources introduces additional
reliability and operational considerations for the application to ensure health is
maintained.

Design recommendations
It's recommended that mission-critical solutions are deployed within Azure virtual
networks where possible to remove unnecessary public endpoints, limiting the
application attack surface to maximize security and reliability.
Use Private Endpoints for connectivity to Azure platform services. Service
Endpoints can be considered for services that don support Private Link,
provided data exfiltration risks are acceptable or mitigated through alternative
controls.
For application scenarios that don't require corporate network connectivity, treat
all virtual networks as ephemeral resources that are replaced when a new regional
deployment is conducted.
When connecting to other Azure or on-premises networks, application virtual
networks shouldn't be treated as ephemeral since it creates significant
complications where virtual network peering and virtual network gateway
resources are concerned. All relevant application resources within the virtual
network should continue to be ephemeral, with parallel subnets used to facilitate
blue-green deployments of updated regional deployment stamps.
In scenarios where corporate network connectivity is required to facilitate
application integration over private networks, ensure that the IPv4 address space

used for regional application virtual networks doesn't overlap with other
connected networks and is properly sized to facilitate required scale without
needing to update the virtual network resource and incur downtime.
It's strongly recommended to only use IP addresses from the address allocation
for private internet (RFC 1918).
For environments with a limited availability of private IP addresses (RFC
1918), consider using IPv6.
If the use of public IP address is required, ensure that only owned address
blocks are used.
Align with organization plans for IP addressing in Azure to ensure that
application network IP address space doesn't overlap with other networks
across on-premises locations or Azure regions.
Don't create unnecessarily large application virtual networks to ensure that IP
address space isn't wasted.
Prioritize the use Azure CNI for AKS network integration, since it supports a richer
feature set.
Consider Kubenet for scenarios with a limited rage available IP addresses to fit
the application within a constrained address space.
Prioritize the use of the Azure CNI network plugin for AKS network integration
and consider Kubenet for scenarios with a limited range of available IP
addresses. See Micro-segmentation and kubernetes network policies for more
details.
For scenarios requiring on-premises network integration, prioritize the use Express
Route to ensure secure and scalable connectivity.
Ensure the reliability level applied to the Express Route or VPN fully satisfies
application requirements.
Multiple network paths should be considered to provide additional redundancy
when required, such as cross connected ExpressRoute circuits or the use of VPN
as a failover connectivity mechanism.
Ensure all components on critical network paths are in line with the reliability and
availability requirements of associated user flows, regardless of whether the
management of these paths and associated component is delivered by the
application team of central IT teams.
７ Note

When deploying within an Azure landing zone and integrating with a broader
organizational network topology, consider the network guidance to ensure
the foundational network is aligned with Microsoft best-practices.
Use Azure Bastion or proxied private connections to access the data plane of Azure
resources or perform management operations.

Internet egress
Internet egress is a foundational network requirement for a mission-critical application
to facilitate external communication in the context of:
1. Direct application user interaction.
2. Application integration with external dependencies outside Azure.
3. Access to external dependencies required by the Azure services used by the
application.
This section explores how internet egress can be achieved while ensuring security,
reliability, and sustainable performance are maintained, highlighting key egress
requirements for services recommended in a mission-critical context.

Design Considerations
Many Azure services require access to public endpoints for various management
and control plane functions to operate as intended.
Azure provides different direct internet outbound connectivity methods, such as
Azure NAT gateway or Azure Load Balancer, for virtual machines or compute
instances on a virtual network.
When traffic from inside a virtual network travels out to the Internet, Network
Address Translation (NAT) must take place. This is a compute operation that occurs
within the networking stack and that can therefore impact system performance.
When NAT takes place at a small scale the performance impact should be
negligible, however, if there are a large number of outbound requests network
issues may occur. These issues typically come in the form of 'Source NAT (or SNAT)
port exhaustion'.
In a multi-tenant environment, such as Azure App Service, there's a limited number
of outbound ports available to each instance. If these ports run out, no new
outbound connections can be initiated. This issue can be mitigated by reducing

the number of private/public edge traversals or by using a more scalable NAT
solution such as the Azure NAT Gateway.
In addition to NAT limitations, outbound traffic may also be subject to requisite
security inspections.
Azure Firewall provides appropriate security capabilities to secure network
egress.
Azure Firewall (or an equivalent NVA) can be used to secure Kubernetes egress
requirements by providing granular control over outbound traffic flows.
Large volumes of internet egress will incur data transfer charges .
Azure NAT Gateway
Azure NAT Gateway supports 64,000 connections for TCP and UDP per assigned
outbound IP address.
Up to 16 IP addresses can be assigned to a single NAT gateway.
A default TCP idle timeout of 4 minutes. If idle timeout is altered to a higher
value, flows will be held for longer, which will increase the pressure on the SNAT
port inventory.
NAT gateway can't provide zonal isolation out-of-the-box. To get zonal
redundancy, a subnet containing zonal resources must be aligned with
corresponding zonal NAT gateways.

Design recommendations
Minimize the number of outgoing Internet connections as this will impact NAT
performance.
If large numbers of internet-bound connections are required, consider using
Azure NAT Gateway to abstract outbound traffic flows.
Use Azure Firewall where requirements to control and inspect outbound internet
traffic exist.
Ensure Azure Firewall isn't used to inspect traffic between Azure services.
７ Note
When deploying within an Azure landing zone, consider using the foundational
platform Azure Firewall resource (or equivalent NVA). If a dependency is taken on a
central platform resource for internet egress, then the reliability level of that

resource and associated network path should be closely aligned with application
requirements. Operational data from the resource should also be made available to
the application in order to inform potential operational action in failure scenarios.
If there are high-scale requirements associated with outbound traffic, consideration
should be given to a dedicated Azure Firewall resource for a mission-critical application,
to mitigate risks associated with using a centrally shared resource, such as noisy
neighbor scenarios.
When deployed within a Virtual WAN environment, consideration should be given
to Firewall Manager to provide centralized management of dedicated application
Azure Firewall instances to ensure organizational security postures are observed
through global firewall policies.
Ensure incremental firewall policies are delegated to application security teams via
role-based access control to allow for application policy autonomy.

Inter-zone and inter-region Connectivity
While the application design strongly advocates independent regional deployment
stamps, many application scenarios may still require network integration between
application components deployed within different zones or Azure regions, even if only
under degraded service circumstances. The method by which inter-zone and interregion communication is achieved has a significant bearing on overall performance and
reliability, which will be explored through the considerations and recommendations
within this section.

Design Considerations
The application design approach for a mission-critical application endorses the use
of independent regional deployments with zonal redundancy applied at all
component levels within a single region.
An Availability Zone (AZ) is a physically separate data center location within an
Azure region, providing physical and logical fault isolation up to the level of a
single data center.
A round-trip latency of less than 2 ms is guaranteed for inter-zone communication.
Zones will have a small latency variance given varied distances and fiber paths
between zones.

Availability Zone connectivity depends on regional characteristics, and therefore
traffic entering a region via an edge location may need to be routed between
zones to reach its destination. This will add a ~1ms-2ms latency given inter-zone
routing and 'speed of light' constraints, but this should only have a bearing for
hyper sensitive workloads.
Availability Zones are treated as logical entities within the context of a single
subscription, so different subscriptions might have a different zonal mapping for
the same region. For example, zone 1 in Subscription A could correspond to the
same physical data center as zone 2 in subscription B.
Communication between zones within a region incurs a data transfer charge

per

GB of bandwidth.
With application scenarios that are extremely chatty between application
components, spreading application tiers across zones can introduce significant
latency and increased costs. It's possible to mitigate this within the design by
constraining a deployment stamp to a single zone and deploying multiple stamps
across the different zones.
Communication between different Azure regions incurs a larger data transfer
charge

per GB of bandwidth.

The applicable data transfer rate depends on the continent of the considered
Azure regions.
Data traversing continents are charged at a considerably higher rate.
Express Route and VPN connectivity methods can also be used to directly connect
different Azure regions together for certain scenarios, or even different cloud
platforms.
For services to service communication Private Link can be used for direct
communication using private endpoints.
Traffic can be hair-pinned through Express Route circuits used for on-premise
connectivity in order to facilitate routing between virtual networks within an Azure
region and across different Azure regions within the same geography.
Hair-pinning traffic through Express Route will bypass data transfer costs
associated with virtual network peering, so can be used as a way to optimize
costs.
This approach necessitates additional network hops for application integration
within Azure, which introduces latency and reliability risks. Expands the role of
Express Route and associated gateway components from Azure/on-premises to
also encompass Azure/Azure connectivity.

When submillisecond latency are required between services, Proximity Placement
Groups can be used when supported by the services used.

Design recommendations
Use virtual network peering to connect networks within a region and across
different regions. It's strongly recommended to avoid hair-pinning within Express
Route.
Use Private Link to establish communication directly between services in the same
region or across regions (service in Region A communicating with service in Region
B.
For application workloads that are extremely chatty between components,
consider constraining a deployment stamp to a single zone and deploying multiple
stamps across the different zones. This ensures zonal redundancy is maintained at
the level of an encapsulated deployment stamp rather than a single application
component.
Where possible, treat each deployment stamp as independent and disconnected
from other stamps.
Use data platform technologies to synchronize state across regions rather than
achieving consistency at an application level with direct network paths.
Avoid 'dog legging' traffic between different regions unless necessary, even in a
failure scenario. Use global routing services and end-to-end health probes to
take an entire stamp out of circulation in the event that a single critical
component tier fails, rather than routing traffic at that faulty component level to
another region.
For hyper latency sensitive application scenarios, prioritize the use of zones with
regional network gateways to optimize network latency for ingress paths.

Micro-segmentation and Kubernetes network
policies
Micro-segmentation is a network security design pattern used to isolate and secure
individual application workloads, with policies applied to limit network traffic between
workloads based on a Zero Trust model. It's typically applied to reduce network attack
surface, improve breach containment, and strengthen security through policy-driven
application-level network controls.

A mission-critical application can enforce application-level network security using
Network Security Groups (NSG) at either a subnet or network interface level, service
Access Control Lists (ACL), and network policies when using Azure Kubernetes Service
(AKS).
This section explores the optimal use of these capabilities, providing key considerations
and recommendations to achieve application-level micro-segmentation.

Design Considerations
AKS can be deployed in two different networking models:
Kubenet networking: AKS nodes are integrated within an existing virtual
network, but pods exist within a virtual overlay network on each node. Traffic
between pods on different nodes is routed through kube-proxy.
Azure Container Networking Interface (CNI) networking: The AKS cluster is
integrated within an existing virtual network and its nodes, pods and services
received IP addresses from the same virtual network the cluster nodes are
attached to. This is relevant for various networking scenarios requiring direct
connectivity from and to pods. Different node pools can be deployed into
different subnets.
７ Note
Azure CNI requires more IP address space compared to Kubenet. Proper
upfront planning and sizing of the network is required. For more information,
refer to the Azure CNI documentation.
By default, pods are non-isolated and accept traffic from any source and can send
traffic to any destination; a pod can communicate with every other pod in a given
Kubernetes cluster; Kubernetes doesn't ensure any network level isolation, and
doesn't isolate namespaces at the cluster level.
Communication between Pods and Namespaces can be isolated using Network
Policies . Network Policy is a Kubernetes specification that defines access policies
for communication between Pods. Using Network Policies, an ordered set of rules
can be defined to control how traffic is sent/received, and applied to a collection of
pods that match one or more label selectors.
AKS supports two plugins that implement Network Policy, Azure and Calico.
Both plugins use Linux IPTables to enforce the specified policies. See Differences
between Azure and Calico policies and their capabilities for more details.
Network policies don't conflict since they're additive.

For a network flow between two pods to be allowed, both the egress policy on
the source pod and the ingress policy on the destination pod need to allow the
traffic.
The network policy feature can only be enabled at cluster instantiation time. It is
not possible to enable network policy on an existing AKS cluster.
The delivery of network policies is consistent regardless of whether Azure or
Calico is used.
Calico provides a richer feature set, including support for windows-nodes and
supports Azure CNI as well as Kubenet.
AKS supports the creation of different node pools to separate different workloads
using nodes with different hardware and software characteristics, such as nodes
with and without GPU capabilities.
Using node pools doesn't provide any network-level isolation.
Node pools can use different subnets within the same virtual network. NSGs can
be applied at the subnet-level to implement micro-segmentation between node
pools.

Design recommendations
Configure an NSG on all considered subnets to provide an IP ACL to secure ingress
paths and isolate application components based on a Zero Trust model.
Use Front Door Service Tags within NSGs on all subnets containing application
backends defined within Azure Front Door, since this will validate traffic
originates from a legitimate Azure Front Door backend IP address space. This
will ensure traffic flows through Azure Front Door at a service level, but header
based filtering will still be required to ensure the use of a particular Front Door
instance and to also mitigate 'IP spoofing' security risks.
Public internet traffic should be disabled on RDP and SSH ports across all
applicable NSGs.
Prioritize the use of the Azure CNI network plugin and consider Kubenet for
scenarios with a limited range of available IP addresses to fit the application
within a constrained address space.
AKS supports the use of both Azure CNI and Kubenet. It is selected at
deployment time.
The Azure CNI network plugin is a more robust and scalable network plugin,
and is recommended for most scenarios.
Kubenet is a more lightweight network plugin, and is recommended for
scenarios with a limited range of available IP addresses.

See Azure CNI for more details.
The Network Policy feature in Kubernetes should be used to define rules for
ingress and egress traffic between pods in a cluster. Define granular Network
Policies to restrict and limit cross-pod communication.
Enable Network Policy for Azure Kubernetes Service at deployment time.
Prioritize the use of Calico because it provides a richer feature set with broader
community adoption and support.

Next step
Review the considerations for quantifying and observing application health.
Health modeling and observability

Health modeling and observability of
mission-critical workloads on Azure
Article • 08/07/2023

Health modeling and observability are essential concepts to maximize reliability, which
focuses on robust and contextualized instrumentation and monitoring. These concepts
provide critical insight into application health, promoting the swift identification and
resolution of issues.
Most mission-critical applications are significant in terms of both scale and complexity
and therefore generate high volumes of operational data, which makes it challenging to
evaluate and determine optimal operational action. Health modeling ultimately strives
to maximize observability by augmenting raw monitoring logs and metrics with key
business requirements to quantify application health, driving automated evaluation of
health states to achieve consistent and expedited operations.
This design area focuses on the process to define a robust health model, mapping
quantified application health states through observability and operational constructs to
achieve operational maturity.
） Important
This article is part of the Azure Well-Architected mission-critical workload series. If
you aren't familiar with this series, we recommend you start with what is a missioncritical workload?
There are three main levels of operational maturity when striving to maximize reliability.
1. Detect and respond to issues as they happen.
2. Diagnose issues that are occurring or have already occurred.
3. Predict and prevent issues before they take place.

Video: Define a health model for your missioncritical workload
https://learn-video.azurefd.net/vod/player?id=fd8c4e50-9d7f-4df0-97cbd0474b581398&embedUrl=%2Fazure%2Fwell-architected%2Fmissioncritical%2Fmission-critical-health-modeling&locale=en-us

Layered application health
To build a health model, first define application health in the context of key business
requirements by quantifying ‘healthy’ and ‘unhealthy’ states in a layered and measurable
format. Then, for each application component, refine the definition in the context of a
steady running state and aggregated according to the application user flows.
Superimpose with key non-functional business requirements for performance and
availability. Finally, aggregate the health states for each individual user flow to form an
acceptable representation of the overall application health. Once established, these
layered health definitions should be used to inform critical monitoring metrics across all
system components and validate operational subsystem composition.
） Important
When defining what 'unhealthy' states, represent for all levels of the application. It's
important to distinguish between transient and non-transient failure states to
qualify service degradation relative to unavailability.

Design considerations
The process of modeling health is a top-down design activity that starts with an
architectural exercise to define all user flows and map dependencies between
functional and logical components, thereby implicitly mapping dependencies
between Azure resources.
A health model is entirely dependent on the context of the solution it represents,
and therefore can't be solved 'out-of-the-box' because 'one size doesn't fit all'.
Applications will differ in composition and dependencies
Metrics and metric thresholds for resources must also be finely tuned in terms
of what values represent healthy and unhealthy states, which are heavily
influenced by encompassed application functionality and target non-functional
requirements.
A layered health model enables application health to be traced back to lower level
dependencies, which helps to quickly root cause service degradation.
To capture health states for an individual component, that component's distinct
operational characteristics must be understood under a steady state that is
reflective of production load. Performance testing is therefore a key capability to
define and continually evaluate application health.

Failures within a cloud solution may not happen in isolation. An outage in a single
component may lead to several capabilities or additional components becoming
unavailable.
Such errors may not be immediately observable.

Design recommendations
Define a measurable health model as a priority to ensure a clear operational
understanding of the entire application.
The health model should be layered and reflective of the application structure.
The foundational layer should consider individual application components, such
as Azure resources.
Foundational components should be aggregated alongside key non-functional
requirements to build a business-contextualized lens into the health of system
flows.
System flows should be aggregated with appropriate weights based on business
criticality to build a meaningful definition of overall application health.
Financially significant or customer-facing user flows should be prioritized.
Each layer of the health model should capture what ‘healthy’ and ‘unhealthy’
states represent.
Ensure the heath model can distinguish between transient and non-transient
unhealthy states to isolate service degradation from unavailability.
Represent health states using a granular health score for every distinct application
component and every user flow by aggregating health scores for mapped
dependent components, considering key non-functional requirements as
coefficients.
The health score for a user flow should be represented by the lowest score
across all mapped components, factoring in relative attainment against nonfunctional requirements for the user flow.
The model used to calculate health scores must consistently reflect operating
health, and if not, the model should be adjusted and redeployed to reflect new
learnings.
Define health score thresholds to reflect health status.
The health score must be calculated automatically based on underlying metrics,
which can be visualized through observability patterns and acted on through
automated operational procedures.
The health score should become core to the monitoring solution, so that
operating teams no longer have to interpret and map operational data to
application health.

Use the health model to calculate availability Service Level Objective (SLO)
attainment instead of raw availability, ensuring the demarcation between service
degradation and unavailability is reflected as separate SLOs.
Use the health model within CI/CD pipelines and test cycles to validate application
health is maintained after code and configuration updates.
The health model should be used to observe and validate health during load
testing and chaos testing as part of CI/CD processes.
Building and maintaining a health model is an iterative process and engineering
investment should be aligned to drive continuous improvements.
Define a process to continually evaluate and fine-tune the accuracy of the
model, and consider investing in machine learning models to further train the
model.

Example - Layered health model
This is a simplified representation of a layered application health model for illustrative
purposes. A comprehensive and contextualized health model is provided in the MissionCritical reference implementations:
Mission-Critical Online
Mission-Critical Connected
When implementing a health model it's important to define the health of individual
components through the aggregation and interpretation of key resource-level metrics.
An example of how resource metrics can be used is the image below:

This definition of health can subsequently be represented by a KQL query, as
demonstrated by the example query below that aggregates InsightsMetrics (Container

insights) and AzureMetrics (diagnostics setting for AKS cluster) and compares (inner
join) against modeled health thresholds.
kql

// ClusterHealthStatus
let Thresholds=datatable(MetricName: string, YellowThreshold: double,
RedThreshold: double) [
// Disk Usage:
"used_percent", 50, 80,
// Average node cpu usage %:
"node_cpu_usage_percentage", 60, 90,
// Average node disk usage %:
"node_disk_usage_percentage", 60, 80,
// Average node memory usage %:
"node_memory_rss_percentage", 60, 80
];
InsightsMetrics
| summarize arg_max(TimeGenerated, *) by Computer, Name
| project TimeGenerated,Computer, Namespace, MetricName = Name, Value=Val
| extend NodeName = extract("([a-z0-9-]*)(-)([a-z0-9]*)$", 3, Computer)
| union (
AzureMetrics
| extend ResourceType = extract("(PROVIDERS/MICROSOFT.)([A-Z]*/[A-Z]*)",
2, ResourceId)
| where ResourceType == "CONTAINERSERVICE/MANAGEDCLUSTERS"
| summarize arg_max(TimeGenerated, *) by MetricName
| project TimeGenerated, MetricName, Namespace = "AzureMetrics",
Value=Average
)
| lookup kind=inner Thresholds on MetricName
| extend IsYellow = iff(Value > YellowThreshold and Value < RedThreshold, 1,
0)
| extend IsRed = iff(Value > RedThreshold, 1, 0)
| project NodeName, MetricName, Value, YellowThreshold, IsYellow,
RedThreshold, IsRed

The resulting table output can subsequently be transformed into a health score for
easier aggregation at higher levels of the health model.
kql

// ClusterHealthScore
ClusterHealthStatus
| summarize YellowScore = max(IsYellow), RedScore = max(IsRed)
| extend HealthScore = 1-(YellowScore*0.25)-(RedScore*0.5)

These aggregated scores can subsequently be represented as a dependency chart using
visualization tools such as Grafana to illustrate the health model.

This image shows an example layered health model from the Azure Mission-Critical
online reference implementation

, and demonstrates how a change in health state for

a foundational component can have a cascading impact to user flows and overall
application health (the example values correspond to the table in the previous image).

Demo video: Monitoring and health modeling
demo
https://www.microsoft.com/en-us/videoplayer/embed/RE55Nd9?postJsllMsg=true

Unified data sink for correlated analysis
Many operational datasets must be gathered from all system components to accurately
represent a defined heath model, considering logs and metrics from both application
components and underlying Azure resources. This vast amount of data ultimately needs
to be stored in a format that allows for near-real time interpretation to facilitate swift
operational action. Moreover, correlation across all encompassed data sets is required
to ensure effective analysis is unbounded, allowing for the layered representation of
health.
A unified data sink is required to ensure all operational data is swiftly stored and made
available for correlated analysis to build a 'single pane' representation of application
health. Azure provides several different operational technologies under the umbrella of
Azure Monitor, and the Log Analytics workspace serves as the core Azure-native data
sink to store and analyze operational data.

Design considerations
Azure Monitor
Azure Monitor is enabled by default for all Azure subscriptions, but Azure Monitor
Logs (Log Analytics workspace) and Application Insights resources must be
deployed and configured to incorporate data collection and querying capabilities.
Azure Monitor supports three types of observability data: logs, metrics, and
distributed traces.
Logs are stored in Log Analytics workspaces, which is based on Azure Data
Explorer. Log queries are stored in query packs that can be shared across
subscriptions, and are used to drive observability components such as
dashboards, workbooks, or other reporting and visualization tools.
Metrics are stored in an internal time-series database. For most Azure resources,
metrics are automatically collected and retained for 93 days. Metric data can
also be sent to the Log Analytics workspace using a diagnostic setting for the
resource.
All Azure resources expose logs and metrics, but resources must be appropriately
configured to route diagnostic data to your desired data sink.
 Tip
Azure provides various Built-In Policies that can be applied to ensure deployed
resources are configured to send logs and metrics to a Log Analytics workspace.
It's not uncommon for regulatory controls to require operational data remains
within originating geographies or countries/regions. Regulatory requirements may
stipulate the retention of critical data types for an extended period of time. For
example, in regulated banking, audit data must be retained for at least seven years.

Different operational data types may require different retention periods. For
example, security logs may need to be retained for a long period, while
performance data is unlikely to require long-term retention outside the context of
AIOps.
Data can be archived or exported from Log Analytics workspaces for long term
retention and/or auditing purposes.
Dedicated Clusters provide a deployment option that enables Availability Zones for
protection from zonal failures in supported Azure regions. Dedicated Clusters
require a minimum daily data ingest commitment.
Log Analytics workspaces are deployed into a specified Azure region.
To protect against loss of data from unavailability of a Log Analytics workspace,
resources can be configured with multiple diagnostic settings. Each diagnostic
setting can send metrics and logs to a separate Log Analytics workspace.
Data sent to each additional Log Analytics workspace will incur extra costs.
The redundant Log Analytics workspace can be deployed into the same Azure
region, or into separate Azure regions for additional regional redundancy.
Sending logs and metrics from an Azure resource to a Log Analytics workspace
in a different region will incur inter-region data egress costs.
Some Azure resources require a Log Analytics workspace within the same
region as the resource itself.
See Best practices for Azure Monitor Logs for further availability options for the
Log Analytics workspace.
Log Analytics workspace data can be exported to Azure Storage or Azure Event
Hubs on a continuous, scheduled, or one-time basis.
Data export allows for long-term data archiving and protects against possible
operational data loss due to unavailability.
Available export destinations are Azure Storage or Azure Event Hubs. Azure
Storage can be configured for different redundancy levels including zonal or
regional. Data export to Azure Storage stores the data within .json files.
Data export destinations must be within the same Azure region as the Log
Analytics workspace. An event hub data export destination to be within the
same region as the Log Analytics workspace. Azure Event Hubs geo-disaster
recovery isn't applicable for this scenario.
There are several data export limitations. Only specific tables in the workspace
are supported for data export.
Archiving can be used to store data in a Log Analytics workspace for long-term
retention at a reduced cost without exporting it.

Azure Monitor Logs has user query throttling limits, which may appear as reduced
availability to clients, such as observability dashboards.
Five concurrent queries per user: if five queries are already running, additional
queries are placed in a per-user concurrency queue until a running query ends.
Time in concurrency queue: if a query sits in the concurrency queue for over
three minutes, it will be terminated and a 429 error code returned.
Concurrency queue depth limit: the concurrency queue is limited to 200 queries,
and additional queries will be rejected with a 429 error code.
Query rate limit: there's a per-user limit of 200 queries per 30 seconds across all
workspaces.
Query packs are Azure Resource Manager resources, which can be used to protect
and recover log queries if the Log Analytics workspace is unavailable.
Query packs contain queries as JSON and can be stored external to Azure
similar to other infrastructure-as-code assets.
Deployable through the Microsoft.Insights REST API.
If a Log Analytics workspace must be re-created the query pack can be
redeployed from an externally stored definition.
Application Insights can be deployed in a workspace-based deployment model,
underpinned by a Log Analytics workspace where all the data is stored.
Sampling can be enabled within Application Insights to reduce the amount of
telemetry sent and optimize data ingest costs.
All data collected by Azure Monitor, including Application Insights, is charged
based on the volume of data ingested and the duration that data is retained.
Data ingested into a Log Analytics workspace can be retained at no additional
charge up to first 31 days (90 days if Sentinel is enabled)
Data ingested into a workspace-based Application Insights is retained for the
first 90 days at no extra charge.
The Log Analytics commitment tier pricing model provides a reduced cost and a
predictable approach to data ingest charges.
Any usage above the reservation level is billed at the same price as the current
tier.
Log Analytics, Application Insights, and Azure Data Explorer use the Kusto Query
Language (KQL).
Log Analytics queries are saved as functions within the Log Analytics workspace
( savedSearches ).

Design recommendations
Use Log Analytics workspace as a unified data sink to provide a 'single pane' across
all operational data sets.
Decentralize Log Analytics workspaces across all used deployment regions. Each
Azure region with an application deployment should consider a Log Analytics
workspace to gather all operational data originating from that region. All global
resources should use a separate dedicated Log Analytics workspace, which
should be deployed within a primary deployment region.
Sending all operational data to a single Log Analytics workspace would
create a single point of failure.
Requirements for data residency might prohibit data leaving the originating
region, and federated workspaces solves for this requirement by default.
There's a substantial egress cost associated with transferring logs and metrics
across regions.
All deployment stamps within the same region can use the same regional Log
Analytics workspace.
Consider configuring resources with multiple diagnostic settings pointing to
different Log Analytics workspaces to protect against Azure Monitor unavailability
for applications with fewer regional deployment stamps.
Use Application Insights as a consistent Application Performance Monitoring
(APM) tool across all application components to collect application logs, metrics,
and traces.
Deploy Application Insights in a workspace-based configuration to ensure each
regional Log Analytics workspace contains logs and metrics from both
application components and underlying Azure resources.
Use cross-Workspace queries to maintain a unified 'single pane' across the
different workspaces.
Use query packs to protect log queries in the event of workspace unavailability.
Store query packs within the application git repository as infrastructure-as-code
assets.
All Log Analytics workspaces should be treated as long-running resources with a
different life-cycle to application resources within a regional deployment stamp.
Export critical operational data from Log Analytics workspace for long-term
retention and analytics to facilitate AIOps and advanced analytics to refine the
underlying health model and inform predictive action.

Carefully evaluate which data store should be used for long-term retention; not all
data has to be stored in a hot and queryable data store.
It's strongly recommended to use Azure Storage in a GRS configuration for
long-term operational data storage.
Use the Log Analytics workspace export capability to export all available data
sources to Azure Storage.
Select appropriate retention periods for operational data types within log analytics,
configuring longer retention periods within the workspace where 'hot'
observability requirements exist.
Use Azure Policy to ensure all regional resources route operational data to the
correct Log Analytics workspace.
７ Note
When deploying into an Azure landing zone, if there's a requirement for centralized
storage of operational data, you can fork data at instantiation so it's ingested into
both centralized tooling and Log Analytics workspaces dedicated to the application.
Alternatively, expose access to application Log Analytics workspaces so that central
teams can query application data. It's ultimately critical that operational data
originating from the solution is available within Log Analytics workspaces dedicated
to the application.
If SIEM integration is required, do not send raw log entries, but instead send critical
alerts.
Only configure sampling within Application Insights if it's required to optimize
performance, or if not sampling becomes cost prohibitive.
Excessive sampling can lead to missed or inaccurate operational signals.
Use correlation IDs for all trace events and log messages to tie them to a given
request.
Return correlation IDs to the caller for all calls not just failed requests.
Ensure application code incorporates proper instrumentation and logging to
inform the health model and facilitate subsequent troubleshooting or root cause
analysis when required.
Application code should use Application Insights to facilitate Distributed
Tracing, by providing the caller with a comprehensive error message that
includes a correlation ID when a failure occurs.
Use structured logging

for all log messages.

Add meaningful health probes to all application components.
When using AKS, configure the health endpoints for each deployment (pod) so
that Kubernetes can correctly determine when a pod is healthy or unhealthy.
When using Azure App Service, configure the Health Checks so that scale out
operations will not cause errors by sending traffic to instances that are not-yet
ready, and making sure unhealthy instances are recycled quickly.
If the application is subscribed to Microsoft Mission-Critical Support, consider
exposing key health probes to Microsoft Support, so application health can be
modelled more accurately by Microsoft Support.
Log successful health check requests, unless increased data volumes can't be
tolerated in the context of application performance, since they provide additional
insights for analytical modeling.
Do not configure production Log Analytics workspaces to apply a daily cap, which
limits the daily ingestion of operational data, since this can lead to the loss of
critical operational data.
In lower environments, such as Development and Test, a daily cap can be
considered as an optional cost saving mechanism.
Provided operational data ingest volumes meet the minimum tier threshold,
configure Log Analytics workspaces to use commitment tier based pricing to drive
cost efficiencies relative to the 'pay-as-you-go' pricing model.
It's strongly recommended to store Log Analytics queries using source control and
use CI/CD automation to deploy them to relevant Log Analytics workspace
instances.

Visualization
Visually representing the health model with critical operational data is essential to
achieve effective operations and maximize reliability. Dashboards should ultimately be
utilized to provide near-real time insights into application health for DevOps teams,
facilitating the swift diagnosis of deviations from steady state.
Microsoft provides several data visualization technologies, including Azure Dashboards,
Power BI, and Azure Managed Grafana (currently in-preview). Azure Dashboards is
positioned to provide a tightly integrated out-of-the-box visualization solution for
operational data within Azure Monitor. It therefore has a fundamental role to play in the
visual representation of operational data and application health for a mission-critical
workload. However, there are several limitations in terms of the positioning of Azure

Dashboards as a holistic observability platform, and as a result consideration should be
given to the supplemental use of market-leading observability solutions, such as
Grafana, which is also provided as a managed solution within Azure.
This section focuses on the use of Azure Dashboards and Grafana to build a robust
dashboarding experience capable of providing technical and business lenses into
application health, enabling DevOps teams and effective operation. Robust
dashboarding is essential to diagnose issues that have already occurred, and support
operational teams in detecting and responding to issues as they happen.

Design considerations
When visualizing the health model using log queries, note that there are Log
Analytics limits on concurrent and queued queries, as well as the overall query rate,
with subsequent queries queued and throttled.
Queries to retrieve operational data used to calculate and represent health scores
can be written and executed in either Log Analytics or Azure Data Explorer.
Sample queries are available here

.

Log Analytics imposes several query limits, which must be designed for when
designing operational dashboards.
The visualization of raw resource metrics, such as CPU utilization or network
throughput, requires manual evaluation by operations teams to determine health
status impacts, and this can be challenging during an active incident.
If multiple users use dashboards within a tool like Grafana, the number of queries
sent to Log Analytics multiplies quickly.
Reaching the concurrent query limit on Log Analytics will queue subsequent
queries, making the dashboard experience feel 'slow'.

Design Recommendations
Collect and present queried outputs from all regional Log Analytics Workspaces
and the global Log Analytics Workspace to build a unified view of application
health.
７ Note
If deploying into an Azure landing zone, consider querying the central platform
Log Analytics workspace if key dependencies on platform resources exist, such as

ExpressRoute for on-premises communication.
A ‘traffic light’ model should be used to visually represent 'healthy' and 'unhealthy'
states, with green used to illustrate when key non-functional requirements are fully
satisfied and resources are optimally utilized. Use "Green", "Amber, and "Red" to
represent "Healthy", "Degraded", and "Unavailable" states.
Use Azure Dashboards to create operational lenses for global resources and
regional deployment stamps, representing key metrics such as request count for
Azure Front Door, server side latency for Azure Cosmos DB, incoming/outgoing
messages for Event Hubs, and CPU utilization or deployment statuses for AKS.
Dashboards should be tailored to drive operational effectiveness, infusing
learnings from failure scenarios to ensure DevOps teams have direct visibility into
key metrics.
If Azure Dashboards can't be used to accurately represent the health model and
requisite business requirements, then it's strongly recommended to consider
Grafana as an alternative visualization solution, providing market-leading
capabilities and an extensive open-source plugin ecosystem. Evaluate the
managed Grafana preview offering to avoid the operational complexities of
managing Grafana infrastructure.
When deploying self-hosted Grafana, employ a highly available and geodistributed design to ensure critical operational dashboards can be resilient to
regional platform failures and cascading error scenarios.
Separate configuration state into an external datastore, such as Azure Database
for Postgres or MySQL, to ensure Grafana application nodes remain stateless.
Configure database replication across deployment regions.
Deploy Grafana nodes to App Services in a highly available configuration across
ones within a region, using container based deployments.
Deploy App Service instances across considered deployment regions.
App Services provides a low-friction container platform, which is ideal for
low-scale scenarios such as operational dashboards, and isolating Grafana
from AKS provides a clear separation of concern between the primary
application platform and operational representations for that platform.
Please refer to the Application Platform deign area for further configuration
recommendations.

Use Azure Storage in a GRS configuration to host and manage custom visuals
and plugins.
Deploy app service and database read-replica Grafana components to a
minimum of two deployment regions, and consider employing a model where
Grafana is deployed to all considered deployment regions.
For scenarios targeting a >= 99.99% SLO, Grafana should be deployed within a
minimum of 3 deployment regions to maximize overall reliability for key operational
dashboards.
Mitigate Log Analytics query limits by aggregating queries into a single or small
number of queries, such as by using the KQL 'union' operator, and set an
appropriate refresh rate on the dashboard.
An appropriate maximum refresh rate will depend on the number and
complexity of dashboard queries; analysis of implemented queries is required.
If the concurrent query limit of Log Analytics is being reached, consider optimizing
the retrieval pattern by (temporarily) storing the data required for the dashboard in
a high performance datastore such as Azure SQL.

Automated incident response
While the visual representations of application health provide invaluable operational
and business insights to support issue detection and diagnosis, it relies on the readiness
and interpretations of operational teams, as well as the effectiveness of subsequent
human-triggered responses. Therefore, to maximize reliability it's necessary to
implement extensive alerting to detect proactively and respond to issues in near realtime.
Azure Monitor provides an extensive alerting framework to detect, categorize, and
respond to operational signals through Action Groups. This section will therefore focus
on the use of Azure Monitor alerts to drive automated actions in response to current or
potential deviations from a healthy application state.
） Important
Alerting and automated action is critical to effectively detect and swiftly respond to
issues as they happen, before greater negative impact can occur. Alerting also
provides a mechanism to interpret incoming signals and respond to prevent issues
before they occur.

Design considerations
Alert rules are defined to fire when a conditional criteria is satisfied for incoming
signals, which can include various data sources, such as metrics, log queries, or
availability tests.
Alerts can be defined within Log Analytics or Azure Monitor on the specific
resource.
Some metrics are only interrogatable within Azure Monitor, since not all diagnostic
data points are made available within Log Analytics.
The Azure Monitor Alerts API can be used to retrieve active and historic alerts.
There are subscription limits related to alerting and action groups, which must be
designed for:
Limits exist for the number of configurable alert rules.
The Alerts API has throttling limits, which should be considered for extreme
usage scenarios.
Action Groups have several hard limits for the number of configurable
responses, which must be designed for.
Each response type has a limit of 10 actions, apart from email, which has a
limit of 1,000 actions.
Alerts can be integrated within a layered health model by creating an Alert Rule for
a saved log search query from the model's 'root' scoring function. For example,
using 'WebsiteHealthScore' and alerting on a numeric value that represents an
'Unhealthy' state.

Design recommendations
For resource-centric alerting, create alert rules within Azure Monitor to ensure all
diagnostic data is available for the alert rule criteria.
Consolidate automated actions within a minimal number of Action Groups, aligned
with service teams to support a DevOps approach.
Respond to excessive resource utilization signals through automated scale
operations, using Azure-native auto-scale capabilities where possible. Where builtin auto-scale functionality isn't applicable, use the component health score to
model signals and determine when to respond with automated scale operations.
Ensure automated scale operations are defined according to a capacity model,

which quantifies scale relationships between components, so that scale responses
encompass components that need to be scaled in relation to other components.
Model actions to accommodate a prioritized ordering, which should be
determined by business impact.
Use the Azure Monitor Alerts API to gather historic alerts to incorporate within
'cold' operational storage for advanced analytics.
For critical failure scenarios, which can't be met with an automated response,
ensure operational 'runbook automation' is in-place to drive swift and consistent
action once manual interpretation and sign out is provided. Use alert notifications
to drive swift identification of issues requiring manual interpretation
Create allowances within engineering sprints to drive incremental improvements in
alerting to ensure new failure scenarios that haven't previously been considered
can be fully accommodated within new automated actions.
Conduct operational readiness tests as part of CI/CD processes to validate key alert
rules for deployment updates.

Predictive action and AI operations (AIOps)
Machine learning models can be applied to correlate and prioritize operational data,
helping to gather critical insights related to filtering excessive alert 'noise' and
predicting issues before they cause impact, as well as accelerating incident response
when they do.
More specifically, an AIOps methodology can be applied to critical insights about the
behavior of the system, users, and DevOps processes. These insights can include
identifying a problem happening now (detect), quantifying why the problem is
happening (diagnose), or signaling what will happen in the future (predict). Such insights
can be used to drive actions that adjust and optimize the application to mitigate active
or potential issues, using key business metrics, system quality metrics, and DevOps
productivity metrics, to prioritize according to business impact. Conducted actions can
themselves be infused into the system through a feedback loop that further trains the
underlying model to drive additional efficiencies.

There are multiple analytical technologies within Azure, such as Azure Synapse and
Azure Databricks, which can be used to build and train analytical models for AIOps. This
section will therefore focus on how these technologies can be positioned within an
application design to accommodate AIOps and drive predictive action, focusing on
Azure Synapse that reduces friction by bringing together the best of Azure's data
services along with powerful new features.
AIOps is used to drive predictive action, interpreting and correlating complex
operational signals observed over a sustained period in order to better respond to and
prevent issues before they occur.

Design considerations
Azure Synapse Analytics offers multiple Machine Learning (ML) capabilities.
ML models can be trained and run on Synapse Spark Pools with libraries
including MLLib, SparkML and MMLSpark, as well as popular open-source
libraries, such as Scikit Learn

.

ML models can be trained with common data science tools like PySpark/Python,
Scala, or .NET.
Synapse Analytics is integrated with Azure ML through Azure Synapse Notebooks,
which enables ML models to be trained in an Azure ML Workspace using
Automated ML.
Synapse Analytics also enables ML capabilities using Azure Cognitive Services to
solve general problems in various domains, such as Anomaly Detection. Cognitive
Services can be used in Azure Synapse, Azure Databricks, and via SDKs and REST
APIs in client applications.

Azure Synapse natively integrates with Azure Data Factory tools to extract,
transform, and load (ETL) or ingest data within orchestration pipelines.
Azure Synapse enables external dataset registration to data stored in Azure Blob
storage or Azure Data Lake Storage.
Registered datasets can be used in Synapse Spark pool data analytics tasks.
Azure Databricks can be integrated into Azure Synapse Analytics pipelines for
additional Spark capabilities.
Synapse orchestrates reading data and sending it to a Databricks cluster, where
it can be transformed and prepared for ML model training.
Source data typically needs to be prepared for analytics and ML.
Synapse offers various tools to assist with data preparation, including Apache
Spark, Synapse Notebooks, and serverless SQL pools with T-SQL and built-in
visualizations.
ML models that have been trained, operationalized, and deployed can be used for
batch scoring in Synapse.
AIOps scenarios, such as running regression or degradation predictions in CI/CD
pipelined, may require real-time scoring.
There are subscription limits for Azure Synapse, which should be fully understood
in the context of an AIOps methodology.
To fully incorporate AIOps it's necessary to feed near real-time observability data
into real-time ML inference models on an ongoing basis.
Capabilities such as anomaly detection should be evaluated within the
observability data stream.

Design recommendations
Ensure all Azure resources and application components are fully instrumented so
that a complete operational dataset is available for AIOps model training.
Ingest Log Analytics operational data from the global and regional Azure Storage
Accounts into Azure Synapse for analysis.
Use the Azure Monitor Alerts API to retrieve historic alerts and store it within cold
storage for operational data to subsequently use within ML models. If Log
Analytics data export is used, store historic alerts data in the same Azure Storage
accounts as the exported Log Analytics data.

After ingested data is prepared for ML training, write it back out to Azure Storage
so that it's available for ML model training without requiring Synapse data
preparation compute resources to be running.
Ensure ML model operationalization supports both batch and real-time scoring.
As AIOps models are created, implement MLOps and apply DevOps practices to
automate the ML lifecycle for training, operationalization, scoring, and continuous
improvement. Create an iterative CI/CD process for AIOps ML models.
Evaluate Azure Cognitive Services for specific predictive scenarios due to their low
administrative and integration overhead. Consider Anomaly Detection to quickly
flag unexpected variances in observability data streams.

Next step
Review the deployment and testing considerations.
Deployment and testing

Deployment and testing for missioncritical workloads on Azure
Article • 02/01/2023

Failed deployments and erroneous releases are common causes for application outages.
Your approach to deployment and testing plays a critical role in the overall reliability of
a mission-critical application.
Deployment and testing should form the basis for all application and infrastructure
operations to ensure consistent outcomes for mission-critical workloads. Be prepared to
deploy weekly, daily, or more often. Design your continuous integration and continuous
deployment (CI/CD) pipelines to support those goals.
The strategy should implement:
Rigorous pre-release testing. Updates shouldn't introduce defects, vulnerabilities,
or other factors that might jeopardize application health.
Transparent deployments. It should be possible to roll out updates at any time
without affecting users. Users should be able to continue their interactions with the
application without interruption.
Highly available operations. Deployment and testing processes and tools must be
highly available to support overall application reliability.
Consistent deployment processes. The same application artifacts and processes
should be used to deploy the infrastructure and application code across different
environments. End-to-end automation is mandatory. Manual interventions must be
avoided because they can introduce reliability risks.
This design area provides recommendations on how to optimize deployment and
testing processes with the goal of minimizing downtime and maintaining application
health and availability.
） Important
This article is part of the Azure Well-Architected Framework mission-critical
workload series. If you aren't familiar with this series, we recommend that you start
with What is a mission-critical workload?.

Zero-downtime deployment
View the following video for an overview of zero-downtime deployment.
https://learn-video.azurefd.net/vod/player?id=1d3846c0-f301-4a25-a49a94f5be3f6605&locale=en-us&embedUrl=%2Fazure%2Fwell-architected%2Fmissioncritical%2Fmission-critical-deployment-testing
Achieving zero-downtime deployments is a fundamental goal for mission-critical
applications. Your application needs to be available all day, every day, even when new
releases are rolled out during business hours. Invest your efforts up front to define and
plan deployment processes in order to drive key design decisions like whether to treat
resources as ephemeral.
To achieve zero-downtime deployment, deploy new infrastructure next to the existing
infrastructure, test it thoroughly, transition end user traffic, and only then decommission
the previous infrastructure. Other practices, like the scale-unit architecture, are also key.
The Mission-Critical Online

and Azure Mission-Critical Connected

reference

implementations illustrate this deployment approach, as shown in this diagram:



Application environments

View the following video for an overview of recommendations for application
environments.
https://learn-video.azurefd.net/vod/player?id=7e6e6390-9f32-4c9e-88da497a604db319&locale=en-us&embedUrl=%2Fazure%2Fwell-architected%2Fmissioncritical%2Fmission-critical-deployment-testing
You need various types of environments to validate and stage deployment operations.
The types have different capabilities and lifecycles. Some environments might reflect the
production environment and be long lived, and others might be short lived and have
fewer capabilities than production. Setting up these environments early in the
development cycle helps to ensure agility, separation of production and preproduction
assets, and thorough testing of operations before release to the production
environment. All environments should reflect the production environment as much as
possible, although you can apply simplifications to lower environments as needed. This
diagram shows a mission-critical architecture:



There are some common considerations:
Components shouldn't be shared across environments. Possible exceptions are
downstream security appliances like firewalls and source locations for synthetic
test data.
All environments should use infrastructure as code (IaC) artifacts like Terraform or
Azure Resource Manager (ARM) templates.

Development environments

View the following video for information about ephemeral development environments
and automated feature validation.
https://www.microsoft.com/en-us/videoplayer/embed/RE50Gm9?postJsllMsg=true

Design considerations
Capabilities. Lower requirements for reliability, capacity, and security are
acceptable for development environments.
Lifecycle. Development environments should be created as required and exist for a
short time. Shorter lifecycles help prevent configuration drift from the code base
and reduce costs. Also, development environments often share the lifecycle of a
feature branch.
Density. Teams need multiple environments to support parallel feature
development. They can coexist within a single subscription.

Design recommendations
Keep the environment in a dedicated subscription with context set for
development purposes.
Use an automated process to deploy code from a feature branch to a development
environment.

Test or staging environments
These environments are used for testing and validation. Many test cycles are performed
to ensure bug-free deployment to production. Appropriate tests for a mission-critical
workload are described in the Continuous validation and testing section.

Design considerations
Capabilities. These environments should reflect the production environment for
reliability, capacity, and security. In the absence of a production load, use a
synthetic user load to provide realistic metrics and valuable health modeling input.
Lifecycle. These environments are short lived. They should be destroyed after test
validations are complete.

Density. You can run many independent environments in one subscription. You
should also consider using multiple environments, each in a dedicated
subscription.
７ Note
Mission-critical applications should be subjected to rigorous testing.
You can perform different test functions in a single environment, and in some cases
you'll need to. For example, for chaos testing to provide meaningful results, you
must first place the application under load so you can understand how the
application responds to injected faults. That's why chaos testing and load testing
are typically performed in parallel.

Design recommendations
Ensure that at that least one staging environment fully reflects production to
enable production-like testing and validation. Capacity within this environment can
flex based on the execution of test activities.
Generate synthetic user load to provide a realistic test case for changes on one of
the environments.
７ Note
The Mission Critical Online

reference implementation provides an example

of a user load generator .
Define the number of staging environments and their purposes within the
development and release cycle.

Production environments
Design considerations
Capabilities. The highest levels of reliability, capacity, and security functionality for
the application are required.
Lifecycle. While the lifecycle of the workload and the infrastructure remains the
same, all data, including monitoring and logging, need special management. For
example, management is required for backup and recovery.

Density. For some applications, you might want to consider using different
production environments to cater to different clients, users, or business
functionalities.

Design recommendations
Have a clear governance boundary for production and lower environments. Place each
environment type in a dedicated subscription to achieve that goal. This segmentation
ensures that resource utilization in lower environments doesn't affect production
quotas. Dedicated subscriptions also set access boundaries.

Ephemeral blue/green deployments
A blue/green deployment model requires at least two identical deployments. The blue
deployment is the active one that serves user traffic in production. The green
deployment is the new one that's prepared and tested to receive traffic. After the green
deployment is completed and tested, traffic is gradually directed from blue to green. If
the load transfer is successful, the green deployment becomes the new active
deployment. The old blue deployment can then be decommissioned via a phased
process. However, if there are problems in the new deployment, it can be aborted, and
traffic can either remain in the old blue deployment or be redirected to it.
Azure Mission-Critical recommends a blue/green deployment approach where
infrastructure and applications are deployed together as part of a deployment stamp. So
rolling out a change to the infrastructure or application always results in a green
deployment that contains both layers. This approach enables you to fully test and
validate the effect of the change against the infrastructure and application end-to-end
before you redirect user traffic to it. The approach increases confidence in releasing
changes and enables zero-downtime upgrades because compatibilities with
downstream dependencies like the Azure platform, resource providers, and IaC modules
can be validated.

Design considerations
Technology capabilities. Take advantage of the built-in deployment features in
Azure services. For example, Azure App Service provides secondary deployment
slots that can be swapped after a deployment. With Azure Kubernetes Service
(AKS), you can use a separate pod deployment on each node and update the
service definition.

Deployment duration. The deployment might take longer to complete because
the stamp contains the infrastructure and application rather than just the changed
component. This, however, is acceptable because the risk of all components not
working as expected overrides the time concerns.
Cost impact. There's an additional cost because of the two side-by-side
deployments, which must coexist until the deployment is complete.
Traffic transition. After the new deployment is validated, traffic must be
transitioned from the blue environment to the green one. This transition requires
orchestration of user traffic between the environments. The transition should be
fully automated.
Lifecycle. Mission-critical deployment stamps should be considered ephemeral.
Using short-lived stamps creates a fresh start each time, before resources are
provisioned.

Design recommendations
Adopt a blue/green deployment approach to release all production changes.
Deploy all infrastructure and the application each time, for any type of change, to
achieve a consistent state and zero downtime. Although you can reuse
environments, we don't recommend it for mission-critical workloads. Treat each
regional deployment stamp as ephemeral with a lifecycle that's tied to that of a
single release.
Use a global load balancer, like Azure Front Door, to orchestrate the automated
transition of user traffic between the blue and green environments.
To transition traffic, add a green back-end endpoint that uses a low traffic to
volume weight, like 10 percent. After you verify that the low traffic volume on the
green deployment works and provides the expected application health, gradually
increase traffic. While doing so, apply a short ramp-up period to catch faults that
might not immediately be apparent.
After all traffic is transitioned, remove the blue back end from existing connections.
For instance, remove the back end from the global load balancer service, drain
queues, and detach other associations. Doing so helps to optimize the cost of
maintaining secondary production infrastructure and ensure that new
environments are free of configuration drift.
At this point, decommission the old and now inactive blue environment. For the
next deployment, repeat the process with blue and green reversed.

Subscription-scoped deployment
Depending on the scale requirements of your application, you might need multiple
production subscriptions to serve as scale units.
View the following video to get an overview of recommendations for scoping
subscriptions for a mission-critical application.
https://learn-video.azurefd.net/vod/player?id=013a2a82-dc85-4282-98edb1afe50afd41&embedUrl=%2Fazure%2Fwell-architected%2Fmission-critical%2Fmissioncritical-deployment-testing&locale=en-us

Design considerations
Scalability. For high-scale application scenarios with significant volumes of traffic,
design the solution to scale across multiple Azure subscriptions so that the scale
limits of a single subscription don't constrain scalability.
） Important
The use of multiple subscriptions necessitates additional CI/CD complexity,
which must be appropriately managed. Therefore, we recommend multiple
subscriptions only in extreme scale scenarios, where the limits of a single
subscription are likely to become a limitation.
Environment boundaries. Deploy production, development, and test environments
into separate subscriptions. This practice ensures that lower environments don't
contribute toward scale limits. It also reduces the risk of lower-environment
updates polluting production by providing a clear management and identity
boundary.
Governance. When you need multiple production subscriptions, consider using a
dedicated application management group to simplify policy assignment via a
policy aggregation boundary.

Design recommendations
Deploy each regional deployment stamp in a dedicated subscription to ensure that
the subscription limits apply only within the context of a single deployment stamp
and not across the application as a whole. Where appropriate, you might consider

using multiple deployment stamps within a single region, but you should deploy
them across independent subscriptions.
Place global shared resources in a dedicated subscription to enable consistent
regional subscription deployment. Avoid using a specialized deployment for the
primary region.

Continuous validation and testing
Testing is a critical activity that allows you to fully validate the health of your application
code and infrastructure. More specifically, testing allows you to meet your standards for
reliability, performance, availability, security, quality, and scale. Testing must be well
defined and applied as part of your application design and DevOps strategy. Testing is a
key concern during the local developer process (the inner loop) and as a part of the
complete DevOps lifecycle (the outer loop), which is when code starts on the path from
release pipeline processes toward the production environment.
View the following video to get an overview of continuous validation and testing.
https://learn-video.azurefd.net/vod/player?id=fc7842c3-7c7a-44dc-ad87838aa51d0000&locale=en-us&embedUrl=%2Fazure%2Fwell-architected%2Fmissioncritical%2Fmission-critical-deployment-testing
This section focuses on outer loop testing. It describes various types of tests.
Test

Description

Unit testing

Confirms that application business logic works as expected. Validates the overall
effect of code changes.

Smoke
testing

Identifies whether infrastructure and application components are available and
function as expected. Typically, only a single virtual user session is tested. The
outcome should be that the system responds with expected values and behavior.
Common smoke testing scenarios include reaching the HTTPS endpoint of a web
application, querying a database, and simulating a user flow in the application.

UI testing

Validates that application user interfaces are deployed and that user interface
interactions function as expected.
You should use UI automation tools to drive automation. During a UI test, a script
should mimic a realistic user scenario and complete a series of steps to execute
actions and achieve an intended outcome.

Test

Description

Load testing

Validates scalability and application operation by increasing load rapidly and/or
gradually until a predetermined threshold is reached. Load tests are typically
designed around a particular user flow to verify that application requirements are
satisfied under a defined load.

Stress

Applies activities that overload existing resources to determine solution limits and

testing

verify the system's ability to recover gracefully. The main goal is to identify
potential performance bottlenecks and scale limits.
Conversely, scale down the computing resources of the system and monitor how
it behaves under load and determine whether it can recover.

Performance
testing

Combines aspects of load and stress testing to validate performance under load
and establish benchmark behaviors for application operation.

Chaos

Injects artificial failures into the system to evaluate how it reacts and to validate

testing

the effectiveness of resiliency measures, operational procedures, and mitigations.
Shutting down infrastructure components, purposely degrading performance, and
introducing application faults are examples of tests that can be used to verify that
the application will react as expected when the scenarios actually occur.

Penetration
testing

Ensures that an application and its environment meet the requirements of an
expected security posture. The goal is to identify security vulnerabilities.
Security testing can include end-to-end software supply chain and package
dependencies, with scanning and monitoring for known Common Vulnerabilities
and Exposures (CVE).

Design considerations
Automation. Automated testing is essential to validate application or infrastructure
changes in a timely and repeatable manner.
Test order. The order in which tests are conducted is a critical consideration
because of various dependencies, like the need to have a running application
environment. Test duration also influences order. Tests with shorter running times
should run earlier in the cycle when possible to increase testing efficiency.
Scalability limits. Azure services have different soft and hard limits. Consider using
load testing to determine whether a system risks exceeding them during the
expected production load. Load testing can also be useful for setting appropriate
thresholds for autoscaling. For services that don't support autoscaling, load testing
can help you fine-tune automated operational procedures.
Inability of system components, like active/passive network components or
databases, to appropriately scale can be restrictive. Stress testing can help identify

limitations.
Failure mode analysis. Introducing faults into the application and underlying
infrastructure and evaluating the effect is essential to assessing the solution's
redundancy mechanisms. During this analysis, identify the risk, impact, and breadth
of impact (partial or full outage). For an example analysis that was created for the
Mission Critical Online
components

reference implementation, see Outage risks of individual

.

Monitoring. You should capture and analyze test results individually and also
aggregate them to assess trends over time. You should continually evaluate test
results for accuracy and coverage.

Design recommendations
View the following video to see how resiliency testing can be integrated with Azure
DevOps CI/CD pipelines.
https://www.microsoft.com/en-us/videoplayer/embed/RE4Y50k?postJsllMsg=true
Ensure consistency by automating all testing on infrastructure and application
components. Integrate the tests in CI/CD pipelines to orchestrate and run them
where applicable. For information about technology options, see DevOps tools.
Treat all test artifacts as code. They should be maintained and version controlled
along with other application code artifacts.
Align the SLA of the test infrastructure with the SLA for deployment and testing
cycles.
Run smoke tests as part of every deployment. Also run extensive load tests along
with stress and chaos tests to validate that application performance and operability
is maintained.
Use load profiles that reflect real peak usage patterns.
Run chaos experiments and failure injection tests at the same time as load tests.
 Tip
Azure Chaos Studio

is a native suite of chaos experimentation tools. The

tools make it easy to conduct chaos experiments and inject faults within Azure
services and application components.

Chaos Studio provides built-in chaos experiments for common fault scenarios
and supports custom experiments that target infrastructure and application
components.
If database interactions, like the creation of records, are required for load or smoke
tests, use test accounts that have reduced privileges and make test data separable
from real user content.
Scan and monitor the end-to-end software supply chain and package
dependencies for known CVEs.
Use Dependabot

for GitHub repositories to ensure the repository is

automatically up to date with the latest releases of packages and applications
that it depends on.

Infrastructure as code deployments
Infrastructure as code (IaC) treats infrastructure definitions as source code that's version
controlled along with other application artifacts. Using IaC promotes code consistency
across environments, eliminates the risk of human error during automated deployments,
and provides traceability and rollback. For blue/green deployments, the use of IaC with
fully automated deployments is imperative.
A mission-critical IaC repository has two distinct definitions that are mapped to global
and regional resources. For information about these types of resources, see the core
architecture pattern.

Design considerations
Repeatable infrastructure. Deploy mission-critical workloads in a way that
generates the same environment every time. IaC should be the primary model.
Automation. All deployments must be fully automated. Human processes are error
prone.

Design recommendations
Apply IaC, ensuring that all Azure resources are defined in declarative templates
and maintained in a source control repository. Templates are deployed and
resources are provisioned automatically from source control via CI/CD pipelines.
We don't recommend the use of imperative scripts.

Prohibit manual operations against all environments. The only exception is fully
independent developer environments.

DevOps tools
The effective use of deployment tools is critical to overall reliability because DevOps
processes affect the overall function and application design. For example, failover and
scale operations might depend on automation that's provided by DevOps tools.
Engineering teams must understand the effect of the unavailability of a deployment
service with respect to the overall workload. Deployment tooling must be reliable and
highly available.
Microsoft provides two Azure-native toolsets, GitHub Actions and Azure Pipelines, that
can effectively deploy and manage a mission-critical application.

Design considerations
Technology capabilities. The capabilities of GitHub and Azure DevOps overlap. You
can use them together to get the best of both. A common approach is to store
code repositories in GitHub.com or GitHub AE

while using Azure Pipelines for

deployment.
Be aware of the complexity that's added when you use multiple technologies.
Evaluate a rich feature set against overall reliability.
Regional availability. In terms of maximum reliability, the dependency on a single
Azure region represents an operational risk.
For example, say traffic is spread over two regions: Region 1 and Region 2. Region
2 hosts the Azure DevOps tooling instance. Suppose there's an outage in Region 2
and the instance isn't available. Region 1 automatically handles all traffic and needs
to deploy extra scale units to provide a good failover experience. But it won't be
able to because of the Azure DevOps dependency in Region 2.
Data replication. Data, including metadata, pipelines, and source code, should be
replicated across regions.

Design recommendations
Both technologies are hosted in a single Azure region, which might make your
disaster recovery strategy restrictive.

GitHub Actions is well-suited for build tasks (continuous integration) but might
lack features for complex deployment tasks (continuous deployment). Given the
rich feature set of Azure DevOps, we recommend it for mission-critical
deployments. However, you should make a choice after you assess trade-offs.
Define an availability SLA for deployment tooling and ensure alignment with
broader application reliability requirements.
For multi-region scenarios that use an active/passive or active/active deployment
configuration, make sure that failover orchestration and scaling operations can
continue to function even if the primary region hosting deployment toolsets
becomes unavailable.

Branching strategy
There are many valid approaches to branching. You should choose a strategy that
ensures maximum reliability. A good strategy enables parallel development, provides a
clear path from development to production, and supports fast releases.

Design considerations
Minimize access. Developers should do their work in feature/* and fix/* branches.
These branches become entry points for changes. Role-based restrictions should
be applied to branches as part of the branching strategy. For example, only allow
administrators to create release branches or enforce naming conventions for
branches.
Accelerated process for emergencies. The branching strategy should allow
hotfixes to be merged into main as soon as practical. That way, future releases
contain the fix, and recurrence of the problem is avoided. Use this process only for
minor changes that address urgent problems, and use it with restraint.

Design recommendations
Prioritize the use of GitHub for source control .
） Important
Create a branching strategy that details feature work and releases as a
minimum, and use branch policies and permissions to ensure that the strategy
is appropriately enforced.

Trigger an automated testing process to validate code change contributions before
they're accepted. Team members must also review changes.
Treat the main branch as a continuously forward-moving and stable branch that's
primarily used for integration testing.
Ensure that changes are made to main only via PRs. Use a branch policy to
prohibit direct commits.
Every time a PR is merged into main, it should automatically kick off a
deployment against an integration environment.
The main branch should be considered stable. It should be safe to create a
release from main at any given time.
Use dedicated release/* branches that are created from the main branch and used
to deploy to production environments. release/* branches should remain in the
repository and can be used to patch a release.
Document a hotfix process and use it only when needed. Create hotfixes in a fix/*
branch for subsequent merging into the release branch and deployment to
production.

AI for DevOps
You can apply AIOps methodologies in CI/CD pipelines to supplement traditional testing
approaches. Doing so enables detection of potential regressions or degradations and
allows deployments to be preemptively stopped to prevent potential negative impacts.

Design considerations
Volume of telemetry data. CI/CD pipelines and DevOps processes emit a wide
variety of telemetry for machine learning models. The telemetry ranges from test
results and deployment outcomes to operational data about test components
from composite deployment stages.
Scalability. Traditional data processing approaches like Extract, Transform, Load
(ETL) might not be able to scale throughput to keep up with the growth of
deployment telemetry and application observability data. You can use modern
analytics approaches that don't require ETL and data movement, like data
virtualization, to enable ongoing analysis by AIOps models.
Deployment changes. Changes in deployment need to be stored for automated
analysis and correlation to deployment outcomes.

Design recommendations
Define the DevOps process data to collect and how to analyze it. Telemetry, like
test execution metrics and time series data of changes within each deployment, is
important.
Expose application observability data from staging, test, and production
environments for analysis and correlation within AIOps models.
Adopt the MLOps workflow

.

Develop analytical models that are context-aware and dependency-aware to
provide predictions with automated feature engineering to address schema and
behavior changes.
Operationalize models by registering and deploying the best-trained models
within deployment pipelines.

Next step
Review the security considerations.
Security

Security considerations for missioncritical workloads on Azure
Article • 03/15/2023

Security is a one of the foundational design principles and also a key design area that
must be treated as a first-class concern within the mission-critical architectural process.
Given that the primary focus of a mission-critical design is to maximize reliability so that
the application remains performant and available, the security considerations and
recommendations applied within this design area will focus on mitigating threats with
the capacity to impact availability and hinder overall reliability. For example, successful
Denial-Of-Service (DDoS) attacks are known to have a catastrophic impact on availability
and performance. How an application mitigates those attack vectors, such as SlowLoris
will impact the overall reliability. So, the application must be fully protected against
threats intended to directly or indirectly compromise application reliability to be truly
mission critical in nature.
It's also important to note that there are often significant trade-offs associated with a
hardened security posture, particularly with respect to performance, operational agility,
and in some cases reliability. For example, the inclusion of inline Network Virtual
Appliances (NVA) for Next-Generation Firewall (NGFW) capabilities, such as deep packet
inspection, will introduce a significant performance penalty, additional operational
complexity, and a reliability risk if scalability and recovery operations are not closely
aligned with that of the application. It's therefore essential that additional security
components and practices intended to mitigate key threat vectors are also designed to
support the reliability target of an application, which will form a key aspect of the
recommendations and considerations presented within this section.
） Important
This article is part of the Azure Well-Architected mission-critical workload series. If
you aren't familiar with this series, we recommend you start with what is a missioncritical workload?

Mission-Critical open source project
The reference implementations are part of an open source project available on
GitHub. The code assets adopt a Zero Trust model to structure and guide the
security design and implementation approach.

Alignment with the Zero Trust model
The Microsoft Zero Trust

model provides a proactive and integrated approach to

applying security across all layers of an application. The guiding principles of Zero Trust
strives to explicitly and continuously verify every transaction, assert least privilege, use
intelligence, and advanced detection to respond to threats in near real-time. It's
ultimately centered on eliminating trust inside and outside of application perimeters,
enforcing verification for anything attempting to connect to the system.

Design considerations
As you assess the security posture of the application, start with these questions as the
basis for each consideration.
Continuous security testing to validate mitigations for key security vulnerabilities.
Is security testing performed as a part of automated CI/CD processes?
If not, how often is specific security testing performed?
Are test outcomes measured against a desired security posture and threat model?
Security level across all lower-environments.
Do all environments within the development lifecycle have the same security
posture as the production environment?
Authentication and Authorization continuity in the event of a failure.
If authentication or authorization services are temporarily unavailable, will the
application be able to continue to operate?
Automated security compliance and remediation.
Can changes to key security settings be detected?
Are responses to remediate non-compliant changes automated?
Secret scanning to detect secrets before code is committed to prevent any secret
leaks through source code repositories.
Is authentication to services possible without having credentials as a part of code?
Secure the software supply chain.
Is it possible to track Common Vulnerabilities and Exposures (CVEs) within utilized
package dependencies?
Is there an automated process for updating package dependencies?
Data protection key lifecycles.
Can service-managed keys be used for data integrity protection?

If customer-managed keys are required, how is the secure and reliable key
lifecycle?
CI/CD tooling should require Azure AD service principals with sufficient
subscription level access to facilitate control plane access for Azure resource
deployments to all considered environment subscriptions.
When application resources are locked down within private networks, is there a
private data-plane connectivity path so that CI/CD tooling can perform
application level deployments and maintenance.
This introduces additional complexity and requires a sequence within the
deployment process through requisite private build agents.

Design recommendations
Use Azure Policy to enforce security and reliability configurations for all service,
ensuring that any deviation is either remediated or prohibited by the control plane
at configuration-time, helping to mitigate threats associated with 'malicious admin'
scenarios.
Use Azure AD Privileged Identity Management (PIM) within production
subscriptions to revoke sustained control plane access to production
environments. This will significantly reduce the risk posed from 'malicious admin'
scenarios through additional 'checks and balances'.
Use Azure Managed Identities for all services that support the capability, since it
facilitates the removal of credentials from application code and removes the
operational burden of identity management for service to service communication.
Use Azure AD Role Based Access Control (RBAC) for data plane authorization with
all services that support the capability.
Use first-party Microsoft identity platform authentication libraries within
application code to integrate with Azure AD.
Consider secure token caching to allow for a degraded but available experience if
the chosen identity platform, isn't available or is only partially available for
application authorization.
If the provider is unable to issue new access tokens, but still validates existing
ones, the application and dependent services can operate without issues until
their tokens expire.
Token caching is typically handled automatically by authentication libraries
(such as MSAL).

Use Infrastructure-as-Code (IaC) and automated CI/CD pipelines to drive updates
to all application components, including under failure circumstances.
Ensure CI/CD tooling service connections are safeguarded as critical sensitive
information, and shouldn't be directly available to any service team.
Apply granular RBAC to production CD pipelines to mitigate 'malicious admin'
risks.
Consider the use of manual approval gates within production deployment
pipelines to further mitigate 'malicious admin' risks and provide additional
technical assurance for all production changes.
Additional security gates may come at a trade-off in terms of agility and
should be carefully evaluated, with consideration given to how agility can be
maintained even with manual gates.
Define an appropriate security posture for all lower environments to ensure key
vulnerabilities are mitigated.
Do not apply the same security posture as production, particularly with regard
to data exfiltration, unless regulatory requirements stipulate the need to do so,
since this will significantly compromise developer agility.
Enable Microsoft Defender for Cloud (formerly known as Azure Security Center) for
all subscriptions that contain the resources for a mission-critical workload.
Use Azure Policy to enforce compliance.
Enable Azure Defender for all services that support the capability.
Embrace DevSecOps and implement security testing within CI/CD pipelines.
Test results should be measured against a compliant security posture to inform
release approvals, be they automated or manual.
Apply security testing as part of the CD production process for each release.
If security testing each release jeopardizes operational agility, ensure a
suitable security testing cadence is applied.
Enable secret scanning

and dependency scanning within the source code

repository.

Threat modeling
Threat modeling provides a risk based approach to security design, using identified
potential threats to develop appropriate security mitigations. There are many possible
threats with varying probabilities of occurrence, and in many cases threats can chain in
unexpected, unpredictable, and even chaotic ways. This complexity and uncertainty is
why traditional technology requirement based security approaches are largely

unsuitable for mission-critical cloud applications. Expect the process of threat modeling
for a mission-critical application to be complex and unyielding.
To help navigate these challenges, a layered defense-in-depth approach should be
applied to define and implement compensating mitigations for modeled threats,
considering the following defensive layers.
1. The Azure platform with foundational security capabilities and controls.
2. The application architecture and security design.
3. Security features built-in, enabled, and deployable applied to secure Azure
resources.
4. Application code and security logic.
5. Operational processes and DevSecOps.
７ Note
When deploying within an Azure landing zone, be aware that an additional threat
mitigation layer through the provisioning of centralized security capabilities is
provided by the landing zone implementation.

Design considerations
STRIDE

provides a lightweight risk framework for evaluating security threats across

key threat vectors.
Spoofed Identity: Impersonation of individuals with authority. For example, an
attacker impersonating another user by using their Identity
Authentication
Tampering Input: Modification of input sent to the application, or the breach of
trust boundaries to modify application code. For example, an attacker using SQL
Injection to delete data in a database table.
Data integrity
Validation
Blocklisting/allowlisting
Repudiation of Action: Ability to refute actions already taken, and the ability of the
application to gather evidence and drive accountability. For example, the deletion
of critical data without the ability to trace to a malicious admin.
Audit/logging
Signing

Information Disclosure: Gaining access to restricted information. An example
would be an attacker gaining access to a restricted file.
Encryption
Data exfiltration
Man-in-the-middle attacks
Denial of Service: Malicious application disruption to degrade user experience. For
example, a DDoS botnet attack such as Slowloris.
DDoS
Botnets
CDN and WAF capabilities
Elevation of Privilege: Gaining privileged application access through authorization
exploits. For example, an attacker manipulating a URL string to gain access to
sensitive information.
Remote code execution
Authorization
Isolation

Design recommendations
Allocate engineering budget within each sprint to evaluate potential new threats
and implement mitigations.
Conscious effort should be applied to ensure security mitigations are captured
within a common engineering criteria to drive consistency across all application
service teams.
Start with a service by service level threat modeling and unify the model by
consolidating the thread model on application level.

Network intrusion protection
Preventing unauthorized access to a mission-critical application and encompassed data
is vital to maintain availability and safeguard data integrity.

Design considerations
Zero Trust assumes a breached state and verifies each request as though it
originates from an uncontrolled network.
An advanced zero-trust network implementation employs micro-segmentation
and distributed ingress/egress micro-perimeters.

Azure PaaS services are typically accessed over public endpoints. Azure provides
capabilities to secure public endpoints or even make them entirely private.
Azure Private Link/Private Endpoints provide dedicated access to an Azure PaaS
resource using private IP addresses and private network connectivity.
Virtual Network Service Endpoints provide service-level access from selected
subnets to selected PaaS services.
Virtual Network Injection provides dedicated private deployments for supported
services, such as App Service through an App Service Environment.
Management plane traffic still flows through public IP addresses.
For supported services, Azure Private Link using Azure Private Endpoints addresses
data exfiltration risks associated with Service Endpoints, such as a malicious admin
writing data to an external resource.
When restricting network access to Azure PaaS services using Private Endpoints or
Service Endpoints, a secure network channel will be required for deployment
pipelines to access both the Azure control plane and data plane of Azure resources
in order to deploy and manage the application.
Private self-hosted build agents deployed onto a private network as the Azure
resource can be used as a proxy to execute CI/CD functions over a private
connection. A separate virtual network should be used for build agents.
Connectivity to the private build agents from CI/CD tooling is required.
An alternative approach is to modify the firewall rules for the resource on-thefly within the pipeline to allow a connection from an Azure DevOps agent public
IP address, with the firewall subsequently removed after the task is completed.
However, this approach is only applicable for a subset of Azure services. For
example, this isn't feasible for private AKS clusters.
To perform developer and administrative tasks on the application service jump
boxes can be used.
The completion of administration and maintenance tasks is a further scenario
requiring connectivity to the data plane of Azure resources.
Service Connections with a corresponding Azure AD service principal can be used
within Azure DevOps to apply RBAC through Azure AD.
Service Tags can be applied to Network Security Groups to facilitate connectivity
with Azure PaaS services.
Application Security Groups don't span across multiple virtual networks.
Packet capture in Azure Network Watcher is limited to a maximum period of five
hours.

Design recommendations
Limit public network access to the absolute minimum required for the application
to fulfill its business purpose to reduce the external attack surface.
Use Azure Private Link to establish private endpoints for Azure resources that
require secure network integration.
Use hosted private build agents for CI/CD tooling to deploy and configure
Azure resources protected by Azure Private Link.
Microsoft-hosted agents won't be able to directly connect to network
integrated resources.
When dealing with private build agents, never open an RDP or SSH port directly to
the internet.
Use Azure Bastion to provide secure access to Azure Virtual Machines and to
perform administrative tasks on Azure PaaS over the Internet.
Use a DDoS standard protection plan to secure all public IP addresses within the
application.
Use Azure Front Door with web application firewall policies to deliver and help
protect global HTTP/S applications that span multiple Azure regions.
Use Header ID validation to lock down public application endpoints so they only
accept traffic originating from the Azure Front Door instance.
If additional in-line network security requirements, such as deep packet inspection
or TLS inspection, mandate the use of Azure Firewall Premium or Network Virtual
Appliance (NVA), ensure it's configured for maximum high availability and
redundancy.
If packet capture requirements exist, use Network Watcher packets to capture
despite the limited capture window.
Use Network Security Groups and Application Security Groups to micro-segment
application traffic.
Avoid using a security appliance to filter intra-application traffic flows.
Consider the use of Azure Policy to enforce specific NSG rules are always
associated with application subnets.
Enable NSG flow logs and feed them into Traffic Analytics to gain insights into
internal and external traffic flows.
Use Azure Private Link/Private Endpoints, where available, to secure access to
Azure PaaS services within the application design. For information on Azure
services that support Private Link, see Azure Private Link availability.

If Private Endpoint isn't available and data exfiltration risks are acceptable, use
Virtual Network Service Endpoints to secure access to Azure PaaS services from
within a virtual network.
Don't enable virtual network service endpoints by default on all subnets as this
will introduce significant data exfiltration channels.
For hybrid application scenarios, access Azure PaaS services from on-premises via
ExpressRoute with private peering.
７ Note
When deploying within an Azure landing zone, be aware that network connectivity
to on-premises data centers is provided by the landing zone implementation. One
approach is by using ExpressRoute configured with private peering.

Data integrity protection
Encryption is a vital step toward ensuring data integrity and is ultimately one of the
most important security capabilities that can be applied to mitigate a wide array of
threats. This section will therefore provide key considerations and recommendations
related to encryption and key management in order to safeguard data without
compromising application reliability.

Design considerations
Azure Key Vault has transaction limits for keys and secrets, with throttling applied
per vault within a certain period.
Azure Key Vault provides a security boundary since access permissions for keys,
secrets, and certificates are applied at a vault level.
Key Vault access policy assignments grant permissions separately to keys,
secrets, or certificates.
Granular object-level permissions to a specific key, secret, or certificate are
now possible.
After a role assignment is changed, there's a latency of up to 10 minutes (600
seconds) for the role to be applied.
There's an Azure AD limit of 2,000 Azure role assignments per subscription.
Azure Key Vault underlying hardware security modules (HSMs) are FIPS 140-2 Level
2 compliant.

A dedicated Azure Key Vault managed HSM is available for scenarios requiring
FIPS 140-2 Level 3 compliance.
Azure Key Vault provides high availability and redundancy to help maintain
availability and prevent data loss.
During a region failover, it may take a few minutes for the Key Vault service to fail
over.
During a failover Key Vault will be in a read-only mode, so it won't be possible
to change key vault properties such as firewall configurations and settings.
If private link is used to connect to Azure Key Vault, it may take up to 20 minutes
for the connection to be re-established during a regional failover.
A backup creates a point-in-time snapshot of a secret, key, or certificate, as an
encrypted blob that can't be decrypted outside of Azure. To get usable data from
the blob, it must be restored into a Key Vault within the same Azure subscription
and Azure geography.
Secrets may renew during a backup, causing a mismatch.
With service-managed keys, Azure will perform key management functions, such
as rotation, thereby reducing the scope of application operations.
Regulatory controls may stipulate the use of customer-managed keys for service
encryption functionality.
When traffic moves between Azure data centers, MACsec data-link layer
encryption is used on the underlying network hardware to secure data in-transit
outside of the physical boundaries not controlled by Microsoft or on behalf of
Microsoft.

Design recommendations
Use service-managed keys for data protection where possible, removing the need
to manage encryption keys and handle operational tasks such as key rotation.
Only use customer-managed keys when there's a clear regulatory requirement
to do so.
Use Azure Key Vault as a secure repository for all secrets, certificates, and keys if
additional encryption mechanisms or customer-managed keys need considered.
Provision Azure Key Vault with the soft delete and purge policies enabled to
allow retention protection for deleted objects.
Use HSM backed Azure Key Vault SKU for application production environments.

Deploy a separate Azure Key Vault instance within each regional deployment
stamp, providing fault isolation and performance benefits through localization, as
well as navigating the scale limits imposed by a single Key Vault instance.
Use a dedicated Azure Key Vault instance for application global resources.
Follow a least privilege model by limiting authorization to permanently delete
secrets, keys, and certificates to specialized custom Azure AD roles.
Ensure encryption keys, and certificates stored within Key Vault are backed up,
providing an offline copy in the unlikely event Key Vault becomes unavailable.
Use Key Vault certificates to manage certificate procurement and signing.
Establish an automated process for key and certificate rotation.
Automate the certificate management and renewal process with public
certificate authorities to ease administration.
Set alerting and notifications, to supplement automated certificate renewals.
Monitor key, certificate, and secret usage.
Define alerts for unexpected usage within Azure Monitor.

Policy-driven governance
Security conventions are ultimately only effective if consistently and holistically enforced
across all application services and teams. Azure Policy provides a framework to enforce
security and reliability baselines, ensuring continued compliance with a common
engineering criteria for a mission-critical application. More specifically, Azure Policy
forms a key part of the Azure Resource Manager (ARM) control plane, supplementing
RBAC by restricting what actions authorized users can perform, and can be used to
enforce vital security and reliability conventions across utilized platform services.
This section will therefore explore key considerations and recommendations
surrounding the use of Azure Policy driven governance for a mission-critical application,
ensuring security and reliability conventions are continuously enforced.

Design considerations
Azure Policy provides a mechanism to drive compliance by enforcing security and
reliability conventions, such as the use of Private Endpoints or the use of
Availability Zones.
７ Note

When deploying within an Azure landing zone, be aware that the enforcement of
centralized baseline policy assignments will likely be applied in the implementation
for landing zone management groups and subscriptions.
Azure Policy can be used to drive automated management activities, such as
provisioning and configuration.
Resource Provider registration.
Validation and approval of individual Azure resource configurations.
Azure Policy assignment scope dictates coverage and the location of Azure Policy
definitions informs the reusability of custom policies.
Azure Policy has several limits, such as the number of definitions at any particular
scope.
It can take several minutes for the execution of Deploy If Not Exist (DINE) policies
to occur.
Azure Policy provides a critical input for compliance reporting and security
auditing.

Design recommendations
Map regulatory and compliance requirements to Azure Policy definitions.
For example, if there are data residency requirements, a policy should be
applied to restrict available deployment regions.
Define a common engineering criteria to capture secure and reliable configuration
definitions for all utilized Azure services, ensuring this criteria is mapped to Azure
Policy assignments to enforce compliance.
For example, apply an Azure Policy to enforce the use of Availability Zones for
all relevant services, ensuring reliable intra-region deployment configurations.
The Mission Critical reference implementation contain a wide array of security and
reliability centric policies to define and enforce a sample common engineering
criteria.
Monitor service configuration drift, relative to the common engineering criteria,
using Azure Policy.
For mission-critical scenarios with multiple production subscriptions under a
dedicated management group, prioritize assignments at the management group

scope.
Use built-in policies where possible to minimize operational overhead of
maintaining custom policy definitions.
Where custom policy definitions are required, ensure definitions are deployed at
suitable management group scope to allow for reuse across encompassed
environment subscriptions to this allow for policy reuse across production and
lower environments.
When aligning the application roadmap with Azure roadmaps, use available
Microsoft resources to explore if critical custom definitions could be
incorporated as built-in definitions.
７ Note
When deploying within an Azure landing zone, consider deploying custom Azure
Policy Definitions within the intermediate company root management group scope
to enable reuse across all applications within the broader Azure estate. In a landing
zone environment, certain centralized security policies will be applied by default
within higher management group scopes to enforce security compliance across the
entire Azure estate. For example, Azure policies should be applied to automatically
deploy software configurations through VM extensions and enforce a compliant
baseline VM configuration.
Use Azure Policy to enforce a consistent tagging schema across the application.
Identify required Azure tags and use the append policy mode to enforce usage.
If the application is subscribed to Microsoft Mission-Critical Support, ensure that the
applied tagging schema provides meaningful context to enrich the support
experience with deep application understanding.
Export Azure AD activity logs to the global Log Analytics Workspace used by the
application.
Ensure Azure activity logs are archived within the global Storage Account along
with operational data for long-term retention.
In an Azure landing zone, Azure AD activity logs will also be ingested into the
centralized platform Log Analytics workspace. It needs to be evaluated in this case if
Azure AD are still required in the global Log Analytics workspace.

Integrate security information and event management with Microsoft Defender for
Cloud (formerly known as Azure Security Center).

IaaS specific considerations when using Virtual
Machines
In scenarios where the use of IaaS Virtual Machines is required, some specifics have to
be taken into consideration.

Design considerations
Images are not updated automatically once deployed.
Updates are not installed automatically to running VMs.
Images and individual VMs are typically not hardened out-of-the-box.

Design recommendations
Do not allow direct access via the public Internet to Virtual Machines by providing
access to SSH, RDP or other protocols. Always use Azure Bastion and jumpboxes
with limited access to a small group of users.
Restrict direct internet connectivity by using Network Security Groups, (Azure)
Firewall or Application Gateways (Level 7) to filter and restrict egress traffic.
For multi-tier applications consider using different subnets and use Network
Security Groups to restrict access in between.
Prioritize the use of Public Key authentication, when possible. Store secrets in a
secure place like Azure Key Vault.
Protect VMs by using authentication and access control.
Apply the same security practices as described for mission-critical application
scenarios.
Follow and apply security practices for mission-critical application scenarios as described
above, when applicable, as well as the Security best practices for IaaS workloads in
Azure.

Next step
Review the best practices for operational procedures for mission-critical application
scenarios.
Operational procedures

Operational procedures for missioncritical workloads on Azure
Article • 02/01/2023

Reliable and effective operations must be based on the principles of automation
wherever possible and configuration as code. This approach requires a substantial
engineering investment in DevOps processes. Automated pipelines are used to deploy
application and infrastructure code artifacts. The benefits of this approach include
consistent and accurate operational outcomes with minimal manual operational
procedures.
This design area explores how to implement effective and consistent operational
procedures.
） Important
This article is part of the Azure Well-Architected Framework mission-critical
workload series. If you aren't familiar with this series, we recommend that you start
with What is a mission-critical workload?.

DevOps processes
DevOps combines development and operational processes and teams into a single
engineering function. It encompasses the entire application lifecycle and uses
automation and DevOps tooling to conduct deployment operations in a fast, efficient,
and reliable way. DevOps processes support and sustain continuous integration and
continuous delivery (CI/CD) while fostering a culture of continuous improvement.
The DevOps team for a mission-critical application must be responsible for these tasks:
Creation and management of application and infrastructure resources via CI/CD
automation.
Application monitoring and observability.
Azure role-based access control (RBAC) and identity management for application
components.
Network management for application components.
Cost management for application resources.

DevSecOps expands the DevOps model by integrating security monitoring, application
audits, and quality assurance with development and operations throughout the
application lifecycle. DevOps teams are needed for security-sensitive and highly
regulated scenarios to ensure that security is incorporated throughout the development
lifecycle rather than at a specific release stage or gate.

Design considerations
Release and update process. Avoid manual processes for any change to
application components or underlying infrastructure. Manual processes can lead to
inconsistent results.
Dependencies on central IT teams. DevOps processes can be difficult to apply
when there are hard dependencies on centralized functions because these
dependencies prevent end-to-end operations.
Identity and access management. DevOps teams can consider granular Azure
RBAC roles for various technical functions, like AppDataOps for database
management. Apply a zero-trust model across DevOps roles.

Design recommendations
Define configuration settings and updates as code. Apply change management
through code to enable consistent release and update processes, including tasks
like key or secret rotation and permissions management. Use pipeline-managed
update processes, like scheduled pipeline runs, rather than built-in auto-update
mechanisms.
Don't use central processes or provisioning pipelines for the instantiation or
management of application resources. Doing so introduces external application
dependencies and additional risk vectors, like those associated with noisy neighbor
scenarios.
If you need to use centralized provisioning processes, ensure that the availability
requirements of the dependencies are fully aligned with mission-critical
requirements. Central teams must provide transparency so that holistic
operationalization of the end-to-end application is achieved.
Dedicate a proportion of engineering capacity during each sprint to driving
fundamental platform improvements and bolstering reliability. We recommend
that you allocate 20-40 percent of capacity to these improvements.

Develop common engineering criteria, reference architectures, and libraries that
are aligned with core design principles. Enforce a consistent baseline configuration
for reliability, security, and operations via a policy-driven approach that uses Azure
Policy.
You can also use the common engineering criteria and associated artifacts, like
Azure policies and Terraform resources for common design patterns, across other
workloads within your organization's broader application ecosystem.
Apply a zero-trust model in critical application environments. Use technologies like
Azure AD Privileged Identity Management to ensure that operations are consistent
and occur only through CI/CD processes or automated operational procedures.
Team members shouldn't have standing write access to any environment. You
might want to make exceptions in development environments to enable easier
testing and debugging.
Define emergency processes for just-in-time access to production environments.
Ensure that break glass accounts exist in case of serious problems with the
authentication provider.
Consider using AIOps to continually improve operational procedures and triggers.

Application operations
The application design and platform recommendations influence operational
procedures. There are also operational capabilities provided by various Azure services,
particularly for high availability and recovery.

Design considerations
Built-in operations of Azure services. Azure services provide built-in (enabled by
default) and configurable platform capabilities, like zonal redundancy and georeplication. An application's reliability depends on these operations. Certain
configurable capabilities incur an additional cost, like the multi-write deployment
configuration for Azure Cosmos DB. Avoid building custom solutions unless you
absolutely need to.
Operational access and execution time. Most required operations are exposed
and accessible through the Azure Resource Manager API or the Azure portal.
However, certain operations require assistance from support engineers. For
example, a restore from a periodic backup of an Azure Cosmos DB database, or the

recovery of a deleted resource, can be performed only by Azure support engineers
via a support case. This dependency might affect the downtime of the application.
For stateless resources, we recommend that you redeploy instead of waiting for
support engineers to try to recover deleted resources.
Policy enforcement. Azure Policy provides a framework for enforcing and auditing
security and reliability baselines to ensure compliance with common engineering
criteria for mission-critical applications. More specifically, Azure Policy forms a key
part of the Azure Resource Manager control plane, supplementing RBAC by
restricting the actions that authorized users can perform. You can use Azure Policy
to enforce vital security and reliability conventions across platform services.
Modification and deletion of resources. You can lock Azure resources to prevent
them from being modified or deleted. However, locks introduce management
overhead in deployment pipelines. For most resources, we recommend a robust
RBAC process with tight restrictions rather than resource locks.

Design recommendations
Automate failover procedures. For an active/active model, use a health model and
automated scale operations to ensure that no failover intervention is required. For
an active/passive model, ensure that failover procedures are automated or at least
codified within pipelines.
Prioritize the use of Azure-native autoscaling for services that support it. For
services that don't support native autoscaling, use automated operational
processes to scale services. Use scale units with multiple services to achieve
scalability.
Use platform-native capabilities for backup and restore, ensuring that they're
aligned with your RTO/RPO and data retention requirements. Define a strategy for
long-term backup retention as needed.
Use built-in capabilities for SSL certificate management and renewal, like those
provided by Azure Front Door.
For external teams, establish a recovery process for resources that require
assistance. For example, if the data platform is incorrectly modified or deleted, the
recovery methods should be well understood, and a recovery process should be in
place. Similarly, establish procedures to manage decommissioned container
images in the registry.

Practice recovery operations in advance, on non-production resources and data, as
part of standard business continuity preparations.
Identify critical alerts and define target audiences and systems. Define clear
channels to reach appropriate stakeholders. Send only actionable alerts to avoid
white noise and prevent operational stakeholders from ignoring alerts and missing
important information. Implement continuous improvement to optimize alerting
and remove observed white noise.
Apply policy-driven governance and Azure Policy to ensure the appropriate use of
operational capabilities and a reliable configuration baseline across all application
services.
Avoid the use of resource locks on ephemeral regional resources. Instead, rely on
the appropriate use of RBAC and CI/CD pipelines to control operational updates.
You can apply resource locks to prevent the deletion of long-lived global
resources.

Update management
Mission-critical design strongly endorses the principle of ephemeral stateless
application resources. If you apply this principle, you can typically perform an update by
using a new deployment and standard delivery pipelines.

Design considerations
Alignment with Azure roadmaps. Align your workload with Azure roadmaps so
that platform resources and runtime dependencies are updated regularly.
Automatic detection of updates. Set up processes to monitor and automatically
detect updates. Use tools like GitHub Dependabot .
Testing and validation. Test and validate new versions of packages, components,
and dependencies in a production context before any release. New versions might
contain breaking changes.
Runtime dependencies. Treat runtime dependencies like you would any other
change to the application. Older versions might introduce security vulnerabilities
and might have a negative effect on performance. Monitor the application runtime
environment and keep it up to date.
Component hygiene and housekeeping. Decommission or remove resources that
aren't used. For example, monitor container registries and delete old image

versions that you aren't using.

Design recommendations
Monitor these resources and keep them up to date:
The application hosting platform. For example, you need to update the
Kubernetes version in Azure Kubernetes Service (AKS) regularly, especially given
that support for older versions isn't sustained. You also need to update
components that run on Kubernetes, like cert-manager and the Azure Key Vault
CSI, and align them with the Kubernetes version in AKS.
External libraries and SDKs (.NET, Java, Python).
Terraform providers.
Avoid manual operational changes to update components. Consider the use of
manual changes only in emergency situations. Ensure that you have a process for
reconciling any manual changes back into the source repository to avoid drift and
issue recurrence.
Establish an automated procedure for removing old versions of images from Azure
Container Registry.

Secret management
Key, secret, and certificate expirations are a common cause of application outage. Secret
management for a mission-critical application must provide the needed security and
offer an appropriate level of availability to align with your maximum-reliability
requirements. You need to perform key, secret, and certificate rotation on a regular basis
by using a managed service or as part of update management, and apply processes for
code and configuration changes.
Many Azure services support Azure Active Directory (Azure AD) authentication instead
of relying on connection strings / keys. Using Azure AD greatly reduces operational
overhead. If you do use a secret management solution, it should integrate with other
services, support zonal and regional redundancy, and provide integration with Azure AD
for authentication and authorization. Key Vault provides these features.

Design considerations
There are three common approaches to secret management. Each approach reads
secrets from the secret store and injects them into the application at a different time.

Deployment-time retrieval. The advantage to this approach is that the secret
management solution needs to be available only at deployment time because
there aren't direct dependencies after that time. Examples include injecting secrets
as environment variables into a Kubernetes deployment or into a Kubernetes
secret.
Only the deployment service principal needs to be able to access secrets, which
simplifies RBAC permissions within the secret management system.
There are, however, disadvantages to this approach. It introduces RBAC complexity
in DevOps tooling with regard to controlling service principal access and in the
application with regard to protecting retrieved secrets. Also, the security benefits
of the secret management solution aren't applied because this approach relies
only on access control in the application platform.
To implement secret updates or rotation, you need to perform a full redeployment.
Application-startup retrieval. In this approach, secrets are retrieved and injected at
application startup. The benefit is that you can easily update or rotate secrets. You
don't need to store secrets on the application platform. A restart of the application
is required to fetch the latest value.
Common storage choices include Azure Key Vault Provider for Secrets Store CSI
Driver

and akv2k8s . A native Azure solution, Key Vault referenced app settings,

is also available.
A disadvantage of this approach is that it creates a runtime dependency on the
secret management solution. If the secret management solution experiences an
outage, application components already running might be able to continue serving
requests. Any restart or scale-out operation would likely result in failure.
Runtime retrieval. Retrieving secrets at runtime from within the application itself is
the most secure approach because even the application platform never has access
to secrets. The application needs to authenticate itself to the secret management
system.
However, for this approach, application components require a direct dependency
and a connection to the secret management system. This makes it harder to test
components individually and usually necessitates the use of an SDK.

Design recommendations

When possible, use Azure AD authentication to connect to services instead of
using connection strings or keys. Use this authentication method together with
Azure managed identities so you don't need to store secrets on the application
platform.
Take advantage of the expiry setting in Key Vault, and configure alerting for
upcoming expirations. Perform all key, secret, and certificate updates by using the
standard release process.
Deploy Key Vault instances as part of a regional stamp to mitigate the potential
effect of a failure to a single deployment stamp. Use a separate instance for global
resources. For information about those resources, see the typical architecture
pattern for mission-critical workloads.
To avoid the need to manage service principal credentials or API keys, use
managed identities instead of service principals to access Key Vault whenever
possible.
Implement coding patterns to ensure that secrets are re-retrieved when an
authorization failure occurs at runtime.
Apply a fully automated key-rotation process that runs periodically within the
solution.
Use the key near expiry notification in Azure Key Vault to get alerts about
upcoming expirations.

IaaS-specific considerations when using VMs
If you need to use IaaS VMs, some of the procedures and practices described earlier in
this document might differ. The use of VMs provides more flexibility in configuration
options, operating systems, driver access, low-level operating system access, and the
kinds of software that you can install. The disadvantages are increased operational costs
and the responsibility for tasks that are usually performed by the cloud provider when
you use PaaS services.

Design considerations
Individual VMs don't provide high availability, zone redundancy, or georedundancy.
Individual VMs aren't automatically updated after you deploy them. For example, a
deployed SQL Server 2019 on Windows Server 2019, won't automatically get

updated to a newer release.
Services running in a VM need special treatment and additional tooling if you want
to deploy and configure them via infrastructure as code.
Azure periodically updates its platform. These updates might require VM reboots.
Updates that require a reboot are usually announced in advance. See Maintenance
for virtual machines in Azure and Handling planned maintenance notifications.

Design recommendations
Avoid manual operations on VMs and implement proper processes to deploy and
roll out changes.
Automate the provisioning of Azure resources by using infrastructure-as-code
solutions like Azure Resource Manager (ARM) templates, Bicep, Terraform, or
other solutions.
Ensure that operational processes for deployment of VMs, updates, and backup
and recovery are in place and properly tested. To test for resiliency, inject faults
into the application, note failures, and mitigate those failures.
Ensure that strategies are in place to roll back to the last known healthy state if a
newer version doesn't function correctly.
Create frequent backups for stateful workloads, ensure that backup tasks work
effectively, and implement alerts for failed backup processes.
Monitor VMs and detect for failures. The raw data for monitoring can come from a
variety of sources. Analyze the causes of problems.
Ensure that scheduled backups run as expected and that periodic backups are
created as needed. You can use Backup center to get insights.
Prioritize the use of Virtual Machine Scale Sets rather than VMs to enable
capabilities like scale, autoscale, and zone redundancy.
Prioritize the use of standard images from Azure Marketplace rather than custom
images that need to be maintained.
Use Azure VM Image Builder or other tools to automate build and maintenance
processes for customized images as needed.
Beyond these specific recommendations, apply best practices for operational
procedures for mission-critical application scenarios as appropriate.

Next step
Review the architecture pattern for mission-critical application scenarios:
Architecture pattern

Well-Architected assessment for
mission-critical workloads
Article • 03/15/2023

The Assessment is a review tool for self-assessing the readiness of your mission-critical
workload in production. Working towards building a resilient mission critical architecture
can be a complex process. The assessment is organized in a way that you'll be able to
methodically check the alignment to the best practices for resiliency, reliability, and
availability.
We recommend that the team doing the assessment is well versed in the architecture of
the specific workload and has a strong understanding of cloud principles and patterns.
These roles include, but aren't limited to, cloud architect, operators, DevOps engineer.
The assessment is a set of questions based on the mission-critical design methodology
as a way of checking the foundational design choices of a workload’s architecture and
the end-to-end operational approach.

These questions are designed to help benchmark a workload’s maturity and alignment
to the recommended approach for operating a mission-critical workload. The outcome
of the assessment is the delivery of technical recommendations and documentation that
provide guidance on how to implement a highly reliable solution on Azure.

Mission-critical review tool

Next steps
See these reference architecture that describe design choices for production-ready
implementations.
Baseline architecture of an internet-facing application
Baseline architecture of an internet-facing application with network controls

Carrier-grade workload documentation
In this series, learn about building highly reliable applications for carrier-grade
workloads on Microsoft Azure. Mission-critical systems primarily focus on maximizing
uptime and they exist in many industries. Within the telecommunications industry,
they're referred to as carrier-grade systems.

Get started

ｅ

OVERVIEW

What is carrier-grade?
Design principles

Design areas

ｐ

CONCEPT

Fault tolerance
Data model
Health modeling
Testing and validation

Reference architecture

Ｙ

ARCHITECTURE

Carrier-grade voicemail solution

Carrier-grade workloads on Azure
Article • 03/20/2023

Mission-critical systems primarily focus on maximizing uptime and they exist in many
industries. Within the telecommunications industry, they're referred to as carrier-grade
systems. These systems are developed due to one or more of the following drivers:
Minimizing loss of life or injury.
Meeting regulatory requirements on reliability to avoid paying fines.
Optimizing service to customers to minimize churn to competitors.
Meeting contractual Service Level Agreements (SLAs) with business or government
customers.
This series of articles applies the design methodology for mission-critical workloads to
inform prescriptive guidance for building and operating a highly reliable, resilient, and
available telecommunication workload on Azure.
７ Note
The articles within this series focus on providing additional mission-critical
considerations when designing for 99.999% ('5 9s') levels of reliability within the
telecommunications industry.

What is a carrier-grade workload?
The term workload refers to a collection of application resources that support a common
business goal or the execution of a common business process, with multiple services,
such as APIs and data stores, working together to deliver specific end-to-end
functionality.
The term mission-critical refers to a criticality classification where a significant financial
cost (business-critical) or human cost (safety-critical) is associated with unavailability or
underperformance.
A carrier-grade workload pivots on both business-critical and safety-critical, where
there's a fundamental requirement to be operational with only minutes or even seconds
of downtime per calendar year. Failure to achieve this uptime requirement can result in
extensive loss of life, incur significant fines, or contractual penalties.

The operational aspect of the workload includes how reliability is measured and the
targets that it must meet or exceed. Highly reliable systems typically target 99.999%
uptime (commonly referred to as '5 9s') or 0.001% downtime in a year (approximately 5
minutes). Some systems target 99.9999% uptime, or 30 seconds downtime per year, or
even higher levels of reliability. This covers all forms and causes of outage – scheduled
maintenance, infrastructure failure, human error, software issues and even natural
disaster.
Although the platform used has evolved from dedicated, proprietary hardware through
commercial, off-the-shelf hardware to OpenStack or VMware clouds, Telecommunication
companies consistently deliver services achieving ≤ 5 minutes of downtime per year,
and in many cases, achieve ≤ 30 seconds of downtime due to unscheduled outages.

What are the common challenges?
Building carrier-grade workloads is a challenge for these main reasons:

Lift and shift approach
Telecommunication companies have well-architected applications that deliver the
expected behavior on their existing infrastructure. However, care should be taken before
assuming that porting these applications as is to a public cloud infrastructure won't
impact their resiliency.
The existing applications make a set of assumptions about their underlying
infrastructure, which are unlikely to remain true when moving from on-premises to
public cloud. The architect must check that they still hold and adjust infrastructure and
application design to accommodate the new reality. The architect should also look for
opportunities where the new infrastructure removes limitations that existed onpremises.
For example, upgrade of on-premises systems must happen in place because it's not
viable to maintain sufficient hardware to create a new deployment alongside and slowly
transition in a safe manner. This upgrade path generates a host of requirements for how
upgrades and rollbacks are managed. These requirements lead to complexity and mean
that upgrades are infrequent and only permitted in carefully managed maintenance
windows.
However, in public cloud, it's reasonable to create a new deployment in parallel with the
existing deployment. This process creates the opportunity for major simplifications in
the application's operational design and improvements in the user's experience, and
expectations.

Monolithic solutions
Experience across a range of mission-critical industries shows that it isn't realistic to
attempt to construct a monolithic solution which will achieve the desired levels of
availability. There are just too many potential sources of failure in a complex system.
Instead, successful solutions are composed of individual independent and redundant
elements. Each unit has relatively basic availability (typically ~99.9%), but when
combined together the total solution achieves the necessary availability goals. The art of
carrier-grade engineering then becomes ensuring that the only dependencies common
to the redundant elements are those which are absolutely necessary since they exert the
most significant influence on overall availability, often orders of magnitude greater than
anything else in the design.

Only building zonal redundancy
Using Microsoft Azure Availability Zones is the basic choice for reducing the risk of
outage due to hardware failure or localized environmental issues. However, it isn't
enough to achieve carrier-grade availability, mainly for these reasons:
Availability Zones (AZs) are designed so that the network latency between any two
zones in a single region is ≤ 2 ms. AZs can't be widely and geographically
dispersed. So, the AZs share a correlated risk of failure due to natural disasters,
such as flooding or massive power outages, which could disable multiple AZs
within a region.
Many Azure services are explicitly designed to be zone-redundant so that the
applications using them don't need explicit logic to benefit from the availability
gain. This redundancy function within the service requires collaboration between
the elements in each zone. There's often an unavoidable risk of software failure in
one zone that causes correlated failures in other zones. For example, any issue with
a secret or certificate used with the zone redundant service could impact all AZs
simultaneously.

What are the key design areas?
When designing a carrier-grade workload, consider the following areas:
Design
area

Description

Design
area

Description

Fault
tolerance

Application design must allow for unavoidable failures so that the application as a
whole can continue to operate at some level. The design must minimize points of
failure and take a federated approach.

Data
model

The design must address the consistency, availability, and partition tolerance needs
of the database. Carrier Grade availability requires that the application is deployed
across multiple regions. This deployment requires careful thought about how the
application's data will be managed across those regions.

Health
modeling

An effective health model provides observability of all components, platform and
application, so that issues can be quickly detected and response is ready either
through self-healing or other remediations.

Testing
and
validation

The design and implementation of the application must be thoroughly tested. In
addition, the integration and deployment of the application as a whole solution must
be tested.

Next step
Start by reviewing the design principles for carrier-grade application scenarios.
Design principles

Design principles of a carrier-grade
workload on Azure
Article • 01/11/2023

Carrier grade workload must be designed as per the guiding principles of the WellArchitected Framework quality pillars:
Reliability
Performance Efficiency
Operational Excellence
Security
Cost Optimization
This article describes the carrier-grade design principles that resonate and extend the
mission-critical design principles. These collective principles serve as a road map for
subsequent design decisions across the critical design areas. We highly recommend that
you get to know these principles to better understand their effects and the trade-offs
associated with non-adherence.
There are obvious cost tradeoffs associated with introducing greater reliability, which
should be carefully considered in the context of workload requirements.
） Important
This article is part of the Azure Well-Architected carrier-grade workload series. If
you aren't familiar with this series, we recommend you start with What is a carriergrade workload?
Keep this high-level architecture model in mind when considering these points.

Assume failure
Start from the assumption that everything can, and will fail. Application design must
allow for these failures with fault tolerance so that an application can continue to
operate at some level.
Minimize single points of failure and implement a federated approach.
Deploy the application across multiple regions with proper data management
across those regions, allowing for the impacts of CAP theorem.
Detect issues automatically and respond within seconds. For more information, see
health monitoring.
Test the full solution including the application implementation, platform
integration, and deployment. This testing should include chaos testing on
production systems to avoid testing bias.

Share nothing
Share nothing is a common and straightforward approach to achieve high availability.
Use this approach when an application can be serviced by multiple, distinct elements,
which are interchangeable. The individual elements must have a well-understood
availability metric, but it doesn't need to be high. However, the elements must be
combined in a way to remain independent, with no shared infrastructure or
dependencies.
To share nothing is often impossible. To start from the position that nothing should be
shared, and only add in the smallest possible set of shared dependencies, should result
in an optimal solution.
Example
Given a single system that has six hours of downtime per year (around 3.5*9s), a solution
that combines four systems where the periods of downtime are uncorrelated will
experience less than 30s of downtime per year. As soon as those four systems rely on a
common service, such as global DNS, their downtime is no longer uncorrelated. The
resulting downtime will be higher.

Next step
Review the fault tolerance design area for carrier-grade workloads.
Design area: Fault tolerance

Fault tolerance for carrier-grade
workloads
Article • 01/06/2023

Telecommunication companies have shown that it's possible to implement applications
that meet and exceed carrier-grade availability requirements, whilst running on top of
unreliable elements. Companies exceed these requirements through fault tolerance,
which includes the following aspects:
Combination of independent, non-highly available elements.
Traffic management failure response mechanisms.
Repair and capacity-recovery and failure response mechanisms.
Use of highly available cross-element databases.
When designing for carrier grade reliability and resiliency, assume that every component
can and will fail. The design will require a layered approach to failure resolution. Part of
validating the design is creating a quantitative probabilistic model of the various failure
modes which clearly identifies the key dependencies that the application has, and shows
that the application can achieve the necessary availability given those dependencies
meet their own Service Level Objectives (SLOs). This model should be retained and
continuously validated after development and in production to assure that the test and
live data match what the model predicts.

High availability through combination
To reach high availability, take independent elements, which aren't highly available, and
combine them so that they remain independent entities. The probability of multiple
elements failing together is much lower than the probability of failure of any single
element. We define this process as the Share Nothing architectural style.

Traffic management failure response
When individual elements fail, or there are connectivity issues, the system's traffic
management layer has various ways it can respond to maintain overall service. The
solution should implement as many of the following mechanisms as possible or
necessary to achieve availability. The first two mechanisms rely on specific behavior in
the client, which may not always be an option:

1. Instantaneous client retry to alternate target—On failure, or after no response
from a retry, the client can instantly retry the request with an alternative element
that's considered likely to succeed, as opposed to submitting a new DNS query to
identify an alternative.
2. Client-based health lists—Maintains lists of elements, which are known-healthy
and known-unhealthy. Requests are always sent to a known-healthy element,
unless that list is empty. If empty, the known-unhealthy elements are tried.
3. Server-based polling—System-based distribution mechanisms, such as DNS or
Azure Traffic Manager (ATM), implement their own liveness detection and
monitoring to enable routing around unhealthy elements.

Repair and recovery failure response
Traffic management responses can work around failures, but where the failure is longlived or permanent, the defective element must be repaired or replaced. There are two
main choices here:
1. Restoring redundancy—Failure of an element reduces overall system capacity.
System design should allow for this capacity reduction through provisioning
redundant capacity; however, multiple overlapping failures will lead to true
outages. There must be an automated process that detects the failure and adds
capacity to restore redundancy to its normal levels. The impact can be determined
from the failure rate analysis. In many cases, automatically restoring the
redundancy level can increase the acceptable downtime of any individual element.
2. (Optional) Repairing the failed element—The solution may need to include a
mechanism that detects the failure and attempts to repair the failed element in
place. This solution may not apply for cases where the process of restoring
redundancy instantiates a new element to replace the failed one, and terminates
and decommissions the failed element.
The following diagram shows how the two modes of failure response cooperate to
provide service-level fault tolerance:

Failure rate analysis
No amount of effort leads to a perfect system, so start with the assumption that
everything can and will fail. Consider how the solution, as a whole, will behave. Failure
rate analysis starts with the lower-level individual systems and combines the results
together to model the full solution.
The analysis is typically done using Markov modeling, which considers all possible
failure modes. For each failure mode, consider the following factors:
Rate
Duration and resolution time
Probability of correlated failure (what is the chance that a failure in service X causes
a failure in service Y)
The outcome is an estimate of the system availability and informs consideration of the
blast radius. The blast radius is the set of things that won't work given a particular
failure. Good design aims to limit the scope and severity of that set, since failure is going
to happen. For example, a failure that blocks creation of new users, but doesn't impact
existing ones is less concerning than a failure that stops service for all users.
Failure rate analysis provides an estimate of the overall system-level availability. The
analysis identifies the critical dependencies on which that availability relies. Where
failure rate analysis indicates that system availability falls short, it also provides specific,
quantitative insights on where improvements are needed and to what extent.
For more details on failure mode analysis in Azure, reference Failure mode analysis for
Azure applications.

Cascading failures and overload
Carrier-grade application design must pay careful attention to the risks of cascading
failures, where failure of one element leads to failure of other elements, often due to

overload. Cascading failures aren't unique to carrier-grade applications, but the
reliability and the response time demand graceful degradation and automated recovery.

Next step
Review the considered data model design area for carrier-grade workloads.
Design area: Data model

Data modeling for carrier-grade
workloads
Article • 09/26/2022

Enterprise applications deployed in a single region can typically ignore this model of
application design and safely delegate responsibility to the database layer to make data
reliably available. This behavior isn't the case for carrier-grade applications, which must
be deployed across multiple regions. Multi-region deployment forces the application
architect to consider the compromises they're willing to accept for their data.

CAP theorem
Databases can provide three key properties for the data they manage for an application,
known as CAP:
Consistency: A data read returns the most recent write.
Availability: Every request receives a valid response.
Partition tolerance: The system continues to operate despite delay or total loss of
communication between elements.
The CAP theorem states that a database layer can't provide all three of these properties
for the same data at the same time in the presence of network partitions. The architect
needs to make explicit design decisions about which of the CAP properties to favor
under which conditions, and how to deal with the edge cases.
According to the CAP theorem, any database can only guarantee two out of three
possible properties for the same data at the same time in the presence of network
partition.
Multi-region deployment means partition tolerance becomes significant. In most cases,
carrier-grade architects prioritize partition tolerance and availability over consistency.
For each type of data, the architect must consider what tradeoffs they're willing to make,
considering the edge cases.
For example, consider the database of system users. Is it acceptable for the user
database to drop to read-only if there's a network partition? This behavior prioritizes
consistency and read availability over write availability. This prioritization may not be
suitable if it's unacceptable for a user to access an isolated site after their permissions
are revoked elsewhere. The described scenario would require all database access to be
blocked if there's a partition, which prioritizes write availability over read availability.

７ Note
The compromises made can be different for different databases within the same
application, since the databases are likely to have different usage profiles.
Where consistency is the compromise, the application must cope with data
inconsistency artifacts by using conflict-free replicated data types (CRDTs). The use of
CRDTs requires discipline in the application design and implementation. Their use means
data merges following partition events can be handled automatically by the data layer
without human intervention or complex application-level logic.
） Important
More details on data platform choices for your mission-critical workload is available
here
The architect must also understand the tradeoffs in the data model, which were made
within the dependent services. Those tradeoffs impact the service delivered to their
application because those tradeoffs may not align with the application-level
requirements.

Next step
Review the health modeling design area for carrier-grade workloads.
Design area: Health modeling

Health modeling for carrier-grade
workloads
Article • 01/11/2023

High availability requires careful health monitoring to automatically detect and respond
to issues within seconds. This monitoring requires built-in telemetry of key
dependencies to reliably detect the failure. The application itself requires additional
telemetry (Service Level Indicators) which accurately report the health of the application
in a way that is perceived by users of the application. Evaluation against SLOs may be
necessary.
The failure rate analysis and general health modeling of the application should generate
clear metrics indicative of the service and health of its constituent elements. These
metrics must be included in the design so that the true service availability can be
monitored. By including metrics, you can track the most useful leading indicators to
trigger the automated failure responses and to generate the necessary alerts for human
intervention.
） Important
More details on how to build a health model for your mission-critical workload are
available here.

Management and monitoring
Monitoring and management require the following thought processes:
How will the application handle bugs in the framework?
How is the application upgraded?
What actions should be taken during an incident?
For example, a solution may rely on Azure DevOps (ADO) to host its Git repository for all
configuration. If the Azure region hosting that ADO repo fails, the recovery time is two
hours. If the solution is deployed in the same region, it's not possible to modify
configuration to add capacity elsewhere for that entire two hour period. As a result, the
application architect must consider correlated failure modes for key services, such as:
Azure Traffic Manager
Azure Key Vault

Azure Kubernetes Service
Correlated failure modes for these key services may be a necessary part of the
application-level response to failure. It's vital to create control planes which aren't
impacted by the same application failure.
The management tooling required to issue diagnosis and troubleshooting must be the
same as tooling used for normal day-to-day operations tasks. Similar tooling ensures
that it's familiar and proven to work. Similar tooling also maximizes the users' familiarity
with the user interface and process steps. Requiring operators to switch to a different
tool set to resolve a high-pressure outage isn't conducive to identifying and resolving
the issue effectively.

Federated model
A highly available application or service must have a highly available management and
monitoring infrastructure built using the same well-architected principles of federation
and fault tolerance. Infrastructures built on these well-architected principles ensure
individual regions can be self sufficient if disconnected.
If there's a disconnect event, the system degenerates into individually functioning
islands, instead of using a primary/backup system. The federated model is flexible and
resilient, and automatically adapts to partition and reconnection events.
For example, logs and metrics are stored in the Availability Zone (AZ) where they're
generated. A query of metrics uses an opaque process of federated search to query the
metric stores in all reachable AZs. It comes down to the requirements of the specific
application about what level of logs, metrics, and alarms data should be replicated to
other regions. Typically, alarms should be replicated, but there may be insufficient
justification to replicate logs and metrics.

Health and unhealth metrics
Internal metrics are useful as unhealth metrics. These metrics reliably indicate the
presence of an issue, but the reverse isn't true. No evidence of poor health isn't
evidence of good health, as the customer perceives health.
For example, a DNS issue indicates requests aren't arriving at the database service. The
DNS error doesn't affect the database read-success metric because this metric isn't
seeing any errors. However, the end user perceives a total outage because they aren't
able to access the database. At least a portion of health metrics must be measured
externally, so that these metrics include everything the end user will experience.

Monitoring and tracing
The support team's ability to detect, diagnose, and resolve issues is an important part of
delivering a highly available application. To ensure success, the monitoring and tracing
element must deliver high levels of visibility, so that the one in a thousand type events
can be found and resolved.
A tracing solution that only logs 0.1% of requests only has a one in one million chance
of recording such events, which means that diagnosis and resolution are highly unlikely.
Yet, failure to resolve such issues will have a meaningful impact on availability.

Next step
Review the testing and validation design area for carrier-grade workloads.
Design area: Testing and validation

Testing and validation for carrier-grade
workloads
Article • 09/26/2022

Continuous testing and validation can detect and help resolve issues before they
become potentially life threatening. Consider well-known testing methodologies such as
chaos testing. Testing should be conducted for the lifetime of the application because
the deployment environment is complex and multi-layered.
） Important
More details on how to implement continuous validation for your mission-critical
workload is available here.
Also, supportability must be strong throughout the application lifetime. Highly available
systems rely on high quality support teams able to rapidly respond to and resolve issues
in the field, conduct root cause analysis and look for systematic design flaws.
Proving that an application is well architected requires testing, ideally use a chaos
testing framework to avoid testing bias. This methodology simulates failures of all
dependent elements. Robust and regular testing should both prove the design and
validate the original failure mode analysis.
A warning flag should be raised for any application or service for which the redundancy
or resiliency measures can't be tested because it's considered too risky.
If redundancy and resiliency measures aren't tested, then the only valid assumption,
from a safety-critical point of view, is that these measures aren't going to work when
needed. Using common paths for software upgrades, configuration updates, and fault
recovery, for example, provide a good mechanism for validating that measures will work.

Human error
Experience from Telcos is that as much as 60% of all outages are actually the result of
human error. A well-architected application recognizes this and seeks to compensate.
Here are some suggested approaches, but the list is not exhaustive, and what is
applicable to a given workload needs to be considered on a case-by-case basis.
Maximizing use of automation avoids human operators having to enter long and
complex commands or conduct repetitive operations across multiple elements.

However, care must be taken to consider the blast radius, as there is a risk of
automation actually magnifying the effect of a configuration error, allowing it to
roll out across a global network in seconds. Strong checks and balances such as
decision gates requiring human approval before proceeding to the next step are
advised.
Leveraging syntax checkers and simulation tools minimize the chance of errors or
unforeseen side effects from changes making their way into widespread
production.
Use of carefully controlled canary deployments ensure that the effect of changes in
full production can be observed and validated at limited scope.
Ensuring that the management interfaces and processes needed for fault recovery
are the same as those used in day-to-day operation avoid operators being
confronted with unfamiliar screens and barely used method of procedures (MOPs)
at times of peak stress.

Clients
Common client libraries are also part of the end-to-end system and need equivalent
analysis and testing. Software issues in common client code that simultaneously impacts
a proportion of the system clients will impact overall availability in the same way as
application server-side issues.

Next step
Revisit the five pillars of architectural excellence to form a solid foundation for your
carrier-grade workloads.
Azure Well-Architected Framework

Overview of a hybrid workload
Article • 03/20/2023

Customer workloads are becoming increasingly complex, with many applications often
running on different hardware across on-premises, multicloud, and the edge. Managing
these disparate workload architectures, ensuring uncompromised security, and enabling
developer agility are critical to success.
Azure uniquely helps you meet these challenges, giving you the flexibility to innovate
anywhere in your hybrid environment while operating seamlessly and securely. The
Well-Architected Framework includes a hybrid description for each of the five pillars:
cost optimization, operational excellence, performance efficiency, reliability, and
security. These descriptions create clarity on the considerations needed for your
workloads to operate effectively across hybrid environments.
Adopting a hybrid model offers multiple solutions that enable you to confidently deliver
hybrid workloads: run Azure data services anywhere, modernize applications anywhere,
and manage your workloads anywhere.

Extend Azure management to any
infrastructure
 Tip
Applying the principles in this article series to each of your workloads will better
prepare you for hybrid adoption. For larger or centrally managed organizations,
hybrid and multicloud are commonly part of a broader strategic objective. If you
need to scale these principle across a portfolio of workloads using hybrid and
multicloud environments, you may want to start with the Cloud Adoption
Framework's hybrid and multicloud scenario and best practices. Then return to
this series to refine each of your workload architectures.
Use Azure Arc enabled infrastructure to extend Azure management to any infrastructure
in a hybrid environment. Key features of Azure Arc enabled infrastructure are:
Unified Operations
Organize resources such as virtual machines, Kubernetes clusters and Azure
services deployed across your entire IT environment.
Manage and govern resources with a single pane of glass from Azure.

Integrate with Azure Lighthouse for managed service provider support.
Adopt cloud practices
Easily adopt DevOps techniques such as infrastructure as code.
Empower developers with self-service and choice of tools.
Standardize change control with configuration management systems, such as
GitOps and DSC.

Run Azure services anywhere
Azure Arc allows you to run Azure Services anywhere. This allows you to build consistent
hybrid and multicloud application architectures by using Azure services that can run in
Azure, on-premises, at the edge, or at other cloud providers.

Run Azure data services anywhere
Use Azure Arc enabled data services to run Azure data services anywhere to support your
hybrid workloads. Key features of Azure Arc enabled data services are:
Run Azure data services on any Kubernetes cluster deployed on any hardware.
Gain cloud automation benefits, always up-to-date innovation in Azure data
services, unified management of your on-premises and cloud data assets with a
cloud billing model across both environments.
Azure SQL Database and Azure PostgreSQL Hyperscale are the first set of Azure
data services that are Azure Arc enabled.

Run Azure Application services anywhere
Use Azure Arc enabled Application services to run Azure App Service, Functions, Logic
Apps, Event Grid, and API Management anywhere to support your hybrid workloads. Key
features of Azure Arc enabled application services are as follows:
Web Apps - Azure App Service makes building and managing web applications
and APIs easy, with a fully managed platform and features like autoscaling,
deployment slots, and integrated web authentication.
Functions - Azure Functions makes event-driven programming simple, with stateof-the-art autoscaling, and with triggers and bindings to integrate with other
Azure services.
Logic Apps - Azure Logic Apps produces automated workflows for integrating
apps, data, services, and backend systems, with a library of more than 400
connectors.

Event Grid - Azure Event Grid simplifies event-based applications, with a single
service for managing the routing of events from any source to any destination.
Azure API Management gateway - Azure API Management provides a unified
management experience and full observability across all internal and external APIs.

Modernize applications anywhere
Use the Azure Stack family to modernize applications without ever leaving the
datacenter. Key features of the Azure Stack family are:
Extend Azure to your on-premises workloads with Azure Stack Hub. Build and run
cloud apps on premises, in connected or disconnected scenarios, to meet
regulatory or technical requirements.
Use Azure Stack HCI to run virtualized workloads on premises and easily connect
to Azure to access cloud management and security services.
Build and run your intelligent edge solutions on Azure Stack Edge, an Azure
managed appliance to run machine learning models and compute at the edge to
get results quickly—and close to where data is being generated. Easily transfer the
full data set to Azure for further analysis or archive.

Manage workloads anywhere
Use Azure Arc management to extend Azure management to all assets in your
workloads, regardless of where they are hosted. Key features of Azure Arc management
are:
Adopt cloud practices
Easily adopt DevOps techniques such as infrastructure as code.
Empower developers with self-service and choice of tools.
Standardize change control with configuration management systems, such as
GitOps and DSC.
Scale across workloads with Unified Operations
Organize resources such as virtual machines, Kubernetes clusters and Azure
services deployed across your entire IT environment.
Manage and govern resources with a single pane of glass from Azure.
Integrate with Azure Lighthouse for managed service provider support.

Next steps

Cost optimization

Cost optimization in a hybrid workload
Article • 11/30/2022

A key benefit of hybrid cloud environments is the ability to scale dynamically and back
up resources in the cloud, avoiding the capital expenditures of a secondary datacenter.
However, when workloads sit in both on-premises and cloud environments, it can be
challenging to have visibility into the cost. With Azure's hybrid technologies, you can
define policies and constraints for both on-premises and cloud workloads with Azure
Arc. By utilizing Azure Policy, you're able to enforce organizational standards for your
workload and the entire IT estate.
Azure Arc helps minimize or even eliminate the need for on-premises management and
monitoring systems, which reduces operational complexity and cost, especially in large,
diverse, and distributed environments. This helps offset additional costs associated with
Azure Arc-related services. For example, advanced data security for Azure Arc enabled
SQL Server instance requires Microsoft Defender for Cloud functionality of Microsoft
Defender for Cloud, which has pricing implications

.

Other considerations are described in the Principles of cost optimization section in the
Microsoft Azure Well-Architected Framework.

Workload definitions
Define the following for your workloads:
Monitor cloud spend with hybrid workloads. Track cost trends and forecast future
spend with dashboards in Azure for your on-prem data estates with Azure Arc.
Keep within cost constraints.
Create, apply, and enforce standardized and custom tags and policies.
Enforce run-time conformance and audit resources with Azure Policy.
Choose a flexible billing model. With Azure Arc enabled data services, you can use
existing hardware with the addition of an operating expense (OPEX) model.

Functionality
For budget concerns, you get a considerable amount of functionality at no cost that you
can use across all of your servers and cluster with Azure Arc enabled servers. You can
turn on additional Azure services to each workload as you need them, or not at all.
Free Core Azure Arc capabilities

Update, management
Search index
Group, tags
Portal
Templates, extensions
RBAC, subscriptions
Paid-for Azure Arc enabled attached services
Azure policy
Azure monitor
Defender for Cloud – Standard
Microsoft Sentinel
Backup
Config and change management

Tips
Start slow. Light up new capabilities as needed. Most of Azure Arc's resources are
free to start.
Save time with unified management for your on-premises and cloud workloads
by projecting them all into Azure.
Automate and delegate remediation of incidents and problems to service teams
without IT intervention.

Azure Architecture Center (AAC) resources
related to hybrid cost
Manage configurations for Azure Arc enabled servers
Azure Arc hybrid management and deployment for Kubernetes clusters
Optimize administration of SQL Server instances in on-premises and multi-cloud
environments by leveraging Azure Arc
Disaster Recovery for Azure Stack Hub virtual machines
Build high availability into your BCDR strategy
Use Azure Stack HCI switchless interconnect and lightweight quorum for Remote
Office/Branch Office
Archive on-premises data to cloud

Infrastructure Decisions

Azure Stack HCI can help in cost-savings by using your existing Hyper-V and Windows
Server skills to consolidate aging servers and storage. Azure Stack HCI pricing follows
the monthly subscription billing model, with a flat rate per physical processor core in an
Azure Stack HCI cluster.
Use Azure Stack HCI to modernize on-prem workloads with hyperconverged infra. Azure
Stack HCI billing is based on a monthly subscription fee per physical processor core, not
a perpetual license. When customers connect to Azure, the number of cores used is
automatically uploaded and assessed for billing purposes. Cost doesn't vary with
consumption beyond the physical processor cores. This means that more VMs don't cost
more, and customers who are able to run denser virtual environments are rewarded.
If you are currently using VMware, you can take advantage of cost savings only available
with Azure VMware Solution. Easily move VMware workloads to Azure and increase your
productivity with elasticity, scale, and fast provisioning cycles. This will help enhance
your workloads with the full range of Azure compute, monitor, backup, database, IoT,
and AI services.
Lastly, you can slowly begin migrating out of your datacenter and use Azure Arc while
you're migrating to project everything into Azure.

Capacity planning
Check out our checklist under the Cost Optimization pillar in the Well-Framework to
learn more about capacity planning, and build a checklist to design cost-effective
workloads.
Define SLAs
Determine regulatory needs

Provision
One advantage of cloud computing is the ability to use the PaaS model. And in some
cases, PaaS services can be cheaper than managing VMs on your own. Some workloads
cannot be moved to the cloud though for regulatory or latency reasons. Therefore,
using a service like Azure Arc enabled services allows you to flexibly use cloud
innovation where you need it by deploying Azure services anywhere.
Click the following links for guidance in provisioning:
Azure Arc pricing
Azure Arc Jumpstart for templates

(in GitHub)

Azure Stack HCI pricing
Azure Stack HCI can reduce costs by saving in server, storage, and network
infrastructure.
Azure VMware Solution pricing - Run your VMware workloads natively on Azure
Run your VMware workloads natively on Azure.
Azure Stack Hub pricing

Monitor and optimize
Treat cost monitoring and optimization as a process, rather than a point-in-time activity.
You can conduct regular cost reviews and forecast the capacity needs so that you can
provision resources dynamically and scale with demand.
Managing the Azure Arc enabled servers agent
Bring all your resources into a single system so you can organize and inventory
through a variety of Azure scopes, such as Management groups, Subscriptions,
and Resource Groups.
Create, apply, and enforce standardized and custom tags to keep track of
resources.
Build powerful queries and search your global portfolio with Azure Resource
Graph.
With Azure Stack HCI
Costs for datacenter real estate, electricity, personnel, and servers can be
reduced or eliminated.
Costs are now part of OPEX, which can be scaled as needed.

Next steps
Operational excellence

Operational excellence in a hybrid
workload
Article • 11/30/2022

Operational excellence consists of the operations processes that keep a system running
in production. Applications must be designed with DevOps principles in mind, and
deployments must be reliable and predictable. Use monitoring tools to verify that your
application is running correctly and to gather custom business telemetry that will tell
you whether your application is being used as intended.
Use Azure Arc enabled infrastructure to add support for cloud Operational Excellence
practices and tools to any environment. Be sure to utilize reference architectures and
other resources from this section that illustrate applying these principles in hybrid and
multicloud scenarios. The architectures referenced here can also be found in the Azure
Architecture Center, Hybrid and Multicloud category.

Build cloud native apps anywhere, at scale
To keep your systems running, many workload teams have architected and designed
applications where components are distributed across public cloud services, private
clouds, data centers, and edge locations. With Azure Arc enabled Kubernetes, you can
accelerate development by using best in class application services with standardized
deployment, configuration, security, and observability. One of the primary benefits of
Azure Arc is facilitating implementation of DevOps principles that apply established
development practices to operations. This results in improved agility, without
jeopardizing the stability of IT environment.
Centrally code and deploy applications confidently to any Kubernetes distribution
in any location.
Centrally manage and delegate access for DevOps roles and responsibilities.
Reduce errors with consistent configuration and policy-driven deployment and
operations for applications and Kubernetes clusters.
Delegate access for DevOps roles and responsibilities through Azure RBAC.
Reduce errors with consistent policy driven deployment and operations through
GitHub and Azure Policy.

Connect Kubernetes clusters to Azure and start
deploying using a GitOps model

GitOps relies on a Git repository to host files that contain the configuration representing
the expected state of a resource. An agent running on the cluster monitors the state of
the repository and, when there is a change on the repository, the agent pulls the
changed files to the cluster and applies the new configuration.
In the context of Azure Arc enabled Kubernetes clusters, a Git repository hosts a
configuration of a Kubernetes cluster, including its resources such as pods and
deployments. A pod or a set of pods running on the cluster polls the status of the
repository and, once it detects a change, it pulls and applies the new configuration to
the cluster.
Azure Arc enabled Kubernetes clusters rely on Flux, an open-source GitOps deployment
tool to implement the pods responsible for tracking changes to the Git repository you
designate and applying them to the local cluster. In addition, the containerized Flux
operator also periodically reviews the existing cluster configuration to ensure that it
matches the one residing in the Git repository. If there is a configuration drift, the Flux
agent remediates it by reapplying the desired configuration.
Each association between an Azure Arc enabled Kubernetes cluster configuration and
the corresponding GitOps repository resides in Azure, as part of the Azure Resource
Manager resource representing the Azure Arc enabled Kubernetes clusters. You can
configure that association via traditional Azure management interfaces, such as the
Azure portal or Azure CLI. Alternatively, you can use Azure Policy to automate this
process, allowing you to apply it consistently to all resources in an entire subscription or
individual resource groups you designate.

Modernize applications anywhere with Azure
Kubernetes Service on Azure Stack HCI
If you are looking for a fully managed Kubernetes solution on-premises in your
datacenters and/or edge locations, AKS on Azure Stack HCI is a great option. Azure
Kubernetes Service on Azure Stack HCI is an on-premises implementation of Azure
Kubernetes Service (AKS), which automates running containerized applications at scale.
Azure Kubernetes Service is now in preview on Azure Stack HCI and Windows Server
2019 Datacenter, making it quicker to get started hosting Linux and Windows containers
in your datacenter.
AKS clusters on Azure Stack HCI can be connected to Azure Arc for centralized
management. Once connected, you can deploy your applications and Azure data
services to these clusters and extend Azure services such as Azure Monitor, Azure Policy
and Microsoft Defender for Cloud.

Azure Stack HCI use cases
Modernize your high-performance workloads and containerized applications
Use Azure Stack HCI to enable automated deployment, scaling and
management of containerized applications by running a Kubernetes cluster on
your hyperconverged infrastructure.
Deploy AKS on Azure Stack HCI using Windows Admin Center or PowerShell.
Deploy and manage workloads in remote and branch sites
Use Azure Stack HCI to deploy your container-built edge workloads, and
essential business applications in highly available virtual machines (VMs).
Bring efficient application development and deployment to remote locations at
the right price by leveraging switchless deployment and 2 node clusters.
Get a global view of your system's health using Azure Monitor.
Upgrade your infrastructure for remote work using VDI
Bring desktops on-premises for low latency and data sovereignty enabling
remote work using a brokerage service like Microsoft Remote Desktop Services.
With Azure Stack HCI you can scale your resources in a simple predictable way.
Provide a secure way to deliver desktop services to a wide range of devices
without allowing users to store data locally or upload data from those local
devices.

Resources and architectures related to
Operational Excellence
The introduction of cloud computing had a significant impact on how software is
developed, delivered, and run. With Azure Arc enabled infrastructure and Azure Arc
components like Azure Arc enabled Kubernetes and Azure Arc enabled data services it
becomes possible to design cloud native applications with a consistent set of principles
and tooling across public cloud, private cloud, and the edge.
Click the following links for architecture details and diagrams that enable application
design and DevOps practices consistent with Operational excellence principles.

Application design
Azure Arc hybrid management and deployment for Kubernetes clusters
Run containers in a hybrid environment
Managing K8 clusters outside of Azure with Azure Arc

Optimize administration of SQL Server instances in on-premises and multi-cloud
environments by leveraging Azure Arc
Azure Data Studio dashboards
microsoft/azure_arc: Azure Arc environments bootstrapping for everyone
github.com)
All Azure Architecture Center Hybrid and Multicloud Architectures

Monitoring
Enable monitoring of Azure Arc enabled Kubernetes cluster
Azure Monitor for containers overview

Application performance management
Hybrid availability and performance monitoring

Manage data anywhere

Next steps
Performance Efficiency

(in

Performance efficiency in a hybrid
workload
Article • 11/30/2022

Performance efficiency is the ability of your workload to scale to meet the demands
placed on it by users in an efficient manner. In a hybrid environment, it is important to
consider how you manage your on-premises or multicloud workloads to ensure they
can meet the demands for scale. You have options to scale up into the cloud when your
on-premises resources reach capacity. Scale up, down, and scale out your databases
without application downtime.
Using a tool like Azure Arc, you can build cloud native apps anywhere, at scale. Architect
and design hybrid applications where components are distributed across public cloud
services, private clouds, data centers and edge locations without sacrificing central
visibility and control. Deploy and configure applications and Kubernetes clusters
consistently and at scale from source control and templates. You can also bring PaaS
services on premises. This allows you to use cloud innovation flexibly, where you need it
by deploying Azure services anywhere. Implement cloud practices and automation to
deploy faster, consistently, and at scale with always up-to-date Azure Arc enabled
services. You can scale elastically based on capacity, with the ability to deploy in
seconds.

Azure Arc design
The first steps with Azure Arc are to connect the machines to Azure. To use Azure Arc to
connect the machine to Azure, you need to install the Azure Connected Machine agent
on each machine that you plan to connect using Azure Arc. You can connect any other
physical or virtual machine running Windows or Linux to Azure Arc.
There are four ways to connect machines:
1. Manual installation
2. Script-based installation
3. Connect machines at scale using service principal
4. Installation using Windows PowerShell DSC
After connecting the machines, you can then manage the VM extensions all from Azure,
which provides consistent extension management between Azure and non-Azure VMs.
In Azure you can use Azure Automation State Configuration to centrally store
configurations and maintain the desired state of Arc enabled servers through the DSC

VM extension. You can also collect log data for analysis with Azure Monitor Logs
enabled through the Log Analytics agent VM extension. With Azure Monitor, you can
analyze the performance of your Windows and Linux VMs and monitor their processes
and dependencies on other resources and external processes.

Azure Arc enabled Kubernetes
With Azure Arc enabled Kubernetes, you need to register the cluster first. You can
register any CNCF Kubernetes cluster that is running. You'll need a kubeconfig file to
access the cluster and cluster-admin role on the cluster for deploying Arc-enabled
Kubernetes agents. You'll use Azure Command-Line Interface (Azure CLI) to perform
cluster registration tasks.

Azure Arc enabled SQL Managed Instance
When planning for deployment of Azure Arc enabled SQL Managed Instance, you
should identify the correct amount of compute, memory, and storage that will be
required to run the Azure Arc data controller and the intended SQL managed instance
server groups.
You have the flexibility to extend the capacity of the underlying Kubernetes or AKS
cluster over time by adding additional compute nodes or storage. Kubernetes or AKS
offers an abstraction layer over the underlying virtualization stack and hardware. Storage
classes implement such abstraction for storage.
７ Note
When provisioning a pod, you need to decide which storage class to use for its
volumes. Your decision is important from a performance standpoint because an
incorrect choice could result in suboptimal performance.
When planning for deployment of Azure Arc enabled SQL Managed Instance, you
should consider a range of factors affecting storage configuration kubernetes-storageclass-factors for both data controller and database instances.

Azure Stack HCI
With the scope of Azure Arc extended to Azure Stack HCI VMs, you'll be able to
automate their configuration by using Azure VM extensions and evaluate their
compliance with industry regulations and corporate standards by using Azure Policy.

In remote office/branch office scenarios, you must consider storage resiliency versus
usage efficiency, versus performance. Planning for Azure Stack HCI volumes involves
identifying the optimal balance between resiliency, usage efficiency, and performance.
The challenge results from the fact that maximizing one of these characteristics typically
has a negative impact on at least one of the other two.
To learn more, see Use Azure Stack HCI switchless interconnect and lightweight quorum
for Remote Office/Branch Office.

Monitoring in a hybrid environment
Monitoring in a hybrid environment can be a challenge. However, with tools like Azure
Arc as you bring Azure services on-premises, you can easily enroll in additional Azure
services such as monitoring, security, and update by simply turning them on.
Across products: Integrate with Microsoft Sentinel, Microsoft Defender for Cloud
Bring Microsoft Defender for Cloud to your on-prem data and servers with Arc
Set security policies, resource boundaries, and RBAC for workloads across the
hybrid infra
Proper admin roles for read, modify, re-onboard, and delete a machine

Monitoring containers
Monitoring your containers is critical. Azure Monitor for containers provides a rich
monitoring experience for the AKS and AKS engine clusters.
Configure Azure Monitor for containers to monitor Azure Arc enabled Kubernetes
clusters hosted outside of Azure. This helps achieve comprehensive monitoring of your
Kubernetes clusters across Azure, on-premises, and third-party cloud environments.
Azure Monitor for containers can provide you with performance visibility by collecting
memory and processor metrics from controllers, nodes, and containers available in
Kubernetes through the Metrics application programming interface (API). Container logs
are also collected. After you enable monitoring from Kubernetes clusters, metrics and
logs are automatically collected for you through a containerized version of the Log
Analytics agent. Metrics are written to the metrics store and log data is written to the
logs store associated with your Log Analytics workspace. For more information about
Azure Monitor for containers, refer to Azure Monitor for containers overview.
Enable Azure Monitor for containers for one or more existing deployments of
Kubernetes by using either a PowerShell or a Bash script. To enable monitoring for Arc

enabled Kubernetes clusters, refer to Enable monitoring of Azure Arc enabled
Kubernetes cluster.
Automatically enroll in additional Azure Arc enabled resources and services. Simply turn
them on when needed:
Strengthen your security posture and protect against threats by turning on
Microsoft Defender for Cloud.
Get actionable alerts from Azure Monitor.
Detect, investigate, and mitigate security incidents with the power of a cloudnative SIEM, by turning on Microsoft Sentinel.

Deploy and manage containerized applications
Deploy and manage containerized applications with GitHub and Azure Policy. Ensure
that applications and clusters are consistently deployed and configured at scale from
source control. Write with your familiar tool set to the same application service APIs that
can run consistently on-premises, across multicloud, and in edge environments. Easily
instrument Azure monitoring, telemetry, and security services into your hybrid apps
wherever they run.

Next steps
Reliability

Reliability in a hybrid workload
Article • 11/30/2022

In the cloud, we acknowledge up front that failures will happen. Instead of trying to
prevent failures altogether, the goal is to minimize the effects of a single failing
component. While historically you may have purchased levels of redundant higher-end
hardware to minimize the chance of an entire application platform failing, in the cloud,
we acknowledge up front that failures will happen.
For hybrid scenarios, Azure offers an end-to-end backup and disaster recovery solution
that's simple, secure, scalable, and cost-effective, and can be integrated with onpremises data protection solutions. In the case of service disruption or accidental
deletion or corruption of data, recover your business services in a timely and
orchestrated manner.
Many customers operate a second datacenter, however, Azure can help reduce the costs
of deploying, monitoring, patching, and scaling on-premises disaster recovery
infrastructure, without the need to manage backup resources or build a secondary
datacenter.
Extend your current backup solution to Azure, or easily configure our application-aware
replication and application-consistent backup that scales based on your business needs.
The centralized management interface for Azure Backup and Azure Site Recovery makes
it simple to define policies to natively protect, monitor, and manage enterprise
workloads across hybrid and cloud. These include:
Azure Virtual Machines
SQL and SAP databases
On-premises Windows servers
VMware machines
７ Note
By not having to build on-premises solutions or maintain a costly secondary
datacenter, customers can reduce the cost of deploying, monitoring, patching, and
scaling disaster recovery infrastructure by backing up their hybrid data and
applications with Azure.

Backup and Recovery

Together, Azure Backup and Azure Site Recovery use the underlying power and
unlimited scale of the cloud to deliver high availability with minimal maintenance or
monitoring overhead. These native capabilities are available through a pay-as-you-use
model that bills only for the storage that is consumed.
Using Azure Site Recovery, users can set up and manage replication, failover, and
failback from a single location in the Azure portal. The Azure hybrid services tool in
Windows Admin Center can also be used as a centralized hub to easily discover all the
available Azure services that bring value to on-premises or hybrid environments.
Windows Admin Center streamlines setup and the process of replicating virtual
machines on Hyper-V servers or clusters, making it easier to bolster the resiliency of
environments with Azure Site Recovery's disaster recovery service.
Azure is committed to providing the best-in-class data protection to keep your
applications running. Azure Backup protects backups of on-premises and cloudresources from ransomware attacks by isolating backup data from source data,
combined with multi-factor authentication and the ability to recover maliciously or
accidentally deleted backup data. With Azure Site Recovery you can fail over VMs to the
cloud or between cloud data centers and secure them with network security groups.
In the case of a disruption, accidental deletion, or corruption of data, customers can rest
assured that they will be able to recover their business services and data in a timely and
orchestrated manner. These native capabilities support low recovery-point objective
(RPO) and recovery-time objective (RTO) targets for any mission-critical workload in
your organization. Azure is here to help customers pivot towards a strengthened BCDR
strategy.

Availability Considerations
For Azure Arc
In most cases, the location you select when you create the installation script should be
the Azure region geographically closest to your machine's location. The rest of the data
will be stored within the Azure geography containing the region you specify, which
might also affect your choice of region if you have data residency requirements. If an
outage affects the Azure region to which your machine is connected, the outage will not
affect the Arc enabled server, but management operations using Azure might not be
able to complete. For resilience in the event of a regional outage, if you have multiple
locations which provide a geographically-redundant service, it's best to connect the
machines in each location to a different Azure region.

Ensure that Azure Arc is supported in your regions by checking supported regions. Also,
ensure that services referenced in the Architecture section are supported in the region
to which Azure Arc is deployed.

Azure Arc enabled data services
With Azure Arc enabled SQL Managed Instance, you can deploy individual databases in
either a single or multiple pod pattern. For example, the developer or general-purpose
pricing tier implements a single pod pattern, while a highly available business critical
pricing tier implements a multiple pod pattern. A highly available Azure SQL managed
instance uses Always On Availability Groups to replicate the data from one instance to
another either synchronously or asynchronously.
With Azure Arc enabled SQL Managed Instance, planning for storage is also critical from
the data resiliency standpoint. If there's a hardware failure, an incorrect choice might
introduce the risk of total data loss. To avoid such risk, you should consider a range of
factors affecting storage configuration kubernetes-storage-class-factors for both data
controller and database instances.
Azure Arc enabled SQL Managed Instance provides automatic local backups, regardless
of the connectivity mode. In the Directly Connected mode, you also have the option of
leveraging Azure Backup for off-site, long-term backup retention.

Azure Stack HCI
Site-level fault domains
Each physical site of an Azure Stack HCI stretched cluster represents distinct fault
domains that provide additional resiliency. A fault domain is a set of hardware
components that share a single point of failure. To be fault tolerant to a particular level,
you need multiple fault domains at that level.

Site awareness
Site awareness allows you to control placement of virtualized workloads by designating
their preferred sites. Specifying the preferred site for a stretched cluster offers many
benefits, including the ability to group workloads at the site level and to customize
quorum voting options. By default, during a cold start, all virtual machines use the
preferred site, although it is also possible to configure the preferred site at the cluster
role or group level.

Resiliency limits
Azure Stack HCI provides multiple levels of resiliency, but because of its hyperconverged architecture, that resiliency is subject to limits imposed not only by the
cluster quorum, but also by the pool quorum. You can eliminate this limit by
implementing cluster sets in which you combine multiple Azure Stack HCI clusters to
create an HCI platform consisting of hundreds of nodes.

Next steps
Security

Security in a hybrid workload
Article • 11/30/2022

Security is one of the most important aspects of any architecture. Particularly in hybrid
and multicloud environments, an architecture built on good security practices should be
resilient to attacks and provide confidentiality, integrity, and availability. To assess your
workload using the tenets found in the Microsoft Azure Well-Architected Framework,
see the Microsoft Azure Well-Architected Review.
Microsoft Defender for Cloud can monitor on-premises systems, Azure VMs, Azure
Monitor resources, and even VMs hosted by other cloud providers. To support that
functionality, the standard fee-based tier of Microsoft Defender for Cloud is needed. We
recommend that you use the 30-day free trial to validate your requirements. Defender
for Cloud's operational process won't interfere with your normal operational procedures.
Instead, it passively monitors your deployments and provides recommendations based
on the security policies you enable.
Microsoft Sentinel can help simplify data collection across different sources, including
Azure, on-premises solutions, and across clouds using built-in connectors. Microsoft
Sentinel works to collect data at cloud scale—across all users, devices, applications, and
infrastructure, both on-premises and in multiple clouds.

Azure Architecture Center (AAC) resources
Hybrid Security Monitoring using Microsoft Defender for Cloud and Microsoft
Sentinel
DevSecOps in Azure
Optimize administration of SQL Server instances in on-premises and multi-cloud
environments by leveraging Azure Arc
Implement a secure hybrid network
Securely managed web applications

Principles
Azure Arc management security capabilities
Access unique Azure security capabilities such as Microsoft Defender for Cloud.
Centrally manage access for resources with Role-Based Access Control.

Centrally manage and enforce compliance and simplify audit reporting with Azure
Policy.

Azure Arc enabled data services security capabilities
Protect your data workloads with Microsoft Defender for Cloud in your
environment, using the advanced threat protection and vulnerability assessment
features for unmatched security.
Set security policies, resource boundaries, and role-based access control for
various data workloads seamlessly across your hybrid infrastructure.

Azure Stack HCI
Protection in transit. Storage Replica offers built-in security for its replication
traffic. This includes packet signing, AES-128-GCM full data encryption, support for
Intel AES-NI encryption acceleration, and pre-authentication integrity man-in-themiddle attack prevention.
Storage Replica also utilizes Kerberos AES256 for authentication between the
replicating nodes.
Encryption at rest. Azure Stack HCI supports BitLocker Drive Encryption for its data
volumes, thus facilitating compliance with standards such as FIPS 140-2 and HIPAA.
Integration with a range of Azure services that provide more security
advantages. You can integrate virtualized workloads that run on Azure Stack HCI
clusters with Azure services such as Microsoft Defender for Cloud.
Firewall-friendly configuration. Storage Replica traffic requires a limited number
of open ports between the replicating nodes.

Design
Azure Arc enabled servers
Implement Azure Monitor
Use Azure Monitor to monitor your VMs, virtual machine scale sets, and Azure Arc
machines at scale. Azure Monitor analyzes the performance and health of your Windows
and Linux VMs. It also monitors their processes and dependencies on other resources
and external processes. It includes support for monitoring performance and application
dependencies for VMs that are hosted on-premises or in another cloud provider.
Implement Microsoft Sentinel

Use Microsoft Sentinel to deliver intelligent security analytics and threat intelligence
across the enterprise. This provides a single solution for alert detection, threat visibility,
proactive hunting, and threat response. Microsoft Sentinel is a scalable, cloud-native,
security information event management (SIEM) and security orchestration automated
response (SOAR) solution that enables several scenarios including:
Collect data at cloud scale across all users, devices, applications, and infrastructure,
both on-premises and in multiple clouds.
Detect previously undetected threats and minimize false positives.
Investigate threats with artificial intelligence and hunt for suspicious activities at
scale.
Respond to incidents rapidly with built-in orchestration and automation of
common tasks.

Azure Stack HCI
A stretched Azure Stack HCI cluster relies on Storage Replica to perform synchronous
storage replication between storage volumes hosted by the two groups of nodes in their
respective physical sites. If a failure affects the availability of the primary site, the cluster
automatically transitions its workloads to nodes in the surviving site to minimize
potential downtime.

Monitor
Across products: Integrate with Microsoft Sentinel and Microsoft Defender for
Cloud.
Bring Microsoft Defender for Cloud to your on-premises data and servers with Arc.
Set security policies, resource boundaries, and RBAC for workloads across the
hybrid infrastructure.
Set the correct admin roles to read, modify, re-onboard, and delete a machine.

Overview of IoT workloads
Article • 04/27/2023

This section of the Microsoft Azure Well-Architected Framework aims to address the
challenges of building IoT workloads on Azure. This article describes the IoT design
areas, architecture patterns, and architecture layers in the IoT workload.
Five pillars of architectural excellence underpin the IoT workload design methodology.
These pillars serve as a compass for subsequent design decisions across the design
areas described in this article. The remaining articles in this series delve into how to
evaluate the design areas using IoT-specific design principles in the reliability, security,
cost optimization, operational excellence, and performance efficiency pillars.
 Tip
To assess your IoT workload through the lenses of reliability, security, cost
optimization, operational excellence, and performance efficiency, see the Azure
Well-Architected Review.

What is an IoT workload?
The term workload refers to the collection of application resources that support a
common business goal or the execution of a common business process. These goals or
processes use multiple services, such as APIs and data stores. The services work together
to deliver specific end-to-end functionality.
Internet of Things (IoT) is a collection of managed and platform services across edge
and cloud environments that connect, monitor, and control physical assets.
An IoT workload therefore describes the practice of designing, building, and operating
IoT solutions to help meet architectural challenges according to your requirements and
constraints.
The IoT workload addresses the three components of IoT systems:
Things, or the physical objects, industrial equipment, devices, and sensors that
connect to the cloud persistently or intermittently.
Insights, information that the things collect that humans or AI analyze and turn into
actionable knowledge.

Actions, the responses of people or systems to insights, which connect to business
outcomes, systems, and tools.

IoT architecture patterns
Most IoT systems use either a connected products or connected operations architecture
pattern. Each pattern has specific requirements and constraints in the IoT design areas.
Connected products architectures focus on the hot path. End users manage and
interact with products by using real-time applications. This pattern applies to
manufacturers of smart devices for consumers and businesses in a wide range of
locations and settings. Examples include smart coffee machines, smart TVs, and
smart production machines. In these IoT solutions, the product builders provide
connected services to the product users.
Connected operations architectures focus on the warm or cold path with edge
devices, alerts, and cloud processing. These solutions analyze data from multiple
sources, gather operational insights, build machine learning models, and initiate
further device and cloud actions. The connected operations pattern applies to
enterprises and smart service providers that connect pre-existing machines and
devices. Examples include smart factories and smart buildings. In these IoT
solutions, service builders deliver smart services that provide insights and support
the effectiveness and efficiency of connected environments.
To learn more about the base solution architecture for IoT workloads, see Azure IoT
reference architecture and Industry specific Azure IoT reference architectures.

Well-Architected Framework pillars in your IoT
workload
The Azure Well-Architected Framework consists of five pillars of architectural excellence,
which you can use to improve the quality of IoT workloads. The following articles
highlight how IoT-specific design principles influence decisions across IoT design areas:
Reliability ensures that applications meet availability commitments. Resiliency
ensures that workloads are available and can recover from failures at any scale.
Reliability in your IoT workload discusses how the IoT design areas of
heterogeneity, scalability, connectivity, and hybridity affect IoT reliability.
Security provides confidentiality, integrity, and availability assurances against
deliberate attacks and abuse of data and systems. Security in your IoT workload

describes how heterogeneity and hybridity affect IoT security.
Cost optimization balances business goals with budget justification to create costeffective workloads while avoiding capital-intensive solutions. Cost optimization in
your IoT workload looks at ways to reduce expenses and improve operational
efficiency across IoT design areas.
Operational excellence covers the processes that build and run applications in
production. Operational excellence in your IoT workload discusses how
heterogeneity, scalability, connectivity, and hybridity affect IoT operations.
Performance efficiency is a workload's ability to scale efficiently to meet demands.
Performance efficiency in your IoT workload describes how heterogeneity,
scalability, connectivity, and hybridity affect IoT performance.

IoT design areas
The key IoT design areas that facilitate a good IoT solution design are:
Heterogeneity
Security
Scalability
Flexibility
Serviceability
Connectivity
Hybridity
The design areas are interrelated and decisions made within one area can affect
decisions across the entire design. To evaluate the design areas, use the IoT-specific
design principles in the five pillars of architectural excellence. These principles help
clarify considerations to ensure your IoT workload meets requirements across
architecture layers.
The following sections describe the IoT design areas, and how they apply to the IoT
connected products and connected operations architecture patterns.

Heterogeneity
IoT solutions must accommodate various devices, hardware, software, scenarios,
environments, processing patterns, and standards. It's important to identify the
necessary level of heterogeneity for each architecture layer at design time.

In connected products architectures, heterogeneity describes the varieties of machines
and devices that need to be supported. Heterogeneity also describes the variety of
environments where you can deploy smart product, such as networks and types of
users.
In connected operations architectures, heterogeneity focuses on support for different
operational technology (OT) protocols and connectivity.

Security
IoT solutions must consider security and privacy measures across all layers. Security
measures include:
Device and user identity.
Authentication and authorization.
Data protection for data at rest and in transit.
Strategies for data attestation.
In connected products architectures, limited control over product use in heterogeneous
and widely distributed environments affects security. According to the Microsoft Threat
Modeling Tool STRIDE model, the highest risk to devices is from tampering, and the
threat to services is from denial of services from hijacked devices.
In connected operations architectures, the security requirements for the deployment
environment are important. Security focuses on specific OT environment requirements
and deployment models, such as ISA95

and Purdue, and integration with the cloud-

based IoT platform. Based on STRIDE, the highest security risks for connected operations
are spoofing, tampering, information disclosure, and elevation of privilege.

Scalability
IoT solutions must be able to support hyper-scalability, with millions of connected
devices and events ingesting large amounts of data at high frequency. IoT solutions
must enable proof of concept and pilot projects that start with a few devices and events,
and then scale out to hyper-scale dimensions. Considering the scalability of each
architecture layer is essential to IoT solution success.
In connected products architectures, scale describes the number of devices. In most
cases, each device has a limited set of data and interactions, controlled by the device
builder, and scalability comes only from the number of devices deployed.
In connected operations architectures, scalability depends on the number of messages
and events to process. In general, the number of machines and devices is limited, but OT

machines and devices send large numbers of messages and events.

Flexibility
IoT solutions build on the principle of composability, which enables combining various
first-party or third-party components as building blocks. A well-architected IoT solution
has extension points that enable integration with existing devices, systems, and
applications. A high-scale, event-driven architecture with brokered communication is
part of the backbone, with loosely coupled composition of services and processing
modules.
In connected products architectures, changing end-user requirements define flexibility.
Solutions should allow you to easily change device behavior and end-user services in
the cloud, and provide new services.
In connected operations architectures, the support for different types of devices defines
flexibility. Solutions should be able to easily connect legacy and proprietary protocols.

Serviceability
IoT solutions must consider ease of maintaining and repairing components, devices, and
other system elements. Early detection of potential problems is critical. Ideally, a wellarchitected IoT solution should correct problems automatically before serious trouble
occurs. Maintenance and repair operations should cause as little downtime or disruption
as possible.
In connected products architectures, the wide distribution of devices affects
serviceability. The ability to monitor, manage, and update devices within end user
context and control, without direct access to that environment, is limited.
In connected operations architectures, serviceability depends on the given context,
controls, and procedures of the OT environment, which may include systems and
protocols already available or in use.

Connectivity
IoT solutions must be able to handle extended periods of offline, low-bandwidth, or
intermittent connectivity. To support connectivity, you can create metrics to track
devices that don't communicate regularly.
Connected products run in uncontrolled consumer environments, so connectivity is
unknown and hard to sustain. Connected products architectures must be able to

support unexpected extended periods of offline and low-bandwidth connectivity.
In connected operations architectures, the deployment model of the OT environment
affects connectivity. Typically, the degree of connectivity, including intermittent
connectivity, is known and managed in OT scenarios.

Hybridity
IoT solutions must address hybrid complexity, running on different hardware and
platforms across on-premises, edge, and multicloud environments. It's critical to
manage disparate IoT workload architectures, ensure uncompromised security, and
enable developer agility.
In connected products architectures, the wide distribution of devices defines hybridity.
The IoT solution builder controls the hardware and runtime platform, and hybridity
focuses on the diversity of the deployment environments.
In connected operations architectures, hybridity describes the data distribution and
processing logic. Scale and latency requirements determine where to process data and
how fast feedback must be.

IoT architecture layers
An IoT architecture consists of a set of foundational layers. Specific technologies support
the different layers, and the IoT workload highlights options for designing and creating
each layer.
Core layers identify IoT-specific solutions.
Common layers aren't specific to IoT workloads.
Cross-cutting layers support all layers in designing, building, and running solutions.
The IoT workload addresses different layer-specific requirements and implementations.
The framework focuses on the core layers, and identifies the specific impact of the IoT
workload on the common layers.

Device management and
modeling
layer

Interac on
and
repor ng
layer

DevOps

Event processing
and analy cs
layer
Device and
gateway
layer

Inges on and
communica on
layer
Storage
layer

Integra on
layer

Transport layer

Core IoT layers

Common layers

The following sections describe the IoT architecture layers and the Microsoft
technologies that support them.

Core layers and services
The IoT core layers and services identify whether a solution is an IoT solution. The core
layers of an IoT workload are:
Device and gateway
Device management and modeling
Ingestion and communication
The IoT workload focuses primarily on these layers. To realize these layers, Microsoft
provides IoT technologies and services such as:
Azure IoT Hub
Azure IoT device SDKs

Azure IoT Edge
IoT Hub Device Provisioning Service (DPS)
Azure Digital Twins
Azure Sphere.
 Tip
Azure IoT Central is a managed application platform that you can use to quickly
evaluate your IoT scenario and assess the opportunities for your business. After
you've used IoT Central to evaluate your IoT scenario, you can then build your
enterprise ready solution by using the power of Azure IoT platform.

Device and gateway layer
This layer represents the physical or virtual device and gateway hardware deployed at
the edge or on premises. Elements in this layer include the operating systems and the
device or gateway firmware. Operating systems manage the processes on the devices
and gateways. Firmware is the software and instructions programmed onto devices and
gateways. This layer is responsible for:
Sensing and acting on other peripheral devices and sensors.
Processing and transferring IoT data.
Communicating with the IoT cloud platform.
Base level device security, encryption, and trust root.
Device level software and processing management.
Common use cases include reading sensor values from a device, processing and
transferring data to the cloud, and enabling local communication.
Relevant Microsoft technologies include:
Azure IoT Edge
Azure IoT device SDKs
Azure RTOS
Microsoft Defender for IoT
Azure Sphere
Windows for IoT

Ingestion and communication layer

This layer aggregates and brokers communications between the device and gateway
layer and the IoT cloud solution. This layer enables:
Support for bi-directional communication with devices and gateways.
Aggregating and combining communications from different devices and gateways.
Routing communications to a specific device, gateway, or service.
Bridging and transforming between different protocols. For example, mediate
cloud or edge services into an MQTT message going to a device or gateway.
Relevant Microsoft technologies include:
Azure IoT Hub
Azure IoT Central

Device management and modeling layer
This layer maintains the list of devices and gateway identities, their state, and their
capabilities. This layer also enables the creation of device type models and relationships
between devices.
Relevant Microsoft technologies include:
IoT Hub device twins
IoT Hub Device Provisioning Service
Azure Digital Twins
IoT Plug and Play

Common layers and services
Workloads other than IoT, such as Data & AI and modern applications, also use the
common layers. The top-level Azure Well-Architected Framework addresses the generic
elements of these common layers, and other workload frameworks address other
requirements. The following sections touch on the IoT-related influence on
requirements, and include links to other guidance.

Transport layer
This layer represents the way devices, gateways, and services connect and communicate,
the protocols they use, and how they move or route events, both on premises and in the
cloud.
Relevant Microsoft technologies include:

OT and IoT protocols, such as MQTT(S), AMQP(S), HTTPS, OPC-UA, and Modbus
IoT Hub routing
IoT Edge routes

Event processing and analytics layer
This layer processes and acts on the IoT events from the ingestion and communication
layer.
Hot path stream processing and analytics happen in near real-time to identify
immediate insights and actions. For example, stream processing generates alerts
when temperatures rise.
Warm path processing and analytics identify short-term insights and actions. For
example, analytics predict a trend of rising temperatures.
Cold path processing and analytics create intelligent data models for the hot or
warm paths to use.
Relevant Microsoft technologies include:
Azure Stream Analytics
Azure Functions
Azure Databricks
Azure Machine Learning
Azure Synapse Analytics

Storage layer
This layer persists IoT device event and state data for some period of time. The type of
storage depends on the required use for the data.
Streaming storage, such as message queues, decouple IoT services and
communication availability.
Time series-based storage enables warm-path analysis.
Long-term storage supports machine learning and AI model creation.
Relevant Microsoft technologies include:
Azure Event Hubs
Azure Data Explorer
Azure Cosmos DB
Azure SQL
Azure Data Lake Storage

Interaction and reporting layer
This layer lets end users interact with the IoT platform and have a role-based view into
device state, analytics, and event processing.
Relevant Microsoft technologies include:
Azure App Service
Power Apps
Power BI
Dynamics 365 Connected Field Service

Integration layer
This layer enables interaction with systems outside the IoT solution by using machineto-machine or service-to-service communications APIs.
Relevant Microsoft technologies include:
Azure Logic Apps
Azure Functions
Azure API Management
Azure Event Grid
Power Automate

Cross-cutting activities
Cross-cutting activities like DevOps help you design, build, deploy, and monitor IoT
solutions. DevOps lets formerly siloed roles, like development, operations, quality
engineering, and security, coordinate and collaborate to produce better, more reliable,
and agile products.
DevOps is well-known in software development, but can apply to any product or
process development and operations. Teams who adopt a DevOps culture, practices,
and tools can better respond to customer needs, increase confidence in the applications
and products they build, and achieve business goals faster.
The following diagram shows the DevOps continuous planning, development, delivery,
and operations cycle:

Development and deployment activities include the design, build, test, and
deployment of the IoT solution and its components. The activity covers all layers
and includes hardware, firmware, services, and reports.
Management and operations activities identify the current health state of the IoT
system across all layers.
Correctly executing DevOps and other cross-cutting activities can determine your
success in creating and running a well-architected IoT solution. Cross-cutting activities
help you meet the requirements set at design time and adjust for changing
requirements over time. It's important to clearly assess your expertise in these activities
and take measures to ensure execution at the required quality level.
Relevant Microsoft technologies include:
Visual Studio
Azure DevOps
Microsoft Security Development Lifecycle (SDL)
Azure Monitor
Azure Arc
Microsoft Defender for IoT
Microsoft Sentinel

Next steps
Reliability in your IoT workload

Security in your IoT workload
Cost optimization in your IoT workload
Operational excellence in your IoT workload
Performance efficiency in your IoT workload

Related resources
Azure IoT reference architecture
Azure IoT documentation

Reliability in your IoT workload
Article • 04/27/2023

IoT workloads, like all workloads, have the potential to malfunction. The key reliability
considerations for well-architected IoT solutions are how quickly you can detect changes
and how quickly you can resume operations.
IoT applications are often distributed at massive scale, and might operate over
unreliable networks without persistent access or visibility into end-to-end data flows.
Because of these factors, you should design your IoT architecture with availability and
resiliency in mind.
Building a reliable IoT solution requires careful consideration of devices, cloud services,
and how they interact. The choices you make for device hardware, connectivity and
protocols, and cloud services affect your solution's reliability requirements and
capabilities.

Assess reliability in your IoT workload
To assess your IoT workload through the lenses of the Well-Architected Framework
Reliability pillar, complete the reliability questions for IoT workloads in the Azure WellArchitected Review. After the assessment identifies key reliability recommendations for
your IoT solution, use the following content to help implement the recommendations.

Design Principles
Five pillars of architectural excellence underpin the IoT workload design methodology.
These pillars serve as a compass for subsequent design decisions across the key IoT
design areas. The following design principles extend the quality pillar of the Azure WellArchitected Framework - Reliability.
Design
principle

Considerations

Design devices
for resiliency

Design your devices to satisfy the uptime and availability requirements of your
end-to-end solution. Ensure that your IoT device can operate efficiently with
intermittent connectivity to the cloud.

Design
principle

Considerations

Design for
business
requirements

Cost implications are inevitable when introducing architectural modifications
to meet service-level agreements (SLAs). For example, to have greater
reliability and high availability you can implement cross-region redundancies
and an automated system to autoscale. This trade-off should be carefully
considered.

Safe, simple
update

An enterprise IoT solution should provide a strategy for how operators
manage devices. IoT operators require simple and reliable update tools and

procedures

practices.

Observe
application
health

Define service-level indicators (SLIs) and service-level objectives (SLOs) based
on observability. Add processes for auditing, monitoring, and alerting beyond
what cloud services provide.

High availability
and disaster

Resilient hardware and software components that build in redundancy,
including cross-region redundancies.

recovery
(HA/DR) for
critical
components.
Plan for

Plan for service quotas and throttles, latency between the detection-action,

capacity

and establish benchmarks at production scale to support uninterrupted data
flow.

IoT architecture layers
Reliability design principles help clarify considerations to ensure your IoT workload
meets requirements across the foundational IoT architecture layers. To achieve overall
solution reliability, each layer should have acceptable levels of reliability.

Device management and
modeling
layer

Interac on
and
repor ng
layer

DevOps

Event processing
and analy cs
layer
Device and
gateway
layer

Inges on and
communica on
layer
Storage
layer

Integra on
layer

Transport layer

Core IoT layers

Common layers

Device and gateway layer
As part of your overall IoT solution, design your devices to satisfy your solution's endto-end uptime and availability requirements. Devices and gateways come in many
forms. IoT devices and gateways can do data collection, supervisory control, and edge
analytics.
Data collection connects devices to sensors or subscribes them to telemetry from
downstream systems, and pushes collected data to the cloud. IoT solution design
should ensure reliable device management and reliable communications from the
device to the cloud.
Devices that provide supervisory control not only collect data to send to the cloud,
but also take actions based on that data. Devices send data back to the machines
or environment to take supervisory actions. The reliability of supervisory control
applications is critical.

Device design
Design and select IoT devices to function reliably in the expected operating conditions
over their expected lifetimes. A reliable device should perform according to its hardware
and software specifications, and any failure should be detected and managed through
mitigation, repair, or replacement. Design devices for reliability, but also plan for failures.

Device lifecycle
Limited service lifetimes affect solution reliability. Assess the consequences of device
failure on the solution, and define a device lifecycle strategy according to solution
requirements.
Device failure impact assessment includes:
Severity, such as single points of failures.
Probability, such as mean time between failures.
Detectability, such as failure mode and effects analysis.
Acceptable downtime period.
The acceptable operational downtime determines the speed and extent of device
maintenance. The availability or longevity of the device and part supply is an important
consideration for device lifecycle.
The more modular the design, the easier it's to swap out parts of the system, especially
if some parts become obsolete earlier than others. Alternative or multi-sourcing of
component and module supply chains are critical for reliable solutions.

Environmental requirements
The conditions in which a device operates affect its reliability. Define your environmental
requirements, and use devices with appropriate feature specifications. These
specifications include parameters such as operating temperature range, humidity,
ingress protection (IP) rating, electromagnetic interference (EMI) immunity, and shock
and vibration immunity.

Operational profile
Performance stress affects the operational behavior of devices, and therefore their
reliability. Define operational profiles that estimate behavior over device lifetime, and
assess device reliability accordingly. Such profiles include operation modes, such as

wireless transmission or low-power modes, and environmental conditions, such as
temperature, over the device lifetime.
In normal operating conditions, the device and software should run safely within the
specified operational profiles. Devices must be able to service and process all the
external sensors and data processing that the solution requires. Avoid running at the
limit of device capabilities.

Regulations and standards
Devices for specific industries are subject to applicable regulations and standards.
Define regulations and standards, and make sure devices meet compliance and
conformity requirements. Regulations include certification and marking, such as FCC or
CE. Standards include industry or agency applications, such as ATEX and MIL-SPEC, and
safety conformance, for example IEC 61508.

Device management and modeling layer
Cloud services provide each device with an identity, and manage devices at scale. The
cloud is often the final data ingress point for all messages flowing from the devices. In
IoT solutions, the cloud services must provide reliability for the IoT devices to integrate
and transmit data.
Device connectivity conditions, including upstream to the cloud and downstream to
local networks, should be part of IoT solution reliability design. Assess the potential
effect of connectivity interruption or interference, and define a connectivity strategy
accordingly.
The connectivity strategy should include robustness, for example fallback capability and
disconnection management, and buffering backup to mitigate cloud dependency for
critical or safety functions.
The following design, error handling, and monitoring practices relate to connectivity.

Connectivity design
An IoT solution should enable the flow of information between intermittently connected
devices and cloud-based services. Make sure your IoT devices can operate efficiently
with intermittent connectivity to the cloud.
Best practices include the following recommendations:

Implement retry and backoff logic in device software.
Synchronize device state with the cloud.
Make sure you can store data on devices if your solution can't tolerate data loss.
Use data sampling and simulations to measure network capacity and storage
requirement baselines.

Connectivity implementation
The Azure IoT device SDKs provide client libraries that you can use on devices or
gateways to simplify connectivity with Azure IoT services. You can use the SDKs to
instrument IoT device clients that:
Connect to the cloud.
Provide a consistent client development experience across different platforms.
Simplify common connectivity tasks by abstracting details of the underlying
protocols and message processing patterns, such as exponential backoff with jitter
and retry logic.

Connectivity monitoring
Connectivity issues for IoT devices can be difficult to troubleshoot because of the many
possible points of failure. Application logic, physical networks, protocols, hardware,
Azure IoT Hub, and other cloud services can have problems.
The ability to detect and pinpoint the source of an issue is critical. However, an IoT
solution at scale could have thousands of devices, so it's not practical to manually check
individual devices. Azure Monitor and Azure Event Grid can help you diagnose
connectivity issues in IoT Hub.

Connectivity resources
Manage connectivity and reliable messaging by using Azure IoT Hub device SDKs
Monitor, diagnose, and troubleshoot Azure IoT Hub device connectivity
Transient fault handling
Error handling for resilient applications in Azure
Circuit Breaker pattern - Cloud Design Patterns
Compensating Transaction pattern - Cloud Design Patterns
Throttling pattern - Cloud Design Patterns

Ingestion and communication layer

The IoT ingestion and communication layer covers service quotas and limits, capacity,
throttling, and autoscale.

Redundant capacity design
When planning thresholds and alerts, consider the latency between detection and action
taken. Make sure the system and operators have enough time to respond to change
requests. Otherwise, for example, you might detect a need to increase the number of
units, but the system could fail by losing messages before the increase can take effect.

Service quota planning
As with all platform services, IoT Hub and the IoT Hub Device Provisioning Service (DPS)
enforce quotas and throttles on certain operations, so Azure can deliver predictable
service levels and costs for its services. Quotas and throttles are tied to the service tier
and number of units you deploy, so you can design your solution with the right number
of resources. Review quotas and throttles in advance, and design your IoT Hub and DPS
resources accordingly.

Production-scale benchmarks
As the number of devices or volume of data increase, the cloud gateway must scale to
support uninterrupted data flow. Due to the distributed nature of IoT solutions, the
number of devices, and the volume of data, it's important to establish scale benchmarks
for the overall solution. These benchmarks help to plan for capacity risks. Use the Azure
IoT Device Telemetry Simulator to simulate production scale volumes.

Autoscaling to dynamically adjust to quotas
A benefit of using platform as a service (PaaS) components is the ability to scale up and
down with little effort according to your needs. To provide the lowest cost and
operational effort, consider implementing an automated system to scale your resources
up and down with the varying needs of your solution.

Quota and throttle management
To ensure solution reliability, continuously monitor resource usage against quotas and
throttles to detect usage increases that indicate the need to scale. Depending on your
business requirements, you can continuously monitor resource usage and alert the
operator when thresholds are met, or implement an automated system to autoscale.

Capacity and scaling resources
Understand Azure IoT Hub quotas and throttling
Quotas and limits in the Azure IoT Hub Device Provisioning Service
Auto-scale your Azure IoT Hub - Code Samples
Azure IoT Device Telemetry Simulator - Code Samples
IoT Hub IP addresses
Best practices for device configuration within an IoT solution
Choose the right IoT Hub tier for your solution
Azure IoT Central quotas and limits
IoT Hub quotas and throttling: Operation throttles
IoT Hub quotas and throttling: Other limits

Transport layer
To connect to the cloud service for data, control, and management, devices need access
to a network. Depending on the type of IoT solution, connectivity reliability might be
your responsibility or that of the network service provider. Networks might have
intermittent connectivity issues, and devices need to manage their behavior accordingly.

DevOps layer
An enterprise IoT solution should provide a strategy for operators to manage the
system. To address reliability, IoT management and operations should use DevOps
processes to handle updates, observability and monitoring, and HA/DR implementation.

Updates
The device aspect of IoT solutions presents challenges compared to cloud-based
solutions. For example, there must be a way to continually update devices to address
vulnerabilities and application changes.
Due to the distributed nature of IoT solutions, it's important to adopt safe and secure
policies for deploying updates. IoT operators require simple and reliable update tools
and practices.
A device update solution must support:
Gradual update rollout through device grouping and scheduling controls.
Support for resilient A/B device updates for seamless rollback.
Detailed update management and reporting tools.

Network optimization based on available bandwidth.
Device Update for IoT Hub is a service that enables safe, secure, and reliable over-the-air
(OTA) IoT device updates. Device Update for IoT Hub can group devices and specify
which devices should receive an update. Operators can view the status of update
deployments and whether each device successfully applies the required updates.
If an update fails, Device Update helps operators identify the devices that failed and see
the failure details. The ability to identify which devices failed can eliminate hours of
trying to manually pinpoint the failure source.
Device Update monitors the status of device deployments and updates, and reports how
many devices are compliant with the highest version available compatible update.

Observability and monitoring
To manage overall solution reliability and define alert procedures, you should monitor
every component of your IoT solution. All Azure IoT services publish metrics that
describe service health and availability. To establish end-to-end observability, also
consider the metrics that you need on the device side. Use these metrics as part of your
overall solution reliability monitoring.
IoT application monitoring and diagnostics are crucial for availability and resiliency. If
something fails, you need to know that it failed, when it failed, and why. By monitoring
the operation of an IoT application and devices against a healthy state, you can detect
and fix reliability issues.
To mitigate issues that affect IoT application reliability, you must be able to capture logs
and signals that help you detect issues in end-to-end operations. Use logging and
monitoring to determine whether an IoT solution is functioning as expected and to help
troubleshoot issues with solution components.
The following actions support observability for IoT solutions:
Establish a mechanism to collect and analyze performance metrics and alerts.
Configure devices, cloud services, and applications to collect and connect with
Azure Monitor.
Use real-time dashboards and alerts to monitor Azure backend services.
Define roles and responsibilities for monitoring and acting on events and alerts.
For more information, see Roles, responsibilities, and permissions.
Implement continuous monitoring.

Azure Monitor

Azure Monitor is the recommended monitoring and visualization platform for Azure IoT
solutions. You can configure devices, cloud services, and applications, regardless of
deployment location, to push log messages directly or through built-in connectors into
Azure Monitor.
Use Azure Monitor built-in metrics integration for remote monitoring of IoT Edge
devices. To enable this capability on your devices, add the IoT Edge Metrics
Collector module to your deployment and configure it to collect and transport
module metrics to Azure Monitor.
With Azure Monitor, you can monitor the state of your IoT Hub environment,
ensure it's running properly, and verify that your devices aren't being throttled or
experiencing connection issues. IoT Hub provides usage metrics, such as the
number of messages used and the number of devices connected. You can relay
this data to Azure Monitor for analysis and to alert other services.
If your solution uses Azure IoT Central, you can use the metrics IoT Central
provides to assess the health of connected devices and active data exports. IoT
Central applications enable metrics by default, which you can access from the
Azure portal. Azure Monitor exposes and provides several ways to interact with
these metrics.
Azure Monitor provides custom log parsing to facilitate the decomposition of
events and records into individual fields for indexing and search.
Implement real-time dashboards and Azure Monitor alerts to monitor Azure
backend services. Alerts proactively notify you about specific conditions in your
monitoring data, so you can identify and address issues before customers
encounter them. You can set alerts on metrics, logs, and the activity log.
Application Insights is a feature of Azure Monitor that provides extensible application
performance management and monitoring for live web apps. If your IoT solution uses
custom Azure App Service, Azure Kubernetes Service, or Azure Functions applications,
you can use Application Insights for app monitoring and analysis.
Application Insights can:
Automatically detect performance anomalies.
Help diagnose issues by using powerful analytics tools.
Show what users actually do with your apps.
Help you continuously improve app performance and usability.

Continuous monitoring

Continuous integration and continuous deployment (CI/CD) is a DevOps practice that
delivers software more quickly and reliably to provide continuous value to users.
Continuous monitoring (CM) is a similar concept that incorporates monitoring across all
phases and components of a DevOps cycle.
CM continuously ensures the health, performance, and reliability of your apps and
infrastructure as they flow through development, production, and release to customers.
For more information, see:
Seven best practices for continuous monitoring with Azure Monitor
Continuous integration and continuous deployment to Azure IoT Edge devices

Monitoring resources
Monitor Azure IoT Hub
Monitor Azure IoT Hub data reference
Trace Azure IoT device-to-cloud messages with distributed tracing
Check Azure IoT Hub service and resource health
Collect and transport metrics - Azure IoT Edge

HA/DR for critical components
As you design and build your IoT solution, you must meet the SLA for failure recovery
across the solution stack. Your SLA should guide you regarding which critical system
components need HA/DR. There are multiple approaches, from redundancy across the
IoT solution stack to redundancy for specific layers. Cost is also a major consideration to
weigh against the importance of meeting SLAs.
Azure IoT services have defined uptime and availability targets. Review the SLAs for
Azure IoT services that are part of your solution to see if they meet your uptime
goals. For example, Azure IoT Hub has an SLA of 99.9%, which means you should
plan for 1 minute and 36 seconds of potential downtime per day. The Azure IoT
Hub SDK provides built-in, configurable logic to handle retries and backoff.
Consider breaking your uptime goals into two categories: device management and
data ingestion operations. For example, it might be critical for a device to
successfully send data to an IoT hub, even if device management services are
unavailable. For more information, see the Azure IoT Hub SDK reliability features.
Consider using redundant hardware for sensors, power, and storage. Redundant
hardware enables devices to function if a critical component isn't available.
Hardware can also help with connectivity issues. For example, you can use a store

and forward approach for data when connectivity isn't available. Azure IoT Edge
has this feature built in.
Devices must also be able to handle cloud outages. Azure region pairing provides
an HA/DR strategy for IoT Hub that meets many SLA requirements. If region
pairing isn't enough, consider implementing a secondary IoT hub. You can also use
DPS to avoid hardcoded IoT Hub configurations on your devices. If your primary
IoT hub goes down, DPS can assign your devices to a different hub.
Consider implementing a heartbeat message pattern for devices you expect to be
online most of the time. This pattern uses a custom IoT Hub route with Azure
Stream Analytics, Azure Logic Apps, or Azure Functions to determine if a heartbeat
has failed. You can use the heartbeat to define Azure Monitor alerts that take
actions as needed.

HA/DR resources
Manage connectivity and reliable messaging by using Azure IoT Hub device SDKs
IoT Hub high availability and disaster recovery
IoT Hub intra-region HA/DR
IoT Hub cross-region HA/DR
How to clone an Azure IoT hub to another region
Testing applications for availability and resiliency

Next steps
Security in your IoT workload

Related resources
Reliability design principles
Azure IoT reference architecture
Azure IoT documentation

Security in your IoT workload
Article • 04/27/2023

IoT solutions have the challenge of securing diverse and heterogeneous device-based
workloads with little or no direct interaction. IoT device builders, IoT application
developers, and IoT solution operators share responsibility for security during the full
IoT solution lifecycle. It's important to design the solution from the start with security in
mind. Understand the potential threats, and add defense in depth

as you design and

architect the solution.
Security planning starts with a threat model. Understanding how an attacker might be
able to compromise a system helps you ensure appropriate mitigations from the start.
Threat modeling offers the greatest value when you incorporate it into the design
phase. As part of the threat modeling exercise, you can divide a typical IoT architecture
into several components or zones: device, device gateway, cloud gateway, and services.
Each zone can have its own authentication, authorization, and data requirements. You
can use zones to isolate damage and restrict the impact of low-trust zones on highertrust zones. For more information, see the Internet of Things (IoT) security architecture.
The following security guidance for IoT Workloads identifies key considerations and
provides design and implementation recommendations.

Assess security in your IoT workload
To assess your IoT workload through the lenses of the Well-Architected Framework
Security pillar, complete the security questions for IoT workloads in the Azure WellArchitected Review. After the assessment identifies key security recommendations for
your IoT solution, use the following content to help implement the recommendations.

Design Principles
Five pillars of architectural excellence underpin the IoT workload design methodology.
These pillars serve as a compass for subsequent design decisions across the key IoT
design areas. The following design principles extend the quality pillar of the Azure WellArchitected Framework - Security.
Design
principle

Considerations

Design
principle

Considerations

Strong
identity

Use a strong identity to authenticate devices and users. Have a hardware root of
trust for trusted identity, register devices, issue renewable credentials, and use
passwordless or multi-factor authentication (MFA). Review general Azure identity
and access management considerations.

Least

Automate and use least-privileged access control to limit impact from

privilege

compromised devices or identities or unapproved workloads.

Device

Evaluate device health to gate device access or flag devices for remediation. Check

health

security configuration, assess vulnerabilities and insecure passwords, monitor for
threats and anomalies, and build ongoing risk profiles.

Device

Continuous updates to keep devices healthy. Use a centralized configuration and

update

compliance management solution and a robust update mechanism to ensure
devices are up-to-date and healthy.

Monitor

Proactively monitor for unauthorized or compromised devices and respond to

system
security,

emerging threats.

plan
incident
response

Zero-trust security model
Unauthorized access to IoT systems could lead to mass information disclosure, such as
leaked factory production data, or elevation of privilege for cyber-physical systems
control, such as stopping a factory production line. A zero trust security model

helps

limit the potential impact of users gaining unauthorized access to cloud or on-premises
IoT services and data.
Instead of assuming everything behind a corporate firewall is safe, zero trust fully
authenticates, authorizes, and encrypts every access request before granting access.
Securing IoT solutions with zero trust starts with implementing basic identity, device,
and access security practices, such as explicitly verifying users, reviewing devices on the
network, and using real-time risk detection to make dynamic access decisions.
The following resources can help you implement a zero-trust IoT solution:
The Microsoft Zero Trust Assessment

analyzes the gaps in your current

protection for identity, endpoints, apps, network, infrastructure, and data. Use the
recommended solutions to prioritize your zero trust implementation, and move
forward with guidance from the Microsoft Zero Trust Guidance Center.

The Zero Trust Cybersecurity for the Internet of Things

technical paper describes

how to apply a zero-trust approach to IoT solutions based on Microsoft's
environment and customer experiences.
The NIST Zero Trust Architecture (nist.gov)

document provides guidance for

creating zero-trust architectures. This document provides general deployment
models and use cases where zero trust could improve an enterprise's overall
information technology security posture.

IoT architecture patterns
Most IoT systems use either a connected products or connected operations architecture
pattern. There are key security differences between these patterns. Connected
operations or operational technology (OT) solutions often have on-premises devices that
monitor and control other physical devices. These OT devices add security challenges
such as tampering, packet sniffing, and the need for out-of-band management and
over-the-air (OTA) updates.
Factories and OT environments can be easy targets for malware and security breaches,
because equipment can be old, physically vulnerable, and isolated from server-level
security. For an end-to-end perspective, review the Azure Well-Architected Framework
security pillar.

IoT architecture layers
Security design principles help clarify considerations to ensure your IoT workload meets
requirements across the foundational IoT architecture layers.
All layers are subject to various threats that can be classified according to the STRIDE
categories: spoofing, tampering, repudiation, information disclosure, denial of service, and
elevation of privilege. Always follow Microsoft Security Development Lifecycle (SDL)
practices when you design and build IoT architectures.

Device management and
modeling
layer

Interac on
and
repor ng
layer

DevOps

Event processing
and analy cs
layer
Device and
gateway
layer

Inges on and
communica on
layer
Storage
layer

Integra on
layer

Transport layer

Core IoT layers

Common layers

Device and gateway layer
This architecture layer includes the immediate physical space around the device and
gateway that allows physical access or peer-to-peer digital access. Many industrial
companies use the Purdue model

included in the ISA 95 standard to ensure their

process control networks both protect their limited network bandwidth and provide
real-time deterministic behavior. The Purdue model provides an extra layer of defensein-depth methodology.

Strong device identity
Tightly integrated capabilities of IoT devices and services provide strong device identity.
These capabilities include:
A hardware root of trust.
Strong authentication using certificates, MFA, or passwordless authentication.

Renewable credentials.
Organizational IoT device registry.
A hardware root of trust has the following attributes:
Secure credential storage that proves identity in dedicated, tamper-resistant
hardware.
Immutable onboarding identity tied to the physical device.
Unique per-device renewable operational credentials for regular device access.
The onboarding identity represents and is inseparable from the physical device. This
identity is typically created and installed during manufacturing, and can't be changed for
the device's lifetime. Given its immutability and lifetime, you should use the device
onboarding identity only to onboard the device into the IoT solution.
After onboarding, provision and use a renewable operational identity and credentials for
authentication and authorization to the IoT application. Making this identity renewable
lets you manage access and revocation of the device for operational access. You can
apply policy-driven gates, such as attestation of device integrity and health, at renewal
time.
The hardware root of trust also ensures that devices are built to security specifications
and conform to required compliance regimes. Protect the supply chain of the hardware
root of trust, or any other hardware components of an IoT device, to ensure that supply
chain attacks don't compromise device integrity.
Passwordless authentication, usually using standard x509 certificates to prove a device's
identity, offers greater protection than secrets such as passwords and symmetric tokens
shared between both parties. Certificates are a strong, standardized mechanism that
provides renewable passwordless authentication. To manage certificates:
Provision operational certificates from a trusted public key infrastructure (PKI).
Use a renewal lifetime appropriate for the business use, management overhead,
and cost.
Make renewal automatic, to minimize any potential access disruption due to
manual rotation.
Use standard, up-to-date cryptography techniques. For example, renew through
certificate signing requests (CSR) instead of transmitting a private key.
Grant access to devices based on their operational identity.
Support credential revocation, such as a certificate revocation list (CRL) when using
x509 certificates, to immediately remove device access, for example in response to
compromise or theft.

Some legacy or resource-constrained IoT devices can't use a strong identity,
passwordless authentication, or renewable credentials. Use IoT gateways as guardians to
locally interface with these less-capable devices, bridging them to access IoT services
with strong identity patterns. This practice lets you adopt zero trust today, while
transitioning to use more capable devices over time.
Virtual machines (VMs), containers, or any service that embeds an IoT client can't use a
hardware root of trust. Use available capabilities with these components. VMs and
containers, which don't have hardware root of trust support, can use passwordless
authentication and renewable credentials. A defense-in-depth solution provides
redundancies where possible, and fills in gaps where necessary. For example, you could
locate VMs and containers in an area with more physical security, such as a data center,
compared to an IoT device in the field.
Use a centralized organizational IoT device registry to manage your organization's IoT
device lifecycle and audit device access. This approach is similar to the way you secure
the user identities of an organization's workforce to achieve zero-trust security. A cloudbased identity registry can handle the scale, management, and security of an IoT
solution.
IoT device registry information onboards devices into an IoT solution by verifying that
the device identity and credentials are known and authorized. After a device is
onboarded, the device registry contains the core properties of the device, including its
operational identity and the renewable credentials used to authenticate for everyday
use.
You can use IoT device registry data to:
View the inventory of an organization's IoT devices, including health, patch, and
security state.
Query and group devices for scaled operation, management, workload
deployment, and access control.
Use network sensors to detect and inventory unmanaged IoT devices that don't connect
to Azure IoT services, for awareness and monitoring.

Least-privileged access
Least-privileged access control helps limit impact from authenticated identities that
might be compromised or running unapproved workloads. For IoT scenarios, grant
operator, device, and workload access by using:

Device and workload access control, for access only to scoped workloads on the
device.
Just-in-time access.
Strong authentication mechanisms such as MFA and passwordless authentication.
Conditional access based on a device's context, such as IP address or GPS location,
system configuration, uniqueness, time of day, or network traffic patterns. Services
can also use device context to conditionally deploy workloads.
To implement effective least-privileged access:
Configure IoT cloud gateway access management to only grant appropriate access
permissions for the functionality the backend requires.
Limit access points to IoT devices and cloud applications by ensuring ports have
minimum access.
Build mechanisms to prevent and detect physical device tampering.
Manage user access through an appropriate access control model, such as rolebased or attribute-based access control.
Layer least-privileged access for IoT devices by using network segmentation.

Network micro-segmentation
Network design and configuration provide opportunities to build defense in depth by
segmenting IoT devices based on their traffic patterns and risk exposure. This
segmentation minimizes the potential impact of compromised devices and adversaries
pivoting to higher-value assets. Network segmentation typically uses next-generation
firewalls.
Network micro-segmentation enables isolation of less-capable devices at the network
layer, either behind a gateway or on a discrete network segment. Use network
segmentation to group IoT devices, and use endpoint protection to mitigate the impact
of potential compromise.
Implement a holistic firewall rule strategy that allows devices to access the network
when required, and blocks access when not allowed. To support defense in depth,
mature organizations can implement micro-segmentation policies at multiple layers of
the Purdue model. If necessary, use firewalls on devices to restrict network access.

Device health
Under the zero-trust principle, device health is a key factor to determine the risk profile,
including trust level, of a device. Use the risk profile as an access gate to ensure only

healthy devices can access IoT applications and services, or to identify devices in
questionable health for remediation.
According to industry standards, device health evaluation should include:
Security configuration assessment and attestation that the device is configured
securely.
Vulnerability assessment to determine whether device software is out of date or
has known vulnerabilities.
Insecure credential assessment to check device credentials, such as certificates, and
protocols, such as Transport Layer Security (TLS) 1.2+.
Active threats and threat alerts.
Anomalous behavioral alerts, such as network pattern and usage deviation.

Zero-trust criteria for devices
To support zero trust, IoT devices should:
Contain a hardware root of trust to provide a strong device identity.
Use renewable credentials for regular operation and access.
Enforce least-privileged access control to local device resources such as cameras,
storage, and sensors.
Emit proper device health signals to enable enforcement of conditional access.
Provide update agents and corresponding software updates for the usable lifetime
of the device to ensure security updates can be applied.
Include device management capabilities to enable cloud-driven device
configuration and automated security response.
Run security agents that integrate with security monitoring, detection, and
response systems.
Minimize physical attack footprint, for example by turning off or disabling any
device features that aren't needed, such as physical USB or UART ports, or WiFi or
Bluetooth connectivity. Use physical removal, covering, or blocking when
necessary.
Protect data on devices. If data at rest is stored on devices, use standard
encryption algorithms to encrypt the data.
Several Azure products and services support IoT device security:
Azure Sphere guardian modules connect critical legacy devices to IoT services with
zero-trust capabilities, including strong identity, end-to-end encryption, and
regular security updates.

Azure IoT Edge provides an edge runtime connection to IoT Hub and other Azure
services, and supports certificates as strong device identities. IoT Edge supports
the PKCS#11 standard for device manufacturing identities and other secrets stored
on a Trusted Platform Module (TPM) or Hardware Security Module (HSM).
The Azure IoT Hub SDKS are a set of device client libraries, developer guides,
samples, and documentation. Device SDKs implement various security features,
such as encryption and authentication, to help you develop a robust and secure
device application.
Azure RTOS provides a real-time operating system as a collection of C-language
libraries that you can deploy on a wide range of embedded IoT device platforms.
Azure RTOS includes a complete TCP/IP stack with TLS 1.2 and 1.3 and basic X.509
capabilities. Azure RTOS and the Azure IoT Embedded SDK also integrate with
Azure IoT Hub, Azure Device Provisioning Service (DPS), and Microsoft Defender.
Features such as X.509 mutual authentication and support for modern TLS cipher
suites such as ECDHE and AES-GCM cover the basics of secure network
communication.
Azure RTOS also supports:
Zero-trust design on microcontroller platforms that support hardware security
features, such as Arm TrustZone, a memory protection and partitioning
architecture.
Secure element devices, such as the STSAFE-A110 from ST Microelectronics.
Industry standards such as the Arm Platform Security Architecture (PSA), which
combines hardware and firmware to provide a standardized set of security
features including secure boot, cryptography, and attestation.
The Azure Certified Device program enables device partners to easily differentiate
and promote devices. The program helps solution builders and customers find IoT
devices built with features that enable a zero-trust solution.
The Edge Secured-core program (preview) validates whether devices meet security
requirements for device identity, secure boot, operating system hardening, device
updates, data protection, and vulnerability disclosures. The Edge Secured-core
program requirements are distilled from various industry requirements and
security engineering points of view.
The Edge Secured-core program enables Azure services such as the Azure
Attestation service to make conditional decisions based on device posture, thus
enabling the zero-trust model. Devices must include a hardware root of trust and
provide secure boot and firmware protection. These attributes can be measured by

the attestation service and used by downstream services to conditionally grant
access to sensitive resources.

Ingestion and communication layer
Data that's ingested into the IoT solution should be protected with the guidance in the
Azure Well-Architected Framework security pillar. Additionally, for IoT solutions it's
critical to ensure that communication from the device to the cloud is secure and
encrypted using the latest TLS standards.

Device management and modeling layer
This architecture layer includes software components or modules running in the cloud
that interface with devices and gateways for data collection and analysis, as well as for
command and control.

Zero-trust criteria for IoT services
Use IoT services that offer the following key zero-trust capabilities:
Full support for zero-trust user access control, for example strong user identities,
MFA, and conditional user access.
Integration with user access control systems for least-privileged access and
conditional controls.
A central device registry for full device inventory and device management.
Mutual authentication, offering renewable device credentials with strong identity
verification.
Least-privileged device access control with conditional access so only devices
fulfilling criteria, such as health or known location, can connect.
OTA updates to keep devices healthy.
Security monitoring of both IoT services and connected IoT devices.
Monitoring and access control for all public endpoints, and authentication and
authorization for any calls to these endpoints.
Several Azure IoT services provide these zero-trust capabilities.
Windows for IoT

helps ensure security across key pillars of the IoT security

spectrum.
BitLocker Drive Encryption, Secure Boot, Windows Defender Application Control,
Windows Defender Exploit Guard, secure Universal Windows Platform (UWP)

applications, Unified Write Filter, a secure communication stack, and security
credential management protect data at rest, during code execution, and in
transit.
Device Health Attestation (DHA) detects and monitors trusted devices to let you
start with a trusted device and maintain trust over time.
Device Update Center and Windows Server Update Services apply the latest
security patches. You can remediate threats to devices by using Azure IoT Hub
device management features, Microsoft Intune or third-party mobile device
management solutions, and Microsoft System Center Configuration Manager.
Microsoft Defender for IoT is an agentless, network layer security platform that
delivers continuous asset discovery, vulnerability management, and threat
detection for IoT and OT devices. Defender for IoT continuously monitors network
traffic using IoT-aware behavioral analytics to identify unauthorized or
compromised components.
Defender for IoT supports proprietary embedded OT devices and legacy Windows
systems commonly found in OT environments. Defender for IoT can inventory all
IoT devices, assess for vulnerabilities, provide risk-based mitigation
recommendations, and continuously monitor devices for anomalous or
unauthorized behavior.
Microsoft Sentinel , a cloud-based security information and event management
(SIEM) and security orchestration, automation, and response (SOAR) platform, that
tightly integrates with Microsoft Defender for IoT. Microsoft Sentinel provides a
cloud-scale view of security across your enterprise by collecting data across all
users, devices, applications, and infrastructure, including firewalls, network access
control, and network switch devices.
Microsoft Sentinel can quickly spot anomalous behaviors that indicate potential
compromise of IoT or OT devices. Microsoft Sentinel also supports third-party
security operations center (SOC) solutions such as Splunk, IBM QRadar, and
ServiceNow.
Azure IoT Hub

provides an operational registry for IoT devices. IoT Hub accepts

device operational certificates to enable strong identity, and can disable devices
centrally to prevent unauthorized connections. IoT Hub supports provisioning of
module identities that support IoT Edge workloads.
Azure IoT Hub Device Provisioning Service (DPS) provides a central device
registry for organizational devices to register for onboarding at scale. DPS

accepts device certificates to enable onboarding with strong device identity and
renewable credentials, registering devices in IoT Hub for their daily operation.
Azure Device Update (ADU) for IoT Hub lets you deploy OTA updates for your
IoT devices. ADU provides a cloud-hosted solution to connect virtually any
device, and supports a broad range of IoT operating systems, including Linux
and Azure RTOS.
Azure IoT Hub support for virtual networks lets you restrict connectivity to IoT
Hub through a virtual network that you operate. This network isolation prevents
connectivity exposure to the public internet, and can help prevent exfiltration
attacks from sensitive on-premises networks.
The following Microsoft products fully integrate hardware and Azure services in overall
IoT solutions.
Azure Sphere is a fully managed integrated hardware, OS, and cloud platform
solution that helps medium and low-power IoT devices attain the seven properties
of highly secured devices to implement zero trust. Devices use explicit verification
and implement certificate-based Device Attestation and Authentication (DAA),
which automatically renews trust.
Azure Sphere uses least-privileged access, where applications are denied access by
default to all peripheral and connectivity options. For network connectivity,
permitted web domains must be included in the software manifest or the
application can't connect outside of the device.
Azure Sphere is built around assumed breach. Defense in depth layers protections
throughout the OS design. A secure world partition running in Arm TrustZone on
Azure Sphere devices helps segment OS breaches from access to Pluton or
hardware resources.
Azure Sphere can be a guardian module to secure other devices, including existing
legacy systems not designed for trusted connectivity. In this scenario, an Azure
Sphere guardian module deploys with an application and interfaces with existing
devices through Ethernet, serial, or BLE. The devices don't necessarily have direct
internet connectivity.
Azure Percept

is an end-to-end edge AI platform that can help you start a proof

of concept in minutes. Azure Percept includes hardware accelerators integrated
with Azure AI and IoT services, pre-built AI models, and solution management.
Azure Percept devices use a hardware root of trust to help protect inference data,
AI models, and privacy-sensitive sensors like cameras and microphones. Azure

Percept enables device authentication and authorization for Azure Percept Studio
services. For more information, see Azure Percept security.

DevOps layer
An enterprise IoT solution should provide a strategy for operators to manage the
system. DevOps methodologies that proactively focus on security include:
Centralized configuration and compliance management, to securely apply policies
and distribute and update certificates.
Deployable updates, to update the full set of software on devices, firmware,
drivers, base OS and host applications, and cloud-deployed workloads.
For more information, see Enable DevSecOps with Azure and GitHub.

Continuous updates
To control device access based on health, you must proactively maintain production
devices in a working, healthy target state. Update mechanisms should:
Have remote deployment capabilities.
Be resilient to changes in environment, operating conditions, and authentication
mechanism, such as certificate changes because of expiry or revocation.
Support update rollout verification.
Integrate with pervasive security monitoring to enable scheduled updates for
security.
You should be able to defer updates that interfere with business continuity, but
eventually complete them within a well-defined time interval after you detect a
vulnerability. Devices that haven't been updated should be flagged as unhealthy.

Security monitoring and response
An IoT solution needs to be able to perform monitoring and remediation at scale for all
its connected devices. As a defense-in-depth strategy, monitoring adds an extra layer of
protection for managed greenfield devices, and provides a compensating control for
legacy, unmanaged brownfield devices that don't support agents and can't be patched
or configured remotely.
You need to decide on logging levels, types of activities to monitor, and responses
required for alerts. Logs should be stored securely and not contain any security details.

According to the Cybersecurity and Infrastructure Security Agency (CISA) , a security
monitoring program should monitor and audit unauthorized changes to controllers,
unusual behavior from devices, and access and authorization attempts. Security
monitoring should include:
Generating an as-is asset inventory and network map of all IoT and OT devices.
Identifying all communication protocols used across IoT and OT networks.
Cataloging all external connections to and from networks.
Identifying vulnerabilities in IoT and OT devices and using a risk-based approach to
mitigate them.
Implementing a vigilant monitoring program with anomaly detection to detect
malicious cyber tactics such as living off the land within IoT systems.
Most IoT attacks follow a kill chain pattern, where adversaries establish an initial
foothold, elevate their privileges, and move laterally across the network. Often, attackers
use privileged credentials to bypass barriers such as next-generation firewalls
established to enforce network segmentation across subnets. Rapidly detecting and
responding to these multistage attacks requires a unified view across IT, IoT, and OT
networks, combined with automation, machine learning, and threat intelligence.
Collect signals from the entire environment, including all users, devices, applications,
and infrastructure, both on-premises and in multiple clouds. Analyze the signals in
centralized SIEM and extended detection and response (XDR) platforms, where SOC
analysts can hunt for and uncover previously unknown threats.
Finally, use SOAR platforms to respond to incidents rapidly and mitigate attacks before
they materially impact your organization. You can define playbooks that automatically
execute when specific incidents are detected. For example, you can automatically block
or quarantine compromised devices so they're unable to infect other systems.

Next steps
Cost optimization in your IoT workload

Related resources
How to apply a Zero Trust approach to your IoT solutions
Zero Trust Cybersecurity for the Internet of Things
Internet of Things (IoT) security architecture
Industry IoT Consortium Security Maturity Model
Security design principles

Azure IoT reference architecture
Azure IoT documentation

Cost optimization in your IoT workload
Article • 04/27/2023

Cost effectiveness is one of the key success factors for IoT projects. In a typical IoT
solution, devices generate large quantities of telemetry that they send to the cloud for
cloud technologies to process and store. How you develop devices and applications,
handle large volumes of data, and design your architecture affects overall costs.
Because an IoT solution is a multilayered technology stack, there are many cost-saving
factors to consider, and many opportunities to optimize costs. Cost optimization is a
process of closed-loop cost control that needs to be continuously monitored, analyzed,
and improved throughout a solution lifecycle.
Solution requirements are the key criteria for IoT architecture decisions. You can
separate requirements into functional and operational requirements. Separate the cost
considerations for each type of requirement, because functional requirements
determine system design, while operational requirements affect system architecture.
Develop multiple use cases based on requirements and compare them before finalizing
your design.
This article presents cost considerations for various combinations of Azure IoT services
and technologies. For cost optimization for specific industries or use cases like
connected factories, predictive maintenance, or remote monitoring, see Industry specific
Azure IoT reference architectures.

Assess cost optimization in your IoT workload
To assess your IoT workload through the lenses of the Well-Architected Framework Cost
Optimization pillar, complete the cost optimization questions for IoT workloads in the
Azure Well-Architected Review. After the assessment identifies key cost optimization
recommendations for your IoT solution, use the following content to help implement
the recommendations.

Design Principles
Five pillars of architectural excellence underpin the IoT workload design methodology.
These pillars serve as a compass for subsequent design decisions across the key IoT
design areas. The following design principles extend the quality pillar of the Azure WellArchitected Framework - Cost Optimization.

Design principle

Considerations

Set up budgets and

Understand total cost of ownership (TCO) by accounting for both direct

maintain cost
constraints

and indirect costs when planning.

Use industrystandard strategies
and approaches

For IoT specific industries with their own ecosystems, for example
manufacturing, energy and environment, or automotive and
transportation, use industry-standard strategies and approaches.

Choose the correct
resources

Define implementation plans for each IoT architecture layer.

Continuously
monitor and
optimize cost

Monitor and optimize costs with ongoing cost optimization activities after
you implement your solution.

management

Total cost of ownership (TCO)
IoT costs are a tradeoff between various technology options. Sometimes it's not a
simple comparison, because IoT is an end-to-end system. Consider the cost benefits of
synergy when reconciling multiple services and technologies. For example, you can use
Azure IoT Hub device twins to handle events in Azure Digital Twins. Device Twins in IoT
Hub are available only in the standard tier of IoT Hub.
It's important to correctly estimate long-term aggregated costs. Review the IoT
technology stacks, and develop a cost model that includes the costs to implement and
operate all services involved. The Azure Pricing Calculator

helps estimate both startup

and operational costs.
In some areas, a one-time cost can be more effective than recurring costs. For example,
in security where hacking techniques are always changing, it can be best to import a
reliable commercial operating system and module such as Azure Sphere. For a one-time
payment, such services provide ongoing monthly device security patches.
Estimate solution costs based on running at scale in production, not proof-of-concept
(PoC) architecture. Architecture and costs evolve rapidly after the PoC. According to the
IoT Signals EDITION 3 report

, the top reason for PoC failure is the high cost of scaling.

The high cost of scaling IoT projects comes from the complexities of integrating across
layers, such as devices, edge connectivity, and compatibility across applications.
Your cost model should include the following areas:

Devices: Starting with a limited number of connected devices, estimate growth in
the number of devices deployed and their messaging patterns. Both devices and
messages can have linear or non-linear growth over time.
Infrastructure: To evaluate infrastructure costs, account for the basics first: storage,
compute, and network. Then account for all services your solution needs to ingest,
egress, and prepare data.
Operations: Include long-term operational costs that increase in parallel with
infrastructure costs, such as employing operators, vendors, and customer support
teams.
Monitoring: Continuously monitor and review costs to identify gaps between
planned and actual costs. A regular cost review meeting helps achieve cost
optimization.

IoT architecture layers
Cost Optimization design principles help clarify considerations to ensure your IoT
workload meets requirements across the foundational IoT architecture layers.
Understanding the IoT architecture layers helps you define a cost baseline and consider
multiple architectures for cost comparison. Each layer has multiple technologies and
ecosystems options, such as devices, telecommunications, or the edge location, so you
need to establish a cost strategy for each layer.
The IoT core layers: device and gateway, device management and modeling, and
ingestion and communication, identify IoT-specific solutions. The other layers and crosscutting activities are also common to, and often shared with, other workloads. However,
TCO and cost optimization must take all costs into account, so you need to consider the
IoT-related costs of common and cross-cutting activities as well as the IoT-specific
layers.

Device management and
modeling
layer

Interac on
and
repor ng
layer

DevOps

Event processing
and analy cs
layer
Device and
gateway
layer

Inges on and
communica on
layer
Storage
layer

Integra on
layer

Transport layer

Core IoT layers

Common layers

Device and gateway layer
This layer is responsible for generating, in some cases optimizing, and transferring data
to the cloud. Cost is a key consideration for designing this layer. Cost optimization
should account for the entire device lifecycle of plan, provision, configure, monitor, and
retire.

Edge solutions require IoT devices to be deployed in the field. The deployment might
need networking and power supply infrastructure that affects costs. Pre-existing
infrastructure can minimize installation costs, but might require ensuring that the
installation doesn't affect existing systems.
Developing or installing IoT devices might require training and employing dedicated
internal or external personnel. Required skills include hardware design, embedded
application development, cloud and local connectivity, security and privacy, and IoT
solution architecture. Industry-specific expertise might also be required. Include these
costs in overall device costs.
Device costs include organizing logistics, such as storage, inventory management, and
transport. Include the cost of any decommissioning activities when devices reach the
end of their operational lifecycle.
For devices connected to the cloud, optimize data transmissions to maintain cost
boundaries. Strategies include minimizing payload sizes, batching messages, and
transmitting during off-peak periods. These optimizations also incur costs to implement.
To learn more about Azure IoT devices, see:
Overview of Azure IoT device types
Best practices for device configuration within an IoT solution

Hardware selection

Most of the device development process depends on hardware selection. A make-orbuy decision for devices takes into account qualitative factors like WiFi certification and
quantitative factors like bill of materials cost and time to market. Choosing between offthe-shelf hardware or a custom design affects IoT device cost and time to market.
Off-the-shelf devices might cost more per unit, but have predictable costs and lead
times. Off-the-shelf devices also remove the need for complex supply chain
management.
Custom devices can reduce unit costs, but involve development time, and incur
non-recurring engineering costs such as design, test, certification submissions, and
manufacture.
Pre-certified system components or modules can reduce time to market and create
a semi-custom device, but are more expensive than discrete chips. You need to
properly resource supply-chain and inventory management.
The Azure Certified Device catalog

offers devices that work well with Azure IoT and

can help reduce costs and time to market. You'll focus on designing and architecting the
IoT solution with the flexibility to select the hardware from an extensive list of certified
devices. IoT Plug and Play devices

can reduce both device and cloud development

costs. When you select an Azure Certified Device, you can skip device customizations
and integration straight to onboarding into your IoT Solution.

Lambda architectural pattern
IoT solutions commonly use the hot/warm/cold lambda architectural pattern in the
cloud. This pattern also applies to the edge when you use more performant edge
devices or the Azure IoT Edge runtime. Optimizing this pattern on the edge reduces
overall solution costs. You can choose the most cost-effective service for cloud data
ingestion and processing.
Hot path processing includes near real-time processing, process alerts, or edge
notifications. You can use Azure IoT Hub event streams to process alerts in the
cloud.
Warm path processing includes using storage solutions on the edge, such as opensource time-series databases or Azure SQL Edge. Azure SQL Edge includes edge
stream processing features and time-series optimized storage.
Cold path processing includes batching lower importance events and using a file
transfer option through the Azure Blob Storage module. This approach uses a
lower cost data transfer mechanism compared to streaming through IoT Hub. After
cold data arrives in Azure Blob storage, there are many options to process the data
in the cloud.

Device security
Both IoT Hub with Device Provisioning Service (DPS) and IoT Central support device
authentication with symmetric keys, trusted platform module (TPM) attestation, and
X.509 certificates. There's a cost factor associated with each option.
X.509 certificates are the most secure option for authenticating to Azure IoT Hub,
but certificate management can be costly. Lack of certificate lifecycle management
planning can make certificates even costlier. Typically, you work with third-party
vendors who offer CA and certificate management solutions. This option requires
using a public key infrastructure (PKI). Options include a self-managed PKI, a thirdparty PKI, or the Azure Sphere security service, which is available only with Azure
Sphere devices.
TPMs with X.509 certificates offer an added layer of security. DPS also supports
authentication through TPM endorsement keys. The main costs are from hardware,
potential board redesign, and complexity.
Symmetric key authentication is the simplest and lowest cost option, but you must
evaluate the impact on security. You need to protect keys on the device and in the

cloud, and securely storing the key on the device often requires a more secure
option.
Review costs associated with each of these options, and balance potentially higher
hardware or services costs with increased security. Integration with your manufacturing
process can also influence overall costs.
For more information, see Security practices for Azure IoT device manufacturers.

Azure RTOS
Azure RTOS is an embedded development suite for devices. Azure RTOS includes a small
but powerful operating system that provides reliable, ultra-fast performance for
resource-constrained devices. Azure RTOS is easy to use and has been deployed on
more than 10 billion devices. Azure RTOS supports the most popular 32-bit
microcontrollers and embedded development tools, so you can make the most of
existing device builder skills.
Azure RTOS is free for commercial deployment using pre-licensed hardware . Azure
RTOS comes with Azure IoT cloud capabilities and features such as device update and
security. These features help reduce both device and cloud development costs.
Azure RTOS is certified for safety and security, helping to reduce the time and cost of
building compliant devices for specific verticals such as medical, automotive, and
manufacturing.

LPWAN devices
If LPWAN devices, such as LoRaWAN, NB-IoT, or LTE-M, are already connected to
another IoT cloud, the Azure IoT Central Device Bridge can help bridge to Azure IoT
Central. Azure IoT Central Device Bridge lets you focus on adding industry knowledge,
and evaluating the solution without incurring costs to change existing devices.
When building your enterprise ready solution, you'll need to consider the costs to
integrate LPWAN devices with Azure IoT Hub.

Azure Sphere
Azure Sphere is a secure, end-to-end IoT solution platform with built-in communication
and security features for internet-connected devices. Azure Sphere comprises a secured,
connected, crossover microcontroller unit (MCU), a custom high-level Linux-based
operating system (OS), and a cloud-based security service that provides continuous,

renewable security. Azure Sphere reduces the effort to build and maintain a secure
environment from device to the cloud.
Azure Sphere provides OS updates and zero-day renewable security for 10 years on top
of X.509 based PKI, user app updates, error reporting, and device management beyond
10 years without extra cost. Azure Sphere reduces the operational cost of keeping
millions of devices up to date with the latest security.

Azure Stack
Azure Stack solutions

extend Azure services and capabilities to environments beyond

Azure datacenters, such as on-premises datacenters or edge locations. Azure Stack
solutions include Azure Stack Edge and Azure Stack HCI.
Azure Stack Edge

is an Azure-managed appliance that's ideal for hardware-

accelerated machine learning workloads at edge locations. Azure Stack Edge runs
on modern technology stacks such as containers, so Azure Stack Edge deployed in
an edge location can serve multiple workloads. Sharing computational power
among workloads reduces TCO.
Azure Stack HCI

is a purpose-built, hyperconverged solution with native Azure

integration. Azure Stack HCI offers scalable virtualization to host IoT solutions.
Virtualization brings extra benefits such as security, scalability, and flexible
environments, which can reduce TCO by sharing the hardware with other
workloads. Azure Stack HCI offers more compute power than Azure Stack Edge
and is ideal for industry process transformation.
Azure Stack solutions bring Azure capability to the edge, but hardware sizing constrains
the total compute power. Identify use cases and estimated compute power, and factor
in sizing to match costs to performance needs.

Azure public or private MEC
IoT devices can generate large quantities of data, and might also have strong
requirements for low power consumption and low costs. Small, inexpensive IoT devices
are designed for one or a few tasks, such as collecting sensor or location data and
offloading it for further processing.
Azure public or private multi-access edge compute (MEC) and 5G help optimize the
costs of offloading data from devices. MEC-based IoT solutions enable low-latency data
processing at the edge instead of on devices or in the cloud. Latency is 1-5 ms instead
of the typical 100-150 ms for the cloud. MEC-based IoT solutions are flexible, and the

devices themselves are inexpensive, operate with minimal maintenance, and use smaller,
cheaper, and long-lasting batteries. MEC keeps data analytics, AI, and optimization
functions at the edge, which keeps IoT solutions simple and inexpensive.
In addition to serving as an edge processing, compute, and 5G communication device
for IoT workloads, MEC serves other workloads as a communication device to establish
high-speed connections to the public cloud or remote sites.

Azure IoT Edge
Azure IoT Edge has built-in capabilities for high message volumes. Azure IoT Edge
managed devices

with gateway capabilities can reduce network costs and minimize

the number of messages through local processing and edge scenarios.
Avoid device-to-device or module-to-module edge communications or device-to-cloud
interactions that use many small messages. Use built-in message batching features to
send multiple telemetry messages to the cloud. These features can help reduce the costs
of using IoT Hub. Reducing both the number of daily messages and the number of
device-to-cloud operations per second can allow choosing a lower tier in IoT Hub. To
learn more, see Stretching the IoT Edge performance limits

.

To reduce data exchange costs, you can deploy Azure services such as Azure Stream
Analytics and Azure Functions to IoT Edge. Azure Stream Analytics and Azure Functions
can aggregate and filter large volumes of data at the edge and send only important
data to the cloud. Azure Blob Storage on IoT Edge can reduce the need to transfer large
quantities of data over the network. Edge storage is useful for transforming and
optimizing large quantities of data before sending it to the cloud.
Free Azure IoT Edge modules for open protocols such as OPC Publisher and Modbus
help connect various devices with minimal development. If upload performance is
critical, choosing a proven IoT Edge module from a vendor can be more cost effective
than building a custom module. You can search for and download IoT Edge modules
from the Azure Marketplace .

Ingestion and communication layer
A cloud IoT gateway is a bridge between devices and cloud services. As a front-end
service to the cloud platform, a gateway can aggregate all data with protocol translation
and provide bi-directional communication with devices.
There are many factors to take into account for device to IoT gateway communications,
such as device connectivity, network, and protocol. An understanding of IoT

